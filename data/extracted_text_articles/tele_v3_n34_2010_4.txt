152
International Journal on Advances in Telecommunications, vol 3 no 3 & 4, year 2010, http://www.iariajournals.org/telecommunications/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Universal Ground Control Station for Heterogeneous Sensors 
Axel Bürkle, Florian Segor, Matthias Kollmann, Rainer Schönbein 
Fraunhofer Institute of Optronics, System Technologies and Image Exploitation IOSB 
Karlsruhe, Germany 
{axel.buerkle, florian.segor, matthias.kollmann, rainer.schoenbein}@iosb.fraunhofer.de 
 
 
Abstract—Today, a wide range of sensors and mobile systems – 
both aerial and ground-based – are available for surveillance 
and reconnaissance tasks. For example, they provide first 
responders with current information about the situation at the 
operation site. In many cases those systems have their own 
dedicated control and exploitation station. The joint control of 
heterogeneous sensors and platforms, as well as the 
exploitation and fusion of heterogeneous data is a challenge. 
The surveillance system AMFIS presented in this paper is an 
integration platform that can be used to interconnect system 
components and algorithms. The specific tasks that can be 
performed using AMFIS include surveillance of scenes and 
paths, detection, localization and identification of people and 
vehicles as well as collection of evidence. The major advantages 
of this ground control station are its capability to display and 
fuse data from multiple sensor sources and the high flexibility 
of the software framework to build a variety of surveillance 
applications. 
Keywords - ground control station; sensors; unmanned aerial 
vehicles; security; surveillance; disaster management 
 
I. 
 INTRODUCTION 
This paper presents a generic surveillance system and its 
control station called AMFIS. AMFIS is a component based 
modular construction kit currently under development as a 
research prototype. It has already served as the basis for 
developing specific products in the military and homeland 
security market. Applications have been demonstrated in 
exercises for the European Union under the PASR program 
(Preparatory Action for Security Research), German Armed 
Forces, and the defense industry. The tasks that have to be 
supported by such products are complex and involve among 
other tasks, control of sensors, mobile platforms and 
coordination with a control center. 
The surveillance system AMFIS [1] is an adaptable 
modular system for managing mobile as well as stationary 
sensors. The main task of this ground control station is to 
work as an ergonomic user interface and a data integration 
hub between multiple sensors mounted on light UAVs 
(unmanned aerial vehicles) or UGVs (unmanned ground 
vehicles), stationary platforms (network cameras), ad hoc 
networked sensors, and a superordinated control center. 
The AMFIS system is mobile and portable, allowing it to 
be deployed and operated anywhere with relative ease. It can 
supplement existing stationary surveillance systems or act as 
a surveillance system on its own if no preexisting 
infrastructure is available. The sensor carriers in this multi-
sensor system can be combined in a number of different 
setups in order to meet a variety of specific requirements. At 
present, the system supports optical sensors (infra-red and 
visible) and alarms (PIR, acoustic, visual motion detection). 
There are plans to add support for chemical sensors in the 
future. 
AMFIS has established standardized interfaces and 
protocols to integrate and control different kinds of sensors. 
This “plug and sense” approach allows the seamless 
integration of new sensors with a minimal effort. If 
necessary, all sensor data is automatically converted by 
dedicated services to a format usable by the ground control 
station. 
After a short survey of related work an overview of the 
application scenarios is presented, followed by a detailed 
description of the apparatus in Section IV. Sections V and VI 
outline selected services and introduce a commercial flight 
platform modified to reach a higher level of autonomy and 
extending the ground control station presented in Section IV. 
Finally, in Section VII some first practical results are shown. 
 
II. 
STATE OF THE ART 
To the best of our knowledge, the combination of 
heterogeneous sensors and sensor platforms (ground, air, 
water) in an open homogeneous system allowing the fusion 
of various sensor data to generate a complex situation picture 
is quite a unique project. The integration of different sensors 
into one system has already been realized in previous 
systems but mainly in order to create specialized individual 
solutions tailored to individual customer requirements.  
Many projects deal with the development of supervision 
systems, new sensor platforms or control of sensors. The 
combination 
of 
these 
innovative 
supervision 
and 
reconnaissance attempts to one modular system has not yet 
been done.  
Systems similar to AMFIS are the ground stations of the 
French company Aerodrones [2] and the American company 
AII [3], both developed as stand-alone control stations for 
multiple airborne drones. Another example is the product of 
the US company Defense Technologies [4], which focuses 
on military standardized interfaces to control different sensor 
platforms on the ground, in the air and in the water. 
In contrast to AMFIS, Aerodrones and AII deal 
exclusively with airborne sensor platforms. Defense 
Technologies does not commit itself in the kind of used 
sensor platforms and is therefore more similar to AMFIS. 

153
International Journal on Advances in Telecommunications, vol 3 no 3 & 4, year 2010, http://www.iariajournals.org/telecommunications/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
AII and Defense Technologies are concentrating on military 
solutions while AMFIS is mainly intended for civil 
applications. 
 
III. 
APPLICATION SCENARIOS 
The security feeling of our society has significantly 
changed during the past years. Besides the risks arising from 
natural disasters, there are dangers in connection with 
criminal or terroristic activities, traffic accidents or accidents 
in industrial environments. 
Even though a lot of effort is put into protecting 
threatened or vulnerable infrastructure, most threats cannot 
be foreseen in their temporal and local occurrence, so that 
stationary in situ security and supervision systems are not 
present. Such ad hoc scenarios require quick situation-related 
action. 
Possible scenarios that deal with these specifications are 
the supervision of big events or convoys for security reasons, 
natural and man-made disasters such as earthquakes or major 
fire control but also intrusion of unauthorized persons into 
sites and buildings, e.g., to take hostages or place explosives. 
Especially in the civil domain, in case of big incidents 
there is a need for a better data basis to support the rescue 
forces in decision making. The search for buried people after 
a building collapses or the clarification and location of fires 
at big factories or chemical plants are possible scenarios 
addressed by our system. Only in the minority of cases the 
rescue forces can rely on an already available sensor 
infrastructure at the incident site. If there were sensors 
available, there is a significant chance they will be destroyed 
or at least partially corrupted. A transportable sensor system 
to be used remotely at the site of the event is proposed to 
close this gap. 
The micro UAVs used in AMFIS can deliver a highly up-
to-date situation picture from the air during a conflagration 
in a chemical factory or a similar scenario. Ground robots 
can enter the building in parallel to the fire-fighting work and 
penetrate areas, which are not yet accessible for the fire 
fighter and search for injured people or unknown sources of 
fire without endangering human life. Additionally, the 
mobile sensor platforms can be complemented by stationary 
systems. These can be temperature sensors for the fire 
aftercare or the measuring of the fire development and 
expansion or vibration and motion sensors to use in a 
collapsed building. These sensors can be used to prevent or 
at least to warn of any further structural changes in a 
collapsed building by detecting vibration and movement in 
the debris. The UAVs or ground robots can also act as 
platforms to deploy sensors at points of interest. 
Besides the system’s capability of ad hoc deployment 
during disasters or accidents, AMFIS can also be used as a 
versatile protection and supervision system. Premises or 
vulnerable infrastructures can be monitored with all types of 
sensors and actuators. Equipping the perimeter with motion 
detectors and cameras is a typical setup. In addition, mobile 
ground robots can patrol the area and respond to events. 
Other tasks that can arise are the detection of danger 
potential, the supervision of scenes and ways or the 
localization, tracking and identification of people and 
vehicles. 
When several sensor systems and platforms are used in a 
complex scenario at the same time, conventional control 
systems designed as single use- and controlling-systems 
quickly reach their limits. First of all, every subsystem needs 
its own console and a specially trained operator due to the 
fact that each system has its own interface. Secondly the 
fusion and synchronized filing of sensor and status data from 
different systems is not an easy task. 
The control of the individual sensors and platforms from 
the situation center is hardly practicable on account of the 
complexity, delay in the data transfer and distance to the 
place of action (often several kilometers). 
As a connection between the sensors and the situation 
center an authority directly on the site of the event is 
necessary, which processes the reconnaissance missions 
independently. That includes steering sensor platforms, 
controlling sensors as well as filtering and densification of 
sensor data so that only information relevant for decisions 
like situation reports, alarms and critical video sequences are 
transmitted in an appropriate way to the situation center. 
An analysis of the demands in complex scenarios 
incorporating micro UAVs has shown that at least two 
operators are necessary on the ground control station to deal 
with the requirements and problems arising from such a 
scenario. One operator is exclusively responsible for the 
control and supervision of the mobile sensor platforms. The 
second operator looks after the evaluation of the sensor data 
streams and the communication with the situation center. 
According to a recent Frost & Sulivan report [5] micro 
UAVs are already used in vast and diverse civil applications. 
Some of the tasks that can be supported with UAVs in 
general include but are not limited to: enhancing agricultural 
practices, police surveillance, pollution control, environment 
monitoring, fighting fires, inspecting dams, pipelines or 
electric lines, video surveillance, motion picture film work, 
cross border and harbor patrol, light cargo transportation, 
natural disaster inspection, search and rescue, and mine 
detection. 
 
 
Figure 1.   A team of micro UAVs 
 
Obviously some of these tasks are not suitable for single 
micro UAVs due to their limited operating range and 
payload. With groups or swarms of micro UAVs (Figure 1) it 
is possible to realize scenarios that are inefficient or even not 

154
International Journal on Advances in Telecommunications, vol 3 no 3 & 4, year 2010, http://www.iariajournals.org/telecommunications/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
feasible with a single micro drone. Some situations where 
cooperating MAVs (micro aerial vehicles) add value include: 
• 
A wider area has to be searched. A team of MAVs 
can increase coverage and reduce the time required.  
• 
A UAV loses connection to the ground control 
station because it moved too far or the signals are 
blocked by an obstacle. In a group of UAVs, one of 
them can be “parked” in reach of the ground control 
station and act as relay station. 
• 
Several intruders enter the site. They later split up, 
each taking different directions. A single drone 
would have to decide, which person to follow, while 
a swarm of UAVs can form subgroups and track 
each intruder individually. 
• 
The duration of surveillance exceeds battery life 
time. In a team, assignments can be planned 
accordingly and another UAV can take over the task 
of an out-of-battery drone. 
• 
A threat has to be monitored with different sensor 
types. For example, an intruder who is tracked 
visually suddenly places an object. Besides the 
visual sensor some CBRNE (chemical, biological, 
radiological, nuclear, and explosive) detection 
devices are needed. Since the payload of a single 
quadrocopter is very limited, a swarm could carry 
different sensors. 
• 
Multi-sensor capability can also be used to visually 
control the action of different drones. For example 
an infrared sensor equipped UAV could be 
employed by the operator located at the ground 
control station to navigate a chemical sensor 
equipped micro UAV through a dark building. 
 
These use cases illustrate that there is a need for the 
coordinated use of micro UAVs. The combination with other 
sensor platforms, such as UGVs (unmanned ground vehicles) 
or stationary sensors, adds further value to the system. 
 
IV. 
SYSTEM OVERVIEW 
In order to be adaptable to a wide range of different 
requirements and applications, AMFIS was developed as a 
mobile and generic system, which delivers an extensive 
situation picture in complex surroundings - even with the 
lack of stationary security technology. In order to achieve 
maximum flexibility, the system is implemented open and 
mostly generalized so that different stationary and mobile 
sensors and sensor platforms can be integrated easily with 
minimal effort (Figure 2), establishing interoperability with 
existing assets in a coalition such as UAVs. 
The system is modular and can be scaled arbitrarily or be 
adapted by choosing the modules suitable to the specific 
requirements. 
Because 
of 
the 
open 
interfaces, 
the 
accumulated data can be delivered on a real-time basis to 
foreign systems (e.g., command and control systems or 
exploitation stations, cf. Figure 3). 
The AMFIS system can be divided into a mobile ground 
control station, which can control and coordinate different 
UAVs, land vehicles or vessels (sensor platforms), as well as 
stationary autonomous ad hoc sensor networks and video 
cameras. Depending on the used sensors and sensor 
platforms, the system is extended with suitable broadcasting 
systems for the transmission of the control signals and the 
sensor data (e.g., video recordings.) 
 
 
Figure 2.   The AMFIS ground control station serves as integration 
platform for various sensors and vehicles 
 
By the universal approach, the system is able to link with 
a wide range of sensors and can be equipped with electric-
optical or infrared cameras, with movement dispatch riders, 
acoustic, chemical or radiation sensors depending on the 
operational aim. If supported or even provided by the 
manufacturer, these sensors can be mounted on mobile 
sensor platforms or be installed in fixed positions. The only 
requirement such sensors have to fulfill in a mobile scenario 
is that they work properly without the need for any pre-
existing infrastructure. 
 
 
Figure 3.   AMFIS interfaces 
 
The AMFIS system is scalable and can be extended to 
any number of workstations. Due to this fact several sensor 
platforms can be coordinated and controlled at the same 
time. The most different sensor platforms can be handled in a 
similar manner by a standardized pilot's working station that 
in turn minimizes the training expenditure of the staff and 
raises the operational safety. The user interface is 

155
International Journal on Advances in Telecommunications, vol 3 no 3 & 4, year 2010, http://www.iariajournals.org/telecommunications/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
automatically adapted according to the sensor or sensor 
platform at hand by using standardized descriptions. 
Data fusion is one of the most important tasks of a multi 
sensor system. Without merging the data from different 
sensors the use of such a system is very limited. Linking data 
of sensors that complement each other can generate an entire 
situation picture. 
All information gathered during the operation is 
immediately available to the crew of the ground control 
station, in which a GIS-supported, dynamic situation picture 
plays a central role. At the same time all received data is 
archived and stored into databases, e.g., a CSD (Coalition 
Shared Data) [6] or SSD (SOBCAH Shared Data) [7]. This 
serves the perpetuation of evidence and allows an additional 
subsequent analysis of the events.  
 
 
Figure 4.   The AMFIS ground control station and its user interface 
 
The open interface concept supports the integration of 
AMFIS in existing security systems so that data can be 
exchanged on a real-time basis with other guidance, 
supervision or evaluation systems. Mission planning, manual 
and automatic vehicle guidance, sensor control, local and 
temporal linking (coalescence) of sensor data, the 
coordination of the people on duty, reporting and the 
communication with the leading headquarters in the situation 
center belongs to the other tasks of a reconnaissance system. 
Combination of sensor events and appropriate actions are 
implemented by predefined rules with an easy to use 
production system for situation specific adaptations. 
A. User Interface 
The user interface of the AMFIS ground control station at 
Fraunhofer IOSB consists of three workstations (Figure 4). 
Basically, the system is designed such that each display can 
be used to interact with each function allocated by AMFIS. 
The standard setup consists of two workstations with one 
operator each, and one situation awareness display in 
between that supports both operators. The duties of the two 
operators can be divided into sensor and vehicle control, 
called pilot working place, and data fusion, archiving, 
exploitation and coordination tasks. 
The user interface of the latter working place primarily 
provides a function for the visualization of sensor data 
streams. Therefore the operator gains access to the 
accumulated data. His task is to obtain and keep an overview 
of the situation and to inform the higher authorities about 
important discoveries. He provides the associated data so 
that external systems or personnel can utilize that 
information. It is incumbent on him to mark important data 
amounts and to add additional information when necessary. 
Furthermore, he is the link to the pilot and coordinates and 
supports the pilot in his work. The analyst as well as the pilot 
relies on the central geographical information system-
supported situation representation that provides an overview 
of the whole local situation. The geographical relation is 
established here and the situation and position of the sensors 
and sensor platforms can be visualized. This includes for 
example, the footprints of cameras or the position and 
heading of UAVs or UGVs. 
The pilot's workstation is designed to control many 
different sensor platforms. It is not clear from the start, 
which sensor platforms will be used in the future and it is 
also not clear, which situation information will be provided 
by the different systems, or which information is needed to 
control the future platforms in a proper way. For this purpose 
the pilot workstation provides a completely adaptable user 
interface, which allows selectively activating or deactivating 
the required displays. For example, an artificial horizon is 
completely useless in order to control a stationary swiveling 
camera but very helpful for controlling an airborne drone. 
The surface can be adapted to the particular circumstances 
and is configurable for a wide range of standard applications. 
No matter what sensor platform the user is currently 
controlling or supervising, the task is the same. He does not 
have to switch between different proprietary control stations. 
The user interface is identical except for individual volitional 
or necessary adaptations. 
B. System and Software Architecture 
The physical sensors and sensor carriers are mapped 
logically to the so-called sensor web. This is a tree structure 
describing the real-world entities: The root node, the sensor 
web itself, connects to different sensor networks, each 
representing a number of similar sensor carriers, for example 
a team of UAVs. Each of those sensor networks is made up 
of one or more sensor nodes, equal to a physical sensor 
carrier (e.g., a single UAV), which in turn contain numerous 
sensors (e.g., camera, GPS receiver, etc.). The sensor web is 
permanently stored in a database, from which an XML file is 
generated at runtime by the central message hub of the 
AMFIS ground control station, the Connector. 
The standard communications protocol within AMFIS is 
based on XML messages transported via TCP socket. To 
ease the use of this protocol, implementations exist in 
various 
runtime 
environments 
(e.g., 
.NET, 
Java), 
encapsulating the XML-handling and offering the user an 
object-oriented view of the messages. When a client 
application connects to the Connector, it first receives the 
aforementioned XML-version of the sensor web followed by 
a steady stream of XML messages, each containing metadata 
(e.g., sensor values, commands, etc.) originating from or 
destined to one of the sensors in the sensor web. 
A client application can be anything from a GUI 
application to a low-level service: 

156
International Journal on Advances in Telecommunications, vol 3 no 3 & 4, year 2010, http://www.iariajournals.org/telecommunications/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
• 
The various GUI applications of the user interface, 
most importantly the analyst’s interface, the situation 
overview 
and 
the 
pilot’s 
interface. 
Those 
applications offer a visual representation of received 
metadata to the user, for example by displaying the 
current geographical locations of the various sensor 
carriers in the map, and transmit commands to the 
sensor carriers, for example a user-generated 
waypoint for a UAV. 
• 
A number of services running in background, 
notably the video server, offering time shifting and 
archiving for both video and metadata (more on 
video management within AMFIS see below), the 
rules engine, the flight path planning tool and the 
multi-agent system, all supporting the user by 
automating certain processes (see Section V). 
• 
Drivers for various sensor carriers, for example a 
dedicated control software for UAVs, which 
translates high-level flight commands like waypoints 
into the proprietary RS232-based control protocol of 
the respective drone, and in turn generates metadata 
XML status messages containing the current 
position, heading, remaining flight time etc. 
• 
Interfaces to third-party applications or networks, for 
example superior command centers. 
 
The current implementation of the communication 
protocol is strictly multicast-based: Every message sent to 
the Connector is relayed to every connected client 
application, leaving it to the respective application to decide 
what to do with it. For future versions, a subscription-based 
model is planned. 
The protocol relies on lower-level protocols such as TCP 
to ensure delivery of the messages. Since most of the 
messages contain live status data, they are transferred in a 
fire-and-forget manner, thus eliminating possible race 
conditions within the Connector. 
While all metadata – be it sensor values, commands or 
user-generated textual comments – is transmitted via XML, 
the extensive amount of video data accumulated by the 
various cameras has to be processed and stored by other 
means. A central application within AMFIS is the video 
server, which receives and records all available network 
video streams along with incoming XML messages. Since 
the analyst’s interface heavily relies on the capability to go 
back in time and review critical situations, the video server 
serves the dual purpose of both recording the video streams 
for later archival as well as providing time shifting-enabled 
streams to other clients. If required the video server can store 
(and later play back) the accumulated data using a standard 
MySQL database. 
In order to interface with third-party systems, it can 
become necessary to transcode available data into another 
format. Upon request, the video server can spawn a so-called 
transcoder process, multiplexing video and metadata into a 
single stream to be transmitted to a remote command center. 
To receive incoming commands from a command center, an 
XMPP client (Extensible Messaging and Presence Protocol, 
formerly Jabber [8][9]) has been implemented, which 
seamlessly integrates into the AMFIS ground control station. 
Figure 5 shows a logical representation of the AMFIS 
system.  
 
Sensor Node
(e.g. UAV)
Sensor
(e.g. Voltmeter)
Sensor
(e.g. GPS 
Receiver)
Sensor
(e.g. Camera)
Sensor Node
(e.g. Stationary 
PTZ Camera)
Sensor
(e.g. Camera)
Sensor
(e.g. Heading 
Sensor)
Analyst Interface
(GUI Application)
Video Server
(Service)
Pilot Interface
(GUI Application)
Database  
Archiving
Dedicated 
Sensor Carrier 
Control Software
(Driver)
Situation 
Overview
(GUI Application)
Plugin-Based Sensor 
Carrier Control 
Software
Video Data
(live)
Playback
Interface to External 
Command Center
(Interface)
Video & Metadata
(Non-AMFIS Format)
Video/
Metadata
Multiplexer
AMFIS Connector
Metadata
(Commands /
Status Data)
Up- / Downlink
(Proprietary 
COTS Format)
Rule Engine / 
Multi-Agent 
System / ...
(Service)
Metadata
(Commands / Status Data)
 
Figure 5.   AMFIS system overview 
 
V. 
AMFIS SERVICES 
In addition to being an integration platform, AMFIS 
offers a number of support services related to surveillance 
and reconnaissance tasks. Those services facilitate resource 
planning, sensor and vehicle coordination, sensor data 
exploitation, and training. Three of these services, i.e., rule-
based event response, photo flight and simulation, are 
described in the following paragraphs. 
A. Rule-Based Event Response 
This service is a support system for the automatic 
combination and selection of sensor data sources in a 
surveillance task. Autonomous reactions, e.g., responding to 
an intrusion alert triggered by a motion detector, are very 
important during a surveillance scenario. Therefore a 
solution was developed, which grants users an easy, 
powerful and versatile platform for defining reactions. 
The implementation is universal so that the support 
system can be adapted to several scenarios at different 

157
International Journal on Advances in Telecommunications, vol 3 no 3 & 4, year 2010, http://www.iariajournals.org/telecommunications/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
individual sites. This is accomplished by the use of rule sets, 
which are created site and task specific. These rules contain 
work flows which are pushed if a certain predefined event 
occurs. Thus, for example, a watchman can be automatically 
informed or a UAV can be sent off for reconnaissance of a 
defined area without any user interaction. 
The support component in AMFIS is implemented with 
the Drools rules engine [10] using production rules for 
representing procedural task knowledge. The engine uses the 
Rete algorithm, which repeatedly assesses the current 
situation and selects the rules to execute. Drools is open 
source and contains a well developed rule language. 
The rule language is based on the when-then schema 
(Figure 6). Additionally, Boolean logic, methods of 
compiled Java source code and their return values can be 
used. 
 
rule “rule” 
 
when 
 
 
sensor triggers alarm 
 
 
<additional preconditions> 
 
then 
 
 
camera turns to sensor position 
 
 
drone flies to sensor position 
end 
Figure 6.   Layout of a rule 
 
The integration of the rule-based event response 
application into the AMFIS system is depicted in Figure 7. 
 
AMFIS 
AmfisCom 
Controller 
Rule Engine 
Sensors 
Cameras 
UAVs 
fetches data and executes actions 
creates and updates data structures 
Rules 
Priorities 
describe actions 
dependent on data 
determine which 
actors react 
AMFIS 
Connector 
 
Figure 7.   Integration of a rule engine 
 
The rule engine uses AmfisCom, a communication 
library for interfacing the AMFIS Connector (cf. Section 
IV.B) to establish a connection to the Connector Service, the 
message hub of AMFIS, and receives data. Data structures 
containing e.g., positions and IDs of sensors, cameras and 
UAVs are created and updated with the received data. When 
there is an alarm message it can be assigned to its 
corresponding sensor through the unique ID. After this 
initialization phase the rule interpretation starts. When a rule 
becomes applicable messages are sent to the corresponding 
assets (cameras, UAVs, etc) to change their positions 
accordingly. Furthermore, every camera has a priority list of 
targets. Before the message is sent, this list is evaluated. 
Only if the new target has at least the same priority as the 
current target, the camera pans to the new target. 
Additionally, a tool providing a graphical user interface 
has been developed. It facilitates the definition of rules and 
the creation of priority lists for cameras. The rules can be 
assembled by clicking and saved as XML file. These files are 
editable by the user at any time. Furthermore, the rule engine 
can import these files and convert the content into the 
specific rule language. 
B. Photo flight 
The photo flight service assists the operator in obtaining 
an up-to-date situation picture of a site. One or more UAVs 
with camera payloads fly to defined waypoints and capture 
high resolution still images. These images are later combined 
to a mosaic. The photo flight service manages the available 
assets (i.e., UAVs and payloads) and automatically calculates 
flight paths based on a user-defined area of interest. 
In the flight path planning tool, the user can define any 
polygonal shape on the map. He then selects one or more 
UAVs from a list of available drones, which is distributed by 
the AMFIS Connector. The Connector provides the 
necessary information about all drones and their current 
payloads. For each selected UAV the user is requested to 
enter some variable parameters like desired photo flight 
height or safety height. The safety height parameter is an 
additional safety feature that defines individual cruising 
heights in order to prevent collisions between UAVs. 
After that, the operator can start the photo flight or 
add/remove additional drones to/from the list. Once all 
necessary information has been entered correctly and the 
“Calculate” button is clicked, the planning algorithm starts 
and shows the resulting flight path on the map (Figure 8). 
 
 
Figure 8.   GUI of the flight path planning service 
 
Additionally, the flight path planning service offers the 
possibility to export the calculated waypoints in an AMFIS 
specific file format or upload the points directly to the 
respective UAV using the Connector. 

158
International Journal on Advances in Telecommunications, vol 3 no 3 & 4, year 2010, http://www.iariajournals.org/telecommunications/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
C. Simulation 
In order to assess different cooperation strategies for 
teams of UAVs and other assets, a simulation tool has been 
developed. The tool is also useful when it comes to training 
and briefing of the operators. Modeling and visualization of 
scenarios was done using a computer game engine with 
corresponding editing tools. An interface between AMFIS 
and the engine has been implemented. It allows full control 
of the virtual entities as well as feedback from the virtual 
world. The simulation tool is fully integrated into the AMFIS 
ground control station allowing seamless combination of 
virtual and real assets. Virtual vehicles can be monitored and 
controlled analogous to real assets through the AMFIS 
situation map, whereas real UAVs can be displayed in the 
virtual world next to simulated components. 
 
 
Figure 9.   The AMFIS simulation component 
 
An example scenario that simulates an intrusion has been 
realized (Figure 9). Besides the UAVs and the actors in the 
scenario, sensors have also been modeled. Different kinds of 
sensors such as motion detectors, cameras, ultra sonic or 
LIDAR (light detection and ranging) sensors can be modeled 
with their specific characteristics. The simulation tool can 
determine if an object lies within the range of a sensor. This 
helps evaluate and optimize the use of different sensing 
techniques. 
The intelligence of team members is implemented in 
software agents as described in Section VI.C. They interface 
with the simulation engine using the same control command 
interface as the actual quadrocopters. This subsequently can 
allow the simulation to be transferred to the real world 
without changes to the agents. 
VI. 
AUTONOMOUS SENSOR PLATTFORMS 
AMFIS as an open and generic system supports the 
simultaneous operation of a large number of sensors and 
sensor platforms. While the handling of single platforms is 
already well understood, control and coordination of several 
mobile platforms can be a challenging task. 
For this purpose one of the research focuses lies on the 
improvement of the application of multiple miniature UAVs. 
Our approach is to increase the level of autonomy of each 
drone. Therefore a vast amount of effort has been put into the 
selection of the flight platform. Such a platform preferably 
comes with a range of sensors and an advanced internal 
control system with autonomous flight features, which 
minimizes the regulation need from outside. When it comes 
to flying autonomously, the system has to be highly reliable 
and possess sophisticated safety features in case of 
malfunction or unexpected events. 
 
 
Figure 10.   Sensor platform AirRobot 100-B 
 
Other essential prerequisites are the possibility to add 
new sensors and payloads and the ability to interface with the 
UAV’s control system in order to allow autonomous flight. 
A platform that fulfils these requirements is the quadrocopter 
AR100-B by AirRobot. It can be both controlled from the 
ground control station through a command uplink and by its 
payload through a serial interface. 
A. UAV Control Hardware 
To support the pilot at his work at the ground control 
station and to give him the possibility to supervise multiple 
flying sensor platforms at the same time, several steps are 
necessary. The first step is to enhance the hardware in order 
to reach a higher level of autonomy. Therefore, a payload 
was developed, which carries a processing unit that can take 
over control and thereby steer the quadrocopter. 
Due to space, weight and power constraints of the 
payload, this module has to be small, lightweight and 
energy-efficient. On the other hand, a camera as a sensor 
system should not be omitted. An elegant solution is the use 
of a “smart” camera, i.e., a camera that not only captures 
images but also processes them. Processing power and 
functionalities of modern smart cameras are comparable to 
those of a PC. Even though smart cameras became more 
compact in recent years, they are still too heavy to be carried 
by a quadrocopter such as the AR100-B. In most 
applications, smart cameras remain stationary whereby their 
weight is of minor importance. However, a few models are 
available as board cameras, i.e., without casing and the usual 
plugs and sockets (Figure 11). Thus, their size and weight are 
reduced to a minimum. The camera that was chosen has a 
freely programmable DSP, a real-time operating system and 
several interfaces (Ethernet, I²C, RS232). With its weight of 
only 60g (without the lens), its compact size and a power 
consumption of 2.4W, it is suitable to replace the standard 
video camera payload. 

159
International Journal on Advances in Telecommunications, vol 3 no 3 & 4, year 2010, http://www.iariajournals.org/telecommunications/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
 
Figure 11.   A programmable camera module controls the UAV 
 
The camera can directly communicate with the drone’s 
controller though a serial interface. The camera receives and 
processes status information from the UAV such as position, 
altitude or battery power, and is able to control it by sending 
basic control commands or GPS-based waypoints. 
A drawback of the board camera is its lack of an 
analogue video output thus rendering the quadrocopter’s 
built-in video downlink useless. Image data is only available 
through the camera’s Ethernet interface. To enable 
communication between the smart camera and the ground 
control station, a tiny WiFi module was integrated into the 
payload. The WiFi communication link allows streaming of 
live video images, still shots and status information from the 
UAV to the ground control station. Furthermore, programs 
can be rapidly uploaded to the camera during operation. 
Currently, the above enhanced UAVs are able to perform 
basic maneuvers, such as take-off, fly to position, and 
landing, all autonomously. Furthermore, a software module 
was implemented, that calculates the footprint of the camera, 
i.e., the geographic co-ordinates of the current field of view. 
In the future we will also use the camera’s image processing 
capabilities to generate control information. As a safety 
feature, it is always possible for the operator to override 
autonomous control and take over control manually. 
B. Communication Infrastructure 
For a single UAV communication usually consists of two 
dedicated channels, an uplink channel for control commands 
and a downlink channel for video and status information. In 
present UAVs, each of these channels has its own 
communication technique in a special frequency band. In 
complex scenarios that require multiple UAVs there has to 
be twice as many RF channels as UAVs used. These 
channels are all point-to-point connections, which, if at all, 
see the other UAVs only as interferer. There is no channel 
between two UAVs; all communication goes via the base 
station.  
Besides this direct control of UAVs, there is a more 
abstract way, which can use the benefits of an intelligent 
payload. The group of UAVs receives complex tasks, which 
they will fulfill autonomously. This kind of control however 
brings the standard system with up- and downlink to its 
limits because it poses demands, which cannot be fulfilled 
with the standard communication: 
• 
No interference between communication of multiple 
UAVs (ideally: use of multihopping) 
• 
Adding UAVs to the swarm must not require a new 
RF channel 
• 
Opening of data channels to transmit the results to 
the base-station  
• 
Opening of control channels to transmit any kind of 
commands to the UAV 
• 
Sending broadcast messages to all UAVs  
• 
Opening direct communication channels between 
UAVs 
 
In addition to the new demands, the standard 
requirements for UAVs still must maintain the following: 
• 
Monitor the status of every UAV in the air 
• 
Manual control of every UAV as fallback function 
 
To fulfill these needs the (video-) downlink is replaced 
by a module capable of using networking communications. 
In our prototype we use a WiFi module because of its high 
data rates and good range, though other technologies might 
be feasible too. The UAV’s uplink channel is retained as 
fallback control option in case of an emergency. 
With 
the 
WiFi 
network, 
we 
implemented 
a 
communication solution that meets the demands listed 
above. This solution differentiates between UAV and base-
station, i.e., the ground control station. There is only one 
base-station within the network. A base-station monitors the 
status of every UAV assigned to it. It also acts as gateway to 
other system components. 
Our communication setup uses four types of channels (cf. 
Figure 12): 
• 
Broadcast channel 
a channel, which offers random access to every 
subscriber in the network 
• 
Control channel 
a dedicated channel between a UAV and the base-
station to transmit status information from the UAV 
and to receive commands from the base-station 
• 
Data channel 
a dedicated channel between UAV and the base-
station to send results of task i.e., images 
• 
Co-op channel 
this channel is opened between two UAVs if one of 
them needs assistance to finish a task 
 
 
Figure 12.   Communication channels between UAVs and the ground 
control station 
 

160
International Journal on Advances in Telecommunications, vol 3 no 3 & 4, year 2010, http://www.iariajournals.org/telecommunications/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
1) Broadcast channel 
The broadcast channel is mainly used for initializing the 
other channels. If a UAV is not assigned to a base-station it 
will look for a base-station on this channel. Also if a UAV 
needs assistance to finish a job, e.g., when its battery runs 
low or it needs a UAV with another sensor, the UAV calls 
for assistance on this channel. Through the broadcast channel 
it is possible to reach all UAVs with a single message. If a 
UAV, for example, detects an obstacle it can inform all other 
UAVs in the group. Another main feature of this channel is 
communicating new tasks. When this task is transmitted to 
the whole group instead of a single UAV, the decision 
regarding which UAV best fits the needs for this task can be 
done by the group. 
 
2) Control channel 
The control channel is a dedicated channel between a 
UAV and a base-station. Over this channel a UAV sends its 
status as well as an “Alive” Message. These data make it 
possible to monitor the UAVs in the base-station. The 
second feature of this channel is a command uplink to the 
UAV. It can be used to transmit tasks as well as to configure 
the UAV. Reconfiguring can be done by changing internal 
parameters of the UAV or by uploading new software 
modules. 
 
3) Data channel 
The data channel sends results (usually video images) to 
the base-station. The format of the data has to be predefined.  
 
4) Co-op channel 
The co-op channel is opened between two UAVs, if 
necessary. If the UAV has a task, which cannot be done on 
its own, it seeks a wingman over the broadcast channel. If 
there is an idle UAV, which can assist, a co-op channel is 
opened between the two drones. Over this channel the UAV 
has the possibility to send subtasks to the wingman. After 
completion, it receives the results over the co-op channel. 
 
Replacing the standard downlink with a networking 
module is a big step towards autonomy of each UAV. With 
this adaptive communication solution it is possible to set up 
an expandable network of UAVs. The implemented channels 
provide communication links between all subscribers in the 
net. 
C. UAV Control Software 
The second step to reaching a higher level of autonomy is 
based on the development of a multi-agent system to 
implement team collaboration. An agent-based framework is 
implemented where the individual entities in a team of 
UAVs are represented by software agents. The agents 
implement the properties and logic of their physical 
counterparts. Their behavior defines the reaction to 
influences in the environment, such as alarms generated by 
sensors in the AMFIS network. 
An agent is “...any entity that can be viewed as 
perceiving its environment through sensors and acting upon 
its environment through effectors” [11]. Incorporating that 
“An agent is a computer system, situated in some 
environment that is capable of flexible autonomous action in 
order to meet its design objectives” [12], a multi-agent 
system appears to perfectly meet the challenges of realizing 
an intelligent swarm of autonomous UAVs. 
Software agents are computational systems that inhabit 
some complex dynamic environment, which sense and act 
autonomously in this environment, and by doing so, realize a 
set of goals or tasks, for which they are designed [13]. 
Hence, they meet the major requirements for a suitable 
architectural framework: to support the integration and 
cooperation of autonomous, context-aware entities in a 
complex environment. 
The agent-based approach allows a natural system 
modeling approach facilitating the integration of flight 
platforms, sensors, actuators and services. The core-agents of 
the multi-agent system presented in this paper are based on 
the following three agent classes: 
• 
Action Listener: This agent has two basic tasks: 
connecting the agent system to the AMFIS ground 
control 
station 
and 
managing 
the 
different 
Teamleader Agents (see below). The Action Listener 
receives messages from the AMFIS Connector and 
sends corresponding commands or data to the 
relevant Teamleader Agents. Through the Action 
Listener, the operator can directly task a UAV, 
bypassing the agents’ logic. 
• 
Teamleader Agent: A team leader agent controls a 
group of agents consisting of at least one other agent. 
It co-ordinates higher tasks and assigns sub-tasks to 
team members, for examples areas they have to 
monitor. A team leader is always aware of the 
positions and capabilities of all team members. A 
team leader itself can be controlled by a 
superordinate team leader. 
• 
Universal Agent: This agent represents a single 
UAV. Every drone must be assigned to a team 
leader. The Universal Agent manages all basic 
behaviors and data of the UAV it represents. Basic 
behaviors are for example the direct flight to a 
waypoint and receiving and handling messages from 
the team leader. 
UAVs in a team can be equipped with different payloads, 
for example cameras or gas sensors. Therefore, specific 
agents exist, such as Video Agents and Sensor Agents, which 
inherit the basic behaviors from the Universal Agent. 
Additionally, they also have their own specific behaviors. By 
this separation in universal and specific agents, adaptations 
of the general behavior or the specific behavior can be made 
very efficiently, i.e., only one agent has to be modified. For 
example, if we want to change the behavior to fly to a 
waypoint we only have to do that in the Universal Agent, not 
in the specific payload agent. 
The communication between the agents of one team is 
direct. It is not possible to directly communicate with a 
Universal Agent of another team. Communication to a 
Universal Agent of another team has to go through the 
corresponding Teamleaders. A communication between two 

161
International Journal on Advances in Telecommunications, vol 3 no 3 & 4, year 2010, http://www.iariajournals.org/telecommunications/
2010, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
agents in different teams is usually not necessary, because 
every team has its own task. 
The use of this multi-agent system is not limited to 
UAVs. As well, it can be applied to coordinate a 
heterogeneous fleet of ground and air assets. 
 
VII. RESULTS 
Figure 13 shows a high-resolution situation picture 
generated with AMFIS using its photo flight tool presented 
in Section V.B. The picture is geo-referenced and can be 
overlaid onto the existing GIS-based, dynamic situation map. 
 
 
Figure 13.   Situation picture generated with the AMFIS photo flight tool 
(ca. 9500 x 9000 pixel) 
 
CONCLUSION 
The presented surveillance and reconnaissance system 
AMFIS with its extensions is constantly under development. 
Due to its generic nature, it forms a rather universal 
integration platform for new sensors, platforms, interfaces, 
supporting application programs and customized solutions. 
The knowledge gained from the participation in various 
exercises is constantly used to optimize the ergonomics of 
the work stations and to improve the algorithms for data 
fusion. The advancement of sensor technology and robotics, 
increasing processing power, progress in information and 
network technology provide continuous input for the 
permanent development and optimization of the AMFIS 
system. 
Presently, the human is still the most important link in 
the supervision chain. The operator must evaluate the data, 
which is delivered by the sensors and derive decisions to 
meet the existing dangers. Lastly, it is a person that is 
accountable and bears the responsibility for the resultant 
action, not the supporting machine. 
The above introduced duties of the situation analysis and 
situation response are so versatile and complicated that 
further research is inevitable to fully automate them. Up to 
now, humans are still indispensable in their varied roles as 
head of operations, pilot, analyst, watchman etc. The AMFIS 
system with its ability to integrate different sensors, sensor 
platforms and data sources can support and assist people 
with those tasks. 
With the use of the mobile AMFIS system in industry, as 
well as at authorities and organizations like fire brigade, 
search and rescue services, and police, the geographical and 
information data bases can be improved decisively. With the 
gathered reconnaissance information fused or linked to 
information extracted from different sources, the task forces 
deploying the AMFIS system can be better protected and 
coordinated and decision making in critical situations can be 
optimized. 
ACKNOWLEDGMENT 
The authors would like to thank their colleagues and 
students who have contributed to the work presented in this 
paper, especially Sandro Leuchter, Thomas Partmann, Sven 
Müller, Steffen Burger, Frederic Schumacher, and Torsten 
Großkurth. 
REFERENCES 
 
[1] F. Segor, A. Bürkle, T. Partmann, R. Schönbein, “Mobile Ground 
Control Station for Local Surveillance,“ In: ICONS 2010, The Fifth 
International Conference on Systems, 11-16 April 2010, Menuires, 
The Three Valleys, France. 
[2] AERODRONES, France, http://www.aerodrones.com, 2011. 
[3] AAI Corporation, USA, http://www.aaicorp.com, 2011. 
[4] Defense Technologies, Inc., USA, http://www.dtiweb.net, 2011. 
[5] Frost & Sullivan, “Advances in platform technologies for unmanned 
aerial vehicles,” Technical Insights Report D1B0, San Antonio, TX, 
2009. 
[6] B. Essendorfer and W. Müller, “Interoperable sharing of data with the 
Coalition Shared Data (CSD) server,” North Atlantic Treaty 
Organization (NATO)/Research and Technology Organization 
(RTO): C3I in crisis, emergency and consequence management, 
2009. 
[7] B. Essendorfer, E. Monari, and H. Wanning, “An integrated system 
for border surveillance,” In: R. Ege (ed.), ICONS 2009: The Fourth 
International Conference on Systems, 1-6 March 2009, Gosier, 
Guadeloupe/France. 
[8] RFC 3920: “Extensible Messaging and Presence Protocol (XMPP): 
Core,” The Internet Engineering Task Force (IETF), 2004. 
[9] RFC 3921: “Extensible Messaging and Presence Protocol (XMPP): 
Instant Messaging and Presence,“ The Internet Engineering Task 
Force (IETF), 2004. 
[10] M. Proctor, M. Neale, M. Frandsen, S. Griffith Jr., E. Tirelli, F. 
Meyer, and K. Verlaenen, “Drools Documentation (V. 4.0.4),” 2008, 
http://downloads.jboss.com/drools/docs/4.0.4.17825.GA/html_single/
index.html 
[11] S. J. Russell and P. Norvig, “Artificial Intelligence: A Modern 
Approach,” Prentice Hall, ISBN 9780137903955, 2003. 
[12] N. R. Jennings, K. Sycara, and M. Wooldridge, “A roadmap of agent 
research and development,” Journal of Autonomous Agents and 
Multi-Agent Systems, 1(1), pp. 7–38, ISSN 1387-2532, 1998. 
[13] P. Maes, “Agents that reduce work and information overload,” 
Communications of the ACM 37, 7 (July 1994), pp. 30-40, ISSN 
0001-0782, 1994. 

