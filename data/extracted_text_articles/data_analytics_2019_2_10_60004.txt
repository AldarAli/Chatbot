Analytical Models of Firing Rate Statistics in Sensory Neuroscience Experiments 
 
Christopher C. Pack 
Montreal Neurological Institute (MNI) 
McGill University 
Montreal, Quebec, Canada 
e-mail: christopher.pack@mcgill.ca 
Charles D. Pack 
Computer Science Dept. 
Monmouth University 
Red Bank, NJ 
e-mail: cpack@monmouth.edu
 
 
Abstract— Motivated by empirical results, we develop 
simple Taylor Series approximations (analytics) for 
statistics associated with neuronal response data in 
sensory (e.g., vision) experiments in a laboratory setting. 
Such responses exhibit a non-negativity constraint and 
additional nonlinearities, such as “normalization”. These 
transformations also change correlations among neuronal 
responses, which are thought to limit the fidelity of sensory 
representations. Simulation studies and cases, where we 
have exact results, show that our models are accurate over 
a wide range of parameter values. Ignoring constraints, 
simple analytical expressions help explain how data 
quality, parameter values and sensitivities affect results. 
Keywords- 
data; 
measurements; 
analytics; 
statistics; 
simulation; neuroscience; vision experiments. 
I. 
 INTRODUCTION 
Many sub-disciplines within the field of brain science are 
concerned with the relationship between sensory (e.g., 
vision) stimuli and electrical activity in groups of neurons. 
The fidelity of this “neural code” is thought to depend on the 
firing rates of individual neurons and the variability or noise 
associated with these rates. Standard models of neural coding 
suggest that noise that is correlated across neurons 
deteriorates sensory representations in the brain [1]. Recent 
work has proposed that a specific transform operation, 
“normalization,” might reduce the impact of correlated noise 
on neural coding. Support for this idea comes from 
simulations [2] and experimental observations. [3][4]. There 
are some analytical models for untransformed signal 
correlations [5][6]. Here, we develop new analytical models 
of pairwise correlations in neural responses to multiple 
replications of various sensory stimuli as well as the 
parametric relationship between a common nonlinear 
transformation and noise correlations. While we obtain some 
exact analytical results, degree-2 Taylor Series (T-S) 
approximations are particularly useful for transformed 
responses 
that 
require 
non-negativity 
constraints. 
Simulations show that our approximations are quite accurate 
for parameter values of interest to neuroscientists, but can be 
inaccurate or unstable at parameter extremes. 
In Section II, we describe our neuroscience experiments, 
data/measurements and important statistics. In Section III, we 
develop 
some 
analytical 
models 
and 
Taylor-Series 
approximations. In Section IV, we provide analytical and 
simulation results. Section V contains a summary and 
important conclusions. 
II. 
NEUROSCIENCE EXPERIMENTS, DATA, STATISTICS 
In 
neuroscience 
experiments, 
data 
come 
from 
electrophysiological recordings of two (or more) neurons, 
during the presentation of a sensory stimulus such as a visual 
image. Each neuron fire spikes, which are discrete events that 
we measure over some time window. The measured spike 
counts are generally assumed to follow a Poisson 
distribution. We record these responses to N repetitions of the 
M different visual stimuli. For any stimulus, i, we have a pair 
of vectors ( 𝜆"#$% , 𝜆"#$&) , corresponding to the measured 
responses of the two neurons across repetitions of the same 
stimulus. From those, we calculate rnoise(i), the correlation 
between these vectors. If we then take the mean, i.e., we 
“smooth” the original spike counts across repetitions, j, we 
get vectors (𝑣)#% , 𝑣)#& ). We next calculate the correlation 
between 𝑣)#%  and 𝑣)#&  to get a measure, rsignal(i), of the 
stimulus preferences of the two neurons. Because of limited 
data, neuroscientists often compute, rsignal~, a “pseudo-
correlation” over M stimuli. Finally, neuroscientists may 
analyze transformations of unsmoothed and smoothed 
responses with correlations rnoise(i)’ and rsignal~’. We 
analyze statistics for our models of these four cases:  1.  (𝜆+ #$%,  
𝜆"#$&) (rnoise(i)); 2.  (𝑣)#%, 𝑣)#&) (rsignal~); 3. Transformed 
responses (𝜆+ #$%
, ,𝜆"#$&
, ) (rnoise(i)’); 4. Transformed, averaged 
(𝑣)#%
, , 𝑣)#&
, ) (rsignal~’). Cases 1-3 can be done exactly; Case 4 
needs analytical approximations. 
III. 
ANALYTICAL MODELS 
We now develop analytical models for each of the four 
above cases. 
A. Case 1: Original Data and rnoise(i) 
For each neuron, k = 1 or 2, there are response 
measurements  𝜆"#$-=𝜆#-.1 + 𝜀#$-2 corresponding to stimuli 
i=1,…,M and replications j=1,…,N. The 𝜆#- and 𝜀#$- are the 
“true” random spike counts and the independent random 
measurement 
error 
factors. 
Let 
𝐸(𝜆#-) = 𝜆̅#-
 and 
𝑣𝑎𝑟(𝜆#-) = 𝜎9:;
& . The correlation between the two neuron 
26
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-741-2
DATA ANALYTICS 2019 : The Eighth International Conference on Data Analytics

responses (signals) is 𝜌(𝜆#%, 𝜆#&) ≥ 𝜌(𝜆"#$%, 𝜆"#$&). We assume 
𝜀#$-  is 𝑁(0, 𝜎A;
& ) . It follows that 𝐸.𝜆"#$-2 = 𝜆̅#-  and 
𝑣𝑎𝑟.𝜆"#$-2 = 𝜎9:;
& + 𝜆̅#-
& 𝜎A;
& + 𝜎9:;
& 𝜎A;
& . If measured responses 
are Poisson, 𝑣𝑎𝑟.𝜆"#$-) = 𝐸.𝜆"#$-2 = (𝜆̅#-2. Then, we have 
𝑐𝑜𝑣.𝜆"#$%, 𝜆"#$&2 = 𝑐𝑜𝑣(𝜆#%, 𝜆#&) = 𝜌(𝜆#%, 𝜆#&)𝜎9:F
& 𝜎9:G
&  and  
 
𝑟𝑛𝑜𝑖𝑠𝑒(𝑖) =
LMN(9+:OF,9+:OG)
PNQR(9+:OF)NQR(9+:OG)
=
S(9:F,9:G)
T(%U9V:F
G WXF
G
WY:F
G
UZXF
G )(%U9V:G
G WXG
G
WY:G
G
UZXG
G )
.                           
                                                                                            (1)  
 
Note that only the denominator involves measurement 
uncertainty. There is one unintended impact of the product-
form analytical model for Case 1 (and Case 3, below). Given 
the MxNx2 measurements from a vision experiment, we 
might use the common variance estimator for sample 
variances: 𝜎9+:O;
&
= 𝑣𝑎𝑟.𝜆"#$-2 ≈ 𝜎)9+:O;
&
=
∑
(9+:O;]
∑
Y+:^;
_
^`F
_
)G
_
O`F
a]%
 
=
∑
(9+:O;]N):;)G
_
O`F
a]%
, where 𝑣)#- is an estimate of 𝐸.𝜆"#$-2. The 
problem is that 𝐸 b𝜎)9+:O;
&
c = 𝜎9+:O;
&
− 𝜎9:;
& ≠ 𝜎9+:O;
&
. which is 
biased low for 𝑣𝑎𝑟.𝜆"#$-2  when 
is the product of 2 
random variables. We will need to correct this bias in our 
simulation. Similarly, if we use the standard covariance 
estimator for 𝜍̂(𝑖) ≈ 𝑐𝑜𝑣.𝜆"#$%, 𝜆"#$&2 = 𝑐𝑜𝑣(𝜆#%, 𝜆#&), we get 
𝜍̂(𝑖)=
∑
(9+:OF]N):F)(9+:OG]N):G)
_
O`F
a]%
, with the result that E[𝜍̂(𝑖)]=0, 
which is also biased low and needs to be corrected in the 
simulation results.                             
B. Case 2: “Smoothed” Data and rsignal(i), rsignal~ 
   Let 𝑣)#- =
∑
9+:O;
_
O`F
a
= 𝜆#-(1 +
∑
A:O;
_
O`F
a
) for i=1,…,M. Then, 
for all i, 𝐸(𝑣)#-) = 𝜆̅#-, 𝑣𝑎𝑟(𝑣)#-) = 𝜎9:;
& +
9V:;
G ZX;
G
a
+
ZY:;
G
ZX;
G
a
. 
Further, 𝑐𝑜𝑣(𝑣)#%, 𝑣)#&) = 𝑐𝑜𝑣.𝜆"#$%, 𝜆"#$&2 = 𝑐𝑜𝑣(𝜆#%, 𝜆#&), 
and correlation 𝑟𝑠𝑖𝑔𝑛𝑎𝑙(𝑖) =
LMN(N):F,N):G)
jNQR(N):F)NQR(N):G) =
S(9:F,9:G)ZY:FZY:G
PZkl:F
G
Zkl:G
G
 =
S(9:F,9:G)
T(%U9V:F
G
WXF
G
_WY:F
G
UZXF
G )(%U9V:G
G
WXG
G
_WY:G
G
UZXG
G )
           (2)                                                                                          
Equation (2) is quite similar to (1) except that measurement 
error variances are divided by N. Then, due in part to data 
limitations, neuroscientists generally use (define) rsignal~, a 
different measure of the similarity of neuronal responses to 
various stimuli. That is,  rsignal~ 
 
 
=
LMN~(N)∗F,N)∗G)
jNQR~(N)∗F)NQR~(N)∗G) =
o ∑
p (N):F]N)∗F)(N):G]N)∗G)
:`F
Po ∑
(N):F]N)∗F)Go ∑
p (N):G]N)∗G)G
:`F
p
:`F
       (3)                                                    
where 𝑣)∗- =
∑
N)∗;
p
:`F
q
. While 𝑐𝑜𝑣(𝑣)#%, 𝑣)#&) = 𝑐𝑜𝑣.𝜆"#$%, 𝜆"#$&2, 
𝑐𝑜𝑣~(𝑣)∗%, 𝑣)∗&) ≠  𝑐𝑜𝑣.𝜆"#$%, 𝜆"#$&2 . 
We 
can 
compute 
rsignal~, in terms of more traditional moments, using 
 
𝐸 ∑
(𝑣)#% − 𝑣)∗%)(𝑣)#& − 𝑣)∗&)
q
#r%
=
q]%
q ∑
s𝑐𝑜𝑣(𝜆#%, 𝜆#&) +
q
#r%
𝜆̅#%𝜆̅#&t −
%
q ∑
∑
s𝑐𝑜𝑣.𝜆#%, 𝜆u&2 + 𝜆̅#%𝜆̅u&t
q
uv#
q
#r%
,                (4)                   
 
𝐸 ∑
(𝑣)#- − 𝑣)∗-)&
q
#r%
=
q]%
q ∑
(𝜎A;
& + 𝜆̅#-
& )
q
#r%
−
%
q ∑
∑
s𝑐𝑜𝑣.𝜆#-, 𝜆u-2 + 𝜆̅#-𝜆̅u-t
q
uv#
q
#r%
.                                (5) 
 
 
Assume inter-stimuli covariances in (4), (5) are negligible.  
C. Cases 3 and 4: Transformed Responses 
First, we define the normalization transformation that we 
use in this report: 
 
𝜆"#$-
,
= 𝑓.𝜆#-, 𝜆#-], 𝜀#$-, 𝜀#$-]2
= max {0,
𝜆"#$-
&
𝑠𝜆"#$-
&
+ 𝑐 − 𝑟
𝜆"#$-]
&
𝑠𝜆"#$-]
&
+ 𝑐| 
= max b0, 𝑔.𝜆#-, 𝜆#-], 𝜀#$-, 𝜀#$-]2c 
 
         = max }0,
[9:;(%UA:O;)]G
€[9:;.%UA:O;2]GUL −
R[9:;•(%UA:O;•)]G
€[9:;•.%UA:O;•2]GUL‚          (6) 
 
where s, c, r are non-negative constants and k- is the opposite 
neuron index from k. The first term in expanded (6) 
corresponds to the standard form of normalization used in 
neuroscience [7]; the terms in the denominator can be thought 
of as corresponding to pools of neurons that are correlated 
(first term) or uncorrelated (second term) with the neuron in 
the numerator. The second term in the equation allows for the 
possibility of opponent processing, a common neural 
operation that is hypothesized to influence noise correlations 
[8]. The max function captures the fact that neural firing rates 
cannot, by definition, be negative. We will see that the 
transformations in Cases 3 and 4 increase complexity because 
they: 
• 
Are highly non-linear and may need to be constrained to 
be non-negative 
• 
Add many parameters to the model 
• 
Reduce the non-negative magnitude of the (transformed) 
responses, 𝜆"#$-
,
, to near 0 
• 
Cause correlation statistics, such as rnoise(i)’ and 
rsignal~’, to become very sensitive to important system 
parameters, e.g., as 𝜆"#$-
,
 approaches 0. 
 
In our simulation of Case 3, i.e., smoothed transformed 
data and rnoise(i)’, we have to correct biases in the classic 
variance and covariance estimators,  
𝜎)9+:O;
ƒ&
 and 𝑐̂.𝜆"#$%
, , 𝜆"#$&
, 2, for
. 
That is, for 𝑗 ≠ 𝑝: 
 
𝐸 }𝜎)9+:O;
ƒ&
‚ = 𝑣𝑎𝑟.𝜆"#$-
,
2 + 𝐸&.𝜆"#$-
,
2 − 𝐸.𝜆"#$-
,
𝜆"#u-
,
2             (7) 
ˆλijk
var( ˆλijk
' ) and cov( ˆλij1
' , ˆλij2
' )
27
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-741-2
DATA ANALYTICS 2019 : The Eighth International Conference on Data Analytics

 
and  
     
𝐸 b𝑐̂.𝜆"#$%
, , 𝜆"#$&
, 2c = 𝑐𝑜𝑣.𝜆"#$%
, , 𝜆"#$&
, 2 + 𝐸.𝜆"#$%
, 2𝐸.𝜆"#$&
, 2 −
𝐸.𝜆"#$%
, 𝜆"#u&
,
2.                                                                          (8) 
 
        We also developed exact and T-S analytical models for 
Case 3. Our exact approach conditions on 𝜆#$- and 𝜆#$-] in 
(6), then assumes a joint distribution for them and integrates 
out to obtain expected values, variances and covariances for 
𝜆"#$-
,
. The limits on the integrals reflect the non-negativity 
constraint. Our analytical approximations focus on degree-2 
Taylor Series (TS-2) models (extensive empirical analyses 
show that the much more complex higher degree models 
“overfit” the data), with a differentiable approximation of the 
non-negativity constraint for 𝜆"#$-
,
 in (6). We use Taylor 
Series models because they often provide very good 
approximations and reveal critical statistical relationships 
between input parameters and results. We have not quantified 
the very small approximation errors. Important TS-2 
moments, including correlations, are linear combination of 
input variances/ covariances, with weights being algebraic 
expressions of input parameters. Of special interest is the 
Simplified TS-2 (STS-2) version of the model that is valid 
when the non-negativity constraint is “non-binding”. To 
develop the TS-2 Model, we replace the non-negativity 
constraint in the f form of (6) with  
 
𝜆"#$-
,
= 𝑓.𝜆#-, 𝜆#-], 𝜀#$-, 𝜀#$-]2 
 
           ≈
‰.9:;,9:;•,A:O;,A:O;•2UP‰.9:;,9:;•,A:O;,A:O;•2
GUŠ
&
            (9) 
 
where d, which is required for differentiability, is a very small 
positive number. Then, the Taylor Series fit to f, as given by  
(9), is around the “point” (𝜆̅#-, 𝜆̅#-], 𝜀̅#$-, 𝜀̅#$-]). We assume 
that the l’s have a bivariate Normal distribution with means 
( 𝜆̅#-, 𝜆̅#-]) , variances ( 𝜎9:;
& , 𝜎9:;•
&
) and correlation 
𝜌(𝜆#-, 𝜆#-]).  The e’s are each (independently) normally 
distributed with 0 means and variances 𝜎A;
&  𝑎𝑛𝑑 𝜎A;•
& , 
respectively. Accuracy is usually best when variances 
(𝜎9:;
& , 𝜎9:;•
&
, 𝜎A;
& , 𝜎A;•
& ) are small.  To get the simpler STS-2 
model results, we ignore the non-negativity constraint, i.e., 
f≡g. With limited space, we present some results. First, the 
TS-2 expression for the mean is: 
 
𝐸.𝜆"#$-
,
2 ≈ 𝑓 +
%
& •𝑓A:O;A:O;𝜎A;
& + 𝑓A:O;•A:O;•𝜎A;•
&
+
𝑓9:;9:;𝜎9:;
& + 𝑓9:;•9:;•𝜎9:;•
&
Ž + 𝑓9:F9:G𝑐𝑜𝑣(𝜆#%, 𝜆#&)           (10) 
 
In (10), the subscripts on f and g represent partial derivatives 
and all expressions in f and g are evaluated at the mean point 
.𝜆̅#-, 𝜆̅#-], 0,02. Also, all of the partials, fxy, are of the form:  
𝑓•• =
%
& [𝑔•𝑔•
Š
(‰GUŠ)F.‘ + }1 +
‰
j‰GUŠ‚ 𝑔••]         (11)                        
 
with the following pairs for (x,y), with corresponding g 
partials (evaluated at mean): 
o 
.𝜀#$-, 𝜀#$-2:   𝑔A:O; =
&L9V:;
G
[€9V:;
G UL)G, 𝑔A:O;A:O; =
&L9V:;
G (L]’€9V:;
G )
[€9V:;
G UL)“
    (12A) 
o 
.𝜀#$-], 𝜀#$-]2:  𝑔A:O;• = 
]&RL9V:;•
G
[€9V:;•
G
UL)G,  𝑔A:O;•A:O;]
]&RL9V:;•
G
(L]’€9V:;•
G
)
[€9V:;•
G
UL)“
                        
                                                                                                     (12B) 
o 
(𝜆#-, 𝜆#-):  𝑔9:; =
&L9V:;
[€9V:;
G UL)G ,  𝑔9:;9:; =
&L(L]’€9V:;
G )
[€9V:;
G UL)“   (12C)            
o 
(𝜆#-], 𝜆#-]): 𝑔9:;• =
]&RL9V:;•
[€9V:;•
G
UL)G,  𝑔9:;•9:;• =
]&RL(L]’€9V:;•
G
)
[€9V:;•
G
UL)“
         
                                                                                           (12D) 
o 
 𝑔9:F9:G = 0                                                                    (12E)                         
 
The TS-2 approximation for the variance, 𝑣𝑎𝑟(𝜆"#$-
,
), is: 
 
𝑣𝑎𝑟(𝜆"#$-
,
) ≈ 𝑓A:O;
& 𝜎A;
& + 𝑓A:O;•
&
𝜎A;•
&
+ 𝑓9:;
& 𝜎9:;
& + 𝑓9:;•
&
𝜎9:;•
&
+
2𝑓9:F9:G𝑐𝑜𝑣(𝜆#%, 𝜆#&),                                                           (13) 
 
where 𝑓9:F9:G is given in (11), the weights fx  are 
 
            𝑓• =
%
& 𝑔•(1 +
‰
j‰GUŠ ,                                                (14)     
                                                                  
and the various cases for the partials, gx, are in (12A)-(12E).  
The TS-2 approximation for the covariance, 
𝑐𝑜𝑣.𝜆"#$%
, , 𝜆"#$&
, 2, is complicated by the fact that it involves two 
(transformed) random responses: 𝜆"#$%
,  and 𝜆"#$&
, : 
 
𝑐𝑜𝑣.𝜆"#$%
, , 𝜆"#$&
, 2 ≈ 𝑓A:OF𝑓⃐A:OF𝜎AF
& + 𝑓A:OG𝑓⃐A:OG𝜎AG
& + 𝑓9:F𝑓⃐9:F𝜎9:F
& +
𝑓9:G𝑓⃐9:G𝜎9:G
& +(𝑓9:F𝑓⃐9:G + 𝑓9:G𝑓⃐9:F) 𝑐𝑜𝑣(𝜆#%, 𝜆#&)                    (15)                       
 
where all of the partials of the form, fx, are given in (14). 
However, the term, 𝑓⃐•, requires some explanation. Because 
of the symmetries of the graded transformation (6), 𝜆"#$-
,
=
𝑓.𝜆#-, 𝜆#-], 𝜀#$-, 𝜀#$-]2  and 𝜆"#$-]
,
= 𝑓.𝜆#-], 𝜆#-, 𝜀#$-], 𝜀#$-2 , 
i.e., 𝜆"#$-
,
 is 𝜆"#$-]
,
 but with k and k- arguments reversed. This 
symmetry suggests that partials on the “reverse f”, call it 𝑓⃐, 
can be calculated as partials on 𝑓 if we reverse k and k- 
everywhere. For example, we use 2 steps to compute 𝑓⃐A:OF in 
the TS-2 expression (15): (1) Find the partial, 𝑓A:OG and then 
(2) Evaluate it at the reverse (mean) argument .𝜆̅#&, 𝜆̅#%, 0,02. 
All expressions needed for 𝑓•  are given in (14). We can 
compute the TS-2 or STS-2 versions of rnoise(i)’. For STS-2 
 
𝑟𝑛𝑜𝑖𝑠𝑒(𝑖), ≈
]R9V:F
G ZY+:OF
G
]R™š9V:G
G ZY+:OG
G
U™G9V:F9V:G.%URG2LMN(9:F,9:G)
T{9V:F
G ZY+:OF
G
URG™š9V:G
G ZY+:OG
G
|{RG9V:F
G ZY+:OF
G
U™š9V:G
G ZY+:OG
G
|
   
                                                                                          (16)             
28
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-741-2
DATA ANALYTICS 2019 : The Eighth International Conference on Data Analytics

where 𝑅 =
9V:F
G UL/€
9V:G
G UL/€ captures all the effects of s and c. Equation 
(16) simplifies at r=0 to be rnoise(i)’ ≈ rnoise(i), i.e., the 
transform has (approximately) no effect; this applies to TS-2 
and STS-2 since the constraint is not needed. 
      For Case 4, i.e., the smoothed transformed data (and 
rsignal~’), exact solutions are impractical because they 
require conditioning on at least 6 partially-dependent random 
variables with integrations over a complicated region of 
feasibility. Now, let 𝑣)#-
, =
∑
9+:O;
ƒ
_
O`F
a
 and 𝑣)∗-
, =
∑
N):;
ƒ
p
:`F
q
. Then, 
we can provide expressions for the TS-2 and STS-2 
components of rsignal~’. That is, the expressions for E(𝑣)#-
, ),  
𝑣𝑎𝑟(𝑣)#-
, ) and 𝑐𝑜𝑣(𝑣)#%
, , 𝑣)#&
, ) are the corresponding versions of 
the untransformed responses, but in TS-2 the measurement-
error variances, 𝜎A;
&  and 𝜎A;]
&  are divided by 𝑁 and in STS-
2, 𝜎 9+:O;
&
 and 𝜎9+:O;•
&
 are replaced by 𝜎N):;
&  𝑎𝑛𝑑 𝜎N):;•
&
.  If we 
used 
the 
classical 
correlation, 
𝑟𝑠𝑖𝑔𝑛𝑎𝑙(𝑖), =
LMN.N):F
ƒ ,N):G
ƒ 2
PNQR.N):F
ƒ 2NQR.N):G
ƒ 2
, we would have all we need for TS-2 and 
STS-2. The STS-2 expression for rsignal(i)’ would be (16) 
with 𝜎9+:O;
&
 replaced by 𝑣)#-
,  for k=1, 2.  Moreover, for TS-2 
and STS-2, we would find that when r=0, rsignal(i)’» 
rsignal(i), i.e., that transform does not affect correlation (as 
for rnoise(i)’). However, the “pseudo-correlation”,  
 
𝑟𝑠𝑖𝑔𝑛𝑎𝑙~, =   
o ∑
(N):F
ƒ
p
:`F
]N)∗F
ƒ )(N):G
ƒ ]N)∗G
ƒ )
Po ∑
(N):F
ƒ
p
:`F
]N)∗F
ƒ )Go ∑
(N):G
ƒ
p
:`F
]N)∗G
ƒ )G ,                  (17)                                                                             
 
𝐸 ∑
(𝑣)#-
,
q
#r%
-𝑣)∗-
, )2=
q]%
q ∑
[𝑣𝑎𝑟(𝑣)#-
, ) + 𝐸
q
#r%
2(𝑣)#-
, )]                                                             
- 
%
 q ∑
∑
[𝑐𝑜𝑣(𝑣)#-
, , 𝑣)¥-
,
) + 𝐸(𝑣)#-
, )𝐸(𝑣)¥-
,
)]
¥v#
q
#r%
 ,           (18)              
 
𝐸 ∑
(𝑣)#%
,
q
#r%
-𝑣)∗%
, )(𝑣)#&
, − 𝑣)∗&
, )=
q]%
q ∑
[𝑐𝑜𝑣(𝑣)#%
, , 𝑣)#&
, ) +
q
#r%
𝐸(𝑣)#%
, )𝐸(𝑣)#&
, )]] −
%
q ∑
∑
[𝑐𝑜𝑣(𝑣)#%
, , 𝑣)¥&
, ) +
¥v#
q
#r%
𝐸(𝑣)#%
, )𝐸(𝑣)¥&
, )]                                                                 (19) 
                                                                                
where we usually assume 𝑐𝑜𝑣(𝑣)#-
, , 𝑣)¥-
,
) and 𝑐𝑜𝑣(𝑣)#%
, , 𝑣)¥&
, )  
are neglibile for 𝑖 ≠ 𝑚. 
IV. 
ANALYTICAL AND SIMULATION RESULTS 
 Before we discuss results, let us overview our simulation. 
Our simulation generates random measured responses (𝜆"#$-) 
to various stimuli in visual neuroscience experiments. While 
are often modeled as Poisson, we assume that they are 
Bi-Normal with 𝐵𝑁 b𝜆̅#%, 𝜎9+:OF
& ; 𝜆̅#&, 𝜎9+:OG
& ; 𝜌( 𝜆"#$%, 𝜆"#$&)c. For 
each simulation sample, we generate 2xNxM random 
responses. We consider various pairs of values for the mean 
responses ( 𝜆̅#%, 𝜆̅#& ), each on [25,50]. In assigning key 
parameter values, we prescribe response correlation to be 
𝜌(𝜆#%, 𝜆#&) = 0.7 for all stimuli, i and let the coefficient of 
variation for 𝜆#-  be 5%, i.e., 𝑣𝑎𝑟(𝜆#-) = (0.05𝐸(𝜆#-))& . 
Next, we derive values for the (Poisson) measurement error 
variances,  𝑣𝑎𝑟(𝜀#$-) . Finally, we compute the statistics 
required for Cases 1-4 and contrast them to those obtained 
analytically. To assure the statistical validity, we generate up 
to S=500 experiment “samples” and we correct biases, for 
Cases 1, 3 variance and covariance estimates, as in Sections 
III-B and III-C. 
In Section IV-A, we provide only brief comments on Cases 
1 and 2, the untransformed responses, because our analytical 
models are “exact”. In Section IV-B, we look at simulation 
and T-S analytical results for the transformed responses; we 
focus on Case 3 statistics, 𝑣𝑎𝑟.𝜆"-$%
,
2 and rnoise(6)’,  because 
Case 4 observations do not change significantly as we vary 
key parameters. This similarity of empirical results is quite 
consistent with the striking similarity of the approximate 
analytical results in Section III-C. 
A. Cases 1 and 2: Untransformed Responses 
For classical means and covariances, the true, measured 
and smoothed analytical results are the same. However, the 
unsmoothed and smoothed classical variances may differ 
significantly because averaging reduces the variance of 
measured responses. The key challenge for these Cases is to 
make sure the simulation yields accurate estimates of the 
associated transformed results for Cases 3 and 4. In that 
regard, we note the following: 
• 
The simulation biases for variances and covariances, are 
corrected as in Section III-B. 
• 
“Pseudo variances and covariances” seem to differ only 
slightly from the classical ones. 
• 
Correlation statistics, rnoise(i) and rsignal~, are difficult 
to estimate, may be unstable, using simulation (or 
experimental data) because they are ratios of other 
statistics. 
 
B. Cases 3 and 4: Transformed Responses 
Using our simulation, we studied extensively the 
accuracy of TS-2 and STS-2 for the moments of transformed 
response statistics. The transformed cases are difficult to 
analyze:  
• 
The transformation, as in (6), introduces many additional 
parameters. 
• 
Transformations are nonlinear, further complicated by 
the non-negativity constraint.  
• 
Transformed responses are quite small, on the order of 
10-3 to 10-4 for Case 4, and approaching 0 for parameter 
extremes, e.g., large values for s, c, or r.  
In addition, variances and covariances are much more 
challenging than means to estimate, at parameter extremes, 
and correlations depend strongly on variance and covariance 
accuracy. We also know that STS-2 will (TS-2 may not) have 
accuracy problems where the non-negativity constraints are 
needed. We will see that, despite these difficulties, TS-2 and 
STS-2 are accurate and fast over important parameter ranges. 
ˆλijk
29
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-741-2
DATA ANALYTICS 2019 : The Eighth International Conference on Data Analytics

Neuroscientists restrict r to be on [0,1], and often in the 
“neighborhood” of 0.5. However, in our analyses, we extend 
the domain to [0,1.5] to better understand the properties of 
the transforms and our models. We use the ratio c/s in our 
examples because, in our studies, we found that individual 
values of s and c matter only at extremes (e.g., s=0, c=0, or 
r>> 0.5). For example, the STS-2 expression for rnoise(i)’ 
(16) only depends on c/s. We assume arbitrarily i=6, k=1.  
 
      1) Accuracy of Analytical Models for Variances. In 
Figure 1, below, we look at the accuracy of TS-2 and STS-2 
(vs. simulation) of 𝑣𝑎𝑟.𝜆"-$%
,
2 as we vary r, c/s and (𝜆̅-%, 𝜆̅-&). 
The rows of graphs correspond to different values of c/s and 
columns to ratios 
9V®G
9V®F  .  For each graph, the y-axis is 
𝑣𝑎𝑟.𝜆"-$%
,
2 and the x-axis is 0 £ r £ 1.5. Some observations: 
• 
For these data, the analytical and simulation results 
match well except for STS-2 when r>>0.5 or 
9V®G
9V®F >>  1. 
The analytical model “errors” are usually small in 
absolute value. 
• 
The  𝑣𝑎𝑟.𝜆"-$%
,
2  graph is a unimodal non-negative 
function of c/s, starting at 0 when c/s=0, becoming 
positive and then approaching 0 again for large c/s. 
While not shown here, all three models do a good job of 
exhibiting this property.  
• 
Because of the non-negativity constraint in (6), 
𝑣𝑎𝑟.𝜆"-$%
,
2à0 for large r. We see that TS-2 generally 
provides accurate estimates for large r, while STS-2 does 
not. However, neuroscientists see little practical 
application in their models for r>>0.5. 
• 
TS-2 estimates of 𝑣𝑎𝑟.𝜆"-$%
,
2 are somewhat inaccurate 
when  
9V®G
9V®F >> 1  because the non-negativity constraint is 
needed. It would be much less important to 𝑣𝑎𝑟.𝜆"-$%
,
2 if  
9V®F
9V®G >>  1.  
 
2) Accuracy and Stability of Correlation Statistics. 
Figure 2 (see the y-axes), below, helps us gain insights into 
the accuracy and stability of our rnoise(i)’ models. Some 
observations: 
• 
The analytical models are accurate (match the simulation 
well) if r £  0.75;  
9V®G
9V®F ≤
±
’ , i.e., ≈ 1 due to symmetries.  
• 
The trajectory of rnoise(6)’ vs. r is inherently volatile but 
necessarily within [-1,1]. It starts near 0 for r=0, 
decreases sharply as r increases and, when the non-
negativity constraint “kicks in”, turns up towards 0 
again. TS-2 tracks the upturn pretty well, but (of course) 
STS-2 does not. 
• 
The TS-2 model shows some apparent numerical 
instability, relative to the simulation, for large r. This is 
because, as the numerator 𝑐𝑜𝑣.𝜆"-$%
,
, 𝜆"-$&
,
 2, and 
denominator, 𝑣𝑎𝑟.𝜆"-$%
,
2𝑣𝑎𝑟.𝜆"-$&
,
2, each approach 0, 
estimates of rnoise (6)’ approach 0/0 and are sensitive 
to the rates of convergence of the ratio components.  
• 
Recall (Section III-C), that at r=0, rnoise(i)’ » rnoise(i), 
i.e., the transform is insensitive to c/s and 
9V®G
9V®F. This c/s 
insensitivity for rnoise(6)’ extends to r £  0.75, 
9V®G
9V®F ≈ 1. 
V. 
SUMMARY AND CONCLUSIONS 
We conclude, based on our example analytical and 
simulation results, that TS-2 and STS-2 are accurate for a 
wide range of parameter values. Because the STS-2 models 
omit non-negativity constraints, their accuracy is best when 
r£ 0.75; 
9V:G
9V:F  £  
±
’  for 𝑣𝑎𝑟.𝜆"#$%
, 2and 
9V:G
9V:F  ≈ 1 for correlation, 
rnoise(i)’. In addition, their simple expressions help us better 
understand relationships between model parameters and 
results. Our approximations are extremely fast, linear 
combinations of input parameters, with algebraic expressions 
for the weights. Two key conclusions for neural coding are: 
• 
Transformations, such as normalization, can influence 
noise correlations, but the relationship between these 
two factors, rnoise(i) and rnoise(i)’ (or between rsignal’ 
and rsignal), is highly sensitive to parameter choices, 
and hence unlikely to be robust in real neural networks. 
• 
Opponent processing, i.e., the combining of response 
data from multiple neurons, has a profound influence on 
noise correlations. 
REFERENCES 
[1] E. Zohary, M. Shadler, and W. Newsome, “Correlated 
Neuronal 
Discharge 
Rate 
amd 
Its 
Implication 
for 
Pshycophysical Performance,” vol. 370, pp. 140-143, 1994. 
[2] B. Tripp, “Decorrelation of Spiking Variability and Improved 
Information 
Transfer 
Through 
Feedforward 
Divisive 
Noralization,” Neural Computation, vol. 24, No. 4, pp. 867-
894, April 2012. 
[3] D. Ruff, J. Alberts, and M. Cohen, “Relating Normalization to 
Neuronal 
Populations 
Across 
Cortical 
Areas,” 
Neurophysiology, vol. 116, Issue 3, pp. 1375-1386, September 
2016. 
[4] L. Liu, R. Haefner, and C. Pack, “A “Neural Basis for the 
Spatial Suppression  of Visual Motion Perception,” eLife 2016; 
5:e16167 doi: 10.7554/eLife.16167, May 2016. 
[5] D. Lyamzin, J. Macke, and N. Lesica, “Modeling Population 
Spike Trains with Specified Time-Varying Spike Rates, Trial-
to-Trial Variability, Pairwise Signal and Noise Correlations,” 
Front. Comp. Neuro., 4:144, doi:10.3389/fncom.2010.00144, 
pmid:21152346, November 2010. 
[6] Y. Hu, J. Zylerberg, and E.Shea-Brown, “The Sign Rule and 
Beyond: Boundary Effects, Flexibility, and Noise Correlations 
in Neural Population Codes,” PLoS Computational Biology, 
vol. 10, e1003469. doi:10.1371/journal.pcbi.1003469, 2014. 
[7] D. Heeger, “Normalization of Cell Respomses in Cat Striate 
Cortex,” Visual Neuroscience, vol. 9, Issue 2, pp. 181-197, 
August 1992.  
[8] Y. Chen Y, W. Geisler, and E. Seidemann, “Optimal Decoding 
of Correlated Neural Population Responses in the Primate 
Visual Cortex,” Nat Neurosci., vol. 9, pp. 1412–1420, 2006.
30
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-741-2
DATA ANALYTICS 2019 : The Eighth International Conference on Data Analytics

 
 
Figure 1. Transformed Variances, var(𝜆"-$%
,
), vs. r 
 
 
 
 
Figure 2. Correlation Statistics, rnoise(6)’, vs. r 
0
0.3
0.6
0.9
1.2
1.5
0
1.2 10
- 3
´
2.4 10
- 3
´
3.6 10
- 3
´
4.8 10
- 3
´
6 10
- 3
´
.006
2.5 10
- 9
´
vV7 rR
(
)
vV8 rR
(
)
vV9 rR
(
)
1.5
0
rR
0
0.3
0.6
0.9
1.2
1.5
0
3 10
- 3
´
6 10
- 3
´
9 10
- 3
´
0.012
0.015
.015
2.5 10
- 9
´
vV4 rR
(
)
vV5 rR
(
)
vV6 rR
(
)
1.5
0
rR
0
0.3
0.6
0.9
1.2
1.5
0
8 10
- 4
´
1.6 10
- 3
´
2.4 10
- 3
´
3.2 10
- 3
´
4 10
- 3
´
.004
0
vV1 rR
(
)
vV2 rR
(
)
vV3 rR
(
)
1.5
0
rR
0
0.3
0.6
0.9
1.2
1.5
0
1.2 10
- 3
´
2.4 10
- 3
´
3.6 10
- 3
´
4.8 10
- 3
´
6 10
- 3
´
.006
2.5 10
- 9
´
vV7 rR
(
)
vV8 rR
(
)
vV9 rR
(
)
1.5
0
rR
0
0.3
0.6
0.9
1.2
1.5
0
3 10
- 3
´
6 10
- 3
´
9 10
- 3
´
0.012
0.015
.015
2.5 10
- 9
´
vV4 rR
(
)
vV5 rR
(
)
vV6 rR
(
)
1.5
0
rR
0
0.3
0.6
0.9
1.2
1.5
0
8 10
- 4
´
1.6 10
- 3
´
2.4 10
- 3
´
3.2 10
- 3
´
4 10
- 3
´
.004
0
vV1 rR
(
)
vV2 rR
(
)
vV3 rR
(
)
1.5
0
rR
0
0.3
0.6
0.9
1.2
1.5
0
1.2 10
- 3
´
2.4 10
- 3
´
3.6 10
- 3
´
4.8 10
- 3
´
6 10
- 3
´
.006
2.5 10
- 9
´
vV7 rR
(
)
vV8 rR
(
)
vV9 rR
(
)
1.5
0
rR
0
0.3
0.6
0.9
1.2
1.5
0
3 10
- 3
´
6 10
- 3
´
9 10
- 3
´
0.012
0.015
.015
0
vV4 rR
(
)
vV5 rR
(
)
vV6 rR
(
)
1.5
0
rR
0
0.3
0.6
0.9
1.2
1.5
0
8 10
- 4
´
1.6 10
- 3
´
2.4 10
- 3
´
3.2 10
- 3
´
4 10
- 3
´
.004
0
vV1 rR
(
)
vV2 rR
(
)
vV3 rR
(
)
1.5
0
rR
!"#,%
!"#,& increases (1, 4/3, 2)
c/s
Increases
(100, 750,
5000)
Simulation 
TS-2 
STS-2 
.004
.0024
0
.015
.009
0
.006
.0036
0
0           .75         1.5
r
0          .75         1.5
r
0          .75          1.5
r
0
0.5
1
1.5
1
-
0.5
-
0
0.062
1
-
rN7 rR
(
)
rN8 rR
(
)
rN9 rR
(
)
0
1.5
0
rR
0
0.5
1
1.5
1
-
0.5
-
0
0.062
1
-
rN4 rR
(
)
rN5 rR
(
)
rN6 rR
(
)
0
1.5
0
rR
0
0.5
1
1.5
1
-
0.5
-
0
0.062
1
-
rN1 rR
(
)
rN2 rR
(
)
rN3 rR
(
)
0
1.5
0
rR
0
0.5
1
1.5
1
-
0.5
-
0
0.062
1
-
rN7 rR
(
)
rN8 rR
(
)
rN9 rR
(
)
0
1.5
0
rR
0
0.5
1
1.5
1
-
0.5
-
0
0.062
1
-
rN4 rR
(
)
rN5 rR
(
)
rN6 rR
(
)
0
1.5
0
rR
0
0.5
1
1.5
1
-
0.5
-
0
0.062
1
-
rN1 rR
(
)
rN2 rR
(
)
rN3 rR
(
)
0
1.5
0
rR
0
0.5
1
1.5
1
-
0.5
-
0
0.062
1
-
rN7 rR
(
)
rN8 rR
(
)
rN9 rR
(
)
0
1.5
0
rR
0
0.5
1
1.5
1
-
0.5
-
0
0.062
1
-
rN4 rR
(
)
rN5 rR
(
)
rN6 rR
(
)
0
1.5
0
rR
0
0.5
1
1.5
1
-
0.5
-
0
0.062
1
-
rN1 rR
(
)
rN2 rR
(
)
rN3 rR
(
)
0
1.5
0
rR
.05
0
-.5
-1
Simulation 
TS-2 
STS-2 
c/s
Increases
(100, 750,
5000)
!"#,%
!"#,& increases (1, 4/3, 2)
.05
0
-.5
-1
.05
0
-.5
-1
0           .75         1.5
r
0           .75         1.5
r
0           .75         1.5
r
31
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-741-2
DATA ANALYTICS 2019 : The Eighth International Conference on Data Analytics

