Analytical Models of Firing Rate Statistics in Sensory Neuroscience Experiments 
 
Christopher C. Pack 
Montreal Neurological Institute (MNI) 
McGill University 
Montreal, Quebec, Canada 
e-mail: christopher.pack@mcgill.ca 
Charles D. Pack 
Computer Science Dept. 
Monmouth University 
Red Bank, NJ 
e-mail: cpack@monmouth.edu
 
 
Abstractâ€” Motivated by empirical results, we develop 
simple Taylor Series approximations (analytics) for 
statistics associated with neuronal response data in 
sensory (e.g., vision) experiments in a laboratory setting. 
Such responses exhibit a non-negativity constraint and 
additional nonlinearities, such as â€œnormalizationâ€. These 
transformations also change correlations among neuronal 
responses, which are thought to limit the fidelity of sensory 
representations. Simulation studies and cases, where we 
have exact results, show that our models are accurate over 
a wide range of parameter values. Ignoring constraints, 
simple analytical expressions help explain how data 
quality, parameter values and sensitivities affect results. 
Keywords- 
data; 
measurements; 
analytics; 
statistics; 
simulation; neuroscience; vision experiments. 
I. 
 INTRODUCTION 
Many sub-disciplines within the field of brain science are 
concerned with the relationship between sensory (e.g., 
vision) stimuli and electrical activity in groups of neurons. 
The fidelity of this â€œneural codeâ€ is thought to depend on the 
firing rates of individual neurons and the variability or noise 
associated with these rates. Standard models of neural coding 
suggest that noise that is correlated across neurons 
deteriorates sensory representations in the brain [1]. Recent 
work has proposed that a specific transform operation, 
â€œnormalization,â€ might reduce the impact of correlated noise 
on neural coding. Support for this idea comes from 
simulations [2] and experimental observations. [3][4]. There 
are some analytical models for untransformed signal 
correlations [5][6]. Here, we develop new analytical models 
of pairwise correlations in neural responses to multiple 
replications of various sensory stimuli as well as the 
parametric relationship between a common nonlinear 
transformation and noise correlations. While we obtain some 
exact analytical results, degree-2 Taylor Series (T-S) 
approximations are particularly useful for transformed 
responses 
that 
require 
non-negativity 
constraints. 
Simulations show that our approximations are quite accurate 
for parameter values of interest to neuroscientists, but can be 
inaccurate or unstable at parameter extremes. 
In Section II, we describe our neuroscience experiments, 
data/measurements and important statistics. In Section III, we 
develop 
some 
analytical 
models 
and 
Taylor-Series 
approximations. In Section IV, we provide analytical and 
simulation results. Section V contains a summary and 
important conclusions. 
II. 
NEUROSCIENCE EXPERIMENTS, DATA, STATISTICS 
In 
neuroscience 
experiments, 
data 
come 
from 
electrophysiological recordings of two (or more) neurons, 
during the presentation of a sensory stimulus such as a visual 
image. Each neuron fire spikes, which are discrete events that 
we measure over some time window. The measured spike 
counts are generally assumed to follow a Poisson 
distribution. We record these responses to N repetitions of the 
M different visual stimuli. For any stimulus, i, we have a pair 
of vectors ( ğœ†"#$% , ğœ†"#$&) , corresponding to the measured 
responses of the two neurons across repetitions of the same 
stimulus. From those, we calculate rnoise(i), the correlation 
between these vectors. If we then take the mean, i.e., we 
â€œsmoothâ€ the original spike counts across repetitions, j, we 
get vectors (ğ‘£)#% , ğ‘£)#& ). We next calculate the correlation 
between ğ‘£)#%  and ğ‘£)#&  to get a measure, rsignal(i), of the 
stimulus preferences of the two neurons. Because of limited 
data, neuroscientists often compute, rsignal~, a â€œpseudo-
correlationâ€ over M stimuli. Finally, neuroscientists may 
analyze transformations of unsmoothed and smoothed 
responses with correlations rnoise(i)â€™ and rsignal~â€™. We 
analyze statistics for our models of these four cases:  1.  (ğœ†+ #$%,  
ğœ†"#$&) (rnoise(i)); 2.  (ğ‘£)#%, ğ‘£)#&) (rsignal~); 3. Transformed 
responses (ğœ†+ #$%
, ,ğœ†"#$&
, ) (rnoise(i)â€™); 4. Transformed, averaged 
(ğ‘£)#%
, , ğ‘£)#&
, ) (rsignal~â€™). Cases 1-3 can be done exactly; Case 4 
needs analytical approximations. 
III. 
ANALYTICAL MODELS 
We now develop analytical models for each of the four 
above cases. 
A. Case 1: Original Data and rnoise(i) 
For each neuron, k = 1 or 2, there are response 
measurements  ğœ†"#$-=ğœ†#-.1 + ğœ€#$-2 corresponding to stimuli 
i=1,â€¦,M and replications j=1,â€¦,N. The ğœ†#- and ğœ€#$- are the 
â€œtrueâ€ random spike counts and the independent random 
measurement 
error 
factors. 
Let 
ğ¸(ğœ†#-) = ğœ†Ì…#-
 and 
ğ‘£ğ‘ğ‘Ÿ(ğœ†#-) = ğœ9:;
& . The correlation between the two neuron 
26
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-741-2
DATA ANALYTICS 2019 : The Eighth International Conference on Data Analytics

responses (signals) is ğœŒ(ğœ†#%, ğœ†#&) â‰¥ ğœŒ(ğœ†"#$%, ğœ†"#$&). We assume 
ğœ€#$-  is ğ‘(0, ğœA;
& ) . It follows that ğ¸.ğœ†"#$-2 = ğœ†Ì…#-  and 
ğ‘£ğ‘ğ‘Ÿ.ğœ†"#$-2 = ğœ9:;
& + ğœ†Ì…#-
& ğœA;
& + ğœ9:;
& ğœA;
& . If measured responses 
are Poisson, ğ‘£ğ‘ğ‘Ÿ.ğœ†"#$-) = ğ¸.ğœ†"#$-2 = (ğœ†Ì…#-2. Then, we have 
ğ‘ğ‘œğ‘£.ğœ†"#$%, ğœ†"#$&2 = ğ‘ğ‘œğ‘£(ğœ†#%, ğœ†#&) = ğœŒ(ğœ†#%, ğœ†#&)ğœ9:F
& ğœ9:G
&  and  
 
ğ‘Ÿğ‘›ğ‘œğ‘–ğ‘ ğ‘’(ğ‘–) =
LMN(9+:OF,9+:OG)
PNQR(9+:OF)NQR(9+:OG)
=
S(9:F,9:G)
T(%U9V:F
G WXF
G
WY:F
G
UZXF
G )(%U9V:G
G WXG
G
WY:G
G
UZXG
G )
.                           
                                                                                            (1)  
 
Note that only the denominator involves measurement 
uncertainty. There is one unintended impact of the product-
form analytical model for Case 1 (and Case 3, below). Given 
the MxNx2 measurements from a vision experiment, we 
might use the common variance estimator for sample 
variances: ğœ9+:O;
&
= ğ‘£ğ‘ğ‘Ÿ.ğœ†"#$-2 â‰ˆ ğœ)9+:O;
&
=
âˆ‘
(9+:O;]
âˆ‘
Y+:^;
_
^`F
_
)G
_
O`F
a]%
 
=
âˆ‘
(9+:O;]N):;)G
_
O`F
a]%
, where ğ‘£)#- is an estimate of ğ¸.ğœ†"#$-2. The 
problem is that ğ¸ bğœ)9+:O;
&
c = ğœ9+:O;
&
âˆ’ ğœ9:;
& â‰  ğœ9+:O;
&
. which is 
biased low for ğ‘£ğ‘ğ‘Ÿ.ğœ†"#$-2  when 
is the product of 2 
random variables. We will need to correct this bias in our 
simulation. Similarly, if we use the standard covariance 
estimator for ğœÌ‚(ğ‘–) â‰ˆ ğ‘ğ‘œğ‘£.ğœ†"#$%, ğœ†"#$&2 = ğ‘ğ‘œğ‘£(ğœ†#%, ğœ†#&), we get 
ğœÌ‚(ğ‘–)=
âˆ‘
(9+:OF]N):F)(9+:OG]N):G)
_
O`F
a]%
, with the result that E[ğœÌ‚(ğ‘–)]=0, 
which is also biased low and needs to be corrected in the 
simulation results.                             
B. Case 2: â€œSmoothedâ€ Data and rsignal(i), rsignal~ 
   Let ğ‘£)#- =
âˆ‘
9+:O;
_
O`F
a
= ğœ†#-(1 +
âˆ‘
A:O;
_
O`F
a
) for i=1,â€¦,M. Then, 
for all i, ğ¸(ğ‘£)#-) = ğœ†Ì…#-, ğ‘£ğ‘ğ‘Ÿ(ğ‘£)#-) = ğœ9:;
& +
9V:;
G ZX;
G
a
+
ZY:;
G
ZX;
G
a
. 
Further, ğ‘ğ‘œğ‘£(ğ‘£)#%, ğ‘£)#&) = ğ‘ğ‘œğ‘£.ğœ†"#$%, ğœ†"#$&2 = ğ‘ğ‘œğ‘£(ğœ†#%, ğœ†#&), 
and correlation ğ‘Ÿğ‘ ğ‘–ğ‘”ğ‘›ğ‘ğ‘™(ğ‘–) =
LMN(N):F,N):G)
jNQR(N):F)NQR(N):G) =
S(9:F,9:G)ZY:FZY:G
PZkl:F
G
Zkl:G
G
 =
S(9:F,9:G)
T(%U9V:F
G
WXF
G
_WY:F
G
UZXF
G )(%U9V:G
G
WXG
G
_WY:G
G
UZXG
G )
           (2)                                                                                          
Equation (2) is quite similar to (1) except that measurement 
error variances are divided by N. Then, due in part to data 
limitations, neuroscientists generally use (define) rsignal~, a 
different measure of the similarity of neuronal responses to 
various stimuli. That is,  rsignal~ 
 
 
=
LMN~(N)âˆ—F,N)âˆ—G)
jNQR~(N)âˆ—F)NQR~(N)âˆ—G) =
o âˆ‘
p (N):F]N)âˆ—F)(N):G]N)âˆ—G)
:`F
Po âˆ‘
(N):F]N)âˆ—F)Go âˆ‘
p (N):G]N)âˆ—G)G
:`F
p
:`F
       (3)                                                    
where ğ‘£)âˆ—- =
âˆ‘
N)âˆ—;
p
:`F
q
. While ğ‘ğ‘œğ‘£(ğ‘£)#%, ğ‘£)#&) = ğ‘ğ‘œğ‘£.ğœ†"#$%, ğœ†"#$&2, 
ğ‘ğ‘œğ‘£~(ğ‘£)âˆ—%, ğ‘£)âˆ—&) â‰   ğ‘ğ‘œğ‘£.ğœ†"#$%, ğœ†"#$&2 . 
We 
can 
compute 
rsignal~, in terms of more traditional moments, using 
 
ğ¸ âˆ‘
(ğ‘£)#% âˆ’ ğ‘£)âˆ—%)(ğ‘£)#& âˆ’ ğ‘£)âˆ—&)
q
#r%
=
q]%
q âˆ‘
sğ‘ğ‘œğ‘£(ğœ†#%, ğœ†#&) +
q
#r%
ğœ†Ì…#%ğœ†Ì…#&t âˆ’
%
q âˆ‘
âˆ‘
sğ‘ğ‘œğ‘£.ğœ†#%, ğœ†u&2 + ğœ†Ì…#%ğœ†Ì…u&t
q
uv#
q
#r%
,                (4)                   
 
ğ¸ âˆ‘
(ğ‘£)#- âˆ’ ğ‘£)âˆ—-)&
q
#r%
=
q]%
q âˆ‘
(ğœA;
& + ğœ†Ì…#-
& )
q
#r%
âˆ’
%
q âˆ‘
âˆ‘
sğ‘ğ‘œğ‘£.ğœ†#-, ğœ†u-2 + ğœ†Ì…#-ğœ†Ì…u-t
q
uv#
q
#r%
.                                (5) 
 
 
Assume inter-stimuli covariances in (4), (5) are negligible.  
C. Cases 3 and 4: Transformed Responses 
First, we define the normalization transformation that we 
use in this report: 
 
ğœ†"#$-
,
= ğ‘“.ğœ†#-, ğœ†#-], ğœ€#$-, ğœ€#$-]2
= max {0,
ğœ†"#$-
&
ğ‘ ğœ†"#$-
&
+ ğ‘ âˆ’ ğ‘Ÿ
ğœ†"#$-]
&
ğ‘ ğœ†"#$-]
&
+ ğ‘| 
= max b0, ğ‘”.ğœ†#-, ğœ†#-], ğœ€#$-, ğœ€#$-]2c 
 
         = max }0,
[9:;(%UA:O;)]G
â‚¬[9:;.%UA:O;2]GUL âˆ’
R[9:;â€¢(%UA:O;â€¢)]G
â‚¬[9:;â€¢.%UA:O;â€¢2]GULâ€š          (6) 
 
where s, c, r are non-negative constants and k- is the opposite 
neuron index from k. The first term in expanded (6) 
corresponds to the standard form of normalization used in 
neuroscience [7]; the terms in the denominator can be thought 
of as corresponding to pools of neurons that are correlated 
(first term) or uncorrelated (second term) with the neuron in 
the numerator. The second term in the equation allows for the 
possibility of opponent processing, a common neural 
operation that is hypothesized to influence noise correlations 
[8]. The max function captures the fact that neural firing rates 
cannot, by definition, be negative. We will see that the 
transformations in Cases 3 and 4 increase complexity because 
they: 
â€¢ 
Are highly non-linear and may need to be constrained to 
be non-negative 
â€¢ 
Add many parameters to the model 
â€¢ 
Reduce the non-negative magnitude of the (transformed) 
responses, ğœ†"#$-
,
, to near 0 
â€¢ 
Cause correlation statistics, such as rnoise(i)â€™ and 
rsignal~â€™, to become very sensitive to important system 
parameters, e.g., as ğœ†"#$-
,
 approaches 0. 
 
In our simulation of Case 3, i.e., smoothed transformed 
data and rnoise(i)â€™, we have to correct biases in the classic 
variance and covariance estimators,  
ğœ)9+:O;
Æ’&
 and ğ‘Ì‚.ğœ†"#$%
, , ğœ†"#$&
, 2, for
. 
That is, for ğ‘— â‰  ğ‘: 
 
ğ¸ }ğœ)9+:O;
Æ’&
â€š = ğ‘£ğ‘ğ‘Ÿ.ğœ†"#$-
,
2 + ğ¸&.ğœ†"#$-
,
2 âˆ’ ğ¸.ğœ†"#$-
,
ğœ†"#u-
,
2             (7) 
Ë†Î»ijk
var( Ë†Î»ijk
' ) and cov( Ë†Î»ij1
' , Ë†Î»ij2
' )
27
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-741-2
DATA ANALYTICS 2019 : The Eighth International Conference on Data Analytics

 
and  
     
ğ¸ bğ‘Ì‚.ğœ†"#$%
, , ğœ†"#$&
, 2c = ğ‘ğ‘œğ‘£.ğœ†"#$%
, , ğœ†"#$&
, 2 + ğ¸.ğœ†"#$%
, 2ğ¸.ğœ†"#$&
, 2 âˆ’
ğ¸.ğœ†"#$%
, ğœ†"#u&
,
2.                                                                          (8) 
 
        We also developed exact and T-S analytical models for 
Case 3. Our exact approach conditions on ğœ†#$- and ğœ†#$-] in 
(6), then assumes a joint distribution for them and integrates 
out to obtain expected values, variances and covariances for 
ğœ†"#$-
,
. The limits on the integrals reflect the non-negativity 
constraint. Our analytical approximations focus on degree-2 
Taylor Series (TS-2) models (extensive empirical analyses 
show that the much more complex higher degree models 
â€œoverfitâ€ the data), with a differentiable approximation of the 
non-negativity constraint for ğœ†"#$-
,
 in (6). We use Taylor 
Series models because they often provide very good 
approximations and reveal critical statistical relationships 
between input parameters and results. We have not quantified 
the very small approximation errors. Important TS-2 
moments, including correlations, are linear combination of 
input variances/ covariances, with weights being algebraic 
expressions of input parameters. Of special interest is the 
Simplified TS-2 (STS-2) version of the model that is valid 
when the non-negativity constraint is â€œnon-bindingâ€. To 
develop the TS-2 Model, we replace the non-negativity 
constraint in the f form of (6) with  
 
ğœ†"#$-
,
= ğ‘“.ğœ†#-, ğœ†#-], ğœ€#$-, ğœ€#$-]2 
 
           â‰ˆ
â€°.9:;,9:;â€¢,A:O;,A:O;â€¢2UPâ€°.9:;,9:;â€¢,A:O;,A:O;â€¢2
GUÅ 
&
            (9) 
 
where d, which is required for differentiability, is a very small 
positive number. Then, the Taylor Series fit to f, as given by  
(9), is around the â€œpointâ€ (ğœ†Ì…#-, ğœ†Ì…#-], ğœ€Ì…#$-, ğœ€Ì…#$-]). We assume 
that the lâ€™s have a bivariate Normal distribution with means 
( ğœ†Ì…#-, ğœ†Ì…#-]) , variances ( ğœ9:;
& , ğœ9:;â€¢
&
) and correlation 
ğœŒ(ğœ†#-, ğœ†#-]).  The eâ€™s are each (independently) normally 
distributed with 0 means and variances ğœA;
&  ğ‘ğ‘›ğ‘‘ ğœA;â€¢
& , 
respectively. Accuracy is usually best when variances 
(ğœ9:;
& , ğœ9:;â€¢
&
, ğœA;
& , ğœA;â€¢
& ) are small.  To get the simpler STS-2 
model results, we ignore the non-negativity constraint, i.e., 
fâ‰¡g. With limited space, we present some results. First, the 
TS-2 expression for the mean is: 
 
ğ¸.ğœ†"#$-
,
2 â‰ˆ ğ‘“ +
%
& â€¢ğ‘“A:O;A:O;ğœA;
& + ğ‘“A:O;â€¢A:O;â€¢ğœA;â€¢
&
+
ğ‘“9:;9:;ğœ9:;
& + ğ‘“9:;â€¢9:;â€¢ğœ9:;â€¢
&
Å½ + ğ‘“9:F9:Gğ‘ğ‘œğ‘£(ğœ†#%, ğœ†#&)           (10) 
 
In (10), the subscripts on f and g represent partial derivatives 
and all expressions in f and g are evaluated at the mean point 
.ğœ†Ì…#-, ğœ†Ì…#-], 0,02. Also, all of the partials, fxy, are of the form:  
ğ‘“â€¢â€¢ =
%
& [ğ‘”â€¢ğ‘”â€¢
Å 
(â€°GUÅ )F.â€˜ + }1 +
â€°
jâ€°GUÅ â€š ğ‘”â€¢â€¢]         (11)                        
 
with the following pairs for (x,y), with corresponding g 
partials (evaluated at mean): 
o 
.ğœ€#$-, ğœ€#$-2:   ğ‘”A:O; =
&L9V:;
G
[â‚¬9V:;
G UL)G, ğ‘”A:O;A:O; =
&L9V:;
G (L]â€™â‚¬9V:;
G )
[â‚¬9V:;
G UL)â€œ
    (12A) 
o 
.ğœ€#$-], ğœ€#$-]2:  ğ‘”A:O;â€¢ = 
]&RL9V:;â€¢
G
[â‚¬9V:;â€¢
G
UL)G,  ğ‘”A:O;â€¢A:O;]
]&RL9V:;â€¢
G
(L]â€™â‚¬9V:;â€¢
G
)
[â‚¬9V:;â€¢
G
UL)â€œ
                        
                                                                                                     (12B) 
o 
(ğœ†#-, ğœ†#-):  ğ‘”9:; =
&L9V:;
[â‚¬9V:;
G UL)G ,  ğ‘”9:;9:; =
&L(L]â€™â‚¬9V:;
G )
[â‚¬9V:;
G UL)â€œ   (12C)            
o 
(ğœ†#-], ğœ†#-]): ğ‘”9:;â€¢ =
]&RL9V:;â€¢
[â‚¬9V:;â€¢
G
UL)G,  ğ‘”9:;â€¢9:;â€¢ =
]&RL(L]â€™â‚¬9V:;â€¢
G
)
[â‚¬9V:;â€¢
G
UL)â€œ
         
                                                                                           (12D) 
o 
 ğ‘”9:F9:G = 0                                                                    (12E)                         
 
The TS-2 approximation for the variance, ğ‘£ğ‘ğ‘Ÿ(ğœ†"#$-
,
), is: 
 
ğ‘£ğ‘ğ‘Ÿ(ğœ†"#$-
,
) â‰ˆ ğ‘“A:O;
& ğœA;
& + ğ‘“A:O;â€¢
&
ğœA;â€¢
&
+ ğ‘“9:;
& ğœ9:;
& + ğ‘“9:;â€¢
&
ğœ9:;â€¢
&
+
2ğ‘“9:F9:Gğ‘ğ‘œğ‘£(ğœ†#%, ğœ†#&),                                                           (13) 
 
where ğ‘“9:F9:G is given in (11), the weights fx  are 
 
            ğ‘“â€¢ =
%
& ğ‘”â€¢(1 +
â€°
jâ€°GUÅ  ,                                                (14)     
                                                                  
and the various cases for the partials, gx, are in (12A)-(12E).  
The TS-2 approximation for the covariance, 
ğ‘ğ‘œğ‘£.ğœ†"#$%
, , ğœ†"#$&
, 2, is complicated by the fact that it involves two 
(transformed) random responses: ğœ†"#$%
,  and ğœ†"#$&
, : 
 
ğ‘ğ‘œğ‘£.ğœ†"#$%
, , ğœ†"#$&
, 2 â‰ˆ ğ‘“A:OFğ‘“âƒA:OFğœAF
& + ğ‘“A:OGğ‘“âƒA:OGğœAG
& + ğ‘“9:Fğ‘“âƒ9:Fğœ9:F
& +
ğ‘“9:Gğ‘“âƒ9:Gğœ9:G
& +(ğ‘“9:Fğ‘“âƒ9:G + ğ‘“9:Gğ‘“âƒ9:F) ğ‘ğ‘œğ‘£(ğœ†#%, ğœ†#&)                    (15)                       
 
where all of the partials of the form, fx, are given in (14). 
However, the term, ğ‘“âƒâ€¢, requires some explanation. Because 
of the symmetries of the graded transformation (6), ğœ†"#$-
,
=
ğ‘“.ğœ†#-, ğœ†#-], ğœ€#$-, ğœ€#$-]2  and ğœ†"#$-]
,
= ğ‘“.ğœ†#-], ğœ†#-, ğœ€#$-], ğœ€#$-2 , 
i.e., ğœ†"#$-
,
 is ğœ†"#$-]
,
 but with k and k- arguments reversed. This 
symmetry suggests that partials on the â€œreverse fâ€, call it ğ‘“âƒ, 
can be calculated as partials on ğ‘“ if we reverse k and k- 
everywhere. For example, we use 2 steps to compute ğ‘“âƒA:OF in 
the TS-2 expression (15): (1) Find the partial, ğ‘“A:OG and then 
(2) Evaluate it at the reverse (mean) argument .ğœ†Ì…#&, ğœ†Ì…#%, 0,02. 
All expressions needed for ğ‘“â€¢  are given in (14). We can 
compute the TS-2 or STS-2 versions of rnoise(i)â€™. For STS-2 
 
ğ‘Ÿğ‘›ğ‘œğ‘–ğ‘ ğ‘’(ğ‘–), â‰ˆ
]R9V:F
G ZY+:OF
G
]Râ„¢Å¡9V:G
G ZY+:OG
G
Uâ„¢G9V:F9V:G.%URG2LMN(9:F,9:G)
T{9V:F
G ZY+:OF
G
URGâ„¢Å¡9V:G
G ZY+:OG
G
|{RG9V:F
G ZY+:OF
G
Uâ„¢Å¡9V:G
G ZY+:OG
G
|
   
                                                                                          (16)             
28
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-741-2
DATA ANALYTICS 2019 : The Eighth International Conference on Data Analytics

where ğ‘… =
9V:F
G UL/â‚¬
9V:G
G UL/â‚¬ captures all the effects of s and c. Equation 
(16) simplifies at r=0 to be rnoise(i)â€™ â‰ˆ rnoise(i), i.e., the 
transform has (approximately) no effect; this applies to TS-2 
and STS-2 since the constraint is not needed. 
      For Case 4, i.e., the smoothed transformed data (and 
rsignal~â€™), exact solutions are impractical because they 
require conditioning on at least 6 partially-dependent random 
variables with integrations over a complicated region of 
feasibility. Now, let ğ‘£)#-
, =
âˆ‘
9+:O;
Æ’
_
O`F
a
 and ğ‘£)âˆ—-
, =
âˆ‘
N):;
Æ’
p
:`F
q
. Then, 
we can provide expressions for the TS-2 and STS-2 
components of rsignal~â€™. That is, the expressions for E(ğ‘£)#-
, ),  
ğ‘£ğ‘ğ‘Ÿ(ğ‘£)#-
, ) and ğ‘ğ‘œğ‘£(ğ‘£)#%
, , ğ‘£)#&
, ) are the corresponding versions of 
the untransformed responses, but in TS-2 the measurement-
error variances, ğœA;
&  and ğœA;]
&  are divided by ğ‘ and in STS-
2, ğœ 9+:O;
&
 and ğœ9+:O;â€¢
&
 are replaced by ğœN):;
&  ğ‘ğ‘›ğ‘‘ ğœN):;â€¢
&
.  If we 
used 
the 
classical 
correlation, 
ğ‘Ÿğ‘ ğ‘–ğ‘”ğ‘›ğ‘ğ‘™(ğ‘–), =
LMN.N):F
Æ’ ,N):G
Æ’ 2
PNQR.N):F
Æ’ 2NQR.N):G
Æ’ 2
, we would have all we need for TS-2 and 
STS-2. The STS-2 expression for rsignal(i)â€™ would be (16) 
with ğœ9+:O;
&
 replaced by ğ‘£)#-
,  for k=1, 2.  Moreover, for TS-2 
and STS-2, we would find that when r=0, rsignal(i)â€™Â» 
rsignal(i), i.e., that transform does not affect correlation (as 
for rnoise(i)â€™). However, the â€œpseudo-correlationâ€,  
 
ğ‘Ÿğ‘ ğ‘–ğ‘”ğ‘›ğ‘ğ‘™~, =   
o âˆ‘
(N):F
Æ’
p
:`F
]N)âˆ—F
Æ’ )(N):G
Æ’ ]N)âˆ—G
Æ’ )
Po âˆ‘
(N):F
Æ’
p
:`F
]N)âˆ—F
Æ’ )Go âˆ‘
(N):G
Æ’
p
:`F
]N)âˆ—G
Æ’ )G ,                  (17)                                                                             
 
ğ¸ âˆ‘
(ğ‘£)#-
,
q
#r%
-ğ‘£)âˆ—-
, )2=
q]%
q âˆ‘
[ğ‘£ğ‘ğ‘Ÿ(ğ‘£)#-
, ) + ğ¸
q
#r%
2(ğ‘£)#-
, )]                                                             
- 
%
 q âˆ‘
âˆ‘
[ğ‘ğ‘œğ‘£(ğ‘£)#-
, , ğ‘£)Â¥-
,
) + ğ¸(ğ‘£)#-
, )ğ¸(ğ‘£)Â¥-
,
)]
Â¥v#
q
#r%
 ,           (18)              
 
ğ¸ âˆ‘
(ğ‘£)#%
,
q
#r%
-ğ‘£)âˆ—%
, )(ğ‘£)#&
, âˆ’ ğ‘£)âˆ—&
, )=
q]%
q âˆ‘
[ğ‘ğ‘œğ‘£(ğ‘£)#%
, , ğ‘£)#&
, ) +
q
#r%
ğ¸(ğ‘£)#%
, )ğ¸(ğ‘£)#&
, )]] âˆ’
%
q âˆ‘
âˆ‘
[ğ‘ğ‘œğ‘£(ğ‘£)#%
, , ğ‘£)Â¥&
, ) +
Â¥v#
q
#r%
ğ¸(ğ‘£)#%
, )ğ¸(ğ‘£)Â¥&
, )]                                                                 (19) 
                                                                                
where we usually assume ğ‘ğ‘œğ‘£(ğ‘£)#-
, , ğ‘£)Â¥-
,
) and ğ‘ğ‘œğ‘£(ğ‘£)#%
, , ğ‘£)Â¥&
, )  
are neglibile for ğ‘– â‰  ğ‘š. 
IV. 
ANALYTICAL AND SIMULATION RESULTS 
 Before we discuss results, let us overview our simulation. 
Our simulation generates random measured responses (ğœ†"#$-) 
to various stimuli in visual neuroscience experiments. While 
are often modeled as Poisson, we assume that they are 
Bi-Normal with ğµğ‘ bğœ†Ì…#%, ğœ9+:OF
& ; ğœ†Ì…#&, ğœ9+:OG
& ; ğœŒ( ğœ†"#$%, ğœ†"#$&)c. For 
each simulation sample, we generate 2xNxM random 
responses. We consider various pairs of values for the mean 
responses ( ğœ†Ì…#%, ğœ†Ì…#& ), each on [25,50]. In assigning key 
parameter values, we prescribe response correlation to be 
ğœŒ(ğœ†#%, ğœ†#&) = 0.7 for all stimuli, i and let the coefficient of 
variation for ğœ†#-  be 5%, i.e., ğ‘£ğ‘ğ‘Ÿ(ğœ†#-) = (0.05ğ¸(ğœ†#-))& . 
Next, we derive values for the (Poisson) measurement error 
variances,  ğ‘£ğ‘ğ‘Ÿ(ğœ€#$-) . Finally, we compute the statistics 
required for Cases 1-4 and contrast them to those obtained 
analytically. To assure the statistical validity, we generate up 
to S=500 experiment â€œsamplesâ€ and we correct biases, for 
Cases 1, 3 variance and covariance estimates, as in Sections 
III-B and III-C. 
In Section IV-A, we provide only brief comments on Cases 
1 and 2, the untransformed responses, because our analytical 
models are â€œexactâ€. In Section IV-B, we look at simulation 
and T-S analytical results for the transformed responses; we 
focus on Case 3 statistics, ğ‘£ğ‘ğ‘Ÿ.ğœ†"-$%
,
2 and rnoise(6)â€™,  because 
Case 4 observations do not change significantly as we vary 
key parameters. This similarity of empirical results is quite 
consistent with the striking similarity of the approximate 
analytical results in Section III-C. 
A. Cases 1 and 2: Untransformed Responses 
For classical means and covariances, the true, measured 
and smoothed analytical results are the same. However, the 
unsmoothed and smoothed classical variances may differ 
significantly because averaging reduces the variance of 
measured responses. The key challenge for these Cases is to 
make sure the simulation yields accurate estimates of the 
associated transformed results for Cases 3 and 4. In that 
regard, we note the following: 
â€¢ 
The simulation biases for variances and covariances, are 
corrected as in Section III-B. 
â€¢ 
â€œPseudo variances and covariancesâ€ seem to differ only 
slightly from the classical ones. 
â€¢ 
Correlation statistics, rnoise(i) and rsignal~, are difficult 
to estimate, may be unstable, using simulation (or 
experimental data) because they are ratios of other 
statistics. 
 
B. Cases 3 and 4: Transformed Responses 
Using our simulation, we studied extensively the 
accuracy of TS-2 and STS-2 for the moments of transformed 
response statistics. The transformed cases are difficult to 
analyze:  
â€¢ 
The transformation, as in (6), introduces many additional 
parameters. 
â€¢ 
Transformations are nonlinear, further complicated by 
the non-negativity constraint.  
â€¢ 
Transformed responses are quite small, on the order of 
10-3 to 10-4 for Case 4, and approaching 0 for parameter 
extremes, e.g., large values for s, c, or r.  
In addition, variances and covariances are much more 
challenging than means to estimate, at parameter extremes, 
and correlations depend strongly on variance and covariance 
accuracy. We also know that STS-2 will (TS-2 may not) have 
accuracy problems where the non-negativity constraints are 
needed. We will see that, despite these difficulties, TS-2 and 
STS-2 are accurate and fast over important parameter ranges. 
Ë†Î»ijk
29
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-741-2
DATA ANALYTICS 2019 : The Eighth International Conference on Data Analytics

Neuroscientists restrict r to be on [0,1], and often in the 
â€œneighborhoodâ€ of 0.5. However, in our analyses, we extend 
the domain to [0,1.5] to better understand the properties of 
the transforms and our models. We use the ratio c/s in our 
examples because, in our studies, we found that individual 
values of s and c matter only at extremes (e.g., s=0, c=0, or 
r>> 0.5). For example, the STS-2 expression for rnoise(i)â€™ 
(16) only depends on c/s. We assume arbitrarily i=6, k=1.  
 
      1) Accuracy of Analytical Models for Variances. In 
Figure 1, below, we look at the accuracy of TS-2 and STS-2 
(vs. simulation) of ğ‘£ğ‘ğ‘Ÿ.ğœ†"-$%
,
2 as we vary r, c/s and (ğœ†Ì…-%, ğœ†Ì…-&). 
The rows of graphs correspond to different values of c/s and 
columns to ratios 
9VÂ®G
9VÂ®F  .  For each graph, the y-axis is 
ğ‘£ğ‘ğ‘Ÿ.ğœ†"-$%
,
2 and the x-axis is 0 Â£ r Â£ 1.5. Some observations: 
â€¢ 
For these data, the analytical and simulation results 
match well except for STS-2 when r>>0.5 or 
9VÂ®G
9VÂ®F >>  1. 
The analytical model â€œerrorsâ€ are usually small in 
absolute value. 
â€¢ 
The  ğ‘£ğ‘ğ‘Ÿ.ğœ†"-$%
,
2  graph is a unimodal non-negative 
function of c/s, starting at 0 when c/s=0, becoming 
positive and then approaching 0 again for large c/s. 
While not shown here, all three models do a good job of 
exhibiting this property.  
â€¢ 
Because of the non-negativity constraint in (6), 
ğ‘£ğ‘ğ‘Ÿ.ğœ†"-$%
,
2Ã 0 for large r. We see that TS-2 generally 
provides accurate estimates for large r, while STS-2 does 
not. However, neuroscientists see little practical 
application in their models for r>>0.5. 
â€¢ 
TS-2 estimates of ğ‘£ğ‘ğ‘Ÿ.ğœ†"-$%
,
2 are somewhat inaccurate 
when  
9VÂ®G
9VÂ®F >> 1  because the non-negativity constraint is 
needed. It would be much less important to ğ‘£ğ‘ğ‘Ÿ.ğœ†"-$%
,
2 if  
9VÂ®F
9VÂ®G >>  1.  
 
2) Accuracy and Stability of Correlation Statistics. 
Figure 2 (see the y-axes), below, helps us gain insights into 
the accuracy and stability of our rnoise(i)â€™ models. Some 
observations: 
â€¢ 
The analytical models are accurate (match the simulation 
well) if r Â£  0.75;  
9VÂ®G
9VÂ®F â‰¤
Â±
â€™ , i.e., â‰ˆ 1 due to symmetries.  
â€¢ 
The trajectory of rnoise(6)â€™ vs. r is inherently volatile but 
necessarily within [-1,1]. It starts near 0 for r=0, 
decreases sharply as r increases and, when the non-
negativity constraint â€œkicks inâ€, turns up towards 0 
again. TS-2 tracks the upturn pretty well, but (of course) 
STS-2 does not. 
â€¢ 
The TS-2 model shows some apparent numerical 
instability, relative to the simulation, for large r. This is 
because, as the numerator ğ‘ğ‘œğ‘£.ğœ†"-$%
,
, ğœ†"-$&
,
 2, and 
denominator, ğ‘£ğ‘ğ‘Ÿ.ğœ†"-$%
,
2ğ‘£ğ‘ğ‘Ÿ.ğœ†"-$&
,
2, each approach 0, 
estimates of rnoise (6)â€™ approach 0/0 and are sensitive 
to the rates of convergence of the ratio components.  
â€¢ 
Recall (Section III-C), that at r=0, rnoise(i)â€™ Â» rnoise(i), 
i.e., the transform is insensitive to c/s and 
9VÂ®G
9VÂ®F. This c/s 
insensitivity for rnoise(6)â€™ extends to r Â£  0.75, 
9VÂ®G
9VÂ®F â‰ˆ 1. 
V. 
SUMMARY AND CONCLUSIONS 
We conclude, based on our example analytical and 
simulation results, that TS-2 and STS-2 are accurate for a 
wide range of parameter values. Because the STS-2 models 
omit non-negativity constraints, their accuracy is best when 
rÂ£ 0.75; 
9V:G
9V:F  Â£  
Â±
â€™  for ğ‘£ğ‘ğ‘Ÿ.ğœ†"#$%
, 2and 
9V:G
9V:F  â‰ˆ 1 for correlation, 
rnoise(i)â€™. In addition, their simple expressions help us better 
understand relationships between model parameters and 
results. Our approximations are extremely fast, linear 
combinations of input parameters, with algebraic expressions 
for the weights. Two key conclusions for neural coding are: 
â€¢ 
Transformations, such as normalization, can influence 
noise correlations, but the relationship between these 
two factors, rnoise(i) and rnoise(i)â€™ (or between rsignalâ€™ 
and rsignal), is highly sensitive to parameter choices, 
and hence unlikely to be robust in real neural networks. 
â€¢ 
Opponent processing, i.e., the combining of response 
data from multiple neurons, has a profound influence on 
noise correlations. 
REFERENCES 
[1] E. Zohary, M. Shadler, and W. Newsome, â€œCorrelated 
Neuronal 
Discharge 
Rate 
amd 
Its 
Implication 
for 
Pshycophysical Performance,â€ vol. 370, pp. 140-143, 1994. 
[2] B. Tripp, â€œDecorrelation of Spiking Variability and Improved 
Information 
Transfer 
Through 
Feedforward 
Divisive 
Noralization,â€ Neural Computation, vol. 24, No. 4, pp. 867-
894, April 2012. 
[3] D. Ruff, J. Alberts, and M. Cohen, â€œRelating Normalization to 
Neuronal 
Populations 
Across 
Cortical 
Areas,â€ 
Neurophysiology, vol. 116, Issue 3, pp. 1375-1386, September 
2016. 
[4] L. Liu, R. Haefner, and C. Pack, â€œA â€œNeural Basis for the 
Spatial Suppression  of Visual Motion Perception,â€ eLife 2016; 
5:e16167 doi: 10.7554/eLife.16167, May 2016. 
[5] D. Lyamzin, J. Macke, and N. Lesica, â€œModeling Population 
Spike Trains with Specified Time-Varying Spike Rates, Trial-
to-Trial Variability, Pairwise Signal and Noise Correlations,â€ 
Front. Comp. Neuro., 4:144, doi:10.3389/fncom.2010.00144, 
pmid:21152346, November 2010. 
[6] Y. Hu, J. Zylerberg, and E.Shea-Brown, â€œThe Sign Rule and 
Beyond: Boundary Effects, Flexibility, and Noise Correlations 
in Neural Population Codes,â€ PLoS Computational Biology, 
vol. 10, e1003469. doi:10.1371/journal.pcbi.1003469, 2014. 
[7] D. Heeger, â€œNormalization of Cell Respomses in Cat Striate 
Cortex,â€ Visual Neuroscience, vol. 9, Issue 2, pp. 181-197, 
August 1992.  
[8] Y. Chen Y, W. Geisler, and E. Seidemann, â€œOptimal Decoding 
of Correlated Neural Population Responses in the Primate 
Visual Cortex,â€ Nat Neurosci., vol. 9, pp. 1412â€“1420, 2006.
30
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-741-2
DATA ANALYTICS 2019 : The Eighth International Conference on Data Analytics

 
 
Figure 1. Transformed Variances, var(ğœ†"-$%
,
), vs. r 
 
 
 
 
Figure 2. Correlation Statistics, rnoise(6)â€™, vs. r 
0
0.3
0.6
0.9
1.2
1.5
0
1.2 10
- 3
Â´
2.4 10
- 3
Â´
3.6 10
- 3
Â´
4.8 10
- 3
Â´
6 10
- 3
Â´
.006
2.5 10
- 9
Â´
vV7 rR
(
)
vV8 rR
(
)
vV9 rR
(
)
1.5
0
rR
0
0.3
0.6
0.9
1.2
1.5
0
3 10
- 3
Â´
6 10
- 3
Â´
9 10
- 3
Â´
0.012
0.015
.015
2.5 10
- 9
Â´
vV4 rR
(
)
vV5 rR
(
)
vV6 rR
(
)
1.5
0
rR
0
0.3
0.6
0.9
1.2
1.5
0
8 10
- 4
Â´
1.6 10
- 3
Â´
2.4 10
- 3
Â´
3.2 10
- 3
Â´
4 10
- 3
Â´
.004
0
vV1 rR
(
)
vV2 rR
(
)
vV3 rR
(
)
1.5
0
rR
0
0.3
0.6
0.9
1.2
1.5
0
1.2 10
- 3
Â´
2.4 10
- 3
Â´
3.6 10
- 3
Â´
4.8 10
- 3
Â´
6 10
- 3
Â´
.006
2.5 10
- 9
Â´
vV7 rR
(
)
vV8 rR
(
)
vV9 rR
(
)
1.5
0
rR
0
0.3
0.6
0.9
1.2
1.5
0
3 10
- 3
Â´
6 10
- 3
Â´
9 10
- 3
Â´
0.012
0.015
.015
2.5 10
- 9
Â´
vV4 rR
(
)
vV5 rR
(
)
vV6 rR
(
)
1.5
0
rR
0
0.3
0.6
0.9
1.2
1.5
0
8 10
- 4
Â´
1.6 10
- 3
Â´
2.4 10
- 3
Â´
3.2 10
- 3
Â´
4 10
- 3
Â´
.004
0
vV1 rR
(
)
vV2 rR
(
)
vV3 rR
(
)
1.5
0
rR
0
0.3
0.6
0.9
1.2
1.5
0
1.2 10
- 3
Â´
2.4 10
- 3
Â´
3.6 10
- 3
Â´
4.8 10
- 3
Â´
6 10
- 3
Â´
.006
2.5 10
- 9
Â´
vV7 rR
(
)
vV8 rR
(
)
vV9 rR
(
)
1.5
0
rR
0
0.3
0.6
0.9
1.2
1.5
0
3 10
- 3
Â´
6 10
- 3
Â´
9 10
- 3
Â´
0.012
0.015
.015
0
vV4 rR
(
)
vV5 rR
(
)
vV6 rR
(
)
1.5
0
rR
0
0.3
0.6
0.9
1.2
1.5
0
8 10
- 4
Â´
1.6 10
- 3
Â´
2.4 10
- 3
Â´
3.2 10
- 3
Â´
4 10
- 3
Â´
.004
0
vV1 rR
(
)
vV2 rR
(
)
vV3 rR
(
)
1.5
0
rR
!"#,%
!"#,& increases (1, 4/3, 2)
c/s
Increases
(100, 750,
5000)
Simulation 
TS-2 
STS-2 
.004
.0024
0
.015
.009
0
.006
.0036
0
0           .75         1.5
r
0          .75         1.5
r
0          .75          1.5
r
0
0.5
1
1.5
1
-
0.5
-
0
0.062
1
-
rN7 rR
(
)
rN8 rR
(
)
rN9 rR
(
)
0
1.5
0
rR
0
0.5
1
1.5
1
-
0.5
-
0
0.062
1
-
rN4 rR
(
)
rN5 rR
(
)
rN6 rR
(
)
0
1.5
0
rR
0
0.5
1
1.5
1
-
0.5
-
0
0.062
1
-
rN1 rR
(
)
rN2 rR
(
)
rN3 rR
(
)
0
1.5
0
rR
0
0.5
1
1.5
1
-
0.5
-
0
0.062
1
-
rN7 rR
(
)
rN8 rR
(
)
rN9 rR
(
)
0
1.5
0
rR
0
0.5
1
1.5
1
-
0.5
-
0
0.062
1
-
rN4 rR
(
)
rN5 rR
(
)
rN6 rR
(
)
0
1.5
0
rR
0
0.5
1
1.5
1
-
0.5
-
0
0.062
1
-
rN1 rR
(
)
rN2 rR
(
)
rN3 rR
(
)
0
1.5
0
rR
0
0.5
1
1.5
1
-
0.5
-
0
0.062
1
-
rN7 rR
(
)
rN8 rR
(
)
rN9 rR
(
)
0
1.5
0
rR
0
0.5
1
1.5
1
-
0.5
-
0
0.062
1
-
rN4 rR
(
)
rN5 rR
(
)
rN6 rR
(
)
0
1.5
0
rR
0
0.5
1
1.5
1
-
0.5
-
0
0.062
1
-
rN1 rR
(
)
rN2 rR
(
)
rN3 rR
(
)
0
1.5
0
rR
.05
0
-.5
-1
Simulation 
TS-2 
STS-2 
c/s
Increases
(100, 750,
5000)
!"#,%
!"#,& increases (1, 4/3, 2)
.05
0
-.5
-1
.05
0
-.5
-1
0           .75         1.5
r
0           .75         1.5
r
0           .75         1.5
r
31
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-741-2
DATA ANALYTICS 2019 : The Eighth International Conference on Data Analytics

