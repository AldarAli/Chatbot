What Are You Doing? Real-Time Activity Recognition using Mobile Phone Sensors
Bernhard Hiesl, Marc Kurz, Erik Sonnleitner
University of Applied Sciences Upper Austria
Faculty for Informatics, Communications and Media
Department of Mobility and Energy
4232 Hagenberg, Austria
email: {firstname.lastname}@fh-hagenberg.at
Abstract—This paper focuses on recognizing different activities
of people by utilizing the smart-phone as sensor delivering unit.
For the sake of simplicity, ﬁve different activities (i.e., modes of
locomotion) are considered: (i) standing, (ii) walking, (iii) running,
(iv) walking upstairs and (v) downstairs. The research challenge
is the fact that the phone is placed on the body of the subjects
dynamically and orientation independent – thus the system has to
adapt autonomously to these characteristics. The sensor data was
collected from the built-in accelerometer, gravity and gyroscope
sensors of a common smartphone (i.e., Google Pixel XL). The data
collection procedure is part of the dynamic placement of this
paper addressing position and orientation independent record-
ing and recognizing of activities. Additionally, to acquire full
orientation independence, data transformation (horizontal and
vertical movement) is applied on the gathered data. Within the
framework of this study, ﬁve different phone positions are taken
into account and are therefore considered in the classiﬁcation
process. To achieve the best approach concerning performance
and recognition, different classiﬁers are evaluated (i.e., (i) k-
nearest neighbours (KNN), (ii) Naive Bayes, (iii) Decision Trees
and (iv) Random Forest).
Keywords–Activity recognition; Mobile sensing; Self-adaptation;
Adaptive application; Adaptive real-time strategies.
I.
INTRODUCTION
This paper is a follow-up of the work-in-progress paper
published in 2019 [1]. There, the idea of utilizing smart phones
for activity recognition (i.e., gait recognition) in real-time with
varying positions of the smart phone has been introduced.
Conceptual ideas have been presented – subsequently, this
paper focuses on the solution to the problem and the evaluation
and results.
The problem statement can be summarized as follows. The
major challenge behind this work is the correct detection of
people’s activities regardless of the mobile phone’s position
and orientation. In other words, the system should be able
to recognize the actual activity of the user by gathering
sensor data from the built-in mobile phone sensors only. This
should be performed independently of the device’s position
and orientation - thus being adaptive to contextual details.
The changing position and orientation of the mobile phone
is also referred to as ”dynamic placement” within this paper.
To actively detect the activity of a user, an application using a
real-time system is required. Therefore, sensor data from the
built-in mobile phone sensors need to be collected, processed
and classiﬁed in real-time. To put it in other words, only
a few seconds should be sufﬁcient to detect the activity
performed by the user. In detail, the research question can be
formulated as follows [1]: How can highly accurate real-time
activity recognition be realized utilizing a dynamically (i.e.,
rotation, position and orientation independent) on-body placed
commercial smartphone? For the sake of simplicity, ﬁve modes
of locomotion are speciﬁcally considered: (i) standing, (ii)
walking, (iii) running, (iv) walking upstairs and (v) downstairs.
Generally – as stated in the previous WiP paper [1] – the
idea of recognizing activities utilizing commercial smartphones
is not new and has been subject to research in numerous
publications [2]–[10]. The challenging aspects are the facts that
the recognition should be executed at real-time and rotation-,
orientation- and position-independent –not forcing the user to
conduct an initial calibration.
The WiP paper [1] presented the data collection progress
and the accompanying smartphone application allowing for
instant labelling of activities upon recording. Additionally,
feature extraction methodologies and machine learning models
have been evaluated. As related work shows, low level features
(e.g., mean, max, min, std deviation, variance, energy, entropy,
etc.) are signiﬁcant enough for activity classiﬁcation [3][4][10].
To classify the activities correctly and user independently, the
system was trained with the help of 15 subjects (10 males and
5 females) and a total sensor data amount of 4 hours and 50
minutes per sensor unit. The focus of this paper is the technical
implementation of the autonomous adaptation to the current
phone position for the recognition task, as well as the aspect
that the recognition should be executed instantly at real-time.
The rest of the paper is structured as follows. Section II
(Methodology & Implementation) explains the methodological
approach to realize the dynamic and orientation independent
placement of the phone on the subjects body. Section III (Re-
sults) summarizes the results achieved during the evaluation.
Section IV (Summary & Outlook) closes with a summary and
an outlook to future work.
II.
METHODOLOGY & IMPLEMENTATION
In order to implement the mobile sensing real-time dy-
namic gait recognition approach, the following steps have to
be done: (i) Data Acquisition, (ii) Data Preprocessing, (iii)
Feature Extraction, (iv) Model Training. Within the process
of the Data Acquisition, subjects are asked to collect sensor
data from the built-in mobile phone sensors including the
accelerometer, gyroscope and gravity sensor. In the next step,
the Data Preprocessing, the gathered data is transformed using
a horizontal and vertical movement calculation which is done
by utilizing the accelerometer and gravity sensor (orientation
1
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-781-8
ADAPTIVE 2020 : The Twelfth International Conference on Adaptive and Self-Adaptive Systems and Applications

and position independent = dynamic placement). Additionally,
the transferred data is ”smoothed” by applying a Savitzky-
Golay ﬁlter and is then split into smaller data segments using
a sliding window approach [11] with an overlap of 50%.
The Feature Extraction routine is then used to extract time
and frequency domain features. In the last step, a machine
learning approach is realized. In total, four different machine
learning models are trained ofﬂine. These machine learning
models are then integrated into the smartphone application for
realizing the recognition task. In order to provide a dynamic
real-time recognition system using a mobile phone the three
steps including (i) Data Acquisition, (ii) Data Preprocessing
and (iii) Feature extraction need to be developed within the
smartphone application (see WiP paper for further details [1]).
A. Real-Time
Burns and Welling [12] describe the term real-time as the
actual response time that a system takes to generate an output
from several input values. In other words, it is the actual time
it takes to get a result of a system after applying an input.
In general, real-time systems can be distinguished between
hard and soft real-time systems. In hard real-time systems, a
delay of a response can lead to an entire system failure. In soft
real-time systems, a delay only degrades the performance of
the system. Depending on the place of use, real-time systems
can have response times from milliseconds to minutes. By
utilizing a technique called sliding window [11] (see Figure
1), the approach in this paper targets a soft real-time activity
recognition system with a response time of 2.56 seconds
for the ﬁrst recognition and 1.28 seconds for the following
ones. Subsequently, each recognition iteration of the developed
real-time approach includes the following steps: (i) gathering
2.56 seconds (time) activity data of a user utilizing mobile
phone sensors, (ii) passing the data to the recognition process
(input), (iii) performing different calculations on the sensor
data and provide the actual gait of the user while performing it
(output). Figure 2 shows an example of horizontal and vertical
movement within one single window.
Figure 1. Illustration of the sliding window approach.
Figure 2. Time series data of a single window showing horizontal and
vertical movement.
B. Dynamic Placement
In order to realize the dynamic placement recognition
system (position and orientation independent system), the
following steps were implemented:
•
Sensor recording of activities within several body
positions to get position independent data
•
Sensor data transformation (horizontal and vertical
transformation) to become orientation independent
•
Usage of entire recorded and transformed data to train
different machine learning models
To be more precise, the ﬁrst step to achieve a dynamic
recognition system was to record sensor data in different
pocket locations on the body. In total, ﬁve pocket positions
are covered in the current version of the system and it was
done during the Data Acquisition process (see Figure 3).
Within the second step, the sensor data was transformed into a
different coordinate system representing the data in horizontal
and vertical accelerations. By applying this transformation,
the data becomes independent regarding the orientation of the
sensing device. The actual transformation is performed using
data from the accelerometer and gravity sensor.
After the second step the preprocessed data (transformed,
smoothed and segmented data) from different pocked positions
was then used to train four different machine learning models.
Therefore, the models are ”generalized” concerning pocket
positions, because each pocket position is within the ”trained”
machine learning model. In other words, the models ”trained”
within the training process are all using the same data and
therefore do not distinguish between the front right or back
left trouser pocket to detect the gait type of a user.
III.
RESULTS
Three different testing scenarios were applied to the de-
veloped system including environment changes, varying per-
forming speeds and changing positions. Each test scenario is
evaluated using gallery dependent (subject within the training
set) and independent (subject not inside the training set) sensor
data. The goal of the evaluation was to prove that the system
is able to classify the activity correctly even despite changing
performing parameters. These changing parameters consider
a position change of the mobile phone as well as changing
speed.
2
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-781-8
ADAPTIVE 2020 : The Twelfth International Conference on Adaptive and Self-Adaptive Systems and Applications

A. Scenarios
1) Environment:
The ﬁrst testing scenario covers the
changing of the environment. It is assumed, that the envi-
ronment during the recognition process can change during or
for each execution. In other words, this means that the ideal
outcome was to create an environmental independent recog-
nition system. Therefore, the developed recognition system
was evaluated using different environments including, different
stairs, different ﬂoor surfaces and indoor (house) or outdoor
environments. The main reason why there is a distinction
between an indoor and outdoor environment is that users
usually put off their shoes indoors.
The recorded data was only collected from subjects moving
forward and straight on a ﬂat surface. The environment testing
routine was also performed moving forward and straight only.
This environment evaluation is marked as S1 in the system
results Section III-C. This environment test was only per-
formed while the phone was in the front right trouser pocket (i)
walking on street / grass / indoor, (ii) running on street / grass,
(iii) ascending stairs indoor / outdoor, and (iv) descending
stairs indoor / outdoor.
2) Speed: Within the second testing scenario covering
speed, varying execution speeds are performed and evaluated
on the activity recognition system. However, the freedom of
the actual execution speed within this evaluation is limited in
terms of direction. In other words, the subjects participating
in the evaluation of the system are only able to change the
speed, not altering the motion process of walking itself in an
abnormal or directional way. This is because the data within
the training set only provides motion data in that recognizes
forward direction limiting the evaluation itself in terms of
direction. To be more precise, the speed test investigates the
correct classiﬁcation if there are changes regarding the walking
speed. This is possible, because subjects within the training
set have already personal speed interpretations performing the
activities. For example, participant P1 walks with a speed
of 3 km/h, whereas participant P2 walks with a speed of
3.6 km/h. Both participants perform the gait ”walking”, but at
individual pace. Therefore, the system should still recognize
the gait correctly with varying speeds. Another example, could
be running or climbing stairs where each participant executes
the activity at a different pace. Within the result overview
Section III-C, this execution evaluation scenario is marked as
S2. The speed adjustment evaluation consists of the following
test cases where the phone was placed in the front right trouser
pocket: (i) walking slow / normal / fast, (ii) running slow /
normal / fast, (iii) ascending stairs slow / normal / fast, and
(iv) descending stairs slow / normal / fast.
3) Position: In order to fulﬁl the main focus of this paper
– the dynamic placement of the mobile phone – the third
evaluation scenario addresses the position evaluation. It is used
to evaluate the correct recognition performance of the system
regardless of the device’s orientation. Nevertheless, there are
limitations concerning the mobile phone placement during the
evaluation. The current version of the system only supports
gait recognition within ﬁve mobile phone positions including
the two front trouser pockets, the two back trouser pockets
and the chest pocket of a shirt or the inner jacket pocket (see
Figure 3). In Section III-C the position evaluation scenario is
marked as S3.
Figure 3. Relevant pocket positions within the position evaluation.
B. Participants
Within the entire evaluation process, two subjects were
asked to participate in the process of testing the system. This
was one of the most important parts of the evaluation, because
the system should either work with participants which are
inside the training set (i.e., gallery dependent) or participants
not enrolled within the training process (i.e., gallery indepen-
dent). In other words, gallery dependent participants are testing
subjects, which are known by the machine learning model. In
contrast, participants which are gallery independent are test
subjects which are not known by the trained machine learning
model beforehand. The gallery dependent tests are annotated as
S1D (environment), S2D (speed) and S3D (position) depending
on the test scenario. Accordingly, the tests for the gallery in-
dependent cases are marked with S1I (different environments),
S2I (different execution speeds) and S3I (the varying position
evaluation).
C. Result Overview
The system is evaluated applying two cases: (i) ofﬂine (by
using newly recorded data and performing recognition ofﬂine)
and (ii) online (at real-time) using sensor data processed in
real-time on the smart-phone. Therefore, the results for the
ofﬂine and online cases are presented in the following two
Sections III-C1 and III-C2.
1) Ofﬂine Evaluation: The ofﬂine evaluation process uses
a computer to recognizing the actual gait of newly provided
sensor data from a subject performing different activities. It
is done by creating sensor data which is not covered by
the training data applied during the machine learning model
creation. Therefore, a subject gallery dependent and a subject
gallery independent were asked to record new sensor data with
changing environments, speeds and mobile phone positions. In
the next step, the data is transferred to a computer to evaluate
the different machine learning models. The following Tables
I to III summarize the accuracies for the cases S1D, S2D and
S3D (gallery dependent case).
TABLE I. RECOGNITION ACCURACIES FOR S1D (ENVIRONMENT).
Model
Grass
Street
Outdoor
Indoor
Combined
KNN
91.6%
100%
100%
93.75%
96.1%
Naive Bayes
97.2%
100%
88.8%
93.75%
94.8%
Decision Tree
94.4%
100%
100%
91.6%
96.1%
Random Forest
94.4%
100%
100%
95.8%
97.4%
3
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-781-8
ADAPTIVE 2020 : The Twelfth International Conference on Adaptive and Self-Adaptive Systems and Applications

TABLE II. RECOGNITION ACCURACIES FOR S2D (SPEED).
Model
Slow
Normal
Fast
Combined
KNN
80.0%
98.3%
91.6%
90.0%
Naive Bayes
83.3%
100%
55.0%
79.4%
Decision Tree
88.3%
91.6%
75.0%
85.0%
Random Forest
96.6%
100%
86.6%
94.4%
The abbreviations used in Table III have the following
meaning: (i) FC = front chest, (ii) FR = front right trouser
pocket, (iii) FL = front left trouser pocket, (iv) BR = back
right trouser pocket and (v) BL = back left trouser pocket (see
also Figure 3).
TABLE III. RECOGNITION ACCURACIES FOR S3D (POSITION).
Model
FC
FR
FL
BR
BL
Combined
KNN
86.6%
100%
100%
100%
100%
97.6%
Naive Bayes
80.0%
98.3%
100%
96.6%
100%
95.0%
Decision Tree
88.3%
88.3%
100%
90.0%
95.0%
92.3%
Random Forest
86.6%
100%
100%
93.3%
100%
96.0%
The gallery independent case (the system is evaluated with
data of a person that is not included in the training set) is
more meaningful. It proves that the system is also capable
of recognizing activities from any person at an acceptable
accuracy. The following Tables IV to VI summarize the
accuracies for the cases S1I, S2I and S3I (gallery independent
case).
TABLE IV. RECOGNITION ACCURACIES FOR S1I (ENVIRONMENT).
Model
Grass
Street
Outdoor
Indoor
Combined
KNN
83.3%
100%
100%
100%
97.4%
Naive Bayes
100%
100%
86.1%
81.2%
91.0%
Decision Tree
77.7%
94.4%
91.6%
91.6%
89.1%
Random Forest
94.4%
100%
100%
100%
98.7%
TABLE V. RECOGNITION ACCURACIES FOR S2I (SPEED).
Model
Slow
Normal
Fast
Combined
KNN
93.3%
100%
96.6%
96.6%
Naive Bayes
81.6%
91.6%
61.6%
78.3%
Decision Tree
75.0%
91.6%
100%
88.8%
Random Forest
96.6%
100%
91.6%
96.1%
The following Figure 4 shows the results of the gallery
dependent test scenarios whereas Figure 5 displays gallery
independent results of each test scenario. As shown in Fig-
ure 4, the Random Forest algorithm performs best regarding
environment changes (97.4%) and speed adjustments (94.4%),
whereas the KNN algorithm achieves the highest accuracy of
97.6% in the last position evaluation. Figure 5 displays the
recognition accuracy rates of a subject which is not considered
in the training set. Even though the activity data provided by
the subject was not used to train the model, the accuracy
rates are very similar to the accuracy rates shown in the
gallery dependent evaluation test. The highest precision of
98.7% was achieved by applying the environment evaluation
on the Random Forest classiﬁer followed by the KNN model
evaluating the speed adjustment by reaching an accuracy of
96.6%. In the last evaluation scenario (position), the KNN and
Random Forest algorithm achieved a recognition rate of 94%.
TABLE VI. RECOGNITION ACCURACIES FOR S3I (POSITION).
Model
FC
FR
FL
BR
BL
Combined
KNN
91.6%
100%
96.6%
83.3%
98.3%
94.0%
Naive Bayes
71.6%
91.6%
88.3%
83.3%
90.0%
85.0%
Decision Tree
71.0%
91.6%
93.3%
93.3%
91.6%
88.0%
Random Forest
71.6%
100%
98.3%
100%
100%
94.0%
Figure 4. Recognition accuracies of each machine learning model (K-nearest
Neighbour, Naive Bayes, Decision Tree and Random Forest) applying the
ofﬂine gallery dependent scenarios evaluation including environment, speed
and position changes.
2) Online Evaluation: In contrast to the ofﬂine evaluation,
a real-time/online evaluation was carried out within this paper.
The real-time evaluation is supported by the developed smart
phone application. Instead of labelling and transferring sensor
data to a computer, the application collects sensor data of
the subject performing an activity, preprocesses it, extracts
time and frequency domain features and classiﬁes the actual
activity/gait in real-time utilizing machine learning models
which were integrated into the application beforehand. In
order to decrease the effort of the evaluation process including
testing different machine learning models, the application was
extended to provide a ”ground truth”. In other words, the re-
sulting feature vector provided by the smart phone application
was extended with the actual gait type the subject was perform-
ing. The feature vector containing the ground truth was then
used to evaluate the machine learning models on the computer.
However, all evaluation scenarios were performed by using
the machine learning models on the mobile device itself. The
following Tables VII to IX summarize the accuracies for the
cases S1D, S2D and S3D (gallery dependent case), whereas
For the real-time (online) environment evaluation a gallery
dependent subject performed the exact same testing routine,
which was executed during the ofﬂine environment evaluation.
TABLE VII. RECOGNITION ACCURACIES BASED ON
ENVIRONMENT CHANGES (S1D) OF A PERSON INSIDE THE
TRAINING DATA FOR THE ONLINE CASE.
Model
Grass
Street
Outdoor
Indoor
Combined
KNN
97.2%
100%
100%
95.8%
98.0%
Naive Bayes
100%
100%
83.3%
83.3%
91.0%
Decision Tree
100%
97.2%
100%
89.5%
96.1%
Random Forest
100%
100%
100%
100%
100%
The next evaluation step were the gallery independent
evaluation scenarios, which means that a subject, whose data
was not taken into account within the training, provided
the real-time sensor data for the online evaluation of the
4
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-781-8
ADAPTIVE 2020 : The Twelfth International Conference on Adaptive and Self-Adaptive Systems and Applications

Figure 5. Recognition accuracies of each machine learning model (K-nearest
Neighbour, Naive Bayes, Decision Tree and Random Forest) applying the
ofﬂine gallery independent scenarios evaluation including environment,
speed and position changes.
TABLE VIII. RECOGNITION ACCURACIES BASED ON SPEED
CHANGES (S2D) OF A PERSON INSIDE THE TRAINING DATA FOR
THE ONLINE CASE.
Model
Slow
Normal
Fast
Combined
KNN
96.6%
100%
73.3%
89.4%
Naive Bayes
80.0%
90.0%
48.3%
72.7%
Decision Tree
80.0%
98.3%
68.3%
82.2%
Random Forest
96.6%
100%
83.3%
93.3%
recognition system. The smartphone application was used to
record, preprocess and classify sensor data in real-time. The
following Tables X to XII summarize the accuracies for the
cases S1I, S2I and S3I (gallery independent case) for the online
scenario.
Figure 6 shows the accuracies of the different gallery
dependent evaluation scenarios including environment (S1D),
speed (S2D) and position (S3D) for each machine learning
model whereas Figure 7 shows the accuracies of the gallery
independent evaluation scenarios (environment, speed and po-
sition). As shown in Figure 6, the Random Forest algorithm
achieved an accuracy of 100% using gallery dependent sensor
data on the evaluation test scenario, whereas utilizing activity
motion data from a subject, whose data is not in the training
set, the accuracy reaches 97.4% (Figure 7). Due to fact of
high accuracies in both evaluations (gallery dependent and
independent), it can be said that the real-time recognition
system is independent of environment changes and users.
The second evaluation scenario is the the speed adjustment
evaluation (S2D and S2I). In other words, the correct detection
of the gait regarding pace changes utilizing different machine
learning models. As shown in the bar charts of Figure 6 and
Figure 7 the overall accuracies decrease. This is because slower
and faster motions can lead to incorrect classiﬁcations. For
example, if a user is walking faster than usual the classiﬁcation
process can lead to incorrect results. Nevertheless, the highest
accuracy of 97.2% concerning execution speed changes was
achieved by the Random Forest algorithm during the real-
time speed gallery independent evaluation. The last position
evaluation scenario that was compared is shown as well (S3D
and S3I). Even though the position of the phone changes
between the front chest, front right, front left, back right and
back left pocket, the KNN algorithm of the gallery dependent
or the Random Forest of the gallery independent test evaluation
achieved high recognition rates (KNN= 94.6% and RF =
TABLE IX. RECOGNITION ACCURACIES BASED ON DIFFERENT
BODY POSITIONS (S3D) OF A PERSON INSIDE THE TRAINING
DATA FOR THE ONLINE CASE.
Model
FC
FR
FL
BR
BL
Combined
KNN
85.0%
100%
100%
96.6%
95.0%
94.6%
Naive Bayes
80.0%
90.0%
95.0%
90.0%
96.6%
90.3%
Decision Tree
86.6%
98.3%
86.6%
83.3%
96.6%
90.3%
Random Forest
88.3%
100%
100%
90.0%
93.3%
94.3%
TABLE X. RECOGNITION ACCURACIES BASED ON ENVIRONMENT
CHANGES (S1I) OF A SUBJECT NOT CONSIDERED IN THE
TRAINING DATA FOR THE ONLINE CASE.
Model
Grass
Street
Outdoor
Indoor
Combined
KNN
97.2%
97.2%
97.2%
97.9%
97.4%
Naive Bayes
97.2%
100%
77.7%
54.1%
80.1%
Decision Tree
91.6%
80.5%
94.4%
83.3%
87.1%
Random Forest
97.2%
97.2%
100%
95.8%
97.4%
95.3%). The other algorithms including the Naive Bayes and
Decision Tree detect the gait types between an accuracy rate
of 85% and 90.3%. All in all it can be said, that the developed
recognition system is able to detect the actual gait of a user
dynamically. It is capable of detecting the activity regardless
the phone position and orientation, is user independent and
recognizes gait types in real- time (within 2.56 seconds for
the ﬁrst recognition) with a satisfying result.
Figure 6. Recognition accuracies of each machine learning model (K-nearest
Neighbour, Naive Bayes, Decision Tree and Random Forest) applying the
online gallery dependent scenarios evaluation including environment (S1D),
speed (S2D) and position (S3D) changes.
Figure 7. Recognition accuracies of each machine learning model (K-nearest
Neighbour, Naive Bayes, Decision Tree and Random Forest) applying the
online gallery in- dependent scenarios evaluation including environment
(S1D), speed (S2D) and position (S3D) changes.
5
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-781-8
ADAPTIVE 2020 : The Twelfth International Conference on Adaptive and Self-Adaptive Systems and Applications

TABLE XI. RECOGNITION ACCURACIES FOR S2I (SPEED).
Model
Slow
Normal
Fast
Combined
KNN
93.3%
98.3%
90.0%
93.8%
Naive Bayes
88.3%
85.0%
51.6%
75.0%
Decision Tree
80.0%
91.6%
83.3%
85.0%
Random Forest
98.3%
98.3%
95.0%
97.2%
TABLE XII. RECOGNITION ACCURACIES FOR S3I (POSITION).
Model
FC
FR
FL
BR
BL
Combined
KNN
90.0%
98.3%
98.3%
93.3%
86.6%
93.3%
Naive Bayes
80.0%
85.0%
98.3%
78.3%
83.3%
85.0%
Decision Tree
91.6%
91.6%
88.3%
86.6%
86.6%
89.0%
Random Forest
88.3%
98.3%
96.6%
93.3%
100%
95.3%
IV.
CONCLUSION
To summarize, the aim to create a system, which is able
to adaptively recognize activities regardless the position and
orientation of the recording device (i.e., commercial smart-
phone) with built-in motion sensors was achieved with a
recognition accuracy of 96.1%. Activity recognition systems
apply supervised classiﬁcation machine learning approaches
in order to ”classify” (detect) an actual activity on new data
provided by users. In total, 15 subjects were asked to partic-
ipate in the data acquisition process where motion data from
the accelerometer, gravity, linear accelerometer and gyroscope
sensor was collected. Each subject provided sensor data from
ﬁve different body positions (position independence) as well as
ﬁve different activities. To achieve orientation independence,
the gathered data was transformed into another coordinate
system (horizontal and vertical movements). Additionally, the
second preprocessing step was the usage of the Savitzky-Golay
ﬁlter in order to ”smooth” the data while preserving high
and low signal peaks. Furthermore, data segmentation (sliding
window) was applied on the transformed and smoothed data
in order to realize a real-time approach. The sliding window
of the recognition system has the length of 2.56 seconds and
an overlap of 50%. In the next step, the segmented data was
then passed to the feature extraction routine in order to derive
time and frequency domain features. The last step was the
machine learning model creation. In order to ﬁnd the best
machine learning model concerning performance and accuracy,
four different algorithms (i.e., k-nearest Neighbour, Naive
Bayes, Decision Tree and Random Forest) were compared
and integrated into the implemented smartphone application.
Subsequently, the real-time dynamic gait recognition system
was evaluated using 12 different evaluation scenarios. Half of
the evaluation scenarios were taken ofﬂine on the computer
and the other half was performed online using the smartphone
application. The activity recognition system was tested on
subjects which are gallery dependent (data included in the
training set) and gallery independent (data not included in
the training set). To underline the hypothesis concerning the
dynamic placement, the recognition system was tested on
recognizing the actual gait in ﬁve different body positions. The
best overall recognition accuracy (ofﬂine and online evaluation
combined) of 96.1% was achieved by the Random Forest
algorithm, which turns out to be the most suitable algorithm
for the developed system. Overall, it can be said that the
developed real-time dynamic gait recognition system running
on a smartphone is able to detect the actual activities (i.e., the
gait) of a user regardless the position and orientation of the
device with an recognition accuracy of up to 96.1%. Having
proven the feasibility of developing a gait recognition system
which is position and orientation independent it is legitimate
to state that the system could be hugely beneﬁcial to tracking
and analysing human activity in different commercial use-cases
(e.g., physiotherapy, elderly care, industrial manufacturing,
etc.).
The Feature Extraction process applied within this paper
uses standard mathematical features and includes among other
things maximal, minimal, mean, standard deviation values
from time and frequency domains. Although standard math-
ematical features are applied, the current version of the real-
time dynamic recognition system achieves a satisfyingly result.
However, the feature extraction process could be further im-
proved by using more ”complex” features (e.g., signal peaks in
time domain) or utilizing different feature selection approaches
(e.g., grid search or relief). For example, by applying a grid
search on the current feature vector (177 features), the number
of signiﬁcant features for the classiﬁcation process could be
improved and therefore most likely increase the performance.
ACKNOWLEDGMENT
This paper and all corresponding results, developments and
achievements are a result of the Research Group for Secure and
Intelligent Mobile Systems (SIMS).
REFERENCES
[1]
M. Kurz, B. Hiesl, and E. Sonnleitner, “Real-time activity recognition
utilizing dynamically on-body placed smartphones,” in The Eleventh
International Conference on Adaptive and Self-Adaptive Systems and
Applications (ADAPTIVE 2019), 2019, pp. 84–87.
[2]
A. Anjum and M. U. Ilyas, “Activity recognition using smartphone
sensors,” 2013 IEEE 10th Consumer Communications and Networking
Conference (CCNC), 2013, pp. 914–919.
[3]
X. Su, H. Tong, and P. Ji, “Activity recognition with smartphone
sensors,” Tsinghua science and technology, vol. 19, no. 3, 2014, pp.
235–249.
[4]
L. Bao and S. S. Intille, “Activity recognition from user-annotated
acceleration data,” in International Conference on Pervasive Computing.
Springer, 2004, pp. 1–17.
[5]
G. Bieber, J. Voskamp, and B. Urban, “Activity recognition for everyday
life on mobile phones,” in International Conference on Universal Access
in Human-Computer Interaction.
Springer, 2009, pp. 289–296.
[6]
M. Derawi and P. Bours, “Gait and activity recognition using commer-
cial phones,” computers & security, vol. 39, 2013, pp. 137–144.
[7]
Y. He and Y. Li, “Physical activity recognition utilizing the built-in
kinematic sensors of a smartphone,” International Journal of Distributed
Sensor Networks, vol. 9, no. 4, 2013, p. 481580.
[8]
A. Henpraserttae, S. Thiemjarus, and S. Marukatat, “Accurate activity
recognition using a mobile phone regardless of device orientation
and location,” in Body Sensor Networks (BSN), 2011 International
Conference on.
IEEE, 2011, pp. 41–46.
[9]
M. Kurz, “Opportunistic activity recognition methodologies,” Ph.D.
dissertation, Johannes Kepler University Linz, Jun. 2013.
[10]
J. R. Kwapisz, G. M. Weiss, and S. A. Moore, “Activity recognition us-
ing cell phone accelerometers,” ACM SigKDD Explorations Newsletter,
vol. 12, no. 2, 2011, pp. 74–82.
[11]
L. Sun, D. Zhang, B. Li, B. Guo, and S. Li, “Activity recognition on
an accelerometer embedded mobile phone with varying positions and
orientations,” in International conference on ubiquitous intelligence and
computing.
Springer, 2010, pp. 548–562.
[12]
A. Burns and A. J. Wellings, Real-time systems and programming
languages: Ada 95, real-time Java, and real-time POSIX.
Pearson
Education, 2001.
6
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-781-8
ADAPTIVE 2020 : The Twelfth International Conference on Adaptive and Self-Adaptive Systems and Applications

