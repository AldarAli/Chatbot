203
International Journal on Advances in Life Sciences, vol 8 no 3 & 4, year 2016, http://www.iariajournals.org/life_sciences/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Constraining the Connectivity of Sparse Neural Associative Memories
Philippe Tigréat, Vincent Gripon, Pierre-Henri Horrein
Electronics Department, Telecom Bretagne
Brest, France
Email: {philippe.tigreat, vincent.gripon, ph.horrein}@telecom-bretagne.eu
Abstract—Willshaw networks are a type of associative memo-
ries with a storing mechanism characterized by a strong redun-
dancy. Namely, all the subparts of a message get connected to
one another. We introduce an additional speciﬁcity, by imposing
the constraint of a minimal space separating every two elements
of a message. This approach results from biological observations,
knowing that in some brain regions, a neuron receiving a stronger
stimulation can inhibit its neighbors within a given radius.
Theoretical arguments are derived to quantify the beneﬁts of this
method in terms of memory usage as well as pattern completion
ability. We experiment with different values of the inhibition
radius introduced, and we study its impact on the error rate
in the retrieval of stored messages. We show that this added
constraint can result in signiﬁcatively better performance of the
Willshaw network, either when reducing its set of connections, or
when extending its set of neurons while maintaining the memory
resource.
Keywords—Willshaw Networks; Clique-Based Neural Networks;
Content-Adressable Memory; Lateral Inhibition; Sparse neural
networks.
I. INTRODUCTION
Associative memories are a type of computer memories that
are part of the broader category of content-adressable mem-
ories. A new model of associative memory based on neural
networks was formerly introduced in [1]. Where adressable
memories associate an adress with a piece of data, associa-
tive memories have the characteristic of associating patterns
to one another. Among this group, we distinguish between
hetero-associative memories, and auto-associative memories.
An hetero-associative memory will associate together patterns
in pairs. For instance, if the pattern p1 was associated with
pattern p2, the request p1 will bring the response p2. Auto-
associative memories follow a different principle, as they will
associate a pattern with itself. The main use case of these
memories is pattern completion, where a request made of a
subpart of a stored message will get as response the completed
pattern. Associative memories can be found in several types of
real-world applications, such as database engines [2], network
routers [3], data compression devices [4], and computer vision
systems [5].
Today, it is widely accepted that the working principle of
the brain can often be likened to the operation of an associative
memory. The focus is put here on the phenomenon observed
in biological neural networks, called lateral inhibition [6].
It can also be referred to as surround suppression [7]. This
translates in the inhibition exerted by some neurons on their
close neighbors when these have an activity inferior to their
own. Starting from the Willshaw model [8], we propose
a neural associative memory that is improved in terms of
plausibility, by the introduction of local inhibition that results
in the prohibition of short-range connections. We show that
this modiﬁcation brings a performance improvement in the
retrieval of stored messages.
Section II introduces three associative memory models
with relevant relationship to this work. Section III gives a
formal presentation of Willshaw networks, including the usual
message retrieval algorithm, and biological considerations
motivating the modiﬁcations we introduce. Section IV details
modiﬁcations in our implementation as compared to the classic
Willshaw model, including the constraint applied on the space
between connected neurons. Section V provides theoretical
arguments showing the better usage of memory brought by
this constraint. Section VI presents the results we obtain in
pattern completion, and gives some theoretical explanations.
II. RELATED WORK
A. Hopﬁeld Networks
The prominent model for associative memories was in-
troduced by John Hopﬁeld [9], [10]. Hopﬁeld networks are
made of a set of N neurons that are fully interconnected. The
training of these networks, given n binary vectors xµ of length
N, consists in modifying the weight matrix W according to
the formula:
wij = 1
n
n
X
µ=1
xµ
i xµ
j ,
(1)
where element wij at the crossing between line i and column
j of W is the real-valued connection weight from neuron i to
neuron j.
As connections are reciprocal and not oriented, we have:
wij = wji
∀i, j ∈ J1, NK
(2)
for any indices i and j in the list of neurons, which makes W
symmetrical.
The binary values considered for the stored messages are
usually -1 and 1, but can be adapted to work with other binary
alphabets. The Hopﬁeld model has a limited efﬁciency, in
particular it does not allow a storage of more than 0.14N
messages [11]. The limits of the model can be explained by
the facts that each entry of the matrix is modiﬁed at every time
step of the storing procedure, and that the changes are made
in both directions and can, therefore, cancel each other out.
This overﬁtted characteristics of associative memories is very
different from that observed in learning applications. Indeed,
an overﬁtted learning system recognizes only the training sam-
ples and fails at generalizing to novel inputs. To the contrary,
an overﬁtted storing system recognizes everything and does
not discriminate anymore between stored and nonstored data.

204
International Journal on Advances in Life Sciences, vol 8 no 3 & 4, year 2016, http://www.iariajournals.org/life_sciences/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
B. Willshaw Networks
Willshaw networks are another model of associative mem-
ories in which information is carried by the existence or
absence of connections [8], [12]. Its material is made of a set
of N neurons and N 2 potential connections between them.
A message is then a ﬁxed size subset of the N neurons,
and can be represented by a sparse vector of length N with
ones at these neurons’ positions and zeros everywhere else.
The connection weights are binary, and the active units in a
message get fully interconnected as soon as it is memorized,
thus forming a clique. Figure 1 gives an example of such a
network. The performances of Willshaw networks are way
superior to those of Hopﬁeld memories, given that stored
messages are sparse (i.e., they contain a small proportion of
nonzero elements). Further theoretical and numerical compar-
ison between Hopﬁeld and Willshaw networks can be found
in [13]–[16].
C. Clustered Cliques Networks
Recently, a novel type of associative memories was pro-
posed by Gripon et al., called Gripon-Berrou Neural Net-
works (GBNNs) or clustered cliques networks (CCNs) [17],
[18]. These associative memories make use of powerful yet
simple error correcting codes. These networks consider input
messages to be nonbinary, and more precisely to be words
in a ﬁnite alphabet of size l. This speciﬁc structure allows
the separation of nodes into different clusters, each being
constituted of the same number l of nodes. Connections
between nodes inside a given cluster are forbidden, only the
connections between nodes in two different clusters are al-
lowed. There again, this model brings a signiﬁcantly improved
performance as compared to the former state-of-the-art of
associative memories, namely Willshaw networks [19]–[21].
For instance, it can be found experimentally that with 2,048
nodes and 10,000 stored messages of order 4 and 2-erasures
queries, a Willshaw network will have an error rate close to
80%, while a clique-based neural network will only make 20%
of wrong retrievals.
In both Hopﬁeld and Willshaw models, the number of
messages the network can store and retrieve successfully
is linearly proportional to the number of nodes, with a
greater proportionality constant for Willshaw networks [14]. In
clique-based neural networks however, storage capacity grows
quadratically as a function of the number of units.
One of the objectives of the present work is to explain the
performance improvement brought by the separation of the
network into clusters. Therefore, we study a network that can
be considered as an intermediate between the Willshaw and
Gripon-Berrou models. More precisely, our proposed model
adds a locally exclusive rule for nodes to be active in the
network.
III. WILLSHAW NETWORKS AND BIOLOGICAL
CONSIDERATIONS
Willshaw networks are models of associative memories
constituted of a given number of neurons. A stored message,
or memory, is a combination of nodes taken in this set.
Figure 1. Willshaw network. A message composed of 8 nodes is displayed,
the inter-connections being the means of its storage in the network.
The storage of this information element corresponds to the
creation of connections with unitary weights between every
two neurons in this message. The graphical pattern thus formed
is termed "clique". The storing process of n binary vectors xµ
of length N, is equivalent to the modiﬁcation of elements of
the network’s connection matrix W, according to the formula:
wij = max
µ
xµ
i xµ
j
(3)
Note that here, the max operator is performed coefﬁcient-
wise. Equivalently, the connection weight between nodes i and
j is equal to 1 if, and only if, those two nodes are both part
of one of the n stored messages.
The network’s density d is deﬁned as the expected ratio of
the number of ones in the matrix W to the number of ones
it would contain if every possible message was stored. For
cliques of order c, the number of connections they contain is

205
International Journal on Advances in Life Sciences, vol 8 no 3 & 4, year 2016, http://www.iariajournals.org/life_sciences/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
network with N nodes, the number of potential connections,
or binary resource, is
Q = N(N − 1)
2
[bits].
(5)
After M messages have been stored in the network, the amount
of information it contains is
B = M

log2
N
c

[bits].
(6)
Hence the efﬁciency of a Willshaw network is
η =
2M

log2

206
International Journal on Advances in Life Sciences, vol 8 no 3 & 4, year 2016, http://www.iariajournals.org/life_sciences/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Figure 2. Cyclic Willshaw network with a constraint on local connections.
shorter-range distance depicts a difference in degree. Local
competition is particularly relevant in this scheme.
During retrieval, the network is stimulated iteratively with
a request that will most often change from one iteration to
the next. Each node of the request will ﬁrst stimulate every
other element it is connected to. Scores are initialized with
zero at the start of every iteration, and each stimulation is a
unitary increment to the score of the receiver unit. For the
ﬁrst iteration, after the stimulation we apply a global Winner-
Takes-All rule, which consists in excluding from the research
scope all units that do not achieve the maximal score observed
in the network. We know indeed that the neurons from the
searched message will all have the maximum possible score,
equal to the number of elements in the request. Once non-
maximum elements are put to zero, we only pay interest in
the remaining neurons during the rest of the retrieval process.
Moreover, for every iteration after the ﬁrst one, neurons in the
new request are the only ones that can receive stimulation as
the algorithm proceeds to only discard neurons from then on.
Thereafter, we can keep using the global WTA principle
iteratively, but other algorithms such as Global Winners-Take-
All (GWsTA) or Global Losers-Kicked-Out (GLsKO) [26]
are more efﬁcient in discriminating the right nodes from the
spurious ones that can appear during retrieval.
GWsTA relies on the calculation of a threshold score to
select winner neurons. This threshold is chosen such that
neurons with an activity above it are in number at least as
large as the order of stored messages.
GLsKO consists in putting off, at each iteration, all the units
that do not have the highest score, or a subgroup sampled
randomly in this ensemble.
These two algorithmic techniques allow to getting rid of
an important proportion of false-positives. In the clique-based
CCN, clusters play a similar role.
The iterative nature of the process means that a message
retrieved as output from the network is typically reinjected in
it until input and output no longer differ. A limited number of
iterations is applied in the case where the network would not
converge to a stable solution, an observable case in which it
can oscillate between two states.
In addition to these two stopping criteria that are the
maximum number of iterations and convergence, comes a
third one which is the identiﬁcation of a clique. Indeed, if
we observe that the units still active after an iteration are in
number equal to the order of stored messages, and that they
all have the same score, this means it is a stored message.
This ensemble is then retained as the response given by the
network for the current request.
Algorithm 2 shows the message retrieval procedure used in
the results we present. Phase II uses GLsKO.
Algorithm 2: Message retrieval procedure in the modiﬁed
Willshaw network with spacing constraint.
Data: Subpart x of a stored message
Result: Set of nodes z active after treatment
Phase I
y = Wx
z = GlobalWinnerTakesAll(y)
Phase II
Repeat
y = Wz
a = active nodes in y
m = nodes in a with minimal score
z = a − m
while (convergence not reached
and max. nb. of iterations not reached)
Return z
We experiment the storage of messages of order c in the
connection matrix of the network. Messages are formed with
the constraint of a minimal space between connected nodes.
Two units in a message must be spaced apart at a distance
superior to a minimum σ. In order to ease computations and
avoid edge effects, we choose to use the L1 distance, even
though we believe this method should work using any distance.
This way, when picking a node x for a message, all nodes
located in a square grid centered on x, of side 2σ+1, are
excluded from the possible choices for the elements of the
message remaining to be ﬁlled. Moreover, this distance is
applied in a cyclic way, meaning a node located on the right
edge of the grid will be considered a direct neighbor of the
element located at the crossing between the same line and the
left edge of the grid. All four corners of the grid will also be
neighbors to one another. We call the network so described a
torus.

207
International Journal on Advances in Life Sciences, vol 8 no 3 & 4, year 2016, http://www.iariajournals.org/life_sciences/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Figure 3. Different overlapping conﬁgurations between the inhibition areas
around two neurons. Here with σ = 1, the union of the two overlapping areas
can contain 17, 16, or 15 neurons.
V. EFFICIENT USE OF THE MEMORY RESOURCE
When applying a constraint σ on the minimal spacing be-
tween connected neurons, the number of potential connections
in the modiﬁed Willshaw network becomes
Q = N(N − (2σ + 1)2)
2
[bits].
(12)
Let the total number of messages one can form in it under the
spatial constraint, be denoted M. The entropy per message b
is given by
b =

208
International Journal on Advances in Life Sciences, vol 8 no 3 & 4, year 2016, http://www.iariajournals.org/life_sciences/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Table I:
POLYNOMIAL
FORMULAS
FOR
THE
NUMBERS
OF
ALLOWED
AND
FORBIDDEN
MESSAGES
UNDER
THE
SPATIALITY
CONSTRAINT
ON
CONNECTIONS, AS A FUNCTION OF THE NUMBER N
OF NEURONS,
FOR DIFFERENT CLIQUE ORDERS AND INHIBITION RADII. CASES WITH
σ = 0 ARE EQUIVALENT TO WILLSHAW NETWORKS. THE VALIDITY OF
THESE FORMULAS HOLDS FOR NETWORK SIZES SUFFICIENTLY LARGE
BEFORE σ.
c
σ
Allowed messages
Forbidden messages
2
0
N2−N
2
0
1
N2−9N
2
4N
2
N2−25N
2
12N
3
N2−49N
2
24N
3
0
N3−3N2+2N
6
0
1
N3−27N2+194N
6
4N 2 − 32N
2
N3−75N2+1514N
6
12N 2 − 252N
3
N3−147N2+5834N
6
24N 2 − 972N
4
0
N4−6N3+11N2−6N
24
0
1
N4−54N3+1019N2−6798N
24
2N 3 − 42N 2 + 283N
2
N4−150N3+7931N2−149550N
24
6N 3 − 330N 2 + 6231N
3
N4−294N3+30539N2−1133958N
24
12N 3 − 1272N 2 + 47248N
Figure 5.
Number of allowed messages as a function of the total number
of neurons, under constraints σ = 1, 2, and 3 compared with a Willshaw
network, for c = 3.
network. Figure 5 shows the evolution of the number of
legitimate messages for different constraints on connection
length, for c = 3.
With c = 3 and σ = 1, the network’s efﬁciency is given by
η =
2M

log2

N 3−27N 2+194N
6

N(N − 9)
,
(18)
while the efﬁciency of the corresponding Willshaw network is
η =
2M

log2

N 3−3N 2+2N
6

N(N − 1)
.
(19)
Figure 6. Evolution of the ratios of the efﬁciency of the networks with spatial
constraints σ = 1, 2, and 3 over the efﬁciency of a Willshaw network, with
increasing network size, for c = 3.
The ratio of these two efﬁciencies is thus given by
ησ=1
ησ=0
=
(N − 1)

log2

N 3−27N 2+194N
6

(N − 9)

209
International Journal on Advances in Life Sciences, vol 8 no 3 & 4, year 2016, http://www.iariajournals.org/life_sciences/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Figure 7.
Evolution of the efﬁciency ratios for σ = 1, 2, and 3 over the
Willshaw efﬁciency as a function of the number of neurons in the network,
for c = 4.
VI. PATTERN COMPLETION
We now pay interest speciﬁcally in the pattern retrieval
ability of the modiﬁed Willshaw network, as compared to the
classical model. During retrieval, only a sample from the nodes
of the complete message are stimulated, the inputs are subparts
of stored messages. Units that are close to elements of an input
will not reach the maximum score in the network, and will
therefore be ruled out after the ﬁrst WTA operation. During the
second phase of the algorithm, nodes in the vicinity of input
neurons will also be more likely to reach a low score if they are
activated, and to be discarded. Hence, the local inhibition used
initially during the creation of messages impacts the retrieval
process as well.
We pay interest in the network’s ability to return the exact
memory associated with a request. Hence every difference,
even marked by a single unit, between the expected pattern
and the network’s output is counted as an error.
We measure the performance of the network as the ratio of
the number of successfully retrieved messages over the total
number of requests.
Various parameters can impact this performance, albeit to
different degrees:
- the length S of the grid’s side
- the number M of stored messages
- the minimal space σ between two elements of a message
- the order c of stored messages
- the number of erasures ce applied on stored messages to
obtain the corresponding request messages
The behavior of this network is interesting in relation to
Willshaw networks and clustered cliques networks, as it is
close to a classic Willshaw network and displays the added
feature of prohibited connections as observed in CCNs. This
modiﬁcation can be viewed as a form of sliding-window
clustering.
Figures 8 and
9 represent the matrices of allowed and
forbidden connections in a clustered clique network with
Figure 8.
Matrix of the potential and forbidden connections in a clustered
clique network with 4 clusters of 16 fanals each. The element at the crossing
of a line i and a column j represents the connection between neurons ni
and nj. White cells represent allowed connections, black cells correspond to
forbidden ones.
Figure 9.
Connection matrix for a modiﬁed Willshaw network with side
length S = 8 and σ = 1.

210
International Journal on Advances in Life Sciences, vol 8 no 3 & 4, year 2016, http://www.iariajournals.org/life_sciences/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
χ = 4 clusters comprising l = 16 neurons each, and in
a modiﬁed Willshaw network of side length S = 8 under
constraint σ = 1, respectively. The two networks have the
same number of neurons, and comparable numbers of allowed
and forbidden connections. Indeed, the number of potential
non-oriented connections in the clustered clique network is
given by
Q = χ(χ − 1)l2
2
[bits],
(21)
which here gives 1,536 allowed connections, against 480 ones
forbidden due to the clustering constraint. On the other hand,
from (12) we have 1,760 allowed connections in the spatially
constrained Willshaw networks, and 256 forbidden ones. The
major difference is the potential overlapping between the inhi-
bition areas around different neurons in the modiﬁed Willshaw
network. It follows that in this network, two neurons ni and
nj can be both prohibited from connecting to a third one nk,
and yet be allowed to connect together. To the contrary, if in
a clustered clique network, connection weights wik and wjk
are forced to remain at 0, then necessarily wij will be as well.
In a ﬁrst series of experiments, we focus on the ability of
the network to ﬁrst store independent, identically distributed
messages, and then complete them properly when probed
with partial cues. For every conﬁguration of the network,
messages and requests we test, we store a set of thousands
of messages in the network. These messages are generated
randomly following the local inhibition pattern described in
section IV. We then request it with the full set of queries
associated with stored messages.
For each network size, we observe that there is an opti-
mal value of the minimal distance σ, that lowers the most
signiﬁcantly the error rate, as compared to the corresponding
Willshaw network without constraint on local connections. For
a given minimal distance, the reduction in error rate depends
on the number of stored messages, with an optimal number of
messages which is a function of the network size. For cliques
of order four and with two erasures, the maximal reachable
improvement is close to 15%, and seems to be the same for
all network sizes. In this conﬁguration, the minimal distance
bringing the best performance is approximately the third of
the network side.
The evolution of the retrieval error rate as a function of
the number of stored messages is slower with the appropriate
constraint on connections than for a classic Willshaw network,
as can be seen in Figure 10.
Figure 11 shows a similar comparison, this time between
the modiﬁed Willshaw network with constraint σ = 5 made
of 400 neurons, and an unconstrained Willshaw network with
335 neurons. Because of the reduction in the number of
connections when σ = 5, the two networks have almost
the same binary resource. Indeed, the Willshaw network has
55,945 connections while the constrained one has 55,800
possible connections, despite having more neurons. Even
though the modiﬁed network has a slightly lower footprint, the
improvement is even more noticeable than for the comparison
with equal size of the neurons sets. In fact for 1, 500 stored
Figure 10. Evolution of the retrieval error rate with and without constraint
σ = 5 in a network of side length 20 with 400 neurons, stored messages of
order 6 and 1 erasure applied to form corresponding requests, with 1 iteration.
Figure 11. Evolution of the retrieval error rate after one iteration as a function
of the number of stored messages, in a classic Willshaw network with 335
neurons, and in a modiﬁed Willshaw network of side length 20 with 400
neurons, with constraint σ = 5. Stored messages have order c = 6 and
associated queries are obtained by erasing one vertex. The two networks have
close numbers of possible connections.
messages, the modiﬁed network gains around 50% in error
rate over the Willshaw network. Like the CCNs approach, this
shows the interest of spreading the binary resource across a
larger set of units with constrained connectivity, as opposed
to allowing any two neurons to link.
For a constant number of stored messages, the graph of the
error rate as a function of σ is characterized by a progressive
decay down to a minimum, followed by a rapid growth for
upper values of σ, as shown in Figure 12.
This can be explained by two phenomena. On the one hand,
the prohibition of a growing part of the possible connections
gradually decreases the probability of a "false message",
characterized by the intrusion of a spurious node in the output.
The existence of a node that is connected to all elements in

211
International Journal on Advances in Life Sciences, vol 8 no 3 & 4, year 2016, http://www.iariajournals.org/life_sciences/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Figure 12. Minimal connection distance effect on performance in a modiﬁed
Willshaw network with local inhibition, made of 2500 neurons. Stored
messages are of order c = 4, and ce = 2 erasures are applied to form
corresponding requests. Seven different numbers of stored messages are tested.
The case where minimal spacing σ = 0 corresponds to a classic Willshaw
network.
a request, yet is not part of the corresponding message, will
potentially cause an error. In fact, forbidding some connections
has the effect of reducing the number of concurrent nodes
susceptible to cause errors. We can estimate the mean number
of concurrent nodes remaining after the choice of k neurons
of a message:
Nc(σ, k) = N
 
1 −
 
(2σ + 1)2
N
!!k
.
(22)
The corresponding number of nodes blocked by the con-
straint on connections is, on average:
Nb(σ, k) = N

1 −
 
1 −
 
(2σ + 1)2
N
!!k
 .
(23)
This explains the decay phase in error rate observed for the
ﬁrst values of σ. Let us note that it comes with a decrease
in the diversity of messages, namely the total number of
different messages that can be stored in the network. Following
this decay, the decrease in the number of concurrent nodes
has another effect: the reuse of connections by different
messages becomes more frequent as the choice for possible
connections gets reduced. This comes to counteract the former
phenomenon and raises the error rate.
The density of the modiﬁed network after the storage of n
messages can be calculated by
d = 1 −
 
1 −

212
International Journal on Advances in Life Sciences, vol 8 no 3 & 4, year 2016, http://www.iariajournals.org/life_sciences/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Figure 14. Maximal improvement obtained over a classic Willshaw network
made of 400 units with messages of order 6, using a single iteration of
decoding.
best performance is approximately proportional to the side of
the network. Consequently, the proportion of neurons in the
network that cannot be connected to the c − ce neurons in the
request remains more or less the same for different network
sizes, with the optimal minimal distance.
The beneﬁts brought by the constraint on connections tends
to be stronger for smaller numbers of erasures. For erasures of
about half the units of the messages, the maximum gain will be
lower, yet for a high amount of erasures the performance may
be more noticeably enhanced by the added constraint. The
performance improvement over a classic Willshaw network
also depends on the number of messages stored in the network.
It reaches a peak for a certain number of stored messages, and
then decays when additional messages get stored. The maximal
improvement tends to be reached earlier during storage for
higher numbers of erasures, as illustrated by Figures 14 and
15 for a network with 400 neurons, messages of order c = 6,
and numbers of erasures ce ranging from one to ﬁve. Figure
16 shows that for a larger network with 900 neurons, this
arrangement is respected for the most part, with the exception
of the case where ce = 1, for which the peak in performance
improvement occurs for a lower number of stored messages
than for ce = 2 or ce = 3. Figures 14 and 15 also show that
the maximum number of iterations applied during retrieval has
a varying effect on performance improvement, depending on
the number of erasures applied to form requests. With 400
neurons, increasing the number of iterations has a clear effect
on performance for ce = 3 and ce = 4, more so than for
ce = 1 and ce = 2.
When comparing networks with the same number of neu-
rons, the greatest performance improvement is most often
observed over a classic Willshaw network and a number of
stored messages originally giving an error rate ranging from
about 40%, up to 70%. The performance gain is then often
close to 15%.
Figure 15. Maximal improvement obtained over a classic Willshaw network
of 400 neurons with messages of order 6, using a maximum of 3 iterations.
Figure 16. Maximal improvement obtained over a classic Willshaw network
of 900 neurons with messages of order 6, using a maximum of 3 iterations.
VII. CONCLUSION AND PERSPECTIVES
We introduced a modiﬁed version of Willshaw neural net-
works that has interesting properties regarding storage capacity
and retrieval performance. By prohibiting certain types of
connections in the network, we observe that the retrieval ability
can be enhanced, and that the value of the threshold on inter-
neuron connection spacing has a direct impact on performance.
As a result, this constraint can be used to enhance pattern
completion performance without modifying the size of the
pool of neurons. In addition, the applied constraint comes
with an improved efﬁciency, as the quantity of information
carried by each single connection is made higher. Also, given a
ﬁxed amount of allotted memory to store connection weights,
a constrained network with more neurons but a comparable
number of connections can be created, which will display even
better performances.
This is relevant to observations on clustered cliques neural

213
International Journal on Advances in Life Sciences, vol 8 no 3 & 4, year 2016, http://www.iariajournals.org/life_sciences/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
networks, in that it shows constraining connections in a
Willshaw network modiﬁes its capacity and efﬁciency in a
way that depends on the nature of the applied constraint. It
is a step forward in understanding why the use of clusters
in CCNs brings signiﬁcantly higher capacity as compared to
Willshaw networks. To some extent, it also emulates biological
observations of lateral inhibition in the brain and sensory
channels, as we prevent neighbor neurons from connecting and
therefore let them compete for activity. This makes sense with
a framework in which close-by neurons encode patterns that
differ only in degree and where only one unit that resonates
most with input stimuli must activate.
Future work may involve experimenting with other con-
straints on connections based on the relative locations of
neurons, such as variable values of the spacing constraint
applied in different subregions of the graph, or preventing
random subsets of connections. It may also focus on theoreti-
cal advances in the understanding of the relationship between
the nature of a connectivity constraint and its inﬂuence on
efﬁciency and performance.
ACKNOWLEDGEMENTS
This work was supported by the European Research Council
under Grant ERC-AdG2011 290901 NEUCOD.
The authors would like to thank NVIDIA for providing us
with a free graphics card allowing to speed up computations
for the experiments performed during this work.
REFERENCES
[1] P. Tigreat, V. Gripon, and P. H. Horrein, “Improved Willshaw networks
with local inhibition,” Proceedings of the Eighth International Confer-
ence on Advanced Cognitive Technologies and Applications, 2016, pp.
96–101.
[2] H. Jarollahi et al., “Algorithm and architecture for a multiple-ﬁeld
context-driven search engine using fully-parallel clustered associative
memories," In IEEE Workshop on Signal Processing Systems (SiPS),
2014, pp. 1–6.
[3] H. Gazit et al., “ Network device architecture to support algorithmic
content addressable memory (CAM),” U.S. Patent No. 9,367,645. Wash-
ington, DC: U.S. Patent and Trademark Ofﬁce, 2016.
[4] L. Y. Liu et al., “CAM-based VLSI architectures for dynamic Huffman
coding,” IEEE Trans. Consum. Electron., vol. 40, no. 3, 1994, pp. 282–
289.
[5] M. E. Valle, “A class of sparsely connected autoassociative morpho-
logical memories for large color images,” IEEE Transactions on Neural
Networks, vol. 20, no. 6, 2009, pp. 1045–1050.
[6] S. A. Shamma, “Speech processing in the auditory system II: Lateral
inhibition and the central processing of speech evoked activity in the
auditory nerve,” The Journal of the Acoustical Society of America,
vol. 78, no. 5, 1985, pp. 1622–1632.
[7] H. Ozeki et al., “Inhibitory stabilization of the cortical network underlies
visual surround suppression,” Neuron, vol. 62, no. 4, 2009, pp. 578–592.
[8] D. J. Willshaw, O. P. Buneman, and H. C. Longuet-Higgins, “Non-
holographic associative memory,” Nature, 1969, pp. 960–962.
[9] J. J. Hopﬁeld, “Neural networks and physical systems with emergent
collective computational abilities,” Proceedings of the national academy
of sciences, vol. 79, no. 8, 1982, pp. 2554–2558.
[10] P. Chossat, and M. Krupa, “Consecutive and non-consecutive hetero-
clinic cycles in Hopﬁeld networks,” Dynamical Systems, 2016, pp. 1–
15.
[11] R. J. McEliece et al., “Neural networks and physical systems with
emergent collective computational abilities,” Proceedings of the national
academy of sciences, vol. 33, no. 4, 1987, pp. 461–482.
[12] M. Laiho et al., “High-dimensional computing with sparse vectors,”
Biomedical Circuits and Systems Conference (BioCAS), 2015, pp. 1–4.
[13] H. C. Longuet-Higgins, D. J. Willshaw, and O. P. Buneman, “Theories
of associative recall,” Quarterly reviews of biophysics, 3(02), 1970, pp.
223-244.
[14] J. D. Keeler, “Comparison between kanerva’s sdm and hopﬁeld-type
neural networks,” Cognitive Science, vol. 12, no. 3, 1988, pp. 299–329.
[15] D. Willshaw and P. Dayan, “Optimal plasticity from matrix memories:
What goes up must come down,” Neural Computation, vol. 2, no. 1,
1990, pp. 85–93.
[16] A. Knoblauch, G. Palm, and F. T. Sommer “Memory capacities for
synaptic and structural plasticity,” Neural Computation, vol. 22, no. 2,
2010, pp. 289–341.
[17] V. Gripon and C. Berrou, “Sparse neural networks with large learning
diversity,” Neural Networks, IEEE Transactions on, vol. 22, no. 7, 2011,
pp. 1087–1096.
[18] A. A. Mofrad et al., “Clique-based neural associative memories with
local coding and precoding,” Neural Computation, 2016
[19] ——, “A simple and efﬁcient way to store many messages using neural
cliques,” Computational Intelligence, Cognitive Algorithms, Mind, and
Brain (CCMB), 2011 IEEE Symposium on, 2011, pp. 1–5.
[20] ——, “Nearly-optimal associative memories based on distributed con-
stant weight codes,” Information Theory and Applications Workshop
(ITA), 2012, pp. 269–273.
[21] B. K. Aliabadi, C. Berrou, and V. Gripon, “Storing sparse messages
in networks of neural cliques,” Proceedings of the national academy of
sciences, vol. 25, no. 5, 2014, pp. 461–482.
[22] G. Palm, “Neural associative memories and sparse coding,” Neural
Networks, vol. 37, 1987, pp. 165–171.
[23] J. R. Cavanaugh, W. Bair, and J. A. Movshon, “Nature and interaction
of signals from the receptive ﬁeld center and surround in macaque v1
neurons,” Journal of neurophysiology, vol. 88, no. 5, 2002, pp. 2530–
2546.
[24] M. A. Heller and E. Gentaz, “Psychology of touch and blindness,”
Psychology Press, 2013.
[25] T. Kohonen, “The self-organizing map,” Neurocomputing, vol. 21, no. 1,
1998, pp. 1–6.
[26] A. Aboudib, V. Gripon, and X. Jiang “A study of retrieval algorithms
of sparse messages in networks of neural cliques,” Proceedings of the
Sixth International Conference on Advanced Cognitive Technologies and
Applications, 2014, pp. 140–146.
[27] M. J. Wainwright, T. S. Jaakkola, and A. S. Willsky, “Tree-based
reparameterization framework for analysis of sum-product and related
algorithms,” IEEE Transactions on information theory, vol. 49, no. 5,
2003, pp. 1120-1146.

