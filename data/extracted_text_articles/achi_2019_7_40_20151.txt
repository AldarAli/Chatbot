Introducing Augmented Reality-Ready Head-Worn Displays
to Support Workers on the Shop Floor of a Car Production Line
Sebastian Felix Rauh∗, Diep Nguyen∗, Stephan Bolch† and Gerrit Meixner∗
∗UniTyLab
Heibronn University, Heilbronn, Germany
Email: ﬁrst.last@hs-heilbronn.de
†N/PN-48E
AUDI AG, Neckarsulm, Germany
Email: stephan.bolch@audi.de
Abstract—In this paper, we present an approach to directly
instruct workers in the quality check division of a car production
line. We simulated the complete manual functional check of a car
to test the possibilities of using one selected binocular optical See-
Through Head-Worn Display. Furthermore, we chose one subtask
to be Augmented Reality-supported to get an impression of the
capabilities of this technology in this application area. Also, we
tested a ﬁnger-worn input-device, representing the type of ”close
to the body”-input-devices. For the whole implementation we
applied the Human Centred Design Approach as described in
ISO 9241 - 210. We evaluated the application with four workers
from the selected division. Even though we could not implement
all requirements exactly as they were requested, the participants
responded positively to the selected Head-Worn Display, the
ﬁnger-worn input modality and the augmentation we provide.
Keywords–Augmented Reality; Interaction Device; Head-Worn
Display; Shop Floor; Production Line.
I.
INTRODUCTION
Modern Head-Worn Displays (HWDs) have the power to
show information directly on site where they are needed while
keeping both hands free to execute tasks. Also, due to the rapid
development of miniaturising high performance hardware, they
have the capability to display sophisticated content when it
is needed. Since the late 1980s, visually extending the real
environment with virtual content is part of scientiﬁc research
and also made its way into popular culture. The so-called
Augmented Reality (AR) was exempliﬁed in many areas like
medicine, eLearning or user navigation. Another promising
ﬁeld of application is the support of workers. From the very
beginning, maintenance, repair and assembly were strategically
identiﬁed as key application ﬁelds of AR [1]. It also can be
split up into remote support and automatic on-site support.
While the ﬁrst is establishing contact to an expert of the ﬁeld
and therefore is highly ﬂexible, the latter is based on analysing
and modelling work-ﬂows to display supportive instructions
whenever they are needed.
This automatic on-site support is less ﬂexible as it only can
access formalised knowledge, usually generated for each use
case, but it especially is able to support people in work areas
with low ﬂexibility and high production rate. One prominent
example of such a work area is the conveyor belt of car
manufacturers, where cars are assembled and quality checks
are performed. In this area we conducted our study on how
AR can support workers performing quality checks after the
assembly of vehicles. While AR was difﬁcult to established
with the available hard- and software, we could identify that
overcoming direct interaction with the HWD (i.e., using the
buttons on the HWD itself, e.g., on the glasses temple) is
crucial for the success of these devices.
The work presented in this paper is structured as follows:
In Section II, we present related work. In Section III, we
describe the results of the analysis of the current work ﬂow. In
Section IV, we describe the implementation of the application,
including the interviews conducted to validate and/or make de-
sign decisions. The results of the ﬁnal evaluation are presented
in Section V. In Section VI, our ﬁndings are summarised and
conclusions are drawn.
II.
STATE OF THE ART
Today, many AR systems are using HWDs, hand-held
computers or smart phones, i.e., small devices including at
least one camera and one kind of graphical display. In the
context of worker support where hand-free is critical, however,
HWDs are signiﬁcantly advantageous. In many aspects, HWDs
are similar to Head-Mounted Displays (HMDs). According
to [2] the main difference is that HWDs can be put on
like a normal pair of glasses while HMDs usually require
the user or an assisting person to adjust the position of the
display, for example via adjusting screws on the frame or head
straps. Typically, HWDs function with the support from either
a mobile or a wearable computer. Since the ﬁrst device in
1968 [3], HMDs have gone through tremendous changes for
maturation of resolution, refresh rate, Field of View (FoV),
etc. The new generation of HMDs, the HWDs, are lightweight
and almost like a normal pair of glasses, instead of their
bulky predecessors. All of these devices come along with
their own powerful computing unit, while older devices often
need to be attached to a desktop-PC to be able to offer AR-
features. This development is mainly based on the fast progress
in the miniaturisation of high performance electronics. With
the ability to function as wearable computers, modern HWDs
broaden the ﬁeld of application for AR, due their higher level
of mobility.
Recently, car manufacturers have started supporting em-
ployees with smart glasses instead of mobile devices. For
example, smart glasses now are a part of standard equipment’s
at Volkswagen in Wolfsburg, Germany [4]. Opel also has a
virtual assistant called ”Smart Helper” [5]. The importance of
AR in the production environment was emphasised through the
117
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

work of Meixner et al. [6] in which AR is portrayed as one
of the most important technologies of user interaction evolu-
tion in future industrial environments. The AR-support offers
instructions and guidelines at different levels to help users
to achieve their tasks while minimising errors and increasing
safety [1]. Moreover, AR also has been applied in providing
remote expert support and environment interaction [7]. Rauh et
al. use HWD to support workers in the quality check division
of a German car manufacturer [8] displaying instructions and
documentation in the Field of View rather than paper-based.
The aim here was to enable any worker, independent of
his/her expertise in this task to increase performance. The work
showed that in general HWD have the capacity to optimise
workers’ performance in the selected application area. Loch
et al. could show that compared to video assistance a worker
supported in AR performs signiﬁcantly better in terms of error
rates in manual work tasks [9]. This is an important ﬁnding
considering that the ﬂexibility of a human worker is needed to
be able to offer customised production services for customers.
With the speciﬁc intention of supporting workers in in-
dustrial production environments, several projects proposed
designs and frameworks where AR is employed to fulﬁl
these visions. Both projects MOON by AIRBUS Military
[10] and the AR Framework for Maintenance Tasks by Re
et al. [11] share similar goals of delivering 3D assembly work
instructions via smart phones and tablets. Even still struggling
with tracking technology and needing further development,
these works have shown promising results of AR usage over
productivity. In 2002, Tschirner et al. presented a welding hel-
met equipped with video See-Through technology to augment
the welders ﬁeld of view with multiple information, based on
his/her current task [12]. Besides a feasibility study to assist
workers while ﬁx broke weft yarns of a industrial waving
machine Kerpen et al. identiﬁed effects of the current trend
of digitisation in factory environment on the worker and the
portfolio of his/her tasks [13]. Among others, they predict
a easier and faster access to machine related data, a higher
ﬂexibility of the work organisation and the need of increased
IT-skills.
Wijesooriya et al. make use out of the increased acces-
sibility in a prototype using AR to visualise machine status
data on site [14]. Maly et al. developed an AR system for
maintenance and collaboration with a robot [15]. During the
ARVIKA project [16], head worn AR-based solutions for
industrial applications have been developed. The project had
brought AR into attention of many players across multiple
areas. Related to the ambition of this work, Eissele et al. de-
scribe a system which successfully combines ”multiple reality
stages” (i.e., AR, Mixed Reality, Augmented Virtuality and/or
Virtual Reality) to decrease assembly-time and error-rate [17].
Potentially related to the usage of a combination of multiple
levels of virtual environments, according to the authors the
actual strength of their system may be in highly complex
tasks. Another prominent example is AR in Smart Factories (in
terms of “Industry 4.0”), as described, e.g., by Paelke [18]. By
using AR instructions, users without experience in the selected
task were able to successfully implemented assembly tasks in
down-scaled model factories. The positive outcomes of this
work had encouraged further tests and investigation in a real
production environment for better applications.
Aside the industrial application areas, AR shows great
potential in many other areas. For example, Delft University
of Technology had run a project for distributed collaboration
in crime scene investigation [19]. The system allows a ﬁeld
investigator, equipped with a HWD, to work with an expert
in distance via video stream, voice communication and a
shared interactive 3D model. The work displayed promising
outcomes in improving mutual understanding between investi-
gators, fastening data exchange, as well as time management.
Furthermore, medicine is another interesting domain for AR.
In [20], Mentler et al. discussed several use cases in medical
practice employing smart glasses. These use cases cover from
triage, diagnosis to surgery. Despite the limitation of current
technologies, health care experts had shown their interest for
the usage of HWD.
Since AR has the power to decrease error rate and also
documentation efforts, e.g., by detecting if a task was executed
successfully or with errors, we believe that if this technology
is introduced in the right way it can increase efﬁciency. In the
described study we introduced a AR-ready HWD for selected
work tasks and compared the direct interaction with the device
(i.e., using the interaction elements on the glasses) with a
clicker device (i.e., a remote control with the same buttons
as the glasses). While AR turned out to be problematic due to
the diminished amount of trackable features in the selected
environment, feasible interaction with HWD-based systems
seems to be a key feature for the users we talked to.
Figure 1. The customised layout of the Hoerbiger Smartcontrol PDA
hand-held terminal as used in the described setting.
III.
ANALYSIS
We conducted the presented work on the shop ﬂoor of a
German car manufacturer in the ﬁnal check division of one
car manufacturing line. In contrast to the assembly division,
the tasks in this subsequent division require higher ﬂexibility
from the workers. Several varying automatic and manual tasks
are executed, based on the car’s conﬁguration, among others
to ensure the product quality. To further understand the work
118
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

tasks and the environment, we analysed both as described in
this section.
A. Environment
We selected the initial start-up and functional check divi-
sion, which is the ﬁrst subdivision of the ﬁnal check division.
This selection mainly is based on the fact that we already
executed previous projects in this division in which we made
good experiences with the ﬂexibility of the workers. In this
division, there is a greater amount of manually executed task,
for which the workers need to be instructed and guided than
in other divisions. Also, workers need to be mobile to execute
these tasks. Finally, the workers are already instructed by
a hand-held terminal, which they also use for documenting
the test results. In this division, the vehicles still are on the
conveyor belt. The lighting conditions, which are important
for object detection in camera images, are good and consistent.
For instructions and documentation of the progress Hoerbiger
Smartcontrol PDA hand-held terminals (see Figure 1) are used.
The unused hand-held terminals are placed on a desk, where
they can be charged. Like all other important tools they are
located aside the conveyor belt so workers can reach them in
short walking distance.
B. Work Tasks
We selected observation and self-exploration as the meth-
ods to analyse the current work ﬂow in this division. For
observation, we watched workers while they were performing
their tasks. We could ask them questions, as long as they were
not interrupted in their work ﬂow. For further questions, one
employee from the technical plant support and development
also was available, standing next to us. After the observation,
we shifted to the rework division (located after the ﬁnal check
division) to experience the work on our own without risking
to delay the manufacturing process or affecting the production
quality by wrong performing. In this division, defects can be
corrected. Due to the diversity of issues, the vehicles no longer
are standing on a conveyor belt but are parked in a reworking
area. We performed one exemplary functional check task ﬂow
here. In the following, the context of use and further results
of this initial analysis are listed.
The workers perform manual functional checks, like veri-
fying the bearing play of the steering wheel on each vehicle,
while the vehicle is connected to an automatic functional
test system. Some of the tasks of the automatic functional
test system require the worker to collaborate with the system
which is displayed on the hand-held device, while others are
performed autonomously. Each vehicle is completely checked
by one worker who shifts to check the next car when ﬁnished
with the current one.
To start checking the car, initially the worker scans a bar
code which among others is printed on a A4-paper attached to
the windscreen. To do so, the worker pushes the scan-button
of the hand-held terminal which enables its bar-code-scanner.
If the scan was successful, the hand-held terminal requests the
according functional checks from the back-end application and
displays them in a list. To start one functional test, the worker
selects it by pushing the list item on the touch screen, the hand-
held terminal requests the work ﬂow. To enable the testing
system performing automatic functional checks, the worker
connects the car to a test control unit which also is identiﬁed
by scanning the attached bar-code. After the test control unit
was identiﬁed and connected successfully, the functional test
starts. In the beginning, various automatic tests are performed.
This is indicated by black font colour on white background (see
Figure 1). The worker is observing the automatic tests until the
background changes to yellow. The systems stops performing
until the worker executes the requested input (e.g., pushing
a switch) or conﬁrms that the test was performed manually
by entering the result (i.e., ‘OK’ or ‘not OK’, represented
by the +/- buttons in Figure 1). While automatic and manual
(or collaborative) tasks alternate the worker is moving around
and through the car counter-clockwise. After the last test was
executed the worker removes the test control unit.
IV.
IMPLEMENTATION
For the implementation we deﬁned the requirements on
the software solution. Afterwards we designed Mock-Ups
which we validated by interviewing ﬁve (respectively four)
employees of the car manufacturer. Three participants were
employees working in the selected division while the other
two (respectively one) were engineers of the technical plant
support and development. After redesigning our Mock-Ups we
developed one prototype.
A. Requirements
For developing an example application instructing the
workers using AR, we concluded the following requirements
as must-have:
a) Replacing the Hand-Held Terminal: The selected
HWD shall completely replace the hand-held terminal for the
selected tasks. It has to offer all the necessary features to
successfully perform complete functional tests. As we aim
to evaluate the general usage of AR in this environment and
the user feedback, no connection to the test control unit or
the back-end-system is needed. It is sufﬁcient to simulate one
exemplary functional check.
b) Hands-Free Operation: The biggest disadvantage of
hand-held devices is that they blocks at least one of the
worker’s hand if it is not put aside. To overcome this problem,
the application shall be hands-free (as long as no interaction
with the device is necessary).
c) Improved Training: While previous projects mainly
focused on text-based worker instructions and served as doc-
umentation system, in this project we intended to observe
the capabilities of AR-ready HWD. We are aiming for using
AR to support novice workers in their training phase. For
demonstrating this capability some input elements from the
cockpit shall be selected. The selection will be made during
the development phase.
d) Ease of Operation: Using the application shall be
as simple as possible and new users shall be able to use the
device quickly.
e) Similar Appearance: To minimise the training phase
for experienced workers, the visual design shall follow the
visual design of the hand-held terminal application as long as
it not requested differently by the participants in the analysis
or any evaluation. The existing design shall be used as starting
point and be enhanced together with the workers.
f) Bar-Code Scan: The bar-codes shall be scanned by
using the HWD‘s camera to identify the car. As stated above,
loading the work ﬂow will be simulated.
119
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

g) Interface to Load Work-Flow Data: The application
shall be able to interpret XML-based test-ﬁles to simulate
different work ﬂows.
Within this deﬁnition phase we also discussed other re-
quirements. As the project was limited to four months we
decided to classify them as nice-to-have. These requirements
are: 1. Saving the protocol to be able to review the single
tests. Also, they should be exportable to a computer. 2.
Conﬁgure input modalities should be possible to reconﬁgure
the interaction-concept. This on the one hand would enable
the workers to personalise the interaction to their needs and
preferences and on the other hand would simplify extending
the features of the application. 3. Offering an exchangeable
work ﬂow parser module to ensure the application to be
compatible with future plant control systems. 4. Extending
the use of AR for more than one training scenario. This can
be for example augmenting another task or also superimpose
the workers’ view with navigational instructions, for example,
using arrows to guide them from task to task. 5. Skilled workers
should be enabled to work more independent than novice
workers by adjusting the Graphical User Interface (GUI).
Therefore, the application has full access to the camera, it
generally is able to detect the execution of tasks, and also to
determine the skill level of each worker.
(a) The instruction screen Mock-Up. Buttons and labels follow the design of
the original application.
(b) The scanning screen Mock-Up. Unused Buttons are greyed out.
Figure 2. Initial Mock-Ups for both types of screens.
B. Design
Based on the analysis we identiﬁed two basic screen
layouts. One for scanning tasks, and one for instruction and
documentation. We decided to choose a consistent structure
for both to reduce the cognitive workload when switching
from the instruction screen to the scanner screen. This is
based on the assumption that the worker will be able orientate
himself/herself faster on two similarly structured screens.
We designed Mock-Ups of both screens as shown in
Figure 2. In Figure 2 (a), the instruction screen is displayed
containing ﬁve interaction elements (Buttons) on the right and
a text-area (“Platzhalter-Text”) for displaying instructing text.
Above that area the date, time, WiFi signal strength and battery
load are shown in a status-bar. The scanner screen, shown
in Figure 2 (b), also contains the two required interaction
elements on the right. The black area on the left is transparent
and functions as viewﬁnder, because we plan to use the
camera to scan bar-codes. Above the viewﬁnder-area the user
is instructed to scan the vehicle’s bar-code (“Bitte Fahrzeug-
Barcode scannen”). On the top again there is the status-bar.
As described in the ISO 9241 - 210 Standard [21] we
evaluated the Mock-Ups together with stakeholders (here:
workers from the division and the engineering department),
which should establish usability and ensures that we, the
developers of the application, do not misunderstand the current
work-ﬂow or requests by the stakeholders. The Mock-Up
evaluation was performed in a conference room located in the
same building as the plant. We initially introduced the project
idea and the selected HWD to each of the ﬁve participants.
Also, we offered the participants to stop whenever they feel
to do so and clariﬁed that all personal data will be treated
conﬁdentially. Some of the participants already participated in
a predecessor project where we evaluated the HWD Google
Glass XE in the same set-up.
We prepared an icon test to validate the identiﬁcation of
the used icons and all other GUI-elements. The participants
were asked to put on the HWD and describe all elements they
can see as detailed as possible. Also, they were requested to
explain what they think the element stands for. The results
of icon test are listed in Table I. Except the WiFi Signal
Strength icon all elements could be identiﬁed as expected. The
high identiﬁcation rate most probably results from reusing the
layout of the elements from the hand-held UI.
TABLE I. RESULTS OF THE ‘ICON TEST’.
Icon
Identiﬁcation Rate
Date / Time
5/5
WiFi Signal Strength
3/5a
Battery Load
5/5
Text Area
5/5
Button OK
5/5
Button Not OK
5/5
Button Repeat
5/5
Button Scan
5/5
Button Cancel
5/5
a Two participants could not identify the icon correctly.
Afterwards we showed the participants 41 screens rep-
resenting one possible example of a functional check. Each
screen was displayed for one second and the participant was
asked to stop the forwarding when he/she thinks based on this
screen an active action is necessary. He/she then was asked to
120
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

TABLE II. RESULTS OF THE QUESTIONNAIRES. a
(a) The Mock-Up
P1
P2
P3
P4
...is well structured (++) / confusing (--).
++
++
++
++
...offers acceptable (++) / unacceptable (--) Field of View size.
--
++
++
++
...shows instruction text in sufﬁcient (++) / insufﬁcient (--) font size.
++
++
++
++
...does (++) / does not (--) offer all features to fulﬁl the task efﬁciently.
++
++
+/-
++
...gives sufﬁcient (++) / insufﬁcient (--) feedback on user input.
++
++
++
++
...does (++) / does not (--) follow a consistent interaction concept.
++
++
++
++
...can be learned in little (++) / long (--) time.
++
+
+
++
...does (++) / does not (--) allow to switch between task easily.
NS
++
+/-
NS
...does not (++) / does (--) force the user the interrupt his work.
-
+
++
-
(b) The Head Worn Display
...simpliﬁes (++) / impedes (--) orienting in space.
-
--
+/-
+/-
...does not (++) / does (--) show reﬂections above the display.
--
++
++
++
...is easy (++) / hard (--) to operate.
NS
++
+
++
aP = participant, NS = not speciﬁed / answer not deﬁnite.
describe which action needs to be performed. All screens were
classiﬁed correctly and the necessary actions were derived.
To gather general information about the structure and
appearance of the application and also about critical points (we
experienced in previous projects) of the HWD, we handed out
questionnaires. We used semantic differential to ﬁnd out which
statements the participants rather agree to. Unfortunately, par-
ticipant 5 had to leave earlier due to other obligations and could
not ﬁll in the questionnaire. Hence, this evaluation phase was
performed with four participants. The results, as well as the
translated statements of the both questionnaires are presented
in Table II. When the answer was not clear we did not count
it.
Because we recognised that the ﬁrst participant did not use
the free text section of our questionnaire, we decided to add
a short discussion about the Mock-Ups and the questionnaires
to ﬁnd out further details, among others that the bright white
text-area stresses the eye. Participants suggested to increase
transparency of the area or move the text-area to the top
(similar to the scanner screen). Also, switching from near to
far distance focus was experienced to result in double vision
and for some participants the screen appeared blurry in the
upper left corner.
All participants were very open and showed interest in
the HWD. Some of them asked questions about the device
or compared this experience with the one from Google Glass
XE. All agreed that the AR-ready HWD ODG R-7 is more
suited to the selected use case.
Based on this evaluation we reviewed our current status
and decided to change the GUI-Layout as follows:
•
Two participants were not able to identify the WiFi
Signal Strength icon, even though this icon already
is used in the GUI of the hand-held terminal. We
assume that this information is of minor priority for
the execution of the work tasks. Hence, we decided to
not implement the WiFi Signal Strength icon.
•
All ﬁve participants stated that the text-area is too big,
three even experienced it as disturbing. We decided to
move the text-area to the top, similar to the scanner
screen (cf. Figure 2 (b)).
•
Two participants stated that the upper left corner of the
screen is blurry. The manufacturer offers replacement
nose-pads for their HWD, which can be bought in
different sizes. But still, if the device is not sitting
perfectly on the nose, for example because the worker
hits it unintentionally, the corners might be blurry
again. We concluded that it is better to not display
any information in this area. Hence, we decided to
centre date, time and battery load in the status-bar.
C. Prototype
To increase to possibilities regarding AR, but also to
simplify the manipulation of AR context for future use, we
decided to use Unity Editor with the Vuforia AR SDK to
implement the application. Another reason for this selection
is Vuforia’s ‘native’ support of the selected AR-ready HWD,
which among others means that the framework has all neces-
sary speciﬁcations to set-up a suitable scene for this particular
HWD.
Figure 3. The Reticle Speed Mouse, a ﬁnger-worn input-device, with its
standard button assignment. The size of the device is about
82 mm * 27 mm * 36 mm and can be put on one ﬁnger with the ring-strap
on the backside of the device.
In previous projects, we already analysed input modalities
of HWDs and their suitability for the context of a factory envi-
ronment. In the selected environment, there is a moderate noise
level randomly interrupted e.g., by ﬂoor-borne vehicles passing
by. This prevents using voice commands without directed
microphones and additional noise cancelling. Also, we did not
want to intervene with the cooperative work environment by
121
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

forcing the workers to talk to devices instead of communicating
with the other workers from their group. Finally, most voice
command frameworks use servers to analyse the spoken text
and send back the transcription. This would require to enable
internet access for the HWDs and therefore allow the operator
of these servers unrestricted access to any voice close to
the HWDs. Due to privacy and conﬁdentiality concerns this
currently is not an option.
We also evaluated hand-gesture detection frameworks and
concluded that even in the selected environment with very
consisted lighting conditions the tracking was too unstable to
guarantee a sufﬁcient detection rate. We assume that with a
camera-technology which is able to detect depth information
the tracking would be stable enough. Unfortunately, the se-
lected HWD ‘only’ has a high resolution colour camera. Fur-
thermore, non-visual gesture-tracking technologies detecting
the muscle tension are available, for example the Thalmic Myo
armband. Based on our experience the stability of the tracking
with these technologies depends on too many variables (e.g.,
the skin resistance) to use it in this set-up.
(a) Front of the ODG R-7 with the touch-pad. Between the lenses is the
camera and above a ﬂash-led.
(b) Bottom view of the ODG R-7.
Figure 4. The standard button assignment of the used HWD.
Finally, we decided to work with the buttons and the touch-
pad of HWD and a ‘Clicker’, a ﬁnger-worn input-device (cf.
Figure 3). We are aware of the potential disruptive character
of the Clicker, but compared to a hand-held it does not occupy
the whole hand and here it serves as mean to compare direct
interaction and remote interaction with the HWD.
Both devices (the Clicker and the HWD) offer a touch-
pad (clickable), a back-button, a menu-button and buttons for
increasing and decreasing volume. In addition the Clicker of-
fers 4 quick-launch buttons and a 9-Axis Inertial Measurement
Unit (IMU) for 3D gesture detection. These gestures usually
require sweeping motions to be detected stably, which is not
possible e.g., when sitting on the drivers seat, so we excluded
this option. Furthermore, we decided to use the same ﬁve
buttons on both devices. While a touch-pad click will mark
a task as ‘OK’, clicking the back-button will mark it as ‘not
OK’ and the menu-button will repeat the task. We assigned
all buttons in the order of appearance on the GUI (top to
down). As shown in Figure 3, the selected buttons on the
Clicker are not labelled and as shown in Figure 4 (a) and
Figure 4 (b) the labels on the HWD are not visible to the user
when wearing the device. Hence, we expect that overwriting
their functionality with new ones is not critical. Especially, for
the back-button, which can be misinterpreted as repeat button,
this is important. Furthermore, the workers will use the device
with the developed application only, so they are not confronted
with another assignment of these buttons.
(a) Initial screen. The worker is informed about the key assignment and
can start the functional check by pushing the touch-pad either on the HWD
or on the ﬁnger-worn input-device.
(b) Cancel screen. The worker is asked if he/she really wants to cancel.
He/she can conﬁrm by pushing the touch-pad or undo by pushing the
back-button either on the HWD or on the ﬁnger-worn input-device.
Figure 5. Further Screens of the App-prototype.
We decided to use long click on the menu-button (‘repeat’)
for cancelling the task (as depicted in Figure 7). Furthermore,
we recognised that we never need the scan function when a
functional check is performed and also do not need the mark
as ‘OK’/‘not OK’ function outside of the checks. We decided
to reuse the menu-button for scanning.
122
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

TABLE III. RESULTS OF THE EVALUATION OF THE PROTOTYPE. a
Question
P1
P2
P3
P4
Do you understand the interaction concept of the HWD on the start
screen?
nob
yes
yes
yes
Do you understand the interaction concept of the ﬁnger-worn input-
device on the start screen?
yes
yes
yes
yes
During automatic tests: Do you see all icons and understand what they
mean?
yes
yes
noc
yes
Can you cancel the tests by long pressing the ‘repeat’-button?
yes
yes
yes
yes
AR: Do you see the green arrow?
yes
yes
yes
yes
AR: Does the arrow point on the right element?
nod
nod
nod
nod
AR: Is the arrow displayed double?
yes
yes
yes
yes
Please rank which of the devices you would prefer working with. e
2,3,1
3,2,1
1,2,3
3,2,1
a P = participant, b ”Problems to ﬁnd ‘repeat’-button”, c ”Wearer of corrective glasses, Field of View of the HWD too low”, d ”Only after closing the left
eye”, e ”1 = hand-held terminal, 2 = AR-ready HWD, 3 = AR-ready HWD + hand-held controller
To ﬁnish the program ﬂow, we added two screens: One
informing the workers about the key assignment and enabling
them to start the tests (cf. Figure 5 (a)) and the other one to
conﬁrm cancellation of the test (cf. Figure 5 (b)).
As stated above, we also wanted to explore the potential of
AR in this set-up. As a ﬁrst step we selected four buttons of
the dashboard (Figure 6). These have been selected, because
they are standard elements of the passenger compartment.
Hence, they are available in all vehicles manufactured in this
manufacturing line.
We experimented with the tracking capabilities of the
Vuforia Augmented Reality SDK. Our ﬁrst attempt was to take
a photo of the selected buttons and use it as a so called image
target for the image feature detection algorithm of the SDK.
Due to the mainly black appearance, we did not manage to get
stable detection, even when using a high resolution camera.
Second, we dismounted the whole dashboard element to scan
it with the Vuforia Object Scanner Smart phone application,
which generates a point cloud. Because we only needed the
front of the element and we were not able to remove unneeded
points we could not apply this technology as well. Last loading
a 3D-Object provided by the car manufacturer currently is not
possible with the selected framework. Hence, we decided to
put an image target (‘Vuforia-Stones’) above the buttons and
adjust the position of the arrow to point on each button as
shown in Figure 7. We are aware that this is not an applicable
solution for everyday business, but it gives an impression of
what AR-support might look like in future systems.
V.
EVALUATION
To evaluate the prototype, we focussed on three factors.
First, we wanted to investigate if the GUI and its elements,
especially those we changed, are interpreted correctly. Second,
the selected input concept could not be tested with workers by
now, even though it is an important factor. The focus here is
on the interaction with the HWD and the ﬁnger-worn input-
device and whether the introduction of the key assignment on
the start screen (cf. Figure 5 (a)) is sufﬁcient or not. Third, we
simulated an actual functional check to allow testing whether
the selected HWD is suitable for supporting workers. Also, we
wanted to test how the workers respond on the AR support.
We simulated one functional check in the reworking area.
Four workers from the initial start-up and functional check
division volunteered, two of them were already participating
in the design evaluation-phase. We introduced the HWD and
the general usage, especially to those participants who did not
Figure 6. The selected element of the passenger compartment: Buttons in the
dashboard.
work with it before, followed by the purpose of the evaluation
and the planned procedure. Also, we informed the participants
that all personal data will be treated conﬁdently and that they
can cancel the evaluation whenever they want.
The evaluation was split in two parts. In the ﬁrst part, we
handed out the HWD only and asked the participants to put
it on. They were asked to describe how they understand the
instructions (i.e., the key assignment) shown on the start screen
and then start the functional check (by pushing the touch-pad).
While the automatic tests were simulated the participants were
asked to describe the elements on the screen to verify that
they could identify all of them correctly. When a manual or
collaborative task was displayed the participants were asked
to start performing it. The participants could not perform
the tasks completely as they were working on a real vehicle
which already was tested and set up completely. When an AR
supported screen was displayed we furthermore investigated
the visibility of the AR content (arrow, cf. Figure 7). In the
second part, we gave the Clicker to the participants and asked
them to do the test again. We did not further investigate on
the GUI or the AR feature. The purpose of part two only
was to experience the usage of the ﬁnger-worn input-device
(representing ”close to the body-interaction).
The results are listed in Table III. Summed up, we found
out that the selected HWD might cause problems for spectacle
wearers. This is a very common problem with this kind of
devices as two glasses have to ﬁt on each other. Especially,
wider spectacle frames lead to problems. Furthermore, all par-
ticipants could not instantly perceive the 3D object correctly.
They saw it duplicated and on the wrong position. Based on
our own experience this issue is solved after a longer period
of use, when the users get used to this new experience. It is
important to be aware that this also temporarily can cause eye
123
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

pain, headaches and other symptoms due to the strain on the
eyes, which should be taken serious. The wrong positioning
also can be a result from displacing the image target slightly.
In a short discussion with each participant, directly after
ﬁnishing the two parts, we asked for their impressions and
points of critique. Also, we asked them for ranking which
of the three possibilities they would prefer for their daily
work: The hand-held terminal, the HWD or the HWD with
the Clicker. The ranking is shown in Table III. Three of four
participants prefer the HWD or the combination of HWD and
ﬁnger-worn input-device over the Hand-held terminal. Only
participant 3 prefers the hand-held terminal, most likely due
to the issues he/she had with the HWD and his/her corrective
glasses, which he/she emphasised in the following discussion.
As in previous projects (with other HWDs) the participants
pointed out that having both hands free to perform their
tasks is a great facilitation. Furthermore, we got the following
feedback: All participants stated that the display is too bright.
Figure 7. An overlay from an App screen-shot on a picture of the used
image target (‘Vuforia-Stones’) attached to the vehicle’s dashboard, to
demonstrate what the worker sees through the HWD.
One participant suggested to not use background colours but
only colouring the test white and yellow and reducing the
button size. One participant suggested to move the keys to the
left temple of the HWD. One participant was concerned about
confusing the OS-‘back’-button and the application-‘repeat’-
button, i.e., the key assignment. About the ﬁnger-worn input-
device one participant said that might damage the car and
another participant stated that pushing a button unintentionally
is very likely. Two of the four participant stated that the image
of the HWD dangles when walking. Also, two of the four said
using the device tires the arms and puts a high strain to the
nose and head, due to its high weight of about 175 g. This
is why the participants doubt that they can use the HWD a
whole work shift (8 h). One participant suggested to try head
gestures to interact with the HWD.
VI.
CONCLUSION
In this project, we developed an application for work
instruction and documentation in the initial start-up and func-
tional check division of a car manufacturing line of a big
German car manufacturer. This prototype included an AR
feature simplifying the guidance for the workers. We followed
the Human Centred Design Approach as deﬁned in ISO 9241
- 210 [21]. The prototype was evaluated by four participants
working in this division. The aim of the project was to develop
a proof-of-concept. A study with more participants is pending
to gather quantitative data verifying the presented results and
potentially reveal further perspectives on the prototype.
While the design of the application was easy to understand
for all participants, some problems with the selected HWD
occurred. Wearers of corrective glasses can be limited in the
usage of this device. Also, we found out that while two-
dimensional GUIs do not cause bigger problems for the users,
three-dimensional objects (for the AR feature) could not be
perceived adequately. All four participants stated that the
selected object is not placed correctly and reported double
vision. During our work with the device, we found out that
these symptoms disappear when the eyes learn to focus on
the objects, but this risks causing eye strain. Except one
participant, who struggled with the combination of the HWD
and his/her own glasses, all participants selected the HWD over
the hand-held terminal. Two out of these three participants
preferred input via an additional ﬁnger-worn input-device
while one preferred input via the buttons of the HWD itself.
Participants who already took part in the evaluation of
a HWD not capable of supporting AR in the same division
stated that the AR-ready HWD is more suited for this use
case. Also, even though all four participants struggled with
the AR feature and limits of the selected AR framework all
participants responded positively to this feature.
Furthermore, testing the ﬁnger-worn input-device gave us
some insights on potentials and limits of ”close to the body”-
interaction in this use case: If tailored to the use case, inter-
action with different types of systems could be facilitated. To
not disrupt work-ﬂows the interaction either should not require
any hardware to be worn, be based on hardware which can be
worn unobtrusively or be in-cooperated in the workers clothes.
We aim to explore this device type for use cases like the one
described in this paper in future work.
REFERENCES
[1]
F. Lamberti et al., “Challenges, Opportunities, and Future Trends
of Emerging Techniques for Augmented Reality-Based Maintenance,”
IEEE Transactions on Emerging Topics in Computing, vol. 2, no. 4, 12
2014, pp. 411–421.
[2]
Vuzix Corporation, “Vuzix Wins SBIR Phase II Award from the Ofﬁce
of Naval Research,” Vuzix Corporation, Tech. Rep., 2014.
[3]
I. E. Sutherland, “A head-mounted three dimensional display,” in Pro-
ceedings of the December 9-11, 1968, fall joint computer conference,
part I on - AFIPS ’68 (Fall, part I), no. Fall, part I.
New York, New
York, USA: ACM Press, 1968, p. 757.
[4]
Volkswagen
AG.
Volkswagen
rolls
out
3D
smart
glasses
as
standard
equipment.
Accessed
09/01/2019.
[Online].
Available:
www.volkswagenag.com/en/news/2015/11/3D smart glasses.htm
[5]
Opel
Automobile
GmbH.
Smart
Helpers.
Accessed
09/01/2019.
[Online]. Available: opelpost.com/02/2017/smart-helpers/
[6]
G. Meixner, N. Petersen, and H. Koessling, “User interaction evolution
in the SmartFactoryKL,” Proceedings of the 24th BCS Interaction
Specialist Group Conference, 2010, pp. 211–220.
[7]
M. Kleiber, T. Alexander, C. Winkelholz, and C. M. Schlick, “User-
centered design and evaluation of an integrated AR-VR system for tele-
maintenance,” in 2012 IEEE International Conference on Systems, Man,
and Cybernetics (SMC).
IEEE, 10 2012, pp. 1443–1448.
[8]
S. Rauh, D. Zsebedits, E. Tamplon, S. Bolch, and G. Meixner, “Using
Google Glass for mobile maintenance and calibration tasks in the AUDI
A8 production line,” in 2015 IEEE 20th Conference on Emerging
Technologies & Factory Automation (ETFA), vol. 2015-Octob.
IEEE,
9 2015, pp. 1–4.
124
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

[9]
F. Loch, F. Quint, and I. Brishtel, “Comparing video and augmented
reality assistance in manual assembly,” in Proceedings - 12th Interna-
tional Conference on Intelligent Environments, IE 2016. IEEE, 9 2016,
pp. 147–150.
[10]
J. Servan, F. Mas, J. L. Menendez, and J. Rios, “Using augmented
reality in AIRBUS A400M shop ﬂoor assembly work instructions,” in
AIP Conf. Proc. 1431.
the American Institute of Physics, 2012, pp.
633–640.
[11]
G. M. Re and M. Bordegoni, “An Augmented Reality Framework for
Supporting and Monitoring Operators during Maintenance Tasks,” in
Virtual, Augmented and Mixed Reality. Applications of Virtual and
Augmented Reality: 6th International Conference, VAMR 2014, Held as
Part of HCI International 2014, Heraklion, Crete, Greece, June 22-27,
2014, Proceedings, Part II, R. Shumaker and S. Lackey, Eds.
Cham:
Springer International Publishing, 2014, pp. 443–454.
[12]
P. Tschirner, B. Hillers, and A. Graser, “A concept for the application
of augmented reality in manual gas metal arc welding,” in Proceedings.
International Symposium on Mixed and Augmented Reality.
IEEE
Comput. Soc, 2002, pp. 257–258.
[13]
D. Kerpen et al, “Effects of cyber-physical production systems on
human factors in a weaving mill: Implementation of digital working
environments based on augmented reality,” in 2016 IEEE International
Conference on Industrial Technology (ICIT). IEEE, 3 2016, pp. 2094–
2098.
[14]
I. Wijesooriya, D. Wijewardana, T. De Silva, and C. Gamage, “Demo
Abstract: Enhanced Real-Time Machine Inspection with Mobile Aug-
mented Reality for Maintenance and Repair,” in 2017 IEEE/ACM
Second International Conference on Internet-of-Things Design and
Implementation (IoTDI), Pittsburgh, PA, USA, 2017, pp. 287–288.
[15]
I. Maly, D. Sedlacek, and P. Leitao, “Augmented reality experiments
with industrial robot in industry 4.0 environment,” in IEEE International
Conference on Industrial Informatics (INDIN). IEEE, 7 2017, pp. 176–
181.
[16]
W. Friedrich, “ARVIKA-augmented reality for development, production
and service,” in Proceedings. International Symposium on Mixed and
Augmented Reality.
IEEE Comput. Soc, 2002, pp. 3–4.
[17]
M. Eissele, O. Siemoneit, and T. Ertl, “Transition of Mixed, Virtual,
and Augmented Reality in Smart Production Environments - An Inter-
disciplinary View,” in 2006 IEEE Conference on Robotics, Automation
and Mechatronics.
IEEE, 12 2006, pp. 1–6.
[18]
V. Paelke, “Augmented reality in the smart factory: Supporting workers
in an industry 4.0. environment,” in Proceedings of the 2014 IEEE
Emerging Technology and Factory Automation (ETFA). IEEE, 9 2014,
pp. 1–4.
[19]
R. Poelman, O. Akman, S. Lukosch, and P. Jonker, “As if being there,”
Proceedings of the ACM 2012 conference on Computer Supported
Cooperative Work - CSCW ’12, no. 5, 2012, p. 1267.
[20]
T. Mentler, H. Berndt, and M. Herczeg, “Optical Head-Mounted
Displays for Medical Professionals: Cognition-supporting Human-
Computer Interaction Design,” Proceedings of the European Conference
on Cognitive Ergonomics, 2016, p. 26:126:8.
[21]
Ergonomics of human-system interaction – Part 210: Human-centred
design for interactive systems. ISO 9241-210:2010.
Geneva, Switzer-
land: ISO, 2010.
125
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

