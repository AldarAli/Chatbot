Change your Perspective: Exploration of a 3D Network created from Open Data in an
Immersive Virtual Reality Environment
Nico Reski and Aris Alissandrakis
Department of Media Technology
Faculty of Technology
Linnaeus University
V¨axj¨o, Sweden
Email: {nico.reski,aris.alissandrakis}@lnu.se
Abstract—This paper investigates an approach of how to naturally
interact and explore information (based on open data) within
an immersive virtual reality environment (VRE) using a head-
mounted display and vision-based motion controls. We present the
results of a user interaction study that investigated the acceptance
of the developed prototype, estimated the workload as well as
examined the participants’ behavior. Additional discussions with
experts provided further feedback towards the prototype’s overall
design and concept. The results indicate that the participants were
enthusiastic regarding the novelty and intuitiveness of exploring
information in a VRE, as well as were challenged (in a positive
manner) with the applied interface and interaction design. The
presented concept and design were well received by the experts,
who valued the idea and implementation and encouraged to be
even bolder, making more use of the available 3D environment.
Keywords–human-computer interaction; virtual reality; immer-
sive interaction; information visualization.
I.
INTRODUCTION
Virtual reality (VR) is not a particularly new research area
[1], however having a fully immersive VR experience required
expensive equipment and elaborate maintenance in the past [2].
Touch input has introduced a natural way of interacting with
digital content, because it is the most intuitive way for humans
to interact with their environment [3]. Through head tracking
technologies and systems like head-mounted displays (HMD),
VR has the potential to achieve a similar effect regarding
naturally handling digital content for human vision [2]. Putting
yourself into a virtual reality environment (VRE) and thus
directly inside things, dramatically changes how you feel,
completely shifting your perspective in a break of tradition
[3][4]. Therefore, there is a lot of potential regarding the
visualization of digital content within a 3D environment suited
for immersive VR [5].
Information visualizations are used by humans to gain
insights and acquire an understanding of data in a more
comfortable and easier way compared to, e.g., reading infor-
mation in text format [6][7]. This is attributable to the human
cognitive capabilities to perceive information visually, interpret
the graphical representation of data, and infer meaning [7].
However, humans usually perceive digital information in a very
limited way, through the small and ﬁxed window of a computer
screen while using counter-intuitive 2D tools, such as keyboard
and mouse, to interact [3][8]. A combination of VR as natural
user interface (NUI) for human vision, and motion controls
(particularly hand tracking [2]), as NUI for the interaction with
digital information, could deliver an immersive and natural
user experience. This is a not a trivial task and deserves deeper
investigation in terms of the interplay of these technologies,
as well as the challenges of crafting interaction and interface
design for presenting digital data in a VRE.
In order to address these challenges and to examine the
suitability of the use of immersive VR technologies in the
context of interactive information visualization (InfoVis), we
have identiﬁed a number of relevant research questions:
RQ1
Is immersive virtual reality suited for exploration of
open data and content retrieved from the web?
RQ2
Is the usage of head-mounted display technologies and
(vision-based) motion controls suited to work together,
and can they beneﬁt from each other in order to create
a natural user experience?
In this paper, we present our approach to explore open
data in an immersive VRE. We designed and implemented a
VR prototype using a HMD and vision-based motion controls,
to visualize and enable the user to explore a network of
European capital cities, based on open data obtained from
DBpedia. The remaining of this paper is organized as follows:
Section II describes the foundations (including the concept
and design approach) and a brief overview of the technical
implementation. Related work is described in Section III.
Details about our applied research methodology are provided
in Section IV. In Section V, we provide the results of our
conducted studies. Section VII presents our conclusions and
proposes possible lines of future work.
II.
FOUNDATIONS
Immersive VR enables its users to visually perceive
computer-generated content as if it was real [2]. For VR
systems, HMDs have usually a closed view in a non-see-
through mode; consequently, the user is visually isolated from
the real world, completely surrounded by computer-generated
content. Using head- and even body-tracking, the user is able to
naturally look around and explore the three-dimensional virtual
scene according to the real world paradigms [9][10].
Vision-based motion controls, sometimes also referred to
as vision-based gesture recognition, have the potential to be
a powerful tool in order to support the user with interaction
and manipulation in the 3D space without the need to hold
physical sensor devices [11]. As the preﬁx “vision-based” indi-
cates, technologies following this approach usually use one or
403
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-468-8
ACHI 2016 : The Ninth International Conference on Advances in Computer-Human Interactions

multiple cameras or infrared sensors to visually recognize the
user’s hand or body movements, and translate these movements
accordingly into the digital space [11][12][13]. As the most
expressive form of human communication, gestures have the
potential to be useful in HCI as their application goes directly
in line with the concept of NUI [14].
The interaction between humans and computers represents
the heart of modern InfoVis [7]. Especially through the grow-
ing amount of collected data, tools of different nature (from
artistic to analytical to mixtures of both) have been developed
in order to provide a novel perspective to our surroundings, and
InfoVis is considered essential in making data more accessible
to a broader audience [15]. Within the area of HCI, particularly
visual analytics deal with questions regarding how to combine
and integrate strengths of both humans and computers into
creative and interactive mechanisms to interpret and extract
knowledge [16]. In the community, the use of 3D in InfoVis
is regarded with skepticism, but in certain cases, such as for
spatial layouts, 3D visualizations can provide advantages over
traditional 2D. However, since moving from 2D to 3D can
be expensive in various ways, it needs careful justiﬁcation
and the beneﬁts must outweigh the costs. If a meaningful
3D representation implicitly exists within the dataset (e.g.
airﬂow, skeleton, location), a 3D visualization approach is
easily justiﬁable. If the spatial model and layout is rather
chosen than given, it is the visualization designer’s task to
carefully map values and relations of the dataset’s items to
appropriate variables within the 3D space [17].
A. Concept and interaction design
While VR using HMDs has possibilities to provide a
visual NUI, approaches to create a NUI for user input in
combination with VR had to be considered to create a natural
user experience. Vision-based motion controls enable manip-
ulations of virtual objects to a realistic degree [11]. This is
particularly important since the user will be visually isolated
of the real world surroundings when wearing a HMD [14]. The
applied prototype scenario was conceptualized around these
technologies.
The identiﬁcation of a concrete user scenario and conse-
quently designing towards exactly that scenario is one of the
most important considerations in the process of crafting an
interactive InfoVis [15]. Keeping the conducted foundational
research about VR in mind, the idea to build a network of
different nodes within the 3D space came up, and a user
scenario with the purpose of exploring available open data
about European capital cities within an immersive VRE was
identiﬁed. Since VR has the potential to drastically change the
user’s perspective [3], the thought of putting the user “inside”
the InfoVis came to mind early in the design process. In
more traditional InfoVis approaches, the user is put “outside”
the visualization quite often, providing an overview about the
data in a role of an observer. With the HMD VR technology
and the available 3D virtual space at hand, the change of the
user’s perspective to the inside rather than to the outside of
the InfoVis seemed attractive and novel to pursue. Features to
enable the user to explore the network of nodes itself as well
as each individual node in more detail had to be implemented.
Two options to implement the exploration of the network
were considered: enabling the user to move completely free
without any restrictions within the VRE, or restricting the user
movement to traversing between the nodes. Since there are
known cases of intensiﬁed motion-sickness when letting the
user move freely around in the VR space (without physically
moving in the real world) [18], the decision was made to
restrict the user movement to only traverse between nodes.
Wikipedia was identiﬁed to serve as the data source. A
practical look at Wikipedia articles about European capital
cities provided the idea to request data items within an article’s
“infobox” (e.g. country, geolocation, area, population, images)
using DBpedia’s available API and its semantic layer [19].
The concept of ﬁltering is a common interaction technique in
InfoVis [6]. Therefore options to ﬁlter all European capital
cities towards their area and population were introduced to
support users in their exploration of the network.
Ultimately, the conceptual interaction and interface design
of our VR prototype can be summarized to contain the
following key features:
•
Present open data about European capital cities re-
ceived from Wikipedia (through DBpedia).
•
Each capital is represented by an individual node in
the 3D space.
•
Put the user perspective inside the network visualiza-
tion rather than looking at it from the outside.
•
Automatic traversal for node-to-node movement, no
“free” movement in the 3D space outside the node
network.
•
Two view modes: exploration and content view.
◦
Exploration view: present minimal information
about each node, explore the entire network.
◦
Content view: present detailed information
about one node, explore the current node.
•
Filter options to display connections to cities with
higher/lower area or population.
•
Scale the sizes of the nodes’ 3D models to proportion-
ally represent the cities’ individual area and population
in respect to all others in the network (making them
visually comparable).
•
Swipe gesture interaction to initiate movement to other
nodes and to trigger/dismiss exploration or content
view mode.
•
Interactive 3D GUI to operate ﬁlter and scaling op-
tions.
•
Visualization of the user’s hands, and providing visual
feedback for gesture interactions.
B. Implementation of the prototype
The developed prototype can be divided into two parts:
a) a server and database and b) the VR application. Figure 1
illustrates the overall system architecture.
Server and database (see Figure 2) are based on Node.js
and MongoDB, completing the following tasks:
1)
Queries data items of a given European capital city
from an open data source (DBpedia).
2)
Persistently stores the queried data items in a struc-
tured and uniformed way.
3)
Provides access to the stored data items, prepared for
visualization.
404
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-468-8
ACHI 2016 : The Ninth International Conference on Advances in Computer-Human Interactions

Figure 1. System architecture
Figure 2. Server and database overview
The visualization engine and thus the complete software
development of the VR application is based on the cross-
platform development system Unity. The Oculus Rift DK2
was used as HMD, while a Leap Motion controller (attached
in front of the HMD) served as the vision-based motion
controller.
The
implemented
VR
application
can
be
divided
into
seven
main
GameObjects.
The
NetworkVisPlayerController GameObject handles
all tasks related to the application’s user, such as providing
instances of the VR camera and the Leap Motion controller
as well as handling the user interaction and visual feedback
upon
successfully
recognized
gesture
interaction.
The
NetworkCreator
GameObject
is
responsible
for
keeping
track
of
the
network’s
current
status
and
is
able to dynamically manipulate the nodes of the network
according to the user’s input. Furthermore it is able to
receive information about the European capital cities from
the complementary server and database and to instantiate
a
VR-NetworkNode
GameObject
for
each
city.
The
NetworkVisualization
GameObject
is
a
wrapper
for holding all VR-NetworkNode GameObjects together,
ultimately taking care of rendering and representing all nodes
within the network. The EuropeMapLayer GameObject
presents an underlying outline of all European countries to
the user. Responsible for rendering detailed information about
a network node is the VR-NetworkNode-ContentView
GameObject,
displaying
description,
numerical
data
as
well as images about the city on three 2D content planes
located in front of the user as well as to the left and
right. The interactive 3D GUI presenting the different ﬁlter
options is handled by the VR-FilterExplorationView
node network
hands
notiﬁcation
content left
content center
content right
ﬂipping image
ﬁlter menu
button push
Figure 3. Multiple screenshots of the VR application
GameObject. Furthermore, the VR application features a
LoggingSystem GameObject, keeping track of every user
interaction by writing detailed information in a .csv ﬁle,
which can later be analyzed.
Figure 3 illustrates multiple screenshots, giving a visual
impression of the created VR application. The video at [20]
demonstrates the features of the developed prototype.
III.
RELATED WORK
Donalek et al. [21] recently described their initial explo-
rations of ﬁnding optimal use practices for the application of
immersive VR as a platform for interactive and collaborative
data visualization and exploration. They focused on the visu-
alization of highly dimensional data based on large digital sky
surveys that can be represented as abstract feature vectors and
experimented with different approaches of multidimensional
data representation in VR [21]. Design challenges for a 2D
gesture interface in a 3D VR environment have been explored
by Lee et al. [22], who developed and evaluated an immersive
VR game, concluding that cross-dimensional interfaces may
signiﬁcantly reduce simulator sickness. Ren and O’Neill [13]
investigated 3D selection with freehand gestures and propose
overall design guidelines. According to their results, designers
should consider mapping 3D hand movements to 2D inter-
action for simpler user interfaces with few elements [13].
Abrash [4] argues that immersion can make VR interesting,
but great VR requires custom software. Bayyari and Tudoreanu
[5] investigated perceptual and interaction characteristics of
immersive displays and conclude that they support better
understanding of data and are thus superior to traditional
desktop visualization.
405
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-468-8
ACHI 2016 : The Ninth International Conference on Advances in Computer-Human Interactions

IV.
METHODOLOGY
The purpose of our investigation is to answer the questions
of whether and how appropriate the application of immersive
VR is in order to visualize and naturally explore content
from the web. Our research approach has rather deductive
characteristics [23].
We carried out a user interaction study to gain insights
and real experiences about the design and operation of the
developed prototype in practice. It was of particular interest
to gather the user’s thoughts about their ability to naturally
interact with the visualized content and explore the data
network. We conducted eleven one hour sessions between the
researcher and each participant. The six female and ﬁve male
participants, aged between 16 and 32 years, were introduced
to the prototype by watching a pre-recorded video [20] that
demonstrated the features of the developed prototype.
The eleven participants were given two tasks: to ﬁnd a city
with a population (or area) of a certain number of inhabitants
(or square kilometers), using all the features of the prototype
such as the ﬁlter options, the movement functionalities as well
as receiving more detailed information about a visited city in
network node. They were encouraged to explore the visualized
network and let the researcher know once they found a city that
they considered to satisfy the request of each task. Before their
sessions, each participant had the chance to become familiar
operating the developed prototype in order to understand the
basic functionalities as well as getting used to wearing a HMD
and using the vision-based motion controls. Based on the
participant’s prior experience with the immersive technologies,
this warm-up phase took no longer than ﬁve to ten minutes.
There were no time constraints regarding the completion of the
tasks and the participants were asked to operate the prototype
at their own speed. However, the completion of a single task
should not exceed ten minutes.
A mixture of both quantitative and qualitative data collec-
tion methods were used. Self-constructed pre- and post-session
questionnaires, featuring a mixture of Likert-scale and open
questions, were used to examine the participants prior experi-
ences with the presented technologies as well as examining the
developed prototype in more detail. To gain focused feedback,
the post-session questionnaire was structured into the following
evaluation categories: Perception of the content generated with
data from the web, Spatial perception of my location within
the 3D network, Interaction using the vision-based motion
control interface and Human factors and ergonomics. The
implementation of a log-ﬁle system enabled us to evaluate all
interactions of the user with the prototype, e.g., metrics such as
the time needed to complete a task, the time spent on individual
nodes (cities) and the amount of user interactions applied to
complete a task. Each study was closely observed by the
researcher, who kept written notes. After completing the tasks,
each participant was asked to estimate the felt workload, based
on the Task Load Index (TLX) workload estimation developed
by NASA [24][25]. Receiving insights about a participant’s
workload helps to analyze and estimate the interaction and
interface design, providing indications if the participants felt,
e.g., bored, neutral or overburdened within the VRE.
Additionally, experts within the ﬁeld of HCI and (interac-
tive) InfoVis were selected from within the staff at the Faculty
of Technology at Linnaeus University and invited to partic-
ipate in approximately one hour long discussions between
Figure 4. Participants interacting within the developed VRE
the researcher and the experts. Within these discussions, the
researcher presented the concept, idea and motivation of the
conducted work to the experts as well as information about the
identiﬁed problem domain, scenario and research questions.
The researcher conducted also a live walkthrough presenting
the developed prototype to the experts, during which they
were encouraged to communicate their feedback, comments
and thoughts towards the project. They were furthermore
encouraged to not only think about the presented scenario,
but to think further, more abstract towards similar interactions
or activities in similar cases.
V.
RESULTS AND ANALYSIS
This section presents the results of the conducted user inter-
action study and expert discussions as described in Section IV.
Figure 4 presents some impressions of the participants during
the user interaction study.
A. User interaction study
The participants were asked to ﬁnd a European capital city
close to 1.5 million inhabitants during the ﬁrst task, and one
featuring an area of close to 750 km2 during the second task.
Valid result sets for both tasks were identiﬁed beforehand and
are illustrated in Table I. All participants had to start from the
node representing V¨axj¨o (where Linnaeus University is based).
As shown in Table II, two participants did not name a
solution within the set of valid results for task 1 (they chose
Copenhagen and Soﬁa instead), but all eleven participants
correctly identiﬁed a city close to the asked parameter in
task 2. Additionally, it can be observed that the majority of
participants were able to name a solution rather close to the
asked parameter within both tasks.
Utilizing the implemented log ﬁle system, a detailed anal-
ysis of the participants’ interactions was undertaken. Figure 5
gives a summary for all participants, including both average
values and standard deviation for tasks 1 and 2. Comparing
the two tasks, it is interesting to note that the averages of
the time spent in a traveled city, as well as the amount of
visited and uniquely visited cities, do not show signiﬁcant
differences, while it is noticeable that the overall amount of
interactions increased and the overall amount of time needed
for the task completion decreased within task 2. It also appears
that the participants interacted more heavily with the ﬁlter
related features during task 2. The task order was the same for
all participants, therefore this expected outcome would indicate
increasing conﬁdence and familiarity with the interface.
406
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-468-8
ACHI 2016 : The Ninth International Conference on Advances in Computer-Human Interactions

TABLE I. SET OF CITIES IDENTIFYING SUCCESSFUL TASK SOLUTION
Task 1
Task 2
City name
Inhabitants (in million)
Vienna
1.724
Budapest
1.722
Warsaw
1.717
Belgrade
1.339
Prague
1.249
City name
Area (in km2)
Berlin
892
Kiev
839
Zagreb
641
Madrid
605.8
TABLE II. PARTICIPANT TASK SOLUTIONS (CITIES ESPECIALLY CLOSE TO
THE ASKED PARAMETER ARE MARKED IN BOLD)
Task 1
Task 2
City name
Count
Vienna
1
Budapest
1
Warsaw
1
Belgrade
5
Prague
1
correct answer
9 / 11
other / incorrect
2
City name
Count
Berlin
2
Kiev
3
Zagreb
6
Madrid
0
correct answer
11 / 11
other / incorrect
0
Figure 5. Results - Summary of log ﬁles for both tasks
B. NASA Task Load Index
The estimated workload of all participants averages at 47.5
percent with a standard deviation of 17.6. Excluding the two
outliers (that estimated their workload signiﬁcantly lower and
higher than everyone else, respectively) the workload average
does not differ signiﬁcantly at 48.54 percent but with a lower
standard deviation of 11.2.
Looking at the participant’s determined weights and ratings
of the individual TLX factors mental and physical demand
as well as performance and effort were rather important to
the participants’ experience of workload, while the factors
temporal demand and frustration were of less importance
during the task completion. The participants experienced com-
paratively higher mental and physical demands as well as the
need to make efforts in order to complete the given tasks
within the VRE. The average ratings also showed that the
participants felt a comparatively low level of both temporal
demand and frustration. Furthermore, the low performance
value corresponds to a good level of performance, indicating
that the participants felt rather successful and satisﬁed with
their accomplishment by completing the tasks. It is particularly
noteworthy that the participants were not given any indication
about the success or failure of the task completion and their
given answer.
C. Post-session questionnaire (PTQ)
Perception of the content generated with data from the
web: The participants felt throughout rather positively about
the perception of the presented content, which was ultimately
generated with data received from the web. The participants
valued the amount of presented information about the cities
within the VRE. The textual description of a city was rated
not too long nor too short. Together with providing important
facts (area, population, country) in bullet point format, the
designed interface enabled the participants to get a quick
and pleasant rated city overview. Additionally, the participants
liked that not only textual data were presented to them, but
also photos about the cities, asking even to provide more
images in the future. Overall, with displaying content on
2D planes in front as well as to the user’s left and right
(see Figure 3 middle row), the layout and the animation
of the city data (VR-NetworkNode-ContentView) was
perceived positive. Some participants commented that they felt
“closer to the information (and its value)”.
Spatial perception of my location within the 3D network:
The participants considered to have had a solid understanding
about their own location within the network at all times.
Observations and answers of the open questions further indi-
cate that the participants are in line with the prior mentioned
design decision to offer node-to-node movement and no free
movement within the 3D space from an interaction point
of view. Furthermore, they expressed the supportive value
of the underlying map, indicating the importance of such
an essential cornerstone towards the visual guidance in the
presented VR prototype. The visual node to node connections
helped identifying potential targets to travel to, as a result
of applying a ﬁlter. Most negatively mentioned within this
category was the ability to change the nodes’ size according to
their area or population value and in relation to all other nodes
within the network. Through the change of perspective and
thus putting the user’s perspective inside the visualization, the
participants expressed their inability to properly compare the
nodes’ sizes, e.g., actually larger nodes in the further distance
would effectively appear smaller compared to nodes closer to
the users location due to the perspective.
Interaction using the vision-based motion control interface:
The participants enjoyed operating the VR prototype by using
gestures made through hand movements, although they had
rather neutral feelings towards a precise interaction due to
the inaccuracy of the gesture recognition. Some participants
had to try multiple times to perform a successful gesture.
The overall interaction using gestures, and thus the user’s
body, was received as fun, even fast and ﬂuent, experience.
The applied gesture set was easy to learn and remember.
The interaction with the 3D GUI was particularly enjoyable.
The implemented visual feedback, indicating the successful
performance and recognition of a gesture interaction, was
considered valuable, ultimately representing a crucial response
from the VR prototype to the user.
407
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-468-8
ACHI 2016 : The Ninth International Conference on Advances in Computer-Human Interactions

Human factors and ergonomics: Providing users with a
swivel chair proved to be a good idea, since the participants
strongly agreed that it supported them in the 360 degree
exploration of the network. Some participants expressed
concern that operating the VR prototype might get physically
too tiring or stressful after longer usage. This would need to be
further investigated in future studies. The actual visualization
of the user’s hands within the 3D space and thus the visual
translation of their physical hands to the virtual space was
mentioned positively. Since the users are not able to see their
physical hands anymore while wearing the HMD, translating
these accordingly into the virtual space seemed to be an
essential bridge between physical and virtual world.
D. Expert discussion
As mention in Section II, the use of 3D within the
InfoVis community is met with skepticism. But in case of the
implemented VR prototype and considering its intention and
purpose, the experts agreed that the presented scenario is one
of the few where 3D interactive InfoVis works ﬁne. Due to its
focus on the exploration of data, the interactive visualization
in the 3D space provides a unique perspective and thus fresh
approach towards data exploration and browsing. The experts
also stated that the developed VR prototype and its scenario is
probably quite valuable as a learning tool, arguing that children
learn from their 3D environment. Therefore being surrounded
by the content instead of looking at it from above is potentially
beneﬁcial in this context. Although the implemented ﬁlter
options work ﬁne within the presented scenario, encouraging
the user to play around, explore and move around as much
as possible, the experts state that for “real” data exploration
these are not enough and need to be assisted by the ability of
putting numbers into the ﬁlters, e.g. using range sliders.
To avoid getting lost in larger networks, the experts sug-
gested to implement features to support additional guidance or
navigation, such as an isometric or aerial perspective. For fu-
ture work the experts also suggested to support and implement
features to select, highlight (e.g. glowing effects) and compare
multiple nodes within the network. The implementation of
concepts such as a radar, a minimap, or a “quest marker”
(as known from computer games), could provide additional
guidance and navigation depending on the given task.
Although the prototype made usage of the 3D space, the
network node arrangement used 2D geometry. The experts
suggested to think about the potential of the 3D environment
to come up with effects and solutions to overcome some real
world problems. An example they provided in the particular
case of overcoming occlusion within a 2D arrangement was
to investigate the implementation of a “world bending” effect,
similar to the ones shown in the movie Inception or the Halo
computer game series. The experts positively acknowledged
the visual feedback as result of recognized gestures, reasoning
it being crucial to the user. The 3D interactive GUI was
received positively as well, noting that it feels very easy to
operate the interface elements.
VI.
DISCUSSION
Although the participants had only minor experience with
VR technologies, if any, in the past, it was particularly
interesting to see their adaption to the presented setup and
Figure 6. Explorative behavior: revisiting (left) vs. straight (right)
thus the interaction with the developed VR prototype. The
participants’ answers show that they were successfully able
to solve the given tasks by exploring the 3D virtual network
and consuming the displayed data. This furthermore conﬁrms
the proper functioning of the VR prototype, both in terms
of design and implementation. Examining the log ﬁle data it
appears that the participants learned and adapted quickly to the
overall interaction, which is interesting given the comparatively
short amount of time they spent with the prototype (approx.
20 minutes, including warm-up phase and the completion of
task 1 and 2).
The implementation of a log ﬁle system for this kind of
application turned out to be an essential asset for analyses as
every recognized interaction between user and the prototype
could be tracked (see Figure 5). Particularly interesting was
the tracking of the participant’s routes, visualizing their explo-
ration while solving the given tasks. This provided some in-
sights in their behavior and even search strategy. Two types of
explorative behavior were identiﬁed, as seen in Figure 6: either
featuring an increased degree of revisiting already explored
network nodes, or going straight from one city to another
until a suitable solution was found. At this point, it is not
possible to derive concrete conclusions why sometimes each
strategy was chosen. To further investigate this matter, more
studies need to be conducted in the future, e.g., including the
presentation of the visualized pathways to the participants after
task completion and interviewing them about it.
The NASA TLX analysis revealed that the participants’
estimated workload averaged at 47.5 percent. Given the par-
ticipants’ prior experiences with VR technologies and the
novel approach of the presented VR prototype, a workload
of around 50 percent can be considered ideal, indicating
that the participants were neither bored nor overburdened
with operating the prototype, especially considering that the
majority of participants were confronted with such a setup and
scenario for the ﬁrst time.
The results of the post-session questionnaire, asking the
participants concretely about their experience with the VR pro-
totype, are encouraging as well. The presentation of the content
was accepted positively, pointing out the overall intuitiveness,
novelty and pleasantness of the designed interface. Textual
content within a 3D environment can arguably be tricky,
but within a VRE with no high resolution display devices,
users could become faster exhausted by exclusively visually
perceiving content, due to potential blurriness. Therefore, the
amount and way of how to display textual content using today’s
408
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-468-8
ACHI 2016 : The Ninth International Conference on Advances in Computer-Human Interactions

broadly available HMD devices can be considered crucial to
the overall user satisfaction (which was ranked satisfactory
in our prototype). In general, the three element layout of
putting visual content to the user’s left, right and front, in
combination with a HMD setup found practical acceptance.
The chosen movement transition, enabling users to move
exclusively from one node to another, was perceived positively
as well, especially in regard to the overall physical stationary
setup. While the users move in the virtual space, they do not
actively translate their location in the physical space. Therefore
transitional movement effects can be considered well chosen
(see also “Swivel chair” experience [26]). The users were able
to quickly learn and remember the functionalities of the VR
prototype and make use of the complete feature set. Both the
gesture and the 3D GUI interaction were perceived enjoyable
and intuitive. Both interaction mechanisms have their value
regarding the application within an immersive VRE. Whether
to use one or the other seems highly related to the functionality
or feature the interaction will embody.
Receiving the comments that the developed VR prototype
and its presented scenario can be assigned to the few cases
successfully demonstrating 3D interactive InfoVis was very
encouraging for our proof of concept. However, at the same
time the experts also argued for the importance of including
more traditional features, e.g., maps or isometric perspectives,
aerial respectively. Although the presented change of perspec-
tive for in this scenario feels fresh and novel, one should not
forget or ignore potential limitations that it might come with
it. Therefore, features to support the guidance and navigation
of the user are more important than in other scenarios and
should be considered when designing an application of that
kind. After all, humans use even in the real world maps or
other applications to assist them in their path-ﬁnding and
exploration.
VII.
CONCLUSIONS AND FUTURE WORK
The aim of this paper was to investigate how an immersive
VRE using a HMD and vision-based motion controls could be
used to let the user naturally explore and interact with content
and data received from open online accessible resources. We
presented the design and implementation of an immersive VR
prototype, ultimately enabling the user to explore a network of
European capital cities based on data from Wikipedia. In order
to gather insights, thoughts and recommendations of the user
experience while operating the implemented VR prototype,
a user interaction study and discussions with experts were
conducted to explore the relevant research questions:
RQ1: Is immersive virtual reality suited for exploration of
open data and content received from the web? The designed
and developed VR prototype, and the gathered results, indicate
that immersive VR is indeed suited for the exploration of
traditional data and content from the web. Using a HMD device
the users were able to perceive visually the content within the
3D space, while interacting using their hands. The results show
that the participants valued the enjoyment, intuitiveness and
novelty provided through the presented approach, while they
learned quickly to make full use of the prototype’s feature set,
even solving the given tasks satisfyingly. Some participants
tried to solve the given tasks very systematically, while others
seemed a bit more open in their course of action. Both
approaches of exploration ended in successful task completion.
The fact that almost all participants were encouraged out of
own will to ﬁnd a particularly good solution towards the given
tasks speaks for the acceptance and enjoyment of operating
in the presented immersive VRE and its scenario. This is in
line to the results of the NASA TLX estimation, indicating
that the participants were positively challenged with the newly
experienced environment and its technological components to
a reasonable extent. Consequently, the developed VR prototype
presents a practical approach for the overall successful design
and implementation of a) translating traditional content from
the web to a 3D environment and b) crafting an interaction
interface enabling users to explore a 3D environment and its
visualized content.
RQ2: Is the usage of head-mounted display technologies
and vision-based motion controls suited to work together
and can they beneﬁt from each other in order to cre-
ate a natural user experience? Although both technologies
presented some minor issues (experienced blurriness, low
comfortability of the HMD, inaccuracy of gesture detection),
the presented VR prototype showed that HMD and vision-
based motion controls can successfully work and, even more
importantly, complement each other. While users are wearing
a HMD, they are visually isolated from their physical environ-
ment. Designing more immersive input mechanisms is a crucial
part towards supporting the user with more natural interaction
possibilities. The study participants used the combination of
both technologies to explore the 3D virtual environment (VE)
and successfully complete their given tasks. Design approaches
of the interaction through gestures and a visible 3D GUI
were explored. Both found acceptance, indicating that inter-
action possibilities following these approaches can work in a
meaningful way within a VRE. Interactive 3D GUI elements
have the beneﬁt of being visible to the user at all times, thus
indicating and reminding the user on their functionalities. At
the same time, interaction through gesture recognition seem to
enable the user to operate fast and ﬂuent interactions. However,
these gestures need to be easy to learn and simple to remember.
Overburdening the user with too complex gestures on the one
hand and a set featuring too many gestures on the other, should
be avoided. Furthermore, through our observations it is rec-
ognizable that the combination of both technologies requires
the user to apply a higher degree of physical movements than
compared to traditional HCI setups. Consequently, the intended
duration of how long an application should be experienced
needs to be considered within the application’s design. While
the combination of HMD and vision-based motion controls
worked well in case of the presented prototype, the user
interaction study and the dialog with the experts also revealed
their curiosity towards additional (input) technologies, such as
voice recognition, 3D audio and haptic interfaces, which seem
promising to investigate in the future.
The presented prototype can be considered a ﬁrst step
and proof of concept into the direction of creating immersive
VREs that feature traditional web content. Although the basic
functionalities and visualization approaches throughout worked
satisfactory, the experts made some valid arguments for future
considerations, such as more features supporting the guidance
of the user or making more use of the unique possibilities
only a 3D VE can offer in order to overcome real world
limitations. In conclusion, it is noticeable that which design
approaches and decisions will work and which will not is
409
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-468-8
ACHI 2016 : The Ninth International Conference on Advances in Computer-Human Interactions

highly dependent on the applied scenario and use-case and
should thus be considered individually. Through its focus on
exploration, the presented prototype demonstrates one of the
few use cases, where immersive 3D VR InfoVis can be useful.
Still, some changes on the existing feature set of the
implemented prototype can be applied. The overall color
scheme could be reworked, also investigating the application
of heat map ﬁlter options in order to support the ability to
better compare network nodes using the newly presented per-
spective. Additional features to support the user’s navigation
and path-ﬁnding can include a minimap, the implementation
of a temporary isometric or aerial perspective, or even the
implementation of world-bending effects. Also, the addition
of other emerging technologies such as voice-input, 3D audio
or haptic interfaces needs to be in investigated in the context
of immersive VREs.
Furthermore, the VR prototype should be presented to a
larger audience in additional user studies in order to gain
further insights. The presented results showed that participants
rated the developed prototype as “enjoyable” and “intuitive”.
Long term studies should be performed to prove the effec-
tiveness of VR or reveal if the participants’ opinions are
just an instant reaction to a new experience. Within this
study, the duration of the participant interacting with the VR
prototype lasted approximately 20 minutes. Longer interaction
sessions and their implications could thus be investigated in
the future as some participants expressed their worries partic-
ularly towards the physical exhaustion in prolonged interaction
sessions.
REFERENCES
[1]
I. E. Sutherland, “A head-mounted three dimensional display,” in AFIPS
’68 (Fall, part I) Proceedings of the December 9-11, 1968, fall joint
computer conference, part I.
New York, NY, USA: ACM, 1968, pp.
757–764.
[2]
D. Lanman, H. Fuchs, M. Mine, I. McDowall, and M. Abrash, “Put
on your 3D glasses now,” in SIGGRAPH ’14 ACM SIGGRAPH 2014
Courses.
New York, NY, USA: ACM, Jul. 2014, pp. 12:1—-12:173.
[3]
J. Carmack, “The Engadget Interview: Oculus Rift’s John Carmack,”
Video Interview, Oct. 13, 2013, URL: https://www.youtube.com/watch?
v=AkasIFGpSHI [retrieved: January, 2016].
[4]
M. Abrash, “What VR Could, Should, and Almost Certainly Will
Be within Two Years,” Presentation at the Steam Dev Days, January
15-16, 2014, URL: https://www.youtube.com/watch?v=G-2dQoeqVVo
[retrieved: January, 2016].
[5]
A. Bayyari and M. E. Tudoreanu, “The impact of immersive virtual
reality displays on the understanding of data visualization,” in Proceed-
ings of the ACM symposium on Virtual reality software and technology
- VRST ’06.
New York, NY, USA: ACM, 2006, pp. 368–371.
[6]
M. O. Ward, G. Grinstein, and D. Keim, Interactive Data Visualization:
Foundations, Techniques, and Applications.
Taylor & Francis, 2010.
[7]
R. Spence, Information Visualization: Design for Interaction, 2nd ed.
Pearson, 2007.
[8]
SpaceX, “The Future of Design,” Video report, Sep. 5, 2013, URL:
https://www.youtube.com/watch?v=xNqs S-zEBY [retrieved: January,
2016].
[9]
D. A. Bowman and R. P. McMahan, “Virtual Reality: How Much
Immersion Is Enough?” Computer, vol. 40, no. 7, Jul. 2007, pp. 36–43.
[10]
A. Sears and J. A. Jacko, The Human-Computer Interaction Hand-
book: Fundamentals, Evolving Technologies and Emerging Applica-
tions, 2nd ed., A. Sears and J. A. Jacko, Eds.
CRC Press, 2007.
[11]
G. R. S. Murthy and R. S. Jadon, “A Review of Vision Based Hand
Gestures Recognition,” International Journal of Information Technology
and Knowledge Management, vol. 2, no. 2, 2009, pp. 405–410.
[12]
D. A. Bowman et al., “3D user interfaces: new directions and perspec-
tives.” IEEE computer graphics and applications, vol. 28, no. 6, Jan.
2008, pp. 20–36.
[13]
G. Ren and E. O’Neill, “3D selection with freehand gesture,” Computers
and Graphics, vol. 37, no. 3, 2013, pp. 101–120.
[14]
J. P. Wachs, M. K¨olsch, H. Stern, and Y. Edan, “Vision-based hand-
gesture applications,” Communications of the ACM, vol. 54, no. 2, Feb.
2011, pp. 60–71.
[15]
M. Lima, Visual Complexity: Mapping Patterns of Information. Prince-
ton Architectural Press, 2011.
[16]
D. Keim et al., “Visual Analytics: Deﬁnition, Process, and Challenges,”
in Information Visualization: Human-Centered Issues and Perspectives,
1st ed.
Springer Berlin Heidelberg, 2008, ch. 7, pp. 154–176.
[17]
T. Munzner, “Process and Pitfalls in Writing Information Visualization
Research Papers,” in Information Visualization: Human-Centered Issues
and Perspectives, 1st ed.
Springer Berlin Heidelberg, 2008, ch. 6, pp.
134–153.
[18]
S. Sharples, S. Cobb, A. Moody, and J. R. Wilson, “Virtual reality
induced symptoms and effects (VRISE): Comparison of head mounted
display (HMD), desktop and projection display systems,” Displays,
vol. 29, no. 2, 2008, pp. 58–69.
[19]
M. Kr¨otzsch, D. Vrandeˇci´c, M. V¨olkel, H. Haller, and R. Studer,
“Semantic Wikipedia,” Journal of Web Semantics, vol. 5, no. 4, 2007,
pp. 251–261.
[20]
N. Reski, “Instruction Video: Exploring data from the web in an
immersive 3D virtual reality environment,” 2015, URL: https://vimeo.
com/126973131 [retrieved: January, 2016].
[21]
C. Donalek et al., “Immersive and collaborative data visualization using
virtual reality platforms,” in 2014 IEEE International Conference on Big
Data (Big Data).
IEEE, 2014, pp. 609–614.
[22]
P.-W. Lee, H.-Y. Wang, Y.-C. Tung, J.-W. Lin, and A. Valstar, “TranSec-
tion,” in Proceedings of the 33rd Annual ACM Conference Extended
Abstracts on Human Factors in Computing Systems - CHI EA ’15.
New York, New York, USA: ACM Press, apr 2015, pp. 73–76.
[23]
D. E. Grey, Doing Research in the Real World, 2nd ed.
SAGE
Publications Ltd, 2009.
[24]
NASA, “National Aeronautics and Space Administration TLX: Task
Load
Index,”
Web
page,
URL:
http://humansystems.arc.nasa.gov/
groups/TLX/ [retrieved: January, 2016].
[25]
S. G. Hart, “Nasa-Task Load Index (NASA-TLX); 20 Years Later,”
in Proceedings of the Human Factors and Ergonomics Society Annual
Meeting, vol. 50, no. 9.
Santa Monica: HFES, 2006, pp. 904–908.
[26]
J. Carmack, “The Dawn of Mobile VR,” Talk at the Game Developers
Conference (GDC), March 2 - 6, 2015, URL: http://www.gdcvault.com/
play/1022304/The-Dawn-of-Mobile [retrieved: January, 2016].
410
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-468-8
ACHI 2016 : The Ninth International Conference on Advances in Computer-Human Interactions

