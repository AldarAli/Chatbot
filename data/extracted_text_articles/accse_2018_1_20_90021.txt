Radar-Camera Sensor Fusion Based Object Detection for Smart Vehicles 
 
Eugin Hyun 
ART Lab., 
DGIST,  
Daegu, Korea 
e-mail: braham@dgist.ac.kr 
Young-Seok Jin 
ART Lab., 
DGIST,  
Daegu, Korea 
e-mail: ysjin@dgist.ac.kr 
Hyeong-Cheol Jeon 
ART Lab.,  
DGIST, 
Daegu, Korea 
e-mail: hamtooree@dgist.ac.kr 
Young-Nam Shin 
DAS  Engineering Team, 
SL Corporation, 
Gyeongsanbuk-do, Korea 
e-mail: ynshin@slworld.com 
 
 
Abstract—We propose a post-data processing scheme for radar 
sensors in a fusion system consisting of a camera and radar 
sensors. The proposed scheme is divided into the recursive least 
square filter, the ghost target and clutter cancellation, and 
region of interest (ROI) selection. Especially for the recursive 
least square filter, we determine whether detections are valid 
tracks or are new tracks to initialize or update the filter 
parameters. Next, we apply the valid detections to the filter to 
reduce detection errors. Next, we cancel ghost targets as 
comparing the current tracks and the last tracks, and suppress 
clutter using the detected radial velocity. Finally, we select the 
ROI and determine the transfer coordinates to provide these 
values to the camera sensor. To verify the proposed method, we 
use Delphi commercial radar and carry out the measurements 
in a chamber and on the real road. 
Keywords- Post processing; Sensor fusion; ADAS. 
I. 
 INTRODUCTION  
At present, the Advanced Driver Assistance System 
(ADAS) is one of the main issues for smart vehicle safety in 
road traffic. To support effective ADASs, the target detection 
sensors used are very important. Among currently available 
sensors, camera and radar sensors are commonly used in 
ADASs [1][2]. 
Because radar detects objects by emitting radio signals and 
analyzing the echo in the reflected signal, this system can 
operate robustly in different weather conditions [1][2]. 
Cameras are also widely used because they can provide rich 
data, similar to that by the human eye [1][2]. However, radar 
measurements are limited in terms of the angle resolution and 
this data is rather noisy due to false alarms. Cameras are also 
sensitive to light and weather conditions, and they have low 
detection accuracy levels, such as for the velocity and range 
detections.  
Owing to these limitations, sensor fusion technology is 
considered as an efficient means of increasing target detection 
performance levels [1]-[3]. Because previous works have 
provided sensor fusion outcomes in the end stage [4][5], the 
computational complexity of camera classification remains 
high. Thus, in order to improve the detection performance and 
reduce the computational intensity, early-stage-based sensor 
fusion was proposed in previous works [6]-[8].  
In the previous works [6][7], radar is used to detect targets 
in the region of interest (ROI) of captured image and searches 
for vehicle features within the ROI. However, the detection 
accuracy is unsatisfactory because the camera sensor detects 
the range of ROI as the pre-processing. In order to overcome 
the limitation, in another work [8], a more robust and efficient 
vision-based vehicle detection method was presented. In that 
case, the radar sensor provides ROIs for the camera sensor. 
Compared to the [6][7], because the radar sensor detects the 
range of target, the method can improve the detection 
accuracy and reduces the false alarm rate.  
For the fusion method, the radar system provides precise 
ROI information to the camera sensor. Specifically, because 
the coordinates between the radar and the camera have 
differences, coordinate matching is also required. Finally, 
because the both sensors’ fields of views (FOVs) are also 
different, the overlap area should be considered.  
Thus, in this paper, we propose a radar post-processing 
scheme, which takes these issues into account for fusion of the 
camera and radar sensor. Section II briefly presents the 
proposed radar post-data processing scheme. Section III 
describes measurement results under the real field. Section IV 
presents the conclusion.  
II. 
POST-DATA PROCESSING SCHEME  
Figure 1 presents the expected fusion results of systems 
incorporating both camera and radar sensors.  
 
 
Figure 1.  Expected fusion results of camera and radar sensors. 
The angle detection of the radar sensor has a low 
resolution while range detection error of the camera sensor is 
very high. Here, the overlap between the two sensors is the 
final detected target position. In order to overcome the 
limitations of each sensor, we propose the sensor fusion based 
processing concept shown in Figure 2.  
From the radar sensor, the detection information is 
received through a controller area network (CAN) with radar 
start commend. After packet receiving and decoding, the track 
5
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-658-3
ACCSE 2018 : The Third International Conference on Advances in Computation, Communications and Services

data and the corresponding flag (or status) data are saved in 
the registers. Subsequently, through the proposed post data 
processing and projective transformation steps, the ROI 
information is transferred to the image processing path. In the 
camera sensor, based on these ROIs, feature extraction and 
target classification are carried out.  
 
 
Figure 2.  Proposed camera and radar sensor fusion processing concept. 
 
The proposed post data processing scheme for radar is 
illustrated in Figure 3. First, we determine where detection is 
valid or not valid using the flag and status values.  
In the second step, we initialize the parameters for error 
minimizing filter if the current track is new. On the other hand, 
for existing tracks, the corresponding parameters are 
calculated using the values updated in previous state. In this 
paper, we employed the recursive least square second order 
filter [9] to improve the angle detection error. On the other 
hand, the range and velocity values of the first step are passed 
without any modifications because the detection errors are 
very low.  
The filter processing is expressed by (1) and (2), where 
K1 ൌ 2ሺ2k െ 1ሻ/ሺkଶ ൅ kሻ, K2 ൌ 6/ሺkଶ ൅ kሻ,  and Res ൌ
ݔሾ݇ሿ െݔොሾ݇ െ 1ሿ െݔොௗሾk െ 1ሿ. In this equation, ݔොሾkሿ is the kth 
the estimated angle and ݔሾ݇ሿ is the kth measurement. For a 
new track (k is 1), we define ݔොሾ0ሿ ൌ  ݔሾ1ሿ and ݔොௗሾ0ሿ ൌ  0.  
 
ݔොሾkሿ ൌݔොሾ݇ െ 1ሿ ൅ݔොௗሾ݇ െ 1ሿ ൅ܭ1∙ܴ݁ݏ    (1) 
ݔොௗሾ݇ሿ ൌݔොሾ݇ െ 1ሿ ൅ܭ2∙ܴ݁ݏ                       (2) 
 
Next, the sudden tracks are cancelled. That is, as 
comparing the current track status and the previous 
information, we can determine the received detection is ghost 
or not.  
 
 
Figure 3.  Post-data processing scheme for radar. 
In the next step, we distinguish whether the current track 
is a target or a clutter. If the velocity of the current track, v[k], 
meets (3), the current track is target, otherwise it can be regard 
as clutter. Here, vmin is maximum velocity of clutter and vego is 
ego velocity of subject vehicle. In addition, vmin is statistically 
estimated through the experiment through trade-off between 
the detection probability and false alarm rate.  
 
ݒሾ݇ሿ ൏ െݒ௘௚௢ െݒ௠௜௡  ݋ݎ  ݒሾ݇ሿ ൐ െݒ௘௚௢൅ݒ௠௜௡    (3) 
 
Then we select the ROI information (range, radial velocity, 
and angle) in the overlap area of the radar and the camera. 
Finally, the projective transformation is carried out. In that 
case, the error bound is also fed to camera sensor considering 
the range detection error of the camera sensor and angle 
detection error of the radar sensor. The camera sensor will 
process images within window size, which reduces the 
complexity of image processing.   
The steps described above are repeated until the scanning 
of final tracks is completed. The number of tracks is 
dependent on the type of commercial radar and the 
corresponding parameter setting.  
 
6
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-658-3
ACCSE 2018 : The Third International Conference on Advances in Computation, Communications and Services

III. 
MEASUREMENT RESULTS 
In this paper, we employ the Delphi 77GHz ESR 
(Electronically Scanning Radar) system. In this radar system 
[10], because the maximum number of tracks is 64, the post-
data processing described in Section II are repeated until the 
scanning of 64 tracks is completed.  
TABLE I.  
DELPHI ESR SPECIFICATIONS 
Category 
Values 
Size 
173.7 × 90.2 x 49.2 mm 
(L x W x H) 
Weight 
575 g 
Scanning frequency 
76.5 GHz 
Field of View 
+/- 45° 
Range 
~ 60m 
Target 
64 
Update rate 
<= 50 ms 
 
In order to verify the post-data processing method for 
radar, we configured a moving target measurement scenario 
in a chamber room, as shown in Figure 4. First, we install the 
Delphi ESR on the positioner and a single target is placed on 
the rail approximately 3.5 m away from the radar. The target 
then moved from 3.5m to 5.9 m in a round trip. 
 
 
Figure 4.  Moving target measurement scenario in chamber room. 
 
Figure 5.  Radar measurement set-up. 
We also utilized the measurement set-up shown in Figure 
5. Here, a PC is connected to the ESR device through CAN to 
a USB converter. We coded the device driver software to start 
the radar sensor and data parsing program so as to log the 
received data using the Matlab simulator. In addition, we also 
completed the aforementioned post-data processing algorithm.  
Figure 6 shows the detection result for several frame times: 
range (top), radial velocity (middle), and angle (bottom). As 
shown in the results, the angle detections contain numerous 
errors. Even when the moving target is placed on the middle 
line of the radar, the detection results were found to vary. Thus, 
we filtered the detection outcomes through a recursive least 
square filter. The corresponding outputs are indicated by the 
red line in Figure 6. 
 
 
Figure 6.  Results of post data processing for radar. 
Next, in order to verify the algorithm on the real road, we 
install the radar and camera on the middle of the vehicle front 
bumper. The electrical power of vehicle was supplied to the 
both sensors. Signal lines were built in the vehicle to acquire 
radar signals and capture camera images together with a PC. 
In addition, we considered four scenarios as shown in 
Figure 7. First, a single human is moving along the middle line 
of the radar sensor at approximately 6 m (a). In the second 
scenario (b), a human is walking along the right edge of radar 
FOV. In next scenario (c), the human is walking 3m away 
from the middle line in the longitudinal direction. Final case 
(d) shows the pedestrian who moves laterally at about 3 m 
away from the radar.  
 
 
7
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-658-3
ACCSE 2018 : The Third International Conference on Advances in Computation, Communications and Services

 
Figure 7.  Results of post data processing for radar. 
 
Figures 8-13 present the measurements and post-
processing results for each scenario. We monitor moving 
human for about 7 seconds. In all figures, the blue points are 
the detected tracks in each frame.  
First, Figures 8 and 9 show the results when the human is 
walking and running for the first scenario, respectively. Here, 
Figures (a) ~ (c) present the valid tracks received from radar 
sensor and Figures (d) ~ (f) show the post-data processing 
results. In Figures (a) and (d), the x-axis is the frame index and 
the y-axis indicates the range (meter). In Figure (b) and (e), 
the x-axis indicates the frame index and the y-axis is the angle 
(degree). Figures 8 (c) and (f) express the corresponding x- 
and y-positions (meter) of tracks over the whole frames. The 
results are calculated using range and angle values of each 
track.  Here, the black line indicates FOV of radar. Figures 9 
(c) and (f) show the radial velocity (m/s) over each frame.  
In the results of Figure 8, we can see that angle errors are 
compensated through the recursive least square filter. 
Moreover, in Figure 9, we can find that the ghost targets and 
clutter are cancelled and the multiple scattering points 
oriented from one target are grouped together.  
Next, in Figures 10, 11, and 12, we present the processing 
results for the scenarios 2, 3, and 4. In the results, we describe 
the x-y positions (meter) of target as calculating with the 
detected range and angle for all frames. We also mark black 
line to be able to see the detectable angle of radar. From all 
results, it was proved that the angle errors are minimized, the 
ghost and clutter were canceled, and the grouping was 
completed. 
 
 
Figure 13.  Example of ROI selection and window generation for the 
camera sensor 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1.  Figure 8.  Detected target tracks (range, angle, and the corresponding xy-position) for the first scenario: (a)~(c) tracks before post-
processing and (d)~(e) tracks after post-processing. 
8
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-658-3
ACCSE 2018 : The Third International Conference on Advances in Computation, Communications and Services

Last, Figure 13 shows an example of the ROI selection 
process (red circle) on the captured image for the first scenario. 
Here, the camera with wide angle was developed by the SL 
Corporation. In the image processing, the window can be 
generated based on selected ROI including the range and 
angle such the example of Figure 13.  
 
 
 
 
 
 
 
Figure 2.  Figure 9.  Detected target tracks (range, angle, and radial velocity) for the second scenario: (a)~(c) tracks before post-processing and (d)~(e) 
tracks after post-processing.  
 
 
Figure 3.  Figure 10.  Detected target tracks (xy-position) over the whole frames for the third scenario. 
 
 
Figure 4.  Figure 11.  Detected target tracks (xy-position) over the whole frames for the fourth scenario. 
9
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-658-3
ACCSE 2018 : The Third International Conference on Advances in Computation, Communications and Services

IV. 
CONCLUSION 
In this paper, we proposed post radar data processing for a 
camera and radar sensor fusion system. To do this, we utilized 
a Delphi 77GHz automotive commercial radar system.  
First, using the flag values received from the radar, we 
determined instances of valid detection and new tracks. Next, 
we employed a recursive least square filter to reduce the 
detected angle error. Next we cancelled the ghost target and 
clutter using the received track information. Finally, based on 
the selected ROI information, the projective transformation is 
carried out for the camera sensor. The performance 
capabilities of the proposed scheme were assessed in a 
chamber and in the outdoor environment.  
In the future, we will verify the proposed processing 
scheme in various scenarios on the real road. Thus, we will 
provide the meaningful results. Moreover, together with the 
camera sensor, we will develop methods of sensor fusion 
processing. Thus, we will compare the results of sensor fusion 
and them obtained by camera database alone.  
 
ACKNOWLEDGMENT 
This research was supported by the Technology Transfer 
and Commercialization Program through INNOPOLIS 
Foundation (2017-DG-0001) and the DGIST R&D Program 
(18-FA-07) funded by the Ministry of Science and ICT, Korea.  
 
REFERENCES 
[1] A. Gavriilidis, D. Müller, S. Müller-Schneiders, J. Velten, and 
A. Kummert, “Sensor System Blockage Detection for Night 
Time Headlight Control Based on Camera and Radar Sensor 
Information,” IEEE ITSC 2015, Anchorage, USA, Sep. 2012. 
[2] E. Hyun and Y. S. Jin, “Multi-level Fusion Scheme for Target 
Classification using Camera and Radar Sensors,” IPCV’17, 
Lasvegas, USA, July 2017, pp. 111-114. 
[3] J. Laneurit, C. Blanc, R. Chapuis, and L. Trassoudaine, 
“Multisensorial data fusion for global vehicle and obstacles 
absolute positioning,” IEEE Intelligent Vehicles Symposium, 
Columbus, USA, Jun. 2003, pp. 138–143. 
[4] R. O. Chavez-Garcia, J. Burlet,  T. D. Vu,  and O. Aycard, 
“Frontal object perception using radar and mono-vision”, IEEE 
Intelligent Vehicles Symposium 2012, Alcala de Henares, 
Spain, June 2012 
[5] U. Kadow, G. Schneider, and A. Vukotich, “Radar-vision 
based vehicle recognition with evolutionary optimized and 
boosted features,” IEEE Intelligent Vehicles Symposium, 
Istanbul, Turkey, June 2007, pp. 749–754. 
[6] A. Sole, O. Mano, G. Stain, H. Kumon, Y. Tamatsu, and A. 
Shashua, “Solid or not solid: Vision for radar target validation,” 
IEEE Intelligent Vehicles Symposium, Parma, Italy, Jun. 2004, 
pp. 819–824. 
[7] G. Alessandretti, A. Broggi, and P. Cerri, “Vehicle and Guard 
Rail Detection using Radar and Vision Data Fusion,” IEEE 
Transactions on Intelligent Transportation Systems, Vol. 8, Iss. 
1, pp. 95-105, Mar. 2007. 
[8] X. Wang,  L. Xu,  H. Sun,  J. Xin,  and N. Zheng, “On-Road 
Vehicle Detection and Tracking Using MMW Radar and 
Mono-vision Fusion,” IEEE Transaction on Intelligent 
Transportation Systems, Vol. 17, No. 7, pp. 2075-2084, July 
2016. 
[9] J. Lee and V. J. Mathews, "A fast recursive least squares 
adaptive second order Volterra filter and its performance 
analysis," IEEE Transactions on Signal Processing, Vol. 41, 
Issue 3, pp. 1087-1102, Mar. 1993. 
[10] Autonimoustuff, “Delphi ESR Startup Guide version 2.1”, Oct. 
2015. 
 
 
 
Figure 5.  Figure 12. Detected target tracks (xy-position) over the whole frames for the fifth scenario. 
10
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-658-3
ACCSE 2018 : The Third International Conference on Advances in Computation, Communications and Services

