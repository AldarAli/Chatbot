Computation of Suitable Grasp Pose for Usage of Objects Based on
Predeﬁned Training and Real-time Pose Estimation
Muhammed Tawﬁq Chowdhury, Shuvo Kumar Paul, Monica Nicolescu, Mircea Nicolescu,
David Feil-Seifer and Sergiu Dascalu
Department of Computer Science and Engineering, University of Nevada, Reno
1664 North Virginia Street, Reno, Nevada 89557, USA
Email Addresses: {mtawﬁqc@nevada.unr.edu, shuvo.k.paul@nevada.unr.edu, monica@cse.unr.edu, mircea@cse.unr.edu,
dave@cse.unr.edu, dascalus@cse.unr.edu}
Abstract—Existing grasping mechanisms focus on executing accu-
rate grasps which are not always suitable for the usage of objects.
We developed a system that can be used to train humanoid
robots with different types of grasp poses. We present a grasping
mechanism using homogeneous transformation that allows a
humanoid robot to grasp objects in such a way that is suitable for
the usage of the objects. The system captures the relative poses
of an object and a robot’s wrist for training such that when the
object’s pose changes, the robot’s gripper attached to the wrist
adjusts its pose accordingly and lines up with the object. For
detecting the objects and estimating their poses, we developed and
used a color-based pose detection and estimation system and a
homography-based planar pose detection and estimation system.
We conducted experiments using a humanoid PR2 robot. We
used the Robot Operating System as the primary framework of
the system and MoveIt Interface for manipulation of grasps. The
grasping system showed robust results for different poses of the
objects using both arms of the robot. Our experiments involved
human validation in which the robot successfully grasped objects
such as a screwdriver, a wrench and books from human hands
in different grasp poses that are appropriate for usage of the
objects.
Keywords–Robotics; Homogeneous Transformation; Pose Esti-
mation; Grasping; Objects Usage.
I.
INTRODUCTION
Grasping is an important aspect of a robot’s capabilities.
Since different objects come in different shapes, it is often
difﬁcult for robots to grasp objects accurately. In order to
use tools, such as a screwdriver and a wrench, a robot needs
to grasp them with high level of precision as they need
to be grasped at speciﬁc locations and also in appropriate
orientations. This is not the case for a tennis ball for which the
grasp pose can be more ﬂexible. A human grasps a screwdriver
from the top of its base and this is an ideal grasp. If a robot
attempts to grasp it for usage, it needs to do the same. Thus,
an approach is required so that robots can grasp tools in ideal
grasp poses. This requires a robust system that can train the
robots to grasp objects in required poses so that regardless of
the objects’ orientations, robots can grasp it properly and use.
In this paper, we have designed a grasping technique for
humanoid robots that will enable the humanoid robots to grasp
objects of diverse shapes precisely based on predeﬁned grasp
poses. We are using a homogeneous transformation matrix to
record the relative poses between the end-effector of a robot
and an object so that when the position and orientation of
the object changes, the end-effector follows it accordingly
based on the recorded relative pose. Since this system allows
users to train the initial poses of objects and robot’s end-
effectors, the robots can be trained with different types of
grasp poses. We used a humanoid PR2 [1] robot for conducting
experiments. In our experiments, we used linear shaped tools,
such as a wrench and used a color-based pose detection and
estimation system for these tools. We also used rectangular-
shaped objects for the experiments. We used a planar pose
estimation system for running experiments with such objects.
During the experiments, the robot could robustly grasp objects
in different grasp poses which are suitable for the usage of the
objects.
The major outcomes of this research are (i) developing
a system that can train a humanoid robot different types of
grasps, (ii) ﬁnding predeﬁned grasp poses that would allow a
robot to use tools such as screwdrivers, hammers, etc., (iii) en-
abling the robot to grasp objects accurately from human hands,
(iv) introducing a homography-based planar pose detection
and estimation technique for objects that have complex shapes
(v) implementing a color-based pose detection and estimation
system using mathematical formulas for objects with linear
shapes
The rest of the paper is structured as follows: Section II
discusses the related works in grasping, Section III presents an
analysis of the grasps for humanoid robots, Section IV elabo-
rates the vision systems that were used for pose estimation of
objects, Section V provides an overview of the system, Section
VI discusses the results and Section VII draws the conclusion
of the paper and discusses future works.
II.
RELATED WORK
There has been a wide range of research on robot grasping.
Designing a grasping system is challenging due to the inﬁnite
nature of the shapes of objects. Kehoe et al. [2] used a
candidate grasp from a set of grasps based on feasibility
analysis conducted by a grasp planner and a humanoid PR2
robot was used for their experiments. For stable horizontal
poses of objects, objects such as a mustard bottle is close
to the width of the PR2’s gripper so the grasps were not
very accurate in such orientations of the object. Huebner et
al. [3] also took a similar approach as they performed grasp
candidate simulation. They created a sequence of grasps and
then computed a random grasp evaluation for each model of
objects. In both works, a grasp was chosen from a list of
candidate grasps. Their research focused on ﬁnding a grasp
that would be successful while we focus on training a robot
91
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-787-0
ICAS 2020 : The Sixteenth International Conference on Autonomic and Autonomous Systems

to grasp objects in a way that is not only successful but also
suitable for usage of the objects.
Aleotti et al. [4] proposed a grasping model that involves
programming by demonstration for teaching proper grasps
with automatic 3D shape segmentation for object recognition
and semantic modeling. They developed a virtual grasping
algorithm for object picking and computing the part of the
object which is grasped. Pinto et al. [5] trained a Convolutional
Neural Network (CNN) for predicting grasp locations without
vast overﬁtting. Graspit was used as a grasp simulator [6][7] to
predict grasping. Supervised learning was used [8][9] to predict
grasp locations from RGB images. These works emphasised on
developing and using learning models for obtaining accurate
grasps. In our work, we designed a training mechanism based
on mathematical concepts which not only generated accurate
grasps but also the grasps could follow a predeﬁned training.
Related methods were also developed on autonomous
grasping [10] based on estimated shapes and poses of the
segmented objects. Weng et al. [11] proposed a system which
recognizes objects and estimates the pose of the objects using
deep neural network and then allows grasping objects using the
centers of their deﬁned pose classes. Robots were also trained
to choose optimum grasp from a set of grasps using machine
learning models based on human demonstration [12]. In order
to ensure robust grasp of unknown objects, a new algorithm
using Bayesian optimization was developed for simulation
[13]. Their work did not focus on the usage of the objects
but rather focused on ﬁnding a grasp that enabled the robots
to appropriately hold the objects.
III.
ANALYSIS OF GRASPS
When a human works with a screwdriver, the ideal grasp
is to grasp it from the top of its base. We classify this type of
grasp as top grasp. There are also objects, such as a hammer
and a wrench for which it is necessary to grasp them from
side. We classify it as side grasp. Figure 1 illustrates top grasp
and side grasp.
Figure 1. Top Grasp and Side Grasp.
For a 7-DOF robot such as the PR2, a robot’s planner could
successfully plan in various complex poses of the objects in
our experiments with redundancy. Planning is inherently more
challenging to plan for a top grasp. Some poses of the linear
shaped tools are only suitable for grasp using the right arm
while some other poses are only suitable for grasp using the left
arm. Although our system is capable of recording the relative
poses of objects and the robot’s gripper in any relative pose,
we used top grasp and side grasp in our experiments as these
are the most suitable grasps for the usage of the objects.
IV.
POSE ESTIMATION
In order for robots to operate effectively, it needs to be
aware of its surrounding environment. One aspect of this
awareness is the knowledge of the 3D positions and orienta-
tions of the objects in the scene in real-time. In order to achieve
this we need to locate the objects and ﬁnd their orientations
so that robots can interact with these objects seamlessly.
While object classiﬁcation, detection, and segmentation have
become relatively easier, pose estimation remains a challenging
problem as the large number of complex shapes of objects
found in real life makes it hard to come up with a general
pose estimation technique. Although, some recent pose esti-
mation methods, named PoseCNN [14] and DOPE [15], show
promising results in terms of accuracy; generating synthetic
data for each newly introduced object requires additional
preprocessing tasks that may require other expertise and can
take a lot of time. Moreover, as these methods utilize neural
networks, training and running these models necessitate high
computing resources. Keeping these difﬁculties in mind, we
applied directional cosines to estimate the pose for objects
with simple linear shapes that extend along a straight or nearly
straight line using color cues, and introduced a homography-
based planar pose estimation technique for other objects that
have more complex shapes.
A. Color-based Pose Detection and Estimation
We used two different colors such as yellow and green
on the two edges of the linear objects. This method can be
applied to any linear shaped tools. We computed the position
of the object with respect to one of the edges of the objects.
We calculated the roll, pitch and yaw rotational angles of the
pose using directional cosine equations shown below.















γ = cos−1( ⃗ux
|⃗u|)
β = cos−1( ⃗uy
|⃗u|)
α = cos−1( ⃗uz
|⃗u|)
(1)
Figure 2 shows the directional cosine in 3D space
Figure 2. Directional Cosine.
B. Planar Pose Estimation Using Homography
For more complex shapes, we used a descriptor based
detection system that utilizes homography and the depth data
to estimate the pose of the plane of an object. First, for
92
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-787-0
ICAS 2020 : The Sixteenth International Conference on Autonomic and Autonomous Systems

each object, we acquired an undistorted image of the object’s
plane that we wanted to detect and take as a reference for
homography computation. Then we applied feature detector to
ﬁnd keypoints [16] and used descriptor to retrieve the feature
vectors. Then, we did the same for the image frames received
from the camera and ﬁnd the matches using FLANN [17] and
compute the homography using RANSAC [18]. We applied
a perspective transform to ﬁnd the corresponding points on
the frame using the homography matrix and approximate the
location of the two axes on the plane on the object. Finally, we
used depth information to estimate the third orthogonal axis
by taking the cross product and recover the pose. Figure 3
demonstrates the planar based pose estimation system and
Figure 4 shows the pose detection and visualization.
(a)
(b)
Figure 3. (a) Homography for Different Planar Rotation. (b) Computed Third
Directional Axis Projected onto Image Plane.
(a)
(b)
Figure 4. (a) Pose Detection in Robot’s Camera (b) Visualization of Corre-
sponding Poses in Rviz.
V.
SYSTEM OVERVIEW
We used Robot Operating System (ROS) [19] as the
primary framework for the system as it is used by humanoid
robots such as PR2 and Baxter. We also used MoveIt Interface
[20] for manipulation of the arms and the grippers of the robot.
We had two different phases: the training phase and the testing
phase. During the training phase, we placed the object and
the robot’s gripper close to each other in our desired training
poses. We got the poses of the objects from our vision systems
while we recorded the pose of the robots’ wrist to which the
gripper is attached using a wrist pose recording system for
the PR2 robot. Then we computed the transformation matrix
using the two poses and used the matrix in testing phase. We
used the ROS Python API for developing the functionality of
the transformation matrix. The transformation matrix captures
the relative poses of the object and the wrist. During testing
phase, we placed the objects in different poses and our system
used mathematical equations to generate a new grasp pose
for the robot’s end-effector. The 3D coordinate frame for the
vision system and the robot’s wrist during the training and
testing phase need to be the same. Once a grasp pose was
computed, we used the C++ API of the MoveIt Interface for
the manipulation of the robot’s arm and the wrist to grasp
objects. Figure 5 shows the high level system architecture.
Figure 5. System Architecture.
A. Training
The system allows us to train a wide range of grasp
poses, allowing the robot to use various grasps for different
object uses. During the training phase, we placed the object
and the robot’s gripper close to each other and recorded the
relative pose. Figure 6 illustrates the training process in which
the robot’s gripper and a screwdriver were placed in close
proximity and the relative poses were recorded for grasping
the objects from top which is the general grasping approach
for a screwdriver. Figure 6 shows a training scenario.
Figure 6. Recording Relative Pose for Top Grasp.
B. Matrix Calculation
We used the following homogeneous transformation matrix
[21]:
ATB =
 ARB
APB
0
1

=


c11
c12
c13
xt
c21
c22
c23
yt
c31
c32
c33
zt
0
0
0
1


(2)
93
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-787-0
ICAS 2020 : The Sixteenth International Conference on Autonomic and Autonomous Systems

ATB refers to the transformation of the coordinate frame B
with respect to the coordinate frame A.
ARB and
APB refer
to the rotation and translation respectively of the coordinate
frame B with respect to the coordinate frame A. We then used
(3) to record the relative pose.
OTG =
OTB ×
BTG where
OTB = ( BTO)−1
(3)
In the equation, O refers to the object, B refers to the
robot’s base and G refers to the wrist of the robot to which
the gripper is attached.
C. Pose Calculation
Once we have a training matrix saved in a ﬁle, we can get
a new pose of the object from vision and generate the ﬁnal
matrix that has the new position and orientation of the robot’s
wrist in matrix form using (4):
BTG =
BTO ×
OTG
(4)
We then calculate rotational angles of the grasp pose using
the calculated matrix from (4) with (5)





γ = tan−1(c32/c33)
β = tan−1(−c31/
p
c322 + c332)
α = tan−1(c21/c11)
(5)
VI.
RESULTS AND DISCUSSION
We tested our system on a comprehensive set of estimated
poses that included different types of orientations of the object.
We conducted 75 experiments in total. We used two types
of grasps-top grasp and side grasp. We conducted two types
of experiments: general validation experiments and human
validation experiments. For the general validations experi-
ments, the objects were attached to a tripod and for human
validations experiments, a human held the objects in hand and
then the robot grasped it. We placed the objects in various
locations in front of the robot and in various orientations in
the 3D space. For the experiments using the color-based pose
detection and estimation system, we used a screwdriver and a
wrench. Figure 7 shows the objects used in the experiments.
Figure 7. Objects for Experiments.
For running experiments using the homography based
planar pose estimation system, we used a sticker-book and
a cartoon book. The pose estimation systems showed robust
performance. Figure 8 shows the pose estimation in ROS
Visualizer (RViz) for color-based pose estimation system. In
the ﬁgure, the green axis is parallel to the object and touches
its base which indicate that the pose estimation is accurate.
Figure 8. Checking Pose Estimation in Rviz.
We ran 45 experiments in general scenario. We conducted
30 and 15 experiments respectively using the right and left
arm. Table I shows the general validation results.
TABLE I. GENERAL VALIDATION RESULTS
Objects
Top Grasp
Side Grasp
Successful Grasp (Top | Side)
Accuracy (Top | Side)
Wrench
12
24
12 | 22
100% | 91.67%
Screwdriver
6
N/A
6 | N/A
100% | N/A
Books
N/A
5
N/A | 5
N/A | 100%
The results from the human validation experiments indicate
that the training and pose estimation have been precise enough
for the robot to accurately grasp objects from human hands.
We ran 24 and 6 experiments using the right and left arm,
respectively. Table II shows the experimental results for human
validation experiments.
TABLE II. HUMAN VALIDATION RESULTS
Objects
Top Grasp
Side Grasp
Successful Grasp (Top | Side)
Accuracy (Top | Side)
Wrench
9
7
9 | 6
100% | 85.72%
Screwdriver
9
N/A
9 | N/A
100% | N/A
Books
N/A
5
N/A | 5
N/A | 100%
The robot could successfully grasp the objects in 72 out
of 75 experiments in different grasp poses which are suitable
for the usage of the objects. In 3 experiments, pose estimation
during testing was not accurate enough for a successful grasp.
Poses of the objects could be detected instantly after they were
placed in the scene. The grasps could be initiated in about a
second after the poses were estimated and be completed in
about 5 seconds. This makes the pose estimation and grasping
a real-time operation. In successful experiments, the robot
grasped the objects perfectly with respect to the training.
It demonstrates that both the grasping system and the pose
estimation systems are robust and they can handle rotations of
objects in multiple axes and in different angles. It also shows
that this system is ideal for training robots to grasp linear
shaped tools, such as screwdrivers, wrenches, saws, hammers,
etc. as well as objects with more complex shapes, such as box,
book, magazine, etc. The pose estimation and the grasping had
been robust and accurate enough for the robot to grasp objects
from human hand. Grasping from human hand is sensitive as
if the robot tries to grasp in incorrect locations, it will place
94
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-787-0
ICAS 2020 : The Sixteenth International Conference on Autonomic and Autonomous Systems

its grippers on human hand but in our experiments, that issue
did not occur. There are some poses which are not reachable
by a 7-DOF robot. For instance, when the object is pointing
inward or back in x-axis in the robot reference frame, it is
not possible for the end-effector to make a top grasp. There
are also poses for which right arm is reachable but left arm
is not reachable and vice versa. Thus, in our experiments, we
used both arms so that we could cover all segments in a 3D
coordinate system. Figure 9 shows the side grasp of a wrench
tied to a tripod which displays that the gripper lined up with
the wrench and the grasp pose is similar to the way human
grasps a wrench.
(a)
(b
Figure 9. (a) Initial Pose of the Right Gripper and a Wrench. (b) Side Grasp
of the Wrench Using the Right Gripper.
This grasp then can be used to work with the tool. The
training ensures that the gripper lines up with the objects
in rotations in all axes in the 3D coordinate. Thus, the
system shows capability of handling complex rotations and
the resultant grasp pose is always suitable for usage of the
objects. The robot grasped objects in a very accurate manner
from human hand. The color-based pose estimations system
worked robustly while the human held the objects in hand and
we were able to receive very accurate pose estimations from
the vision systems for complex rotations of the tools. Figure 10
and Figure 11 show grasps of a screwdriver from human hand
in which the robot was able to grasp the screwdriver from the
top of its base.
(a)
(b
Figure 10. (a) Initial Pose of the Right Gripper and a Screwdriver. (b) Top
Grasp of the Screwdriver Pointing Towards the Robot.
(a)
(b
Figure 11. (a) Initial Pose of the Right Gripper and a Screwdriver. (b) Top
Grasp of the Screwdriver Pointing Towards the Human.
The system had also been successful in using both of the
robot’s arms. The use of the left arm allows the robot to grasp
objects in poses that are not feasible to grasp with the right
arm. Figure 12 shows the side grasp of a wrench using the left
arm. The robot also successfully grasped books from human
hand. Figure 13 and Figure 14 show the results.
(a)
(b
Figure 12. (a) Initial Pose of the Left Gripper and a Wrench. (b) Side Grasp
of the Wrench Using the Left Gripper.
(a)
(b
Figure 13. (a) Initial Pose of the Right Gripper and a Sticker-book. (b) Side
Grasp of the Sticker-book.
(a)
(b
Figure 14. (a) Initial Pose of the Right Gripper and a Cartoon-book. (b) Side
Grasp of the Cartoon-book.
VII.
CONCLUSION AND FUTURE WORK
This paper discussed an approach that enables humanoid
robots to grasp objects for usage using two different vision
systems for object pose detection and estimation. Application
of mathematical theories and development of software systems
were integrated in our work. The system had been robust
enough for grasping objects such as a screwdriver and a wrench
from human hand and a comprehensive set of poses had been
tested for grasping with human validation. The predeﬁned
training generated accurate grasps which are suitable for usage
of the objects. The accuracy of the results indicate that the
system is robust.
We plan to extend the project to add more features to it.
An important addition to the project would be an introduction
of the movement of both arms of the robot simultaneously. If
we receive two different poses coming from the vision system
simultaneously then the robot could grasp both the objects at
95
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-787-0
ICAS 2020 : The Sixteenth International Conference on Autonomic and Autonomous Systems

the same time. We would like to introduce a dialogue feature
in the work for collision avoidance [22]. If a robot attempts to
grasp an object, it would initiate a dialogue with humans in
its surrounding environment. If it gets positive response from
the humans, it will execute the grasp. Otherwise, it will not
move its arm. The dialogue will enhance the safety in the
movement of the robots’ arms and will ensure that the robot
avoids obstacles in its surrounding environment while grasping
an object. We would also like to add an automated planning
system for robots so that if the robot planner fails to plan for
a grasp pose using one arm, it would automatically try with
the other. This would increase the robustness of the grasping
system.
ACKNOWLEDGMENTS
We acknowledge the ﬁnancial support for this work by the
Ofﬁce of Naval Research (ONR) award #N00014-16-1-2312,
N00014-14-1-0776.
REFERENCES
[1]
S. Cousins, “ROS on the PR2 [ROS topics],” IEEE Robotics &
Automation Magazine, vol. 17, no. 3, pp. 23–25, Sep. 2010. [Online].
Available: https://doi.org/10.1109/mra.2010.938502
[2]
B. Kehoe, A. Matsukawa, S. Candido, J. Kuffner, and K. Goldberg,
“Cloud-based robot grasping with the google object recognition
engine,”
in
2013
IEEE
International
Conference
on
Robotics
and Automation.
IEEE, May 2013. [Online]. Available: https:
//doi.org/10.1109/icra.2013.6631180
[3]
K.
Huebner,
S.
Ruthotto,
and
D.
Kragic,
“Minimum
volume
bounding
box
decomposition
for
shape
approximation
in
robot
grasping,”
in
2008
IEEE
International
Conference on
Robotics
and Automation.
IEEE, May 2008. [Online]. Available: https:
//doi.org/10.1109/robot.2008.4543434
[4]
J. Aleotti and S. Caselli, “Part-based robot grasp planning from
human demonstration,” in 2011 IEEE International Conference on
Robotics and Automation.
IEEE, May 2011. [Online]. Available:
https://doi.org/10.1109/icra.2011.5979632
[5]
L. Pinto and A. Gupta, “Supersizing self-supervision: Learning to
grasp from 50k tries and 700 robot hours,” in 2016 IEEE International
Conference on Robotics and Automation (ICRA).
IEEE, May 2016.
[Online]. Available: https://doi.org/10.1109/icra.2016.7487517
[6]
A. Miller and P. Allen, “Graspit! a versatile simulator for robotic
grasping,” IEEE Robotics & Automation Magazine, vol. 11, no. 4, pp.
110–122, Dec. 2004. [Online]. Available: https://doi.org/10.1109/mra.
2004.1371616
[7]
A. Miller, S. Knoop, H. Christensen, and P. Allen, “Automatic
grasp planning using shape primitives,” in 2003 IEEE International
Conference on Robotics and Automation (Cat. No.03CH37422). IEEE.
[Online]. Available: https://doi.org/10.1109/robot.2003.1241860
[8]
A. Saxena, J. Driemeyer, and A. Y. Ng, “Robotic grasping of
novel objects using vision,” The International Journal of Robotics
Research, vol. 27, no. 2, pp. 157–173, Feb. 2008. [Online]. Available:
https://doi.org/10.1177/0278364907087172
[9]
L. Montesano and M. Lopes, “Active learning of visual descriptors for
grasping using non-parametric smoothed beta distributions,” Robotics
and Autonomous Systems, vol. 60, no. 3, pp. 452–462, Mar. 2012.
[Online]. Available: https://doi.org/10.1016/j.robot.2011.07.013
[10]
A. Uckermann, C. Elbrechter, R. Haschke, and H. Ritter, “3d
scene
segmentation
for
autonomous
robot
grasping,”
in
2012
IEEE/RSJ International Conference on Intelligent Robots and Systems.
IEEE, Oct. 2012. [Online]. Available: https://doi.org/10.1109/iros.2012.
6385692
[11]
J. Yu, K. Weng, G. Liang, and G. Xie, “A vision-based robotic
grasping system using deep learning for 3d object recognition and
pose estimation,” in 2013 IEEE International Conference on Robotics
and Biomimetics (ROBIO).
IEEE, Dec. 2013. [Online]. Available:
https://doi.org/10.1109/robio.2013.6739623
[12]
O.
Kroemer,
R.
Detry,
J.
Piater,
and
J.
Peters,
“Active
learning
using
mean
shift
optimization
for
robot
grasping,”
in
2009
IEEE/RSJ
International
Conference
on
Intelligent
Robots
and
Systems.
IEEE,
Oct.
2009.
[Online].
Available:
https://doi.org/10.1109/iros.2009.5354345
[13]
J. Nogueira, R. Martinez-Cantin, A. Bernardino, and L. Jamone,
“Unscented
bayesian
optimization
for
safe
robot
grasping,”
in
2016
IEEE/RSJ
International
Conference
on
Intelligent
Robots
and
Systems
(IROS).
IEEE,
Oct.
2016.
[Online].
Available:
https://doi.org/10.1109/iros.2016.7759310
[14]
Y. Xiang, T. Schmidt, V. Narayanan, and D. Fox, “Posecnn: A convolu-
tional neural network for 6d object pose estimation in cluttered scenes,”
2018.
[15]
J. Tremblay, T. To, B. Sundaralingam, Y. Xiang, D. Fox, and
S. Birchﬁeld, “Deep object pose estimation for semantic robotic
grasping of household objects,” in Conference on Robot Learning
(CoRL), 2018. [Online]. Available: https://arxiv.org/abs/1809.10790
[16]
S. K. Paul, M. T. Chowdhury, M. Nicolescu, M. Nicolescu, and D. Feil-
Seifer, “Object detection and pose estimation from rgb and depth data
for real-time, adaptive robotic grasping,” in International Conference on
Image Processing, Computer Vision
Pattern Recognition, Las Vegas,
NV, July 2020.
[17]
M. Muja and D. G. Lowe, “Fast approximate nearest neighbors with
automatic algorithm conﬁguration,” in International Conference on
Computer Vision Theory and Application VISSAPP’09).
INSTICC
Press, 2009, pp. 331–340.
[18]
M. A. Fischler and R. C. Bolles, “Random sample consensus: A
paradigm for model ﬁtting with applications to image analysis and
automated cartography,” Commun. ACM, vol. 24, no. 6, p. 381–395,
Jun. 1981. [Online]. Available: https://doi.org/10.1145/358669.358692
[19]
M. Quigley, K. Conley, B. P. Gerkey, J. Faust, T. Foote, J. Leibs, R. C.
Wheeler, and A. Y. Ng, “Ros: an open-source robot operating system,”
in ICRA 2009, 2009.
[20]
S. Chitta, “MoveIt!: An introduction,” in Studies in Computational
Intelligence.
Springer International Publishing, 2016, pp. 3–27.
[Online]. Available: https://doi.org/10.1007/978-3-319-26054-9 1
[21]
K. Lynch and F. Park, Modern Robotics: Mechanics, Planning, and
Control.
Cambridge University Press, 2017. [Online]. Available:
http://hades.mech.northwestern.edu/images/7/7f/MR.pdf
[22]
B. A. Anima, J. Blankenburg, M. Zagainova, S. P. H. Alinodehi, M. T.
Chowdhury, D. Feil-Seifer, M. Nicolescu, and M. Nicolescu, “Col-
laborative human-robot hierarchical task execution with an activation
spreading architecture,” in Social Robotics.
Springer International
Publishing, 2019, pp. 301–310.
96
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-787-0
ICAS 2020 : The Sixteenth International Conference on Autonomic and Autonomous Systems

