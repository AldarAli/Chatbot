Robust Object Tracking Using Unreliable Object Recognizers  
 
Li Li 
Shannon Cognitive Computing Lab 
Huawei 
Bridgewater, New Jersey, USA 
email: li.nj.li@huawei.com 
Masood Mortazavi 
Shannon Cognitive Computing Lab 
Huawei 
Santa Clara, California, USA 
email: masood.mortazavi@huawei.com
 
 
Abstract—This paper presents a method for video surveillance 
systems to correct noisy observations from distributed object 
recognizers that are unreliable. An unreliable recognizer 
consists of a hardware sensor (e.g., a camera) and a recognition 
program (e.g., facial recognition) that may produce random 
errors including false positives, false negatives, or failures. To 
address this issue, we use a Bayesian Network (BN) to connect 
multiple factors that can cause the noisy observations with 
random errors.  We then incorporate the BN into an extended 
Hidden Markov Model (HMM) to infer optimal object paths 
from noisy observations. A prototype system is implemented 
and the simulation tests show that the Forward-Backward 
algorithm can achieve 77.3% accuracy on average with 47.9% 
relative improvement over the Viterbi algorithm. Both 
algorithms are robust to increasing noises and errors in the 
observations, even when 100% observations have over 66% 
errors.  
Keywords-video 
surveillance; 
object 
tracking; 
camera 
network; baysian network; hidden markov model; forward-
backward algorithm; Viterbi algorithm. 
I. 
 INTRODUCTION 
A video surveillance system consists of distributed 
cameras that cover the critical areas or even an entire city. 
These cameras are connected by wireless and wired 
networks to a regional control center, where the video 
streams are processed, stored, analyzed and searched by 
various algorithms to assist the authorities in controlling and 
preventing hazardous and criminal activities.   
A main challenge in video surveillance systems is to 
automatically track moving objects, such as a person or a 
vehicle.  Traditional approaches focus on monolithic 
algorithms that integrate three distinct functions: 1) extract 
features from video frames; 2) recognize the objects across 
video frames based on the features; and 3) determine object 
trajectories based on object recognitions and extracted 
features. While the monolithic approaches allow a tracking 
system to jointly optimize these functions, they make it 
difficult to combine different object recognition algorithms 
and tracking algorithms on the market. To achieve such 
flexibility, we propose to divide a tracking system into two 
layers: object recognition and object tracking that can change 
independently as blackboxes through a well-defined 
interface. A layered tracking system can easily include 
additional sensors and recognizers, such as radar, sonar, 
LIDAR, Infrared, etc. without changing the tracking model. 
On the other hand, we can easily port a tracking model to 
different domains with different sensors and recognizers.   
Facial recognition can match an unknown human face in 
image or video to one of the known faces in a database (face 
identification), or determine if two human faces are the same 
(face verification). Despite 98% accuracy in some datasets, 
facial recognition in uncontrolled environment only achieves 
74.7% classification accuracy [1]. Gait recognition can 
identify a person at a distance based on how he or she walks 
as recorded by camera or motion sensors. A recent survey [2] 
shows the gait recognition accuracies of different approaches 
vary from 60% to 90%.  
Automatic License Plate Recognition can extract and 
recognize license plate from images and videos of vehicles. 
A 2013 survey [3] shows that the accuracies of different 
methods vary from 80% to 90%. Recent researches in 
computer vision [4][5] can recognize vehicle make and 
model, as well as distinct marks, with accuracy in the range 
of 70% to 84%.  
Visual recognizers are unreliable due to constraints 
imposed by physical environment, such as variations in 
visibility, reflection, scale, view point, occlusion and motion. 
An unreliable object recognizer can produce noisy 
observations with 3 types of random errors: 1) false positive: 
when it recognizes an object that is actually not in its field of 
view; 3) false negative: when it does not recognize an object 
that is actually in its field of view; and 4) failure: when the 
camera breaks, the network fails or the recognition program 
crashes. When a tracking system receives an observation 
from an object recognizer, it should not completely trust the 
observation. Instead, the tracking system should identify and 
correct the random errors in the observation.   
For this purpose, we introduce three probabilistic models: 
1) competence model; 2) intention model; and 3) motion 
model, to describe the uncertain behaviors of the objects and 
recognizers.  We then use a Bayesian Network (BN) to 
connect the factors in these models based on how they cause 
noisy observations. The BN is then incorporated into an 
extended Hidden Markov Model (HMM) that can correct the 
noisy 
observations 
using 
the 
well-known 
inference 
algorithms, such as the Forward-Backward and Viterbi 
algorithms. Simulated tests show that the approach can 
correct 77.3% noisy observations on average and is robust to 
increasing noises and errors in the observations, even when 
100% observations have over 66.66% errors.  
21
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-559-3
SIGNAL 2017 : The Second International Conference on Advances in Signal, Image and Video Processing

The rest of the paper is organized as follows. Section II 
reviews the related work. Section III describes the BN and 
extended HMM for object tracking. Section IV shows the 
experimental results and we conclude with Section V.  
II. 
RELATED WORK 
Taj et al. [6] surveys the two main types of distributed 
camera network architectures and well-known tracking 
methods, including Graph matching, HMM, Particle filtering 
and Kalman filters. The decentralized recognizers send their 
results to the fusion centers which combine the results into a 
global trajectory. In contrast, distributed recognizers act as 
peers that exchange and combine results from their 
neighbors. In both cases, recognizers in a camera network 
can form dynamic clusters that move along with the object, 
in order to reduce search space, save energy and mitigate 
video traffic. 
Within such camera networks, various methods [7]-[10] 
have been proposed to track objects across different cameras. 
These methods tightly integrate feature extraction, object 
recognition and object tracking as each method develops its 
own set of features.  
Our approach is motivated by Fleuret et al. [10] who 
describes a probabilistic generative model that combines a 
motion model, an appearance model and a color model to 
infer the locations of objects from video streams. The 
method can track up to six people across two to four 
synchronized video streams taken at eye level and from 
different angles. In addition, the model can derive accurate 
trajectories of each individual using a grid map that divides 
an area into cells, each of which can only be occupied by one 
person at a time.  The motion model calculates the 
conditional probability that an individual at cell a will travel 
to cell b at time k based on the distance between the cells, 
while the appearance and color models estimate the 
likelihood that a video stream is observed when an individual 
is at a cell.  
However, our approach differs from [10] in some 
significant ways.  First, our approach decouples object 
recognition from object tracking whereas [10] integrates 
them tightly. Second, our approach runs parallel HMM 
inferences for multiple objects whereas [10] is sequential by 
inferring the most likely object trajectory first and uses it to 
constrain the next object trajectory. Third, our motion model 
considers travel modes whereas [10] does not. Fourth, our 
approach can correct noisy observations, whereas [10] does 
not. 
III. 
TRACKING SYSTEM BASED ON BN AND HMM 
The logic architecture of our tracking system is illustrated 
in Figure 1, where N recognizers receive video streams from 
distributed cameras, perform object recognitions and send 
the observations to the tracker. The tracker estimates object 
paths based on extended HMM.  
The architecture is independent of camera resolutions and 
frame rates as well as video compression and transmission 
technologies. This allows the tracking system to reduce 
network bandwidth without losing tracking accuracy by 
matching the camera operations with the speed ranges of the 
target objects. The tracking system provides sufficient 
computing power to run the recognizers and trackers in real-
time on the video streams.  
Each recognizer R is treated as a blackbox function 
R(C,O)=(t,St) that maps a video stream from camera C and 
an object O to observation St at time t. For convenience, 
St=M>0 denotes that R observes O at some mode M, St=0 
denotes that R does not observe O, and St=−1 denotes that R 
fails. Travel mode M includes various means of mobility, 
such as walking, running, bicycle, car, boat, airplanes.  
  
Figure 1. Logic architecture of tracking system 
When object O moves within a camera network, the 
cameras it visits form an object path. When the recognizers 
are unreliable, the observed cameras may differ from the 
actual object path due to random errors: 
 
Type 0 false negative: St=0 but the correct values is 
St>0 since O was at C; 
 
Type 1 failure: St=−1 but the correct value is St>0 
since O was at C; 
 
Type 2 false positive: St>0, but the correct value is 
St=0 since O was not at C.  
To correct these random errors, we model the object path 
as the hidden states of an extended HMM that generates 
noisy 
observations 
according 
to 
some 
probabilistic 
distributions. The probabilistic distributions are derived from 
a BN that connects three probabilistic behavioral models: 
intention, motion and competence, to define the causes of 
noisy observations. With these probabilistic distributions, 
efficient inference algorithms are used to estimate the object 
paths as the optimal states of the HMM given the noisy 
observations.  
A. Probablistic Behavioral Models 
The intention model captures the uncertainty of object 
paths with three conditional probabilities. PI(Ci|O) is the 
probability that object O will enter the camera network at 
camera Ci. PI(Cj|Ci,O) is the probability that object O selects 
a place covered by camera Cj to visit from a place covered 
by camera Ci. PI(M|Ci,Cj,O) is the probability that object O 
will travel from camera Ci to Cj in transportation mode M. 
For simplicity, this model assumes that where an object 
tracker 
recognizer 1  
recognizer N  
 
 
Motion  
Competence  
Intention  
path 
 
 
Extended HMM  
22
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-559-3
SIGNAL 2017 : The Second International Conference on Advances in Signal, Image and Video Processing

travels and what mode it uses only depend on the current and 
next places. 
The motion model captures the uncertainty of travel time 
Δt in mode M from camera Ci to camera Cj with conditional 
probability PM(Δt|Ci,Cj,M). For simplicity, we assume that 
PM(Δt|Ci,Cj,M) is a Gaussian (Normal) distribution N(µ,σ) 
with mean µ and standard deviation σ. The motion model is 
independent of objects and abstracts complex indoor and 
outdoor physical environments as a 3 dimensional matrix.  
The competence model captures the uncertainty of 
recognizers with three probability distributions: 
 
true_pos(C)=PC(St>0|O at C  at t) is the conditional 
probability that recognizer R observes object O when 
it is at C at time t;   
 
false_neg(C)=PC(St=0|O at C  at t) is the conditional 
probability that recognizer R misses object O when it 
is at C at time t, where false_neg(C) = 
1−true_pos(C);  
 
failure(C)=PC(St=−1|O at C  at t) is the probability 
that recognizer R fails at time t. 
B. Bayesian Network 
The BN [11] in Figure 2 connects the 3 probabilistic 
models according to the causal processes that produce the 
random observations when object O transitions between 
cameras. The BN is represented as a directed factor graph 
[11], where the circles represent random events and the 
rectangle, called factors, represent the conditional probability 
distributions between the random events. O is omitted from 
the factors for clarity.   
 
Figure 2: BN represented as a directed factor graph 
Object O enters the places covered by the cameras 
according to factor 1. While at camera Xi at time ti, object O 
selects a place to go next according to factor 3, while the 
place is covered by camera Xj. Then object O chooses mode 
M according to factor 4. Finally, object O transitions to the 
selected place during interval Δt according to factor 5. We 
can combine factors 3, 4 and 5 into one that defines the joint 
probability distribution of next place, mode and interval 
given the current place: 

P(Xj,M,Δt|Xi)= PI(Xj|Xi)PI(M|Xi,Xj)PM(Δt|Xi,Xj,M) 
When object O is at camera Xi or Xj, the recognizer will 
produce observations according to factors 2 and 6, which are 
defined below: 

P(Si|Xi)=(1−falure(Xi))true_pos(Xi) if Si>0


P(Si|Xi)=(1−falure(Xi))false_neg(Xi) if Si=0


P(Si|Xi)=falure(Xi) if Si=−1

C. Extended HMM 
 As object O moves across N cameras in T-1 transitions, 
the causal processes in the BN (Figure 2) are repeated T-1 
times to produce T observations. The repetitions of T-1 
copies of the BN forms an extended HMM for O in Figure 3, 
which has N×T states that represent all possible object paths 
of O that could have produced the T observations.   
 
Figure 3: Extended HMM 
Since any recognizers can generate false positive 
observations, concurrent observations of object O at multiple 
states can occur at the same time, although only one may be 
true. However, conventional HMM [12] does not permit 
concurrent observations. To address this problem, we use 
vector Yi to represent concurrent observations of N 
recognizers at time ti, where Yi[j]=Si of the j-th recognizer. 
For example, Yi=[1, 0, 2, −1] indicates that at time ti, 
recognizer R1 observes object O in mode 1 and R3 observes O 
in mode 2, while R2 does not observe O and R4 fails. 
Furthermore, we assume that each recognizer R observes 
object O independently at any time. Under this assumption, 
(5) reduces the probability of concurrent observations to the 
probability of individual observations. 

P(Yi|Xi)=P(Yi[Xi]|Xi)=P(Si|Xi)

Equations (6)-(8) define the parameters of the extended 
HMM using the notations in [12], where: 
1. aij is the state transition probability from Xi to Xj; 
2. bj(k) is the probability of observation k at state Xj;  
3. πi is the initial state distribution.  

aij=P(Xj,M,Δt|Xi)


bj(k)=P(Yj[k]|Xj)


πi=PI(Xi)

Equation (6) is tied to the intention and motion models 
by (1), while (7) is tied to the competence model by (2)-(5).  
Two well-known algorithms can be used to infer the 
optimal state sequence with O(N2T) operations for N states 
and T observations. The Forward-Backward (FB) algorithm 
Xi  
M 
Δt=tj−ti  
Xj  
 
tj,Sj 
3:PI(Xj|Xi) 
ti,Si 
4:PI(M|Xi,Xj) 
5:PM(Δt|M,Xi,Xj) 
6:P(Sj|Xj) 
2:P(Si|Xi) 
1:PI(Xi) 
X1 
Xi 
Xj 
XT 
P(Xj,M,Δt|Xi) 
 
ti,Yi 
 
tj,Yj  
t1,Y1  
tT,YT  
P(Yi|Xi) 
P(Yj|Xj) 
P(Y1|X0) 
 
P(YT |XT) 
 
M,Δt=tj−ti 
PI(X1) 
 
23
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-559-3
SIGNAL 2017 : The Second International Conference on Advances in Signal, Image and Video Processing

[12] selects T optimal states Xi that independently maximize 
P(Xi|Y1…YT) as follows: 







N
i
t
t
t
t
t
N
i
t
t
i
i
i
i
i
i
q
1
1
( )
)
(
( )
( )
( )
( )],
argmax[








The right-hand side of (9) is computed using the forward 
pass (10) and the backward pass (11) based on (6)-(8). 

N
j
T
t
i a b Y
j
N
i
b Y
i
t
j
ij
N
i
t
t
i
i


 

 





1,
1 ),
(
]
( )
[
( )
 )
2
1 ),
(
( )
 )1
1
1
1
1
1










 
 

 

N
j
t
t
j
ij
t
T
N
i
T
t
j
a b Y
i
N
i
i
1
1
1
1,
1 ),
(
)
(
( )
 )
2
1 ,1
( )
 )1




The Viterbi (VI) algorithm [12] selects T optimal states 
that jointly maximize P(X1… XT|Y1…YT)  as follows:  

T
t
q
q
i
q
i
p
N
j
T
t
j a
i
b Y
j a
j
N
i
i
b Y
i
t
t
t
T
N
i
T
T
N
i
ij
t
N
i
t
t
j
ij
t
i N
t
i
i
 





 


 




 








1 ),
(
 )
4
( )]
argmax[
( )],  
max[
 
3)
1 ,
2 ],
( )
argmax[
( )
 
2b)
)
(
]
( )
max[
 ( )
2a)
1 ,0
( )
 ),
(
( )
 )1
*
1
1
*
1
*
1
*
1
1
1
1
1
1
1











We extend the FB and VI algorithms to return N-best 
optimal states at each timestamp, called N-best path to be 
distinguished from object path.  A N-best path contains T N-
best ranks, and each rank is a sequence of triples (ti, Xi, Pi) 
sorted by descending order of Pi, where Xi is the optimal 
state (camera), and Pi is the probability of the object in state 
Xi at timestamp ti given T observations.  
IV. 
EXPERIMENTAL RESULTS 
A prototype tracking system was implemented in Python 
3.4.3 based on the extended HMM and tested on simulated 
data. The test procedures are illustrated in Figure 4, where the 
rectangle boxes represent the procedures and the curved 
boxes represent data. Based on a US city distance map, we 
bootstrapped the intention, motion and competence models. 
We then generated object paths and noisy observations from 
these models. The noisy observations are fed to the extended 
HMM to infer the N-best paths. Finally, we compare the N-
best paths against the object paths to measure the accuracy 
and performance of the HMM. 
The US city map was downloaded from a website [13], 
which includes distance dij between 31 cities, where dij=dji 
and dii=0. We simulate a situation where an object (a 
vehicle) is driving across the cities, while the cameras in the 
cities may produce noisy observations about the object. The 
tracker must decide which cities the object has actually 
visited based on the noisy observations. 
A. Probablistic Model Generations 
The intention model of the object is derived from the 
distances by setting PI(Cj|Ci,O)=dij-1/Z1 if dij≠0, and 
PI(Cj|Ci,O)=0.0 if dij=0, where Z1=∑jdij-1 normalizes the 
numbers into a probability distribution. These assignments 
make the object more likely to travel to closer cities. The 
initial 
state 
distribution 
is 
obtained 
by 
setting 
PI(Ci|O)=1/Z2∑jPI(Cj|Ci,O), where Z2 normalizes the sum 
into a probability distribution, such that the object is more 
likely to be in a city which has a more close neighbors. 
 
Figure 4: Test procedures 
The motion model is derived from distances dij by setting 
µ=dij/65 and σ=dij/(4×65), based on the average US highway 
speed limit of 65 miles/hour, and the fact that travel time 
tends to vary more as the distance increases. As the result, 
PM(Δt|Ci,Cj,M)~N(dij/65, dij/260) for driving mode M. 
The competence model is derived from a range of 
random choices. For each camera Ci, we randomly selected 
true_pos(Ci) from {1.0, 0.90, 0.80, 0.70}, false_neg(Ci) from 
{0.1, 0.01, 0.001}, and failure(Ci) from {0.01, 0.001, 
0.0001}, all with uniform distributions, to simulate a tracking 
system with mixed high-end and low-end recognizers. 
B. Object Path and Noisy Observation Generations 
We first generated the object paths from the models using 
a Random Walk procedure and then perturbed the paths to 
obtain the noisy observations using a Path Perturbation 
procedure.  An object path is a sequence of (ti, Zi, Mi) triples, 
where ti is the timestamp, Zi the camera, and Mi the mode. A 
noisy observation Y=[Y0,..,YL]  is a sequence of observation 
vectors Yi as described in Section III, where each Yi may 
contain random errors. 
The Random Walk procedure accepts three arguments: 
start time t0, a list of modes ML and step count L and it 
produces an object path, which is a random sample of the 
HMM states. The procedure first randomly selects the initial 
camera Z0 according to distribution PI(Ci|O) and then 
randomly selects a mode M0 from ML. These selections 
constitute the first triple (t0, Z0, M0) of the object path. To 
construct the next triple, the procedure randomly selects the 
Extended 
HMM 
US City 
Map 
Intention 
Motion 
Competence 
N-best 
paths 
Object 
paths 
Noisy 
Observations 
Accuracy 
Model 
Generation 
Random 
Walk 
Path 
Perturbation 
Compare 
24
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-559-3
SIGNAL 2017 : The Second International Conference on Advances in Signal, Image and Video Processing

next camera Z1 according to distribution PI(Cj|Z0,O) and 
mode M1 from ML. It then generates a random interval t1 
from Gaussian distribution PM(Δt|Z0,Z1,M1) to construct the 
second triple (t1, Z1, M1). These steps are repeated L times to 
produce a path of L+1 triples.  
The Path Perturbation procedure accepts an object path 
of L+1 triples and produces a noisy observation Y of L+1 N-
dim vectors Yi. For each triple (ti, Zi, Mi) in the object path, 
the procedure sets the timestamp of Yi to ti, Yi[Zi]=Mi and 
Yi[k]=0 for k≠Zi. It then randomly selects an error type from 
set {0, 1, 2} to modify Yi as follows: 
 
If type=0, set Yi[Zi]=0 with probability false_neg(Zi) 
to simulate false negative errors; 
 
If type=1, set Yi[Zi]=−1 with probability failure(Zi) to 
simulate failures; 
 
If type=2, randomly select two indexes m, n not equal 
to Zi and set Yi[m]=Yi[n]=Mi to simulate false positive 
errors by cameras Cm and Cn.  
For each vector Yi, there is false_neg(Zi)/3 chance of 1 
type 0 error, failure(Zi)/3 chance of 1 type 1 error, and 1/3 
chance of 2 type 2 errors. Type 0 and 1 errors create 
observation gaps in the object path, while type 3 errors 
introduce distractions into the object path. These errors 
significantly deviate a noisy observation from the true object 
path at multiple timestamps. 
We use two measures, error count and noise ratio, to 
quantify the deviation of noisy observations from the object 
paths. The error count of Y is the total number of errors in Y. 
The noise ratio of N observations Y is M/N, where M is the 
number of observations Y whose error count is greater than 
0. Since the error count is proportional to L and the noise 
ratio is proportional to the error count, both measures will 
increase as L increases. The plot in Figure 5 illustrates these 
relations when L increases from 1 to 20. 
C. Accuracy Measurement 
After a tracking system produces an N-best path Sb 
(described in Section III) from noisy observation Y perturbed 
from an object path So, we can measure its accuracy by three 
metrics: recall, precision and F-measure. Recall measures 
how many triples in So are in Sb, while precision measure 
how many triples in Sb belong to So. If Sb is a perfect match 
of So, then both recall and precision will be 1. If they have no 
common triple, then both recall and precision will be 0. 
Higher recall and precision yield higher F-measure. The 
metrics are calculated by (13), where NB>0 decides the size 
of the N-best ranks in Sb. Since length(Sb)≥length(So), these 
metrics are in range [0,1]. 
The rank function determines if a triple Ri=(ti, Zi, Mi) in 
So can be found in Sb which consists of ranked triples (ti, Xi, 
Pi). A match is found if both triples have the same ti and 
Zi=Xi. When a match is found, the rank position 0≤k<NB of 
the N-best triple is returned. The hit function weights and 
accumulates the results of rank functions, such that a higher 
rank receives more weight up to 1. Since S0 and Sb have the 
same length in our case, the recall, precision and F-measure 
metrics become equal. For this reason, we only include F-
measure in our test results. 

]1,0
[
)
,
(
)
,
(
)
,
(
)
,
(
2
)
,
(
]1,0
[
)
(
)
,
(
)
,
(
]1,0
[
)
(
)
,
(
)
,
(
)]
(
[ ,0
)
(
)
,
(
]
[ ,0
if 
 
 if 
)
(



















b
o
b
o
b
o
b
o
b
o
b
b
o
b
o
o
b
o
b
o
o
S
R
i
b
o
b
i
b
i
i
S
precision S
S
S
recall
S
precision S
S
recall S
S
S
F
S
length
S
hit S
S
S
precision
S
length
S
hit S
S
S
recall
length S
NB
rank R
NB
S
S
hit
NB
S
R
k
S
R
NB
R
rank
o
i

D. Test data and Results 
To simulate object tracking with small amount of 
observations, we selected path lengths L=1, 2, 3, 4, 5, 10, 15 
and 20 to generate 8 sets of 500 object paths and noisy 
observations. Figure 5 plots the error count and noise ratio of 
the noisy observations. The noisy observations deviate 
significantly from the object paths and the deviation 
increases monotonically with path length. At path length 1, 
60% observations have over 2 errors. At path length 20, 
100% observations have over 14 errors, which means that 
66.66% vectors in an observation have some errors. 
 
Figure 5. Error counts and noise ratios of 8 datasets 
For each of the 8 datasets, we ran 6 configurations (Table 
I) of the tracking system, by activating different models and 
inference algorithms. We then averaged the F-measures of 
the 500 N-best paths as the F-measure of each configuration. 
The results are shown in Figure 6, where the FB family of 
configurations 1, 3, 5 clearly outperforms the VI family of 
configurations 2, 4, 6. The average F-measure of the FB 
family is 77.3%, with 47.9% relative improvement over the 
52.2% of the VI family.  
One possible reason that the VI family is more prone to 
the observation errors than the FB family is because the 
Viterbi algorithm selects the next optimal state based on the 
previous one such that one early mistake can derail the entire 
selections. 
25
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-559-3
SIGNAL 2017 : The Second International Conference on Advances in Signal, Image and Video Processing

TABLE I.  
6 CONFIGURATIONS OF TRACKING SYSTEM 
 
intention 
motion 
algorithm 
1 
used 
 
not used 
 
Forward-Backward (FB) 
2 
Viterbi (VI) 
3 
used 
 
used 
 
FB 
4 
VI 
5 
not used 
 
used 
 
FB 
6 
VI 
 
Figure 6. F-measures of 6 tracking configurations on 8 datasets, where the 
Top line connects the best configurations for each dataset 
The top configurations for the 8 tests alternate between 
configurations 1 and 5, indicating that using the intention and 
motion models alone is better than combining them. We 
suspect that using both models may have over-fitted the 
HMM to the object paths that deviate significantly from the 
noisy observations.  
The F-measures of the FB and VI algorithms are quite 
stable as the path length increases, with standard deviations 
of 0.010 and 0.017 respectively. This shows that the 
algorithms are robust to the increasing noises and errors in 
the observations shown in Figure 5.  
All the tests were run on a 32-bit Windows 7 Lenovo 
T410 notebook computer with 2.67Ghz Dual Core CPU and 
3GB RAM. The average tracking time is 26.6 milliseconds 
with a standard deviation of 0.68.  
V. 
CONCLUSIONS 
This paper described a probabilistic method to correct 
concurrent noisy observations about moving objects from 
unreliable recognizers (cameras). Our main contributions 
are:  
 
A BN to connect 3 probabilistic models of object 
behaviors and recognizer competence into causal 
relations to explain the noisy observations;  
 
An extension to HMM to support inference on 
concurrent observations from multiple cameras; 
 
An extension to FB and VI algorithms to return N-
best optimal states. 
As the simulated tests demonstrated that it is feasible to 
build a robust tracking system from unreliable recognizers, 
future work is needed to improve the accuracy. One 
important direction for future research is to learn the 
probabilistic models from noisy observations using statistical 
machine learning techniques [11][12]. It is also interesting to 
extend the tracking model to deal with more challenging 
errors in more realistic datasets. 
REFERENCES 
[1] M. Gunther, L. E. Shafey, and S. Marcel, “Face Recognition 
in 
Challenging 
Environments: 
An 
Experimental 
and 
Reproducible Research Survey,” Face Recognition Across the 
Imaging Spectrum, Springer, pp 247-280, 2016. 
[2] A. P. Patel, V. C. Gandhi, “Survey on Human Gait 
Recognition,” IJCST Vol. 5, Issue 4, Oct - Dec 2014, pp 275-
277, 2014. 
[3] S. Du, M. Ibrahim, M. Shehata, and W. Badawy, “Automatic 
license plate recognition (ALPR): A state-of-the-art review,” 
Circuits and Systems 
for 
Video Technology, IEEE 
Transactions on, 23(2), 311-325, 2013.  
[4] J. Sochor, A. Herout, J. Havel, “BoxCars: 3D Boxes as CNN 
Input for Improved Fine-Grained Vehicle Recognition,” The 
IEEE 
Conference 
on 
Computer Vision 
and Pattern 
Recognition (CVPR), pp. 3006-3015, 2016. 
[5] H. Liu, Y. Tian, Y. Wang, L. Pang, and T. Huang, “Deep 
Relative Distance Learning: Tell the Difference Between 
Similar Vehicles,” The IEEE Conference on Computer Vision 
and Pattern Recognition (CVPR), pp. 2167-2175, 2016. 
[6] M. Taj and A. Cavallaro, “Distributed and decentralized 
multi-camera tracking,” IEEE Signal Processing Magazine, 
Volume 28, Issue 3, pp. 46-58, May 2011. 
[7] P. Climent-Perez, D. N. Monekosso, and P. Remagnino, 
“Multi-view event detection in crowded scenes using tracklet 
plots,” The 22nd International Conference on Pattern 
Recognition (ICPR), pp. 4370-4375, 2014.  
[8] Y. Xu, X. Liu, Y. Liu and S.-C. Zhu, “Multi-view People 
Tracking via Hierarchical Trajectory Composition,” The 
IEEE 
Conference 
on 
Computer Vision 
and Pattern 
Recognition (CVPR), pp. 4256-4265, 2016. 
[9] D.-T. Lin and K.-Y. Huang, “Collaborative Pedestrian 
Tracking and Data Fusion With Multiple Cameras,” IEEE 
Transactions on Information Forensics and Security, Vol. 6, 
No. 4, pp. 1432-1444, December 2011. 
[10] F. Fleuret, J. Berclaz, R. Lengagne, and P. Fua, “Multicamera 
People Tracking with a Probabilistic Occupancy Map,” IEEE 
Transactions on Pattern Analysis and Machine Intelligence, 
Vol. 30, No. 2, pp. 267-282, February 2008. 
[11] C. M. Bishop, Pattern Recognition and Machine Learning, 
Springer, 2006. 
[12] L. R. Rabiner, “A Tutorial on Hidden Markov Models and 
Selected Applications in Speech Recognition,” Proceedings of 
IEEE, Vol. 77, No. 2, pp. 257-286, February 1989. 
[13] MapCrow, 
Cities 
in 
United 
States, 
http://www.mapcrow.info/united_states.html, last accessed: 
April 3, 2017. 
 
 
 
 
26
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-559-3
SIGNAL 2017 : The Second International Conference on Advances in Signal, Image and Video Processing

