Towards a Cloud-Based Architecture for 3D Object Comprehension in Cognitive 
Robotics 
Charlotte Sennersten, Ahsan Morshed, Martin Lochner, and Craig Lindley 
CSIRO Computational Informatics (CCI) Autonomous Systems (AS)  
Commonwealth Scientific and Industrial Research Organization (CSIRO)  
Hobart, Australia 
{charlotte.sennersten, ahsan.morshed, martin.lochner, craig.lindley}@csiro.au 
 
 
Abstract—Cognitive 
robotics 
can 
take 
advantage 
of 
distributed, web-based information as a foundation for 
comprehending 3D objects in a 3D scanned world. The 
proposed CogOnto model makes possible grounding a 
cognitive computing system with sensor data gathered from 
diverse and heterogeneous sources, associated with humanly 
crafted symbolic descriptors. The system supports cognitive 
embodiment within the totality of an information ecology, and 
not just within the physical world where an individual robot, 
essentially a mobile peripheral device, is located. The informed 
system uses 3D objects as common denominators for shared 
world comprehension.      
Keywords-Cognitive 
Modelling; 
Eye 
Tracking/Steering; 
Human Robot Interaction; Knowledge base; Ontology.   
I. 
 INTRODUCTION 
Field robotics technology has matured to the point where 
commercial robotics platforms are available for diverse 
applications, such as surveillance, sample and data 
collection, analysis and return, construction, agriculture and 
mining operations. Communications links with robots now 
have high data capacities. A consequence of these advances, 
however, is that human operators receive increasing amounts 
of data streamed from robots that they must perceptually and 
cognitively process, often in real time, in order to perform 
real-time tele-robotic tasks. This data input is often of an 
overwhelming volume and complexity. One approach to 
dealing with this task performance demand is to offload 
some or all of the required cognitive processing onto the 
robot platform itself. Hence cognitive robotics aims to 
develop intelligent software capable of performing highly 
automated cognitive task performance by robots in order to 
optimize their use and take best advantage of the latest 
hardware developments. Artificial Intelligence (AI) has long 
sought solutions for making robots more intelligent, with 
rather limited success. 
The formation and use of representations, and the 
possibility of making representations meaningful, is a key 
attribute of intelligence, and is one of the areas where AI has 
met 
challenges 
due 
to 
the 
human 
authorship 
of 
representations in traditional AI systems; the representations 
are too abstract to be grounded for the technical artifact, do 
not change with their contexts, require human interpretation 
to provide their meaning, and have arbitrary bounds [1].  
Grounding the formation of symbolic representations in 
dynamic and embedded processes as biological systems do 
provides one approach to trying to avoid these issues in 
knowledge 
representation. 
However, 
the 
increasing 
availability 
of 
extensive 
broadband 
communications 
networks, high capacity computer memory and processing 
services, and extensive on-line data, suggests an alternative 
approach to symbol grounding and embedded cognition. 
This is by the use of repositories of previously captured 
sensor data together with real-time sensor data that have 
labels and semantic annotations supporting their discovery 
and reuse in AI systems. A cognitive robotics system 
realized on this basis can have the following features: 
 
 
Agency can be nested, where a robot consists 
minimally of a hardware platform.  
 
The on-board processing ability of a robot can 
scale, from low level interfaces for sensor 
transmission and command reception, through 
increasing levels of on-board autonomy, to full 
autonomous operation [2]. 
 
Intelligence in the system does not need to be 
physically encapsulated or localized.  
 
Intelligent agency can be mapped across one or 
more robot platforms and hardware processing 
networks, with cognitive processing that is partially 
or wholly cloud-based. 
 
An intelligent agent can use the cloud-based 
memory of past perceptions of other robotic and 
human sensory data as a technical analog of human 
episodic memory. 
 
All ongoing and past sensor streams, decision 
processes 
and 
generated 
actions 
(i.e. 
the 
‘experiences’) of agents can be stored for analysis 
and application in ongoing and future task 
performance. 
 
The scope of an agent can be scaled in proportion 
to the task that it is performing and the 
environment in which the task is performed. 
220
Copyright (c) The Government of Australia, 2014. Used by permission to IARIA.     ISBN:  978-1-61208-340-7
COGNITIVE 2014 : The Sixth International Conference on Advanced Cognitive Technologies and Applications

Robotics research is beginning to explore ideas like these 
in a number of scenarios [3]. In this paper, we focus on the 
use of robotic 3D object perception and propose the use of a 
cloud-based infrastructure to implement a machine vision 
paradigm inspired by Marr’s theory [4] of visual cognition. 
We also propose a method of using 3D simulation integrated 
with this perceptual approach. The derivation of 3D model 
data from perception provides world state information as 
input to an ongoing world simulation. The simulation 
provides predictions about future states. Those predictions 
facilitate rapid processing in future perception. The 
comparison of predicted states with perceived states also 
provides foundations for tuning the simulation and its 
parameters, that can also be represented declaratively to 
support higher level reasoning. The proposed CogOnto 
model described below stores the process and object(s) 
information in a knowledge system that can guide the robot 
in physical collaboration, manipulation and navigation. 
The structure of the paper brings you as a reader from a 
‘high level architecture’ to ‘3D visual processing’ and 
thereafter to ‘intelligent action in a structured world’. The 
actual contribution of this paper is presented in the section 
‘proposed model’, followed by ‘integrating semantic web 
concepts, resources, and technologies to the final summary.  
II. 
HIGH LEVEL ARCHITECTURE 
When the robot is operating in the physical world it may 
be controlled by a cognitive agent residing off-board, see 
Fig. 1. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. Robot hardware assisted by the cognitive agent where real 
time 3D object perception and recognition are supported by the 3D virtual 
world and knowledge cloud. 
 
If the robot shall observe and manipulate a 3D object, it 
must have real time perception, comprehension and memory 
recall so that the robot knows how to execute the 
manipulation task(s) via motor action. The linked data cloud 
is operatively called to match already stored 3D object 
information parallel to real time 3D object extraction from 
the scanned physical world: this is the recognition phase. The 
3D world state model constitutes a virtual world derived 
from a scanned volume of view where objects and object 
motions are captured, digitized and recorded. The recorded 
information is fed into memory and can be used for 
simulation scenarios and prediction of scenario events. 
Notice in relation to ‘first order predicate calculus’ [5] that 
3D objects here are both subjects and objects, while 
adjectives and verbs are predicates. 
III. 
3D VISUAL PROCESSING 
The ability to scan a real world environment makes it 
possible to extract digital information about the physical  
world and how it functions. Three dimensional perception is 
a key technology for robotics applications where obstacle 
detection, mapping and localization are core capabilities for 
operating in unstructured environments. Laser scanning 
creates a surface point cloud of a 3D physical environment 
[6] making it possible to map any environment in a rather 
short time (the Leaning Tower of Pisa was scanned in 20 
minutes). This technology can be used in a robotic 
intelligence system for Simultaneous Localization Mapping 
(SLAM) and higher level reasoning regarding location and 
position. However, object recognition and manipulation 
requires deriving 3D object information from the overall 
point cloud and building cognitive models with task 
reasoning for using object and scene data in real time.     
 
Object extraction [7][8][9] makes it possible to know 
what a robot is looking at, supporting manipulation or 
collection actions. This can be achieved by an Environmental 
Scanning-Object Extraction (ES-OE) engine. For human-
robot collaboration, a robot can be enabled to use deictic 
visual references from human gaze by integrating an eye 
tracker with the ES-OE engine.  
A. Background 
In a previous work [10], a 3D simulation engine was 
integrated with an eye tracker. The integrated system allows 
the human point of gaze on 3D objects within a 3D digital 
world projected onto a computer screen to be tracked 
automatically. This development made it possible to log gaze 
in various task-related environments in a simulated world. 
From a Human Factor’s perspective, the simulation and 
human 
observation 
can 
be 
investigated, 
including 
collaborative actions performed by groups with various 
workloads, stressors and decisions. There have been several 
studies made using the technological framework with 
different stimuli [11][12][13], but no substantial theoretical 
framework has been developed in relation to this object-
based approach per se. A bottleneck in relation to this visual 
approach has been that 2D image, film and visual stimuli 
have not met the requirements for incorporating a 
knowledge-based approach for dynamic 3D worlds, whether 
the real physical world or a digitized 3D world. The object 
approach needs to address how both modeled and real world 
objects can be perceived and manipulated [14] by a robot, 
allowing the system to sense, think and act in real time: the 
computer needs to understand how to define an object and 
how to ontologically and semantically make sense out of 
such an object in a dynamic spatial world. 
 
1) 3D objects in a 3D world 
In [10], a simulation engine integrated with an eye tracker 
took a gaze fixation (x and y screen coordinates) and ray 
 
221
Copyright (c) The Government of Australia, 2014. Used by permission to IARIA.     ISBN:  978-1-61208-340-7
COGNITIVE 2014 : The Sixth International Conference on Advanced Cognitive Technologies and Applications

casted/traced from that position onto the underlying 3D 
virtual object’s collision box, a volume corresponding with 
the shape of a virtual object as recognized and processed by 
a physics engine that is also used to designate objects by 
interface devices, like a mouse. This made it possible to track 
gazed objects in real time every 17 ms (using a 60Hz eye 
tracker). The same principle can be used in a physical world 
context where an ES-OE engine could be integrated with eye 
tracking glasses to allow a computational system to know 
what object a person wearing the glasses is looking at. 
 
2) Structuring a noisy world 
The 3D world scenario, simulated or physically real, 
constitutes an event or scene. A scenario includes objects 
that are instances of their classes. A class could be something 
like a CarClass, HumanClass, FlowerClass, etc. 
In a constrained world, we can name all objects 
beforehand so when they are logged we know what they are 
and what position (x, y, z, 1, 2, 3) they are in. In an 
unconstrained environment that is scanned and has extracted 
objects, we must also have a capability to know what the 
objects are and to be able to classify them. A cloud-based 
approach of the kind proposed in this paper presents a middle 
ground, being more open than a highly constrained 
environment, but still being limited to objects of types that 
are represented and labeled within the cloud. 
IV. 
INTELLIGENT ACTION IN A STRUCTURED WORLD 
Knowledge by definition is “1. Facts, information, and 
skills acquired through experience or education; the 
theoretical or practical understanding of a subject and 2. 
Awareness or familiarity gained by experience of a fact or 
situation.” [15]. To gain an understanding of how robots 
might learn and operate on knowledge, we have looked at 
several established models that can fit within an initial 
architecture that enhances these established models by the 
ingestion of information from the web.  Our overall aim is to 
build a computational comprehension system for 3D object 
information, assisted by a hybrid computational ontology 
(i.e., combining several existing and new ontologies). 
A. Existing Models 
Extensive effort has been put into the task of 
understanding and attempting to re-create/simulate the 
processes by which a human being thinks. Using the 
underlying assumption that intelligence is wholly “the simple 
accrual and tuning of many small units of knowledge” [16], 
production-based models of cognition have had success in 
displaying human-like performance on a number of tasks 
(e.g., visual search [17] and natural language processing 
[18]). While there are debates regarding the similarity of 
what humans actually do to what we have achieved using the 
above assumption [19], there is little doubt that such systems 
can produce intelligent-seeming behavior, that can facilitate 
the development of vitally useful control structures in the 
field of robotics and computational intelligence [18]. 
One of the most influential models of human cognition is 
the ACT-R, or “Adaptive Character of Thought – Rational” 
model [16], developed over many years by John Anderson, 
who was a student of the seminal Cognitive Scientist Alan 
Newell (1927-1992). Anderson’s model is a hybrid 
symbolic/sub-symbolic system that incorporates various 
“modules” that are deemed necessary for rational behavior, 
and are thought to have biological correlates.  These include 
the modules Declarative (manages creation, storage and 
activation of memory “chunks”), Procedural (stores and 
executes 
productions 
based 
on 
expected 
utility), 
Intentional/Imaginal 
(goal 
formulation 
for 
directed 
behavior), and Visual (2D)/Audio (theoretically plausible 
implementation of visual and auditory perception), see Fig. 
2. An internal pattern-matching function searches for a 
production that matches the current state of the buffers. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2. A schematic representation of the canonical ACT-R cognitive 
model. 
 
ACT-R is formed as a knowledge model where the 
“chunks” are the elements of declarative knowledge in the 
ACT-R theory and are used to communicate information 
between modules through the buffers. A chunk is defined by 
its chunk type, that is described by its slots (here compared 
with properties), see table 1. Chunk types can be organized 
as a hierarchy of parent (SuperType)-child (SubType) 
relationships. The subtype will inherit all of the slots 
(properties) of the parent node(s).  
Other models that take a similar symbolic approach to 
model human cognition include Soar [20], EPIC [21], 
CLARION [22], and others (for a detailed review see [23]). 
While these have been successful to varying degrees at 
modeling specific human cognitive task(s) performance, it is 
becoming evident that such models are intrinsically limited 
by their disconnections from the real world in which humans 
(or robots) operate. A production based system is only as 
adaptive as its rule set allows given the inputs provided to it, 
that have generally been limited to “screen as eye” and 
“keyboard/mouse as hands” mappings.  A new wave of 
thought surrounding the development of cognitive models is 
embracing the need for “embodied” cognition, improving the 
ability of the system to sense and act.  One example of this is 
the ACT-R/E framework, used as an operating system for 
 
222
Copyright (c) The Government of Australia, 2014. Used by permission to IARIA.     ISBN:  978-1-61208-340-7
COGNITIVE 2014 : The Sixth International Conference on Advanced Cognitive Technologies and Applications

mobile robotics developed by the American Naval Research 
Lab [24], depicted in Fig. 3. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 3. The “embodied” (Visual 3D) modifications introduced by Trafton 
et al. 2012.  Additions in the ACT-R/E are highlighted in red. 
 
The Object-Attribute-Relation (OAR) model of Wang, 
2007 [25], specifies the elements of a cognitive model in the 
fashion of an ontology, the logical model of memory.  In an 
attempt to formally describe the mechanism of human Long 
Term Memory (LTM), which he states is the “foundation of 
all forms of natural intelligence” (p. 66), Wang decomposes 
the construct into three elemental components – Objects, 
Attributes and Relations. This OAR model allows the 
computational specification of the human LTM formation 
and storage process, and is put forth as having sufficient 
explanatory power as to describe the “mental process and 
cognitive 
mechanisms 
of 
learning 
and 
knowledge 
representation” (p.72). This model has a strong parallel with 
the specification of knowledge in information processing 
Ontologies. This parallel is direct, as described by the 
relations given in Table I. 
TABLE I.  
COMPARISON OF MODEL TYPE CONSTRUCTS 
 
 
 
 
 
 
 
A critical issue for any of these kinds of models is the 
relationship of their constructs to the environments in which 
they are expected to provide foundations for action. The core 
notion of embodiment is to provide the heretofore 
functionally “disembodied” computational model with 
sensors and effectors that allow its direct interaction with the 
physical world. In such a way, the inherent limitation of 
human-defined input may be overcome. In addition to 
physical sensory perception and manipulative ability, a 
human may have access to a detailed semantic understanding 
of the surrounding world.  In the quest to produce a non-
human intelligent actor within a physical space, we must 
provide the actor with an understanding of underlying 
structures, i.e. specific denotations in the physical world. 
V. 
PROPOSED MODEL 
In the CogOnto model, we propose a further 
augmentation of the cognitive models discussed above, 
providing 
the 
robot 
with 
detailed 
3D 
schematic 
representations of objects that it encounters in real time, 
supported via task models, knowledge models and 
ontologies. 
The CogOnto model is composed of five parts   
 
<Si,Ci,Ai,Oi,Ri> , where i = 1.. N, and where  Si  is a finite set 
of situations,  Ci is a finite set of classes, Ai is a finite set of 
attributes for characterizing a class, Oi is a finite set of 
objects in a class, and Ri is a finite set of relationships among 
the objects.  In the CogOnto model (Fig. 4), we consider the 
following features [26][27]: 
 
Situation: represents an interactive (i.e. dynamic) 
real world scenario.  
 
ConceptNet: is a network of class-to-class 
relationships applicable in a given situation. 
 
ObjectNet: an object is an instance of a class. 
ObjectNet is a network of object-to-object 
relationships. 
 
AttributeNet: is a network between properties of 
classes and objects. 
 
Relation: is a function associating concepts, 
classes, objects and attributes; e.g. a robot is part-
of an Intelligent Agent (IA), were the “part-of” 
relation connects two concepts. The relations 
(associations) may be modeled or created by an 
autonomous learning process. 
These constructs are not defined in detail here, but 
unlike the other models are not limited to textual/linguistic 
meanings. The CogOnto model illustrated in Fig. 4 has four 
major functional elements that share information: 1) the ES-
OE engine, 2) the eye tracking system interconnected with 
the ES-OE engine, 3) the OAR model functioning as the 
basis of the Cognitive System, and 4) the knowledge cloud, 
including external resources such as WordNet or Cyc. The 
latter is also called the Linked Open Data  and may be used 
to illustrate the intelligent process for sharing and exposing 
information in machine readable form by using uniform 
resource identifiers based on Berners-Lee’s [27][29] 
principles. These principles enable data communication 
guiding perception from procedural memory. 
The knowledge system of the CogOnto model can be 
perceived as a storage system that accesses real world object 
information and external semantic resource information via 
the existing knowledge cloud [29]. 
 
 
223
Copyright (c) The Government of Australia, 2014. Used by permission to IARIA.     ISBN:  978-1-61208-340-7
COGNITIVE 2014 : The Sixth International Conference on Advanced Cognitive Technologies and Applications

 
Figure 4. An illustration of the CogOnto model and its operative states. 
 
The knowledge system represents the integration of 
formal symbolic and free text descriptors of an object.  
VI. 
INTEGRATING SEMANTIC WEB CONCEPTS, 
TECHNOLOGIES AND RESOURCES  
CogOnto integrates its own knowledge resources with 
external resources accessible via the web. For example, 
WordNet is a lexical database where nouns, verbs, adjectives 
and adverbs are grouped into sets of cognitive synonyms 
(synsets). To recall an object, the ‘synsets (WordNet 2.1)’ 
[30] and the W3C [31] standard can be used at a text level, to 
describe what an object is when it is text-labeled. Ontologies 
can be expressed by using Semantic Web tools, e.g. Web 
Ontology Language (OWL) [32] and the Resource 
description framework Schema (RDFS) [33]. 
 The OAR model, with its Object, Attribute and Relation 
parts, 
and 
the 
ontological 
framework, 
containing 
Class/Instance, Relationship and Properties, can be inter-
mapped so the object world can be comprehended using 
existing resources and using the 3D information represented 
internally within an object model. The 3D object’s internal 
structure and shape can either be structured as Free Form 
Geometry (FFG) with surfaces and curves, or as Polygonal 
Geometry (PG) with points, lines and faces. The objects can 
be extracted and exported into different file formats, such as 
e.g. .obj files, .stl files. The .stl file format is a triangular 
representation of a 3D object, where each triangle is uniquely 
defined by its normal and three points representing its 
vertices. The format is native to the stereolithography 
Computer Aided Design (CAD) software created by 3D 
Systems (in this kind of format it is also possible to print the 
object out from a 3D printing machine).   
The 3D object file contains different layers cognitively 
(form, volume, size, other descriptive attributes, etc.), 
supporting our senses and perception operating in parallel 
when performing allocated manipulation tasks. A human 
looking at an object can relate to the object both on a 
denotative- and on a connotative level. The denotative level 
is understood as a pure noun level without any cultural 
associations, nor any emotional or associative signifiers to 
the object, it is purely instrumental. The connotative layer is, 
on the other hand, the level of cultural and personal 
associations attached to an object with experience over time.  
Geometrical information within the 3D object can be 
represented using the X3D XML-based file format, an ISO 
standard for representing 3D computer graphics.  
VII. CONCLUSION AND FUTURE WORK 
The CogOnto model with support from the technological 
implementation of the eye tracker system with the ES-OE 
engine can represent cognitive relations that can be 
processed by a robot operating in a spatial world [34].  
Formal knowledge structures within CogOnto face 
similar challenges to other knowledge representation 
formalisms, and this paper has shown isomorphism with a 
number of examples. However, the primary advance 
proposed is to use cloud-based resources that are not limited 
to formal representations to enhance the robustness of 
knowledge processing by the integration of similarity-based 
search. Those cloud-based resources may use text and 
images. But more interesting extensions for future work 
include new forms of cloud content, such as multi-spectral 
images, point clouds and behavior tracks. The main ongoing 
research challenge is to provide suitable similarity metrics 
for these data forms, integrating search results with formal 
structures, and developing methods for integrating them in 
unified search, or meta-search, results.  
REFERENCES 
[1] C. A. Lindley, “Synthetic Intelligence: Beyond A.I. and 
Robotics,” in Integral Biomathics: Tracing the Road to 
Reality, 
Simeonov, 
Plamen 
L.; 
Smith, 
Leslie 
S.; 
Ehresmann, Andrée C. (Eds.), 2012, Springer. 
[2] T. B. Sheridan, “Humans and Automation: System Design 
and Research Issues,” A John Wiley and Sons, Inc., 2002. 
[3] J. J. Kuffner, “Cloud-Enabled Robots,” IEEE-RAS 
International Conference on Humanoid Robotics, Nashville, 
TN, USA, 6-8 Dec, 2010. 
[4] D. Marr, “Vision: A Computational Investigation into the 
Human 
Representation 
and 
Processing 
of 
Visual 
Information,” MIT Press, 1982. 
[5] J. Barwise, Jon (1977); "An Introduction to First-Order 
Logic", in Barwise, Jon, ed.. Handbook of Mathematical 
Logic. Studies in Logic and the Foundations of 
Mathematics. Amsterdam, NL: North-Holland. ISBN 978-
0-444-86388-1, 1982. 
[6] M. Bosse, R. Zlot, and P. Flick, “Zebedee: Design of a 
Spring-Mounted 3-D Range Sensor with Application to 
Mobile Mapping,” IEEE Transactions on Robotics, vol. 28, 
no. 5, 2012. 
 
224
Copyright (c) The Government of Australia, 2014. Used by permission to IARIA.     ISBN:  978-1-61208-340-7
COGNITIVE 2014 : The Sixth International Conference on Advanced Cognitive Technologies and Applications

[7] R. Zeibak and S. Filin, “Object extraction from Terrestrial 
Laser Scanning Data”, TS 8E –Terrestrial Laser Scanning, 
Visualization and LIDAR, FIG Working Week 2009, 
Surveyors Key Role in Accelerated Development, Eilat, 
Israel. 
[8] S. Westerberg and A. Shiriaev, “Virtual Environment-Based 
Teleoperation of Forestry Machines: Designing Future 
Interaction Methods”, Journal of Human-Robot Interaction, 
vol. 2, no. 3, 2013. 
[9] A. El Daher and S. Park, “Object Recognition and 
Classification from 3D Point Cloud”, student project, 2006, 
http://cs229.stanford.edu/proj2006/ElDaherPark-
ObjectRecognitionAndClassificationFrom3DPointClouds.p
df [retrieved: March 2013], Stanford Education at Stanford 
University, USA. 
[10] C. Sennersten and C. Lindley, “Evaluation of Real-time Eye 
Gaze Logging by a 3D Game Engine”, 12th IMEKO TC1 & 
TC7 Joint Symposium on Man Science and Measurement, 
Annecy, France, 2008. 
[11] C. Sennersten, M. Castor, R. Gustavsson, and C.A. Lindley, 
“Decision Processes in Simulation-Based Training for ISAF 
Vehicle Patrols,” NATO-OTAN, MP-HFM-202-17,  2010.   
[12] P. Jerčić, et al., “A Serious Game Using Physiological 
Interfaces for Emotion Regulation Training In The Context 
of Financial Decision Making,” ECIS 2012 Proceedings. 
AIS Electronic Library (AISeL), 2012. 
[13] H. Cederholm, O. Hillborn, C. Lindley, C. Sennersten, and 
J. Eriksson, “The Aiming Game: Using a Game with 
Biofeedback for Training in Emotion Regulation”, 5th 
Digital Games Research Association (DIGRA) Conference 
THINK DESIGN PLAY 2011, Utrecht, Netherlands. 
[14] J. R. Flanagan, G. Rotman, A. F. Reichelt, and R. S. 
Johansson, “The role of observers’ gaze behavior when 
watching object manipulation tasks: predicting and 
evaluating the consequences of action,” Philosophical 
Transactions of the Royal Society –B: Biological Sciences, 
2013, UK. 
[15] http://www.oxforddictionaries.com/definition/english/know
ledge, [retrieved: March 2013]. 
[16] J. 
R. 
Anderson, 
“ACT,” 
American 
Psychological 
Association, Vol. 51, No.4, 1995,  p. 355-365. 
[17] D. Kieras and D. Meyer, “An overview of the EPIC 
Architecture 
for 
Cognition 
and 
Performance 
with 
Application to Human-Computer Interaction”, University of 
Michigan, EPIC report No. 5 (TR-95/ONR-EPIC-5), 1995,       
DTIC Document 1995, 43 pages. 
[18] D. P. Benjamin, D. Lonsdale, D. Lyons, and S. Patel,  
“Using Cognitive Semantics to Integrate Perception and 
Motion in a Behavior-Based Robot,” In Learning and 
Adaptive Behaviors for Robotic Systems, 2008, LAB-
RS’08. 
ECSIS 
Symposium 
on, 
pp. 
77–82. 
http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=45994
31[retrieved: March 2013]. 
[19] E. Hutchins, Cognition in the Wild (book).  Chapter 9 – 
Cultural Cognition, 1996, pp. 353-374. 
[20] J. Laird, A. Newell, and P. Rosenbloom, “SOAR: An 
Architecture for General Intelligence,” Technical report 
AIP-9, 
University 
of 
Michigan, 
Carnegie-Mellon 
University, Stanford University, Artificial Intelligence 33, 
1987, pp. 1–63, DTIC Document 1988, 63 pages. 
[21] D. Kieras and D. Meyer, “The EPIC architecture for 
modeling human information-processing and performance: 
A brief introduction,” EPIC Report No.1 (TR-94/ONR-
EPIC-1), University of Michigan, 1994, DTIC Document 
1994, 43 pages. 
[22] R. Sun, “The CLARION Cognitive Architecture: Extending 
Cognitive Modeling to Social Simulation,” Cognition and 
Multi-Agent Interaction, Oct. 2004, Cambridge University 
Press, New York, 2006. 
[23] D. Vernon, G. Metta, and G. Sandini, “A Survey of 
Artificial 
Cognitive 
Systems: 
Implications 
for 
the 
Autonomous Development of Mental Capabilities in 
Computational 
Agents,” 
IEEE 
Transactions 
on 
Evolutionary Computation 11 (2), April 2007, pp. 151–180, 
doi:10.1109/TEVC.2006.890274. 
[24] G. Trafton, et al., “ACT-R/E: An Embodied Cognitive 
Architecture for Human-Robot Interaction,” Journal of 
Human-Robot Interaction, vol.2, No.1, 2013, pp. 30-55. 
[25] Y. Wang, “The OAR model for knowledge representation,” 
Proc. The 2006 IEEE Canadian Conference on Electrical 
and Computer Engineering (CCECE’06), Ottawa, Canada, 
pp.1692-1699. 
[26] A. Oltramari and C. Lebiere, “Extending Cognitive 
Architechtures with Semantic Resources,” Department of 
Psychology, Artificial General Intelligence Lecture Notes in 
Computer Science, Vol. 6830, 2011, pp. 222-231. 
[27] T. Berners-Lee, “Linked data-the story so far,” International 
Journal on Semantic Web and Information Systems, 5(3), 
2009, pp.  1-22. 
[28] Linked_Data_Design_Issue. 
http://www.w3.org/DesignIssues/LinkedData.html 
[retrieved: March 2013]. 
[29] C. D’este et al., “Sustainability, Scalability, and Sensor 
Activity with Cloud Robotics”, Proceedings of Australasian 
Conference on Robotics and Automation, 2-3 Dec 2013, 
University of New South Wales, Sydney, Australia. 
[30] WordNet: 
http://wordnetweb.princeton.edu/perl/webwn[retrieved: 
March 2013]. 
[31] World Wide Web Consortium is the main international 
standards organisation for the World Wide Web 
[32] OWL W3C: http://www.w3.org/TR/owl-features/[retrieved: 
March 2013]. 
[33] RDFS W3C: http://www.w3.org/TR/rdf-schema/[retrieved: 
March 2013]. 
[34] M. Lochner, C. Sennersten, A. Morshed, and C. Lindley, “ 
Modelling Spatial Understanding:  Using knowledge 
representation to enable spatial awareness  in a robotics 
platform”, The 6th International Conference on Advanced 
Cognitive Technologies and Applications, 25-29th of May 
2014, Venice, Italy.  
 
 
 
225
Copyright (c) The Government of Australia, 2014. Used by permission to IARIA.     ISBN:  978-1-61208-340-7
COGNITIVE 2014 : The Sixth International Conference on Advanced Cognitive Technologies and Applications

