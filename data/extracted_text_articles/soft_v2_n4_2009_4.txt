The Impact of Source and Channel Coding in the Communication Efﬁciency of
Wireless Body Area Networks
Richard Mc Sweeney, Christian Spagnol, and Emanuel Popovici
Department of Microelectronics Engineering
University College Cork
Cork, Ireland
richardmcs@ue.ucc.ie, c.spagnol@ue.ucc.ie, e.popovici@ucc.ie
Luigi Giancardi
Department of Biophysical and Electronic Engineering
University of Genoa
Genoa, Italy
luigi.giancardi@ingegneria.studenti.unige.it
Abstract—This paper examines the system level energy per-
formance of Wireless Sensor Motes for Electroencephalography
(EEG) patient monitoring application by the use of concate-
nated source and channel coding. The addition of coding in
a power constraint system has its advantages by reducing the
energy per bit, but it also has its drawback in the cost of the
power consumption in the encoding and decoding processes. In
this work Huffman code is implemented as the source coding,
and a shortened Reed-Solomon code is used for channel coding.
The reliability and energy savings of the scheme is presented
and the impact of the coding procedure on the communications
performance is analyzed. The results show that it is possible
to have Bit Error Rate (BER) and compression gains in the
system, and that the computational time of purely software
oriented implementations are not optimal. Also, the possibility
for future extensions of this coding scheme, which would
introduce better efﬁciency and accuracy, are shown. The error
patterns that occur in the channel are investigated, and a design
space for a possible Hybrid Automatic Repeat reQuest (HARQ)
scheme that would minimize the power consumption of this
implementation is proposed.
Keywords-Medical application; power reduction; WBAN;
Huffman source coding; Reed-Solomon channel coding
I. INTRODUCTION
The growing interests and developments in the area of
wireless sensor networks have opened up many avenues
for the applications of such systems in remote monitoring,
whether they may be in medical, environment, security,
surveillance, or industrial. Patient monitoring and personal
healthcare are the focus of this paper, and power optimiza-
tion in the wireless communication by the use of source and
channel coding are presented [1].
Wireless Body Area Network (WBAN) is one type of
network that is considered for patient monitoring, where the
sensors are distributed around the body and their commu-
nications range is limited to the immediate vicinity. They
monitor the body and process the acquired data on the
battery operated sensor node and then wirelessly transmit
them to a monitoring station for further analysis or alarm.
The main constraints of such systems include reliability,
area, timing, and efﬁciency, however the main bottleneck
that has been generally accepted is the issue of power
consumption [2].
The power consumption of the wireless sensor node is dis-
tributed among several different areas of the device, where
the most common conﬁgurations include one or several
sensors, a microprocessor/microcontroller/DSP referred as
software component, a Custom Digital Signal Processing
unit referred as digital hardware component (FPGA/ASIC),
memory, and a transceiver. Since one of the most power-
consuming devices is the RF module, in order to achieve
minimum system power consumption the most effective
solution is to buffer the data and operate the transmitter at the
highest possible data rate at low duty cycle, thus minimizing
the time in which the communication occurs. This can lead
to design constraints such as data rate and packet size,
and can have a signiﬁcant effect on the communication
efﬁciency.
Since WBAN in patient monitoring is employed in an
indoor environment with communication occurring through
the patients body, it is expected that the channel quality
would vary signiﬁcantly. This would require frequent re-
transmissions, which is not energy efﬁcient if a number
of these errors in the corrupted packets can be corrected,
thereby mitigating repeat transmissions. This is done by the
use of Forward Error Correction (FEC), which introduces
systematic redundancies allowing transmission of data at a
reduced energy per bit, achieving the same bit error rate. The
cost of FEC is additional decoder power consumption at the
receiver. To further increase the throughput of the commu-
nication, source coding is used to remove redundancies that
are inherent in the data. Compression reduces the amount
of energy required per bit-of-information in transmission
through the channel. Both methods work on the assumption
that power savings in the wireless data transmission can
be achieved at the expense of power consumption in the
encoding/decoding stages of either the processor or the
dedicated hardware.
The work proposed in this paper focuses on a software
implementation of a source and channel coding scheme on
an 8-bit micro-controller. Huffman code is used for com-
337
International Journal on Advances in Software, vol 2 no 4, year 2009, http://www.iariajournals.org/software/

Figure 1.
The proposed system ﬂow, illustrating the steps taken from
source to sink
pression, and a shortened Reed-Solomon RS(28,24) code
over Galois Field GF(28) is used for FEC. The system is
designed with the applications in the Electroencephalogra-
phy (EEG) patient monitoring in WBAN architecture. The
system itself is implemented on the Tyndall 25mm mote
[3] that has an Atmel mega128L processor, and a Nordic
nrf2401 transceiver that operate on the 2.4 GHz Industrial,
Scientiﬁc and Medical (ISM) band. By using the sample
EEG data [4], the power measurements are performed in a
laboratory environment. The system logical architecture is
illustrated in Fig. 1.
Section II discusses the constraints of WBAN and the
type of channel that is encountered. Section III of this
paper presents the theory and background of source coding
and illustrates the factors that affect the quantization and
Huffman compression scheme. The theory and methods used
in Reed-Solomon channel coding is presented in Section
IV. Section V of the paper gives the results of the power
reduction and gains in terms of compression and error
correcting code. Section VI presents the power savings of
using RS to mitigate retransmissions. The conclusion of the
ﬁndings and the future works are presented in Section VII.
II. WBAN CONSTRAINTS
It is well known that wireless communication has vari-
able channel characteristics that are determined by many
factors such as transmitter/receiver power, communication
frequency, modulation scheme, reﬂection, scattering, obsta-
cles, and interferences from other radiating sources. Indoor
wireless sensor networks are more susceptible to these fac-
tors since the motes operate at lower communication power
levels and in environments that contains more obstacles and
reﬂecting sources. The result is that the Receiver Signal
Strength (RSS) proﬁle shows pockets of low sensitivity
determined by the wave reﬂections and scattering effects
or other factors such as interference and direct obstructions.
This in turn affects the Bit Error Rate (BER), and can result
in poor communication even at small distances.
Fig. 2 is a path loss model from [5] that illustrates the
impact of RSS considering only free space with ground
reﬂections at a power level of 0dBm. It is observed that
Figure 2.
Path loss model for free space with ground reﬂection at a power
level of 0dBm. [5]
there is quite a large degradation in the RSS even at small
distances.
When implementing wireless communication in WBAN,
which has communication distances that are even smaller
and that often place the persons body in between the sensor
nodes, the problem of reliability in the communication
becomes even more pronounced. Therefore it becomes in-
evitable that severely varying BER would be encountered
and the solution becomes that either the transmitter power
needs to be increased or some form of error correction needs
to be used to alleviate this increase in signal power.
The structure of a WBAN system with a gateway is
shown in Fig. 3. The links between the sensors (S) and
the master nodes (MN) are the WBAN, and the link with
the MN to the monitoring station (MS) is the WLAN.
These two types of links have channel characteristic that
are different and demand physical (PHY) and media access
control (MAC) layers that differ from one another. In the
case of WLAN for example, the network can adopt either
singlehop or multi-hop schemes, and can have RSS proﬁle
that are affected signiﬁcantly by reﬂections and scattering
from various sources. They may also have different data rate
and may use a different frequency for communications than
that of WBAN. The WBAN on the other has very small
communications range, is usually single-hop architecture
and pose a different challenge to the WLAN by often placing
direct obstruction (the body) in the line of sight.
One of the works in channel performance and the effect of
path loss in the body is presented in [6], where an investi-
gation of the path loss in ﬂat biological tissue at 2.4GHz
ISM band is performed. The research draws conclusions
on that among the tissue types investigated, the thickness
of skin and fat layers have the most variable inﬂuence on
the path loss, and that proper sizing of the antenna is an
important factor. The Research reported in [7] investigates
the path loss for the human arm and torso, and path loss
parameters were derived from experimental measurements
338
International Journal on Advances in Software, vol 2 no 4, year 2009, http://www.iariajournals.org/software/

Figure 3.
Possible WBAN system connected to WLAN gateway [8]
and are then compared with the model stated in [6]. The
study shows that the path loss along the arm and the torso
follow the same course but the magnitude of the loss along
the torso is higher due to increased absorption, and suggests
that the path loss model for a ﬂat homogeneous tissues may
underestimate the effect near a human body. These studies
show that there is quite a signiﬁcant amount of degradation
in communications efﬁciency near the human body, and that
the designs of WBAN needs to have a high communications
power or an appropriate FEC to keep the desired BER.
III. SOURCE CODING
Data compression algorithms can be divided into two
families, namely lossless compression and lossy compres-
sion techniques [9]. The lossless compression scheme allows
perfect reconstruction of the original data, while lossy com-
pression returns an approximation that can achieve better
compression rates. The choice of compression scheme used
depends heavily on the application and the performance
required by the system.
Due to lack of approved standards, medical data is often
required by clinical practice physicians to be lossless, and
is believed to be an essential requirement for a correct
diagnosis. However as it is noted in [10], the diagnosis of
8-bit resolution EEG data gives enough precision to ensure
a correct interpretation of the signal by a physician. In this
work the EEG data used was sampled with a 12-bit resolu-
tion ADC, and an 8-bit micro-controller is used to perform
the compression. Basing our system upon the observations
made in [10], the ADC data is uniformly quantized from 12-
bits to 8-bits. The operation affects the quality of the signal,
but also has its advantages by providing easier computations
for the 8-bit microcontroller, and by providing less memory
requirements for the Huffman codeword LUT.
Huffman code has been chosen due to its simplicity. Given
a known probability function, the huffman tree can be built
and stored in the SRAM or the ﬂash in the micro-controller.
The encoding is performed on the sensor node immediately
after data acquisition, using just memory accesses to the
codeword LUT. However decoding has higher complexity
since it has to assess individual bits to ﬁnd the corresponding
Figure 4.
The procedure for source encoding
codeword in the LUT. But this is performed on the master
node, which is a more powerful machine, hence is not
considered critical.
Therefore for this work a trade-off solution is proposed,
where some error (loss) is introduced before the lossless
compression stage. The steps of this technique are illustrated
in Fig. 4.
The initial step quantizes the 12-bit EEG data to 8-bits,
which results in resolution-loss of the amplitudes that is
uniformly distributed over the whole dynamic range of the
data set. The quantized EEG data is then compressed using
the static Huffman encoder, and is subsequently passed into
a payload setup, where bit packing is performed. The packet
is then either encoded by the channel-encoding scheme, or
wirelessly transmitted directly.
A. Quantization
The procedure for the quantization is as follows. The
EEG data set is uniformly quantized over the entire dynamic
range. The error introduced by the quantizer varies with the
maximum amplitude that has to be represented after the
quantization, where the peak of the Gaussian distribution
plays an integral role in the choice of the quantization
interval. This peak amplitude parameter V M is chosen so as
to minimize the negative effect of the quantization without
signiﬁcantly affecting the compression performance. The
cost-function in (1) is used to select V M.
min(V M)

J = αErr(V M) +
(1 − α)
Gain(V M)

,
(1)
where the term Gain(V M) is an expression of the source
coding gain (explained in details in Section V.A), and the
term Err(V M) is an error-function deﬁned by:
Err(V M) =
v
u
u
t 1
S
S
X
i=1
|qi − di|2wi,
(2)
where S is the number of EEG samples in the entire training
set, |qi − di| is the difference between the quantized value
and the original sample, and wi is the weight-coefﬁcient. i.e.
the frequency of occurrence of each value in the database.
The α value is a term used to vary the weight between the
error and the coding gain.
339
International Journal on Advances in Software, vol 2 no 4, year 2009, http://www.iariajournals.org/software/

Figure 5.
(a) Illustration of the local minima for the 2D Cost function
without α (b) 3D Cost function
The plot of the cost function J(V M, α) is shown in Fig.
5(b). It is seen from the ﬁgure that this is not a convex
function, there are no local minima, and the global minima
occurs at α = 0. The conclusion is that α does not contribute
to the cost function and is trivial when trying to determine
the optimum value of V M. Therefore the cost function is
analysed without α. This is illustrated in Fig. 5(a). The
minima of this new function J(V M) is found to be at V M
= 1330, which is an integer representation of the voltage
value that was measured on the sensor.
After the quantization, the data will assume only 256
different values. This is the base-requirement for the next
step of the source coding algorithm.
B. Huffmann Coding
Given a discrete source of symbols, the Huffman coding
procedure builds for that source a variable-length minimum-
redundancy preﬁx code. The minimum redundancy code is
often referred as optimum in the sense that it leads to the
lowest possible average codeword length, given N symbols
and M digits. It is important to note that for every source
there are several optimum codes. We considered only preﬁx
codes, in which a codeword cannot be a preﬁx of some
other codeword, because there is no loss of generality in
considering only the preﬁx ones. In fact it can be shown that
given a general optimum code, there exists a preﬁx code that
achieves the same results [10].
The Huffman code tree [11] has a number of leaves
proportional to the number of different symbols coming
from the discrete source: in this case they are 256, as the
Discrete Source is the Quantizer from the previous step.
Each leaf contains a string of bits, here called codeword
that corresponds to an input symbol. Going from the root
deep through the code tree, the symbols become step-by-
step less likely, while the related codewords much longer.
The average length of the code is deﬁned as,
Lav =
N−1
X
i=0
bivi,
(3)
where bi is the codeword, and vi is the corresponding error
probability. The Huffman compression scheme is designed
so that the average length of the code is smaller than the
uncoded version.
An issue related to this code is that it is impossible to put
a limit on the maximum length of a codeword. This is due
to fact that the maximum depth of the Huffman tree results
from the code-design procedure without any possibility of
control. This is a problem since a ﬁxed maximum codeword
length is required when working with an 8-bit micropro-
cessor that cannot efﬁciently handle variables exceeding the
size of 8-bits.
To solve this problem a hybrid technique based on the
Collapsed Huffman Tree (CHT) is used [10]. Each Huffman
codeword larger than length 8-bits are appended a CHT
codeword that ﬂag such a case. The resulting ﬁnal codeword
has 16-bits. It has been observed that this does not have a
signiﬁcant negative impact on the average length of the code,
because the CHT leaf collects the most unlikely symbols of
a given source.
C. Multiple Huffman codes
A possible variant of both classic Huffman and CHT
coding is the use of Multiple Huffman codes, i.e. a family
of codes that allow the encoder and the decoder to switch
between them, by following a certain rule.
By deﬁnition, Huffman codes are built (and optimized)
to best represent the source of symbols they have been
constructed from. This means that such a code has good
performances when the training sequence used to build the
code is truly representative of the source of symbols that are
to be encoded. These considerations imply that a Huffman
code is optimum when the source that has been compressed
emits the symbols with a homogeneous time-independent
distribution. Even if this is true over a large time scale
(which is the assumption that the Huffman coding takes),
it could be not true over a small time window. In the case
of EEG signal processing, it is observed that the distribution
of amplitudes differs from normal activity to seizure activity
as shown in Fig. 6.
Figure 6.
Distribution of amplitudes for a large number of samples taken
from normal activity (left) and abnormal activity (right)
340
International Journal on Advances in Software, vol 2 no 4, year 2009, http://www.iariajournals.org/software/

Figure 7.
Modiﬁed compression scheme for seizure-aware coding
The knowledge of when abnormal activity occurs during
a long-term EEG examination would permit to introduce
further optimizations to this scheme by the use of multiple
Huffman codes. A possible scheme to realize this idea is
shown in Fig. 7.
In such a case where the sensor node is aware of the
current brain state by means of either an internal procedure
or an external indication, the seizure detection block chooses
the appropriate Huffman tree to perform the encoding. In this
implementation, two Huffman trees are used; one optimized
for normal activity, and another one for seizures activity.
The codewords are packed in the payload setup as before.
IV. REED SOLOMON CHANNEL CODING
The channel code that was chosen for this particular
implementation of FEC is the shortened Reed- Solomon
(28,24,4) over the Galois Field size of GF(28). The RS
code [12] is traditionally popular and has been a longtime
industry-standard that has found uses in various applications
such as satellite communications, Digital Video Broadcast-
ing (DVB), Compact Disks (CD), Digital Versatile Disks
(DVD), mass storage, and in wireless communications. The
main reasons for such wide-ranging popularity stems from
the fact that the RS code has efﬁcient encoding and decoding
algorithms, and targets (multiple) burst errors.
A. Reasons for code choice
The RS(28,24,4) over the GF(28) is implemented in this
scheme due to a variety of reasons. One of the requirements
for a power-constrained wireless sensor mote is to operate
the most power consuming devices as seldom as possible.
The transceiver is then needed to work at as small time
frame as possible and at high data rate. These constraints
lead to the need of the packet to be small but also at
the same time to keep the power consumption of the FEC
as low as possible. Therefore the FEC should keep the
size small and be computationally easy. This is especially
the case for the power-constrained sensor nodes, but is
also true for the master node in terms of time taken to
decode, which becomes critical in networks that utilize
Time-Division Multiple Access (TDMA) protocols.
Various research groups have performed works in the ﬁeld
of feasibility of FEC in Wireless Sensor Networks (WSN).
The research done in [13] explores the power estimation of
Hamming codes, Convolutional codes (CC), and RS codes
and has proposed a framework for the design space of FEC
for WSN. It was established that RS codes perform the best
in terms of total energy consumed by the motes, and it
was seen that packet size of 31 bytes consumes the least
amount of power at varying node distances. Through the
BER analysis it is shown that the computational power of
RS(31,29,3) is the lowest at BER of 10−4 and RS(31,27,5)
is the lowest at path loss exponent of 4, which equates
to a dense noisy environment. The authors of [14] have
performed works on power estimation of various BCH, RS,
and CC cores on tsmc180nm ASIC process and Xilinx
Spartan III FPGA platform. It is shown that the power
consumption of linear block codes are much less than that
of CC, and that BCH and RS codes are useful in WSN
applications. In [15], empirical research has been performed
on the BER of the motes in short range (< 1.5 m) WSN,
and investigated the packet loss and packet reception of the
sensors. The packets are sized to 64 bytes, and through their
analysis the authors found that the average number of bit
errors in a packet that passed synchronization is 16.68 bits.
The maximum packet size of the burst mode in the
nrf2041 is 32 bytes, of which 4 bytes are set for synchroniza-
tion and address. The maximum size of the packet payload is
28 bytes, and thus the code is built with n=28. The reason
for choosing the ﬁeld size of GF(28) is due to the 8-bit
micro-controller, where all operations are performed in 8-
bit blocks, therefore for ease of operation, and to reduce
computational power, the code is built over this ﬁeld size.
The combination of these reasons not only shows that the
choice of RS(28,24) is acceptable for short range WSN, but
also justify them from an implementation point of view and
that the performance should be similar to the stated ﬁgures.
B. Code Architecture
Following the encoding of the EEG data by the Huffman
compression scheme on the sensor node, the resulting mes-
sage m(x) is systematically encoded by the concatenated
code and is sent wirelessly through a highly noisy indoor
environment. The systematic encoding ensures that the data
symbols appear in the codeword, and is expressed, using
polynomial notation as follows,
c(x) = p(x) + xn−km(x),
(4)
where the parity symbols p(x) is chosen such that the
codeword c(x) is divisible by the generator polynomial.
Due to the complexity of the Galois ﬁeld multiplication,
and the limitations of the processor to perform only one
operation per clock cycle, a bit serial multiplier is used for
multiplications in GF(28), and a look-up-table is made for
inversion to facilitate maximum speed of the operation [16].
Once the packet is received on the master node, the
word may be corrupted and this is expressed as r(x) =
341
International Journal on Advances in Software, vol 2 no 4, year 2009, http://www.iariajournals.org/software/

Figure 8.
Reed-Solomon Decoder architecture
c(x) + e(x), where r(x) is the received data, c(x) is
the codeword, and e(x) is the added channel noise. The
RS decoder attempts to correct the errors by means of
polynomial operations. The decoding ﬂow can be seen in
Fig. 8.
The (n − k = 2t) syndromes Si are computed and these
are used to calculate the Error Locator Polynomial ELi(x)
in an iterative fashion. The commonly used Berlekamp-
Massey algorithm [17] for the calculation of the error locator
polynomial is here replaced with the Fitzpatrick algorithm
[18] due to its faster performance.
Once the Fitzpatrick algorithm has calculated the error
locator polynomial, the roots of the polynomial, X −1
i
are
then found by the shortened Chien search algorithm, which
only cycle through the last 28 elements of the GF(28) since
the code has been shortened to RS (28,24). This observation
leads to a considerably shortened computational time for the
search calculation.
The resulting roots X−1
i
are the inverses of the error loca-
tions in the received word and are used for the calculation of
the error values Yi. These error values are calculated without
the use of the error evaluator polynomial, as was proposed
in [19]. The equation used for the error value calculations
is shown in (5).
Yi =
X−2t+1
i
ELi′(X−1
i
)EL
′
i(X−1
i
),
(5)
where ELi′(x) is the update polynomial obtained through
the Fitzpatrick algorithm, and EL′
i(x) is the formal deriva-
tive of ELi(x). This algorithm has the advantage over the
traditional Forney algorithm [20] in that it does not have
to calculate the error evaluator polynomial, thereby saving
signiﬁcant computational effort.
Finally the error is corrected with the use of the error
values Yi and their corresponding error positions X −1
i
.
V. POWER MEASUREMENTS AND RESULTS
Several implementations are considered for the analysis of
the power consumption and the performances of the source
and channel codes. These are summarized in Table I.
Table I
COMMUNICATION VERSIONS USED
Version
Description
Orig.
12-bit EEG data that is transmitted as a 16-bit word
Quant.
Quantized 8-bit EEG data
QH
Quantized 8-bit EEG data with Huffman coding
QRS
Quantized 8-bit EEG data with RS coding
QHRS
Quantized 8-bit EEG data with Huffman and RS coding
Figure 9.
TX system current consumption vs. time
The data is sent wirelessly in burst mode of the nrf2041
at data rate of 250 kb/s and at TX power level of -20dBm.
The transmission packets are sized to 32 bytes per packet,
with 28 bytes of information payload.
The result current consumption waveforms at the trans-
mitter for the versions 1 to 4 are shown in Fig. 9, where
each peak corresponds to a packet being sent. The wave-
forms are captured via voltage measurement across a 1.7
Ω external resistor with the supply voltage for the Tyndall
mote set at 5V. The micro-controller operates on 3.3V, and
the transceiver operates on 2.5V. The power consumption
of the voltage regulators and the surrounding circuitry are
assumed to be negligible. It should also be noted that
342
International Journal on Advances in Software, vol 2 no 4, year 2009, http://www.iariajournals.org/software/

Table II
TX POWER AND ENERGY CONSUMPTION
Version
Pac rate
Pwr TX
En /Pac
Pwr RX
En /Pac
(s−1)
(mW)
TX (mJ)
(mW)
RX (mJ)
Orig.
320.5
65.334
0.2038
88.2
0.2752
QH
159.2
59.168
0.3716
88.2
0.5540
QRS
15.21
54.274
3.5683
88.2
5.7988
QHRS
14.79
54.247
3.6678
88.2
5.9635
the development board on which the measurements are
performed adds 20.38mV shown in Fig. 9, and needs to
be subtracted in order to calculate the power consumption
of the mote itself.
Table II summarizes the communication performance and
the energy usage of the various schemes at the transceiver
power level of -20dBm.
Moving down the column for the different versions, it is
observed that the power consumption of the system drops
due to a decrease in the packet rate, but the overall energy
consumption rises due to the heavier processing demands
of the codes. This is due to the limitations of the 8-bit
microprocessor to carry out tasks such as efﬁcient bit-
wise Galois-Field multiplication, hence high amounts of
computational time were introduced that led to reduced
packet rate.
In regards to Orig. and QH, it is observed that Orig. has
twice the packet rate of QH due to the necessary computa-
tional effort required by the Huffman coding operation in
QH. However as seen in Table V, the compression gain
works out to be around 60% for most of the time, and
therefore the overall throughput of the Huffman encoded
data is superior.
In terms of channel coding, it is seen from Table II
that communication has higher throughput and hence better
performance when the code is not used. However these
results do not take into account of the coding gain provided
by the FEC nor the channel characteristics. Investigation and
results on these aspects are provided in Section V.C and
Section VI respectively. It is shown that there are advantages
to be had in using FEC in terms of coding gain and in
mitigating retransmissions as well as the system power.
A. Compression Performance
The Compression rate is strictly dependent on the average
length of the source code, Lav as deﬁned in (3). We
consider the case of a Huffman tree built using the entire
EEG database as the training set with V M=1330 and 8-bit
quantization. The compression performance of huffman and
collapsed huffman trees are shown in Table III.
It is seen from the table that the efﬁciency of the CHT
technique ηcht is lower than the original one η , and the
average length of the CHT Lavcht is higher than the original
Lav. Although this is a slight trade-off in the performance
Table III
COMPRESSION FIGURES
Term
Term Description
H = 5.345
Entropy of the signal from quantizer
Lav=5.366
Avg. length for the standard Huffman tree
Lavcht =5.96
Avg. length for the Huffman tree with the CHT leaf
CHT = 38
Position of the CHT leaf in Huffman tree
η = 0.996
Efﬁciency of Huffman coding
ηcht = 0.897
Efﬁciency of Huffman coding with the CHT leaf
Mem=507
Memory usage of original LUT in Bytes
Memcht=38
Memory usage of CHT LUT in Bytes
of the code, it is also seen from the comparison of Mem
and Memcht that there are large memory savings of approx-
imately 92.5% to be had from using CHT.
Although the ﬁgures of Table III give the performace of
the code that is optimized for the given statistics, it should be
noted that the EEG sample used to build the tree contains
20% seizure activity. Since it is rare to see 20% seizure
activity in the real case, a test that was devised to explore the
compression scheme on various EEG data types. i.e. Seizure
and Non-seizure activity. The test consists of transmitting
the EEG data in 200 packets through the wireless link, and
counting how many information bits are communicated. The
average packet length is sized to 24 bytes, where for QH one
byte is reserved for information on the number of signiﬁcant
bits in the data payload. Fragment 1 holds EEG data activity
of seizure prone patients, Fragment 2 holds the data of the
patients when in seizure, and Fragment 3 is the normal EEG
data of healthy adult subjects. The results are summarized
in Table IV.
Table IV
COMMUNICATION PERFORMANCE WITH VARIOUS TYPES OF DATA
FRAGMENT 1
Orig.
Quant.
QH
Total number of packets
200
200
200
Total EEG samples sent
2400
4800
7725
Avg. data payload length (bits)
192/192
192/192
179/184
FRAGMENT 2
Orig.
Quant.
QH
Total number of packets
200
200
200
Total EEG samples sent
2400
4800
3605
Avg. data payload length (bits)
192/192
192/192
177/184
FRAGMENT 3
Orig.
Quant.
QH
Total number of packets
200
200
200
Total EEG samples sent
2400
4800
8690
Avg. data payload length (bits)
192/192
192/192
181/184
The table shows that the quantized values, Quant. sends
twice as many samples than that of original values Orig. and
that QH achieves even better performance for Fragments
1 and 3. The average data payload length varies for QH
due to the variable length nature of Huffman coding. To
343
International Journal on Advances in Software, vol 2 no 4, year 2009, http://www.iariajournals.org/software/

Table V
HUFFMAN PERFORMANCE WITH VARIOUS EEG
FRAGMENT 1
FRAGMENT 2
FRAGMENT 3
Avg. length
4.682
9.878
4.1988
Gaincht
2.560
1.210
2.850
Gain (%)
60.91
17.35
64.91
summarize the compression performance, the experimental
average code length and the corresponding compression gain
of the Huffman encoded data for each Fragment can be seen
in Table V, where only QH is considered. The estimation
of the overall compression gain is made using a simple
relationship in (6), where 12 is the original resolution of
the sample.
Gain = 12
Lav
,
Gaincht =
12
Lavcht
,
(6)
The results show that coding gain is strictly dependent on
the nature of the signal, and if considering that segments
such as fragments 1 and 3 occur for more than 99% of the
time, the little increment from 8 to 9.878 bits per sample
(during the seizure activity) does not affect the compression
capabilities of the system over a macro-scale.
B. Multiple Huffman codes
Previous considerations on EEG signal compression with
CHT Huffman code show that the coding performances in
terms of accuracy and coding gain are strictly related to
the composition of the signal entering the encoder. This
is inherent to the Huffman-code building algorithm, since
seizures (and abnormal activity in general) are less likely
than normal activity over a large time scale, than a general
activity Huffman code. Even if the encoding scheme here
presented can represent any sample with similar level of
quality (due to uniform quantization), the performance of
the encoder decreases when abnormal activity is present.
In this section, a mixed-activity signal with known oc-
currences of seizure activity has been encoded using both
single and multiple codes. Using the same criterion as (1),
the best value for V M (which is a parameter related to
quantization intervals size, i.e. to accuracy) are chosen,
and two more Huffman codes are built. Based on these, a
mixed code is implemented that takes advantage of seizure
awareness (available by construction, in this particular case)
to introduce further optimization.
The results of the comparison are shown in Table VI,
where f is the expected frequency of occurrence of seizures.
It is seen that the seizure-aware (multiple Huffman) coding
shown in column 4 introduces a double advantage.
First, it encodes the mixed-activity samples with an in-
creased efﬁciency in comparison to the general code (column
3). This can be observed by looking at the larger number
of samples-per-packet sent. Secondly, since the value V M
Table VI
A PERFORMANCE COMPARISON BETWEEN DIFFERENT HUFFMAN
CODES
Signal
Normal
Seizure
Mixed activity
activity
activity
Code
Normal act.
Seizure act.
General
Multiple
code
code
code
code
V M
250
1680
1330
Norm, Seiz
ERR
0.07
0.46
0.64
(1-f)*0.07
(V M)
+ f*0.46
EEG / Pac
25.06
20.03
20.15
24.29
is targeted on the particular class of brain activity, the
effects of quantization on signal quality are lower, i.e. the
reconstructed (decoded) samples are more similar to the
original ones. For example if f = 0.1 (which implies 10%
seizure activity), the average error would result to be 6
times lower than the case when the general code is used.
This means that the reconstructed signal would have 6 times
more accuracy than before. This shows that the multiple-tree
scheme brings improvements with respect to the single-tree
implementation in terms of compression and the accuracy
of the data due to the change in the quantization intervals.
C. Reed Solomon Coding Gain
The coding gain of the RS(28,24) code is usually cal-
culated by setting the desired bit error rate of the un-
coded BPSK and the coded BPSK and then measuring
the difference between the Signal-to-Noise Ratio (SNR)
required to reach such BER. This is achieved by varying
the transmitter power levels. However due to the limitations
of the transceiver of the Tyndall mote in setting the TX
power levels, the SNR was modiﬁed by varying the distance
between the TX and RX at a constant transceiver power level
of -20dBm. It is also noted that the modulation scheme used
by the nrf2041 is Gaussian Frequency Shift Keying (GFSK).
The results of the measurement are shown in Fig. 10. It
should be noted that the distance is inversely proportional
to the SNR, and also that the BER decreases with rising
SNR.
It is observed visually that as the BER decreases, the
gap between the GFSK(uncoded) and that of RS(28,24)
become larger. It is interesting to note that there are certain
points when the communication becomes worse even at
small distances where one would expect lower BER, such
as the case at the distance 0.15m. At other distances such as
0.95m to 1.3m, the effect is more prominent. This is due to
path loss and reﬂections from the ground and walls, which
interact with the original signal to produce low receiver
signal strength. To measure the coding gain of the RS code,
the following model is used [21],
PT X,U[W] = ηU
Eb
N0
N
4π
λ
2
dn,
(7)
344
International Journal on Advances in Software, vol 2 no 4, year 2009, http://www.iariajournals.org/software/

0
0.2
0.4
0.6
0.8
1
1.2
10
−6
10
−5
10
−4
10
−3
10
−2
Distance (m)
Bit Error Rate (BER)
 
 
RS(28,24)
GFSK
Figure 10.
Plot of TX-RX distance vs. Bit Error Rate
where:
ηU = Spectral efﬁciency (=1 for BPSK)
Eb/N0 = SNR (energy ratio)
N = Signal noise (Thermal noise*Bandwidth)
λ = Transmitted wavelength
d = Distance between TX and RX
n = Path loss exponent
By placing the coded and uncoded case in proportion, the
following relationship is obtained.
PT X,RS[W]
PT X,U[W] =
ηMAX

Eb
N0

RS N

0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
0
10
20
30
40
50
60
70
Number of byte errors in packet
Frequency of Occurance (%)
 
 
10−to−50
55−to−90
95−to−130
105−&−125
Figure 11.
Types of errors occuring for various channel characteristics
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
10
−2
10
−1
10
0
10
1
10
2
Error correction capability
Retransmission rate (%)
 
 
10−to−50
55−to−90
95−to−130
105−&−125
Figure 12.
Required retransmission rate for error free communication
where ARQm is the retransmission rate, ej is the error types
shown in Fig. 10, TPac is the total number of packets sent
over the channel, and m is the range of error correction
capability. The calculation of the retransmission rate is
performed and the results are shown in Fig. 12.
The plot shows that with a larger BER such as segment
105 & 125, the overall retransmission rate is higher of
around a factor of 102 than that of almost error free commu-
nication of segment 10 to 50. Also it can be observed that
the addition of FEC alleviates the need for retransmissions,
and this value goes down as the error correcting capability
rises.
B. Power savings
Considering that the communication occurs at the same
packet rate for the uncoded and the coded versions in the
link, it is possible to calculate the power savings of the
system. Such assumption can be achieved by using a code
thats implemented in hardware such as an FPGA or an ASIC.
Therefore one can derive an optimal operating point for
FEC that best utilizes the energy when working at varying
BER levels at this packet size and data rate. Although the
experiment did not directly place any obstructions between
the TX and RX, it did however reach a distance in which
the receiver sensitivity failed. The experiment was also
performed in a closed laboratory environment that provided
strong reﬂections of the transmission. Therefore it is as-
sumed that at higher transmit power (e.g. -10dBm) on a
real patient would provide a similar type of scenario, and
that the results could be used to design a system speciﬁc
HARQ.
The power and energy savings are calculated by com-
paring the original values without any coding with that of
increasing error correction capability. Equation (11) is used
to calculate the new packet rate Pacnewm from the original
packet rate Pacorig and the retransmission requirements
RTm that is in terms of percentage.
Pacnewm =
100Pacorig
Pacorig +

PacorigRTm
100
.
(11)
The total transmission time, Ttot is calculated using (12),
where TT X is the time taken to transmit a packet.
Ttotm = TT XPacnewm.
(12)
The average power is found using the information about
the power consumption of the micro-controller PµC, and the
power consumption of the transceiver PT X. By knowing the
time it takes to transmit the packets in a given window, the
product of the respective times and the known powers give
the average power consumption of the system by following
the relationship shown in (13).
PAvgm = Ttotm(PµC + PT X) + (1 − Ttotm)PµC
(13)
= PµC + PT X(Ttotm).
PAvgm is used to ﬁnd the average energy EAvgm that is
the average energy consumed by the system to transmit one
packet. This is achieved using (14).
EAvgm =
PAvgm
Pacnewm
,
(14)
The energy savings is calculated by taking the difference
in the average energy consumption per packet for trans-
mission with no coding, EAvgNo F EC and that of energy
consumption of coded transmission EAvgm, as shown in
(15).
ESavingsm = EAvgNo F EC − EAvgm,
(15)
From the energy savings, the average power savings or the
difference in terms of power between the uncoded and the
coded transmission is calculated at the respective required
packet rates as shown in (16).
PSavingsm = ESavingsmPacnewm
(16)
Fig. 13 illustrates the amount of power saved for the
various segment types plotted against the error correcting
346
International Journal on Advances in Software, vol 2 no 4, year 2009, http://www.iariajournals.org/software/

0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
0
5
10
15
20
25
30
35
40
Error correction capability
Power savings (mW)
 
 
10−to−50
55−to−90
95−to−130
105−&−125
Figure 13.
Power savings using shortened Reed-Solomon (28,k,t) with
various error correcting capabilities
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
0
20
40
60
80
100
120
Error correction capability
Required extra packets (%)
 
 
10−to−50
55−to−90
95−to−130
105−&−125
Figure 14.
Error correction capability vs. Extra transmissions needed (%)
capability. The zero point on the horizontal axis refers to
the case when no FEC is used. It is observed that the power
savings approach a certain threshold where the addition
of more FEC does not give signiﬁcant savings. It is also
seen that worse the channel, the more beneﬁt in terms of
power savings of the wireless sensor mote is achieved by
the addition of FEC, and that hardly any power savings are
to be had in segments 10 to 50 and 55 to 90.
Although the addition of FEC can mitigate retransmis-
sions, the cost of this operation is the additional parity bits
in the packets, which reduces the overall throughput of the
data. Therefore it is also important to consider the cost of
total throughput versus the gains achieved by the utilization
of FEC. This is illustrated in Fig. 14, where the additional
packets needed for a complete transmission of a data set
in terms of percentage is shown versus the error correction
capability. The point where no error correction is available
refers to a link that utilizes just retransmissions.
The ﬁgure shows that for segments 10 to 50 and
55 to 90, the addition of FEC does not result in bet-
ter throughput. However at a higher BER of segment
95 to 130, it is observed that there is a slight improvement
at m=1. Moving up to segment 105 & 125 shows that there
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
−30
−25
−20
−15
−10
−5
0
5
10
15
Error correction capability
Power savings (mW)
 
 
10−to−50
55−to−90
95−to−130
105−&−125
Figure 15.
Total TX system power savings using FEC considering
throughput
is signiﬁcant savings as the error correction capability rises,
and that the optimum for this BER is at m=3.
The total power savings is calculated by applying (11)
to (16), to the results shown in Fig. 13 and Fig. 14. The
retransmission requirement RTm in (11) is modiﬁed to
incorporate the effect of the throughput. The calculated total
power savings are plotted in Fig. 15.
The total power savings shown in the ﬁgure strongly
reﬂects the effect that the throughput has on the effectiveness
of the FEC. Again it is seen that there are hardly any savings
by the FEC at low BER, but it does start to become energy
efﬁcient at above BER of 10−4. In segment 95 to 130 it is
observed that there is small savings of 0.6mW at m=1, and
at segment 105 & 125 the optimum error correction value
is at m=3 with savings of 14mW.
VII. CONCLUSION AND FUTURE WORK
The possibility of energy savings using a software imple-
mentation of a serially concatenated Huffman-RS code was
presented. The analysis of the Huffman compression show
that EEG compressions gains are achieved in a general case,
and that the use of multiple trees for normal and seizure
activity present even more gains.
In terms of the mote system performance however, the im-
plementation presented long computational time that made
the coding seem less practical. The main reason is due
to the limitations of using the 8-bit microprocessor, where
the computationally difﬁcult Galois Field operation for the
Reed-Solomon code presented signiﬁcant addition to the
processing time. Thus the packet rate was reduced and the
energy consumption per packet was increased. This would
result in faster depletion of the battery on the real system,
which would have a negative effect on the maintenance of
the patient monitoring wireless sensor node.
The error distribution for the channel at various BER
levels was also performed, and the power savings by the
mitigation of retransmissions was calculated. The ﬁndings
show that FEC can save power at BER levels of 10−4 or
347
International Journal on Advances in Software, vol 2 no 4, year 2009, http://www.iariajournals.org/software/

higher, and approach optimum values at error correction
capabilities of up to 3 bytes for this implementation.
Future work will explore the hardware-software codesign
of the Reed-Solomon code along with a HARQ scheme,
and also investigate the advantages of joint source-channel
coding for medical applications WBAN.
ACKNOWLEDGMENT
The authors wish to thank the Tyndall National Institute
for their support in the provision of hardware through the
SFI-NAP scheme and for the facilitation of the testing
process. This work is funded by SFI-EEDSP for Mobile
Digital Health, grant number: 07/SRC/I1169.
REFERENCES
[1] R. Mc Sweeney, L. Giancardi, C. Spagnol, and E. Popovici,
“Implementation of source and channel coding for power
reduction in medical application wireless sensor network,” in
Third International Conference on Sensor Technologies Ap-
plications (SENSORCOMM’09), Athens, Greece, Jun. 2009,
pp. 271–276.
[2] S. Drude, “Requirement and application scenarios for body
area networks,” in Mobile and Wireless Communications
Summit 2007: 16th IST, Budapest, Hungary, Jul. 2007, pp.
1–5.
[3] Tyndall
National
Institute,
Available
at:
http://www.tyndall.ie/mai/25mm.htm.
[4] R. G. Andrezjak, K. Lehnertz, F. Mormann, C. Rieke,
P. David, and C. Elger, “Indications of nonlinear deterministic
and ﬁnite dimensional structures in time series of brain
electrical activity: Dependence on recording region and brain
state,” Physical Review E, vol. 64, no. 6, p. 061907, Nov.
2001.
[5] T. Stoyanova, F. Kerasiotis, A. Prayati, and G. Papadopou-
los, “A practical rf propagation model for wireless network
sensors,” in Third International Conference on Sensor Tech-
nologies Applications (SENSORCOMM’09), Athens, Greece,
Jun. 2009, pp. 194–199.
[6] L. Roelens, W. Joseph, and L. Martens, “Characterization
of the path loss near ﬂat and layered biological tissue for
narrowband wireless body area networks,” in International
Workshop on Wearable and Implantable Body Sensor Net-
works, (BSN’06), Cambridge, Massachusetts, U.S.A, Apr.
2006, pp. 50–56.
[7] E. Reusens, W. Joseph, G. Vermeeren, and L. Martens,
“On-body measurements and characterization of wireless
communication channel for arm and torso of human,” in
International Workshop on Wearable and Implantable Body
Sensor Networks, (BSN’07), Aachen University, Germany,
Mar. 2007, pp. 264–269.
[8] S. Marinkovic, C. Spagnol, and E. Popovici, “Energy-efﬁcient
tdmabased mac protocol for wireless body area networks,” in
Third International Conference on Sensor Technologies Ap-
plications (SENSORCOMM’09), Athens, Greece, Jun. 2009,
pp. 604–609.
[9] A. Gersho and R. M. Gray, Vector Quantization and Signal
Compression.
Norwell, MA: Kluwer Academic Publishers,
1992.
[10] G. Antoniol and P. Tonella, “Eeg data compression tech-
niques,” IEEE Transactions on Biomedical Engineering, vol.
44, 2, pp. 105–114, Feb. 1997.
[11] D. A. Huffman, “A method for the construction of minimum-
redundancy codes,” Proceedings of the I.R.E., vol. 40, 9, pp.
1098–1101, Sep. 1952.
[12] I. Reed and G. Solomon, “Polynomial codes over certain
ﬁnite ﬁelds,” Journal of the Society for Industrial and Applied
Mathematics, vol. 8, 2, pp. 300–304, Jun. 1960.
[13] S. Chouhan, R. Bose, and M. Balakrishnan, “A framework
for energy-consumption-based design space exploration for
wireless sensor nodes,” IEEE Transactions on Computer-
Aided Design of Integrated Circuits and Systems, vol. 28, 7,
pp. 1017–1024, Jul. 2009.
[14] G. Balakrishnan, M. Yang, Y. Jiang, and Y. Kim, “Perfor-
mance analysis of error control codes for wireless sensor
networks,” in International Conference on Information Tech-
nology, (ITNG’07), Las Vegas, Nevada, USA, Apr. 2007, pp.
876–879.
[15] A. Willig and R. Mitschke, “results of bit error measurements
with sensor nodes and casuistic consequences for design of
energy-efﬁcient error control schemes,” in Proc. 3rd European
Workshop on Wireless Sensor Networks, Zrich, Switzerland,
Jan. 2006, pp. 310–325.
[16] S. Lin and D. Costello, Error Control Coding: Fundamentals
and Applications.
New Jersey, USA: Prentice Hall, 1983.
[17] J. L. Massey, “Shift register synthesis and bch decoding,”
IEEE Transactions on Information Theory, vol. 15, pp. 122–
127, Jan. 1969.
[18] P. Fitzpatrick and S. Jenning, “Comparison of two algorithms
for decoding alternant codes,” Applicable Algebra In Engi-
neering, Communication and Computing, vol. 9, 3, pp. 211–
220, 1998.
[19] E. Popovici, “Algorithms and architectures for decoding reed-
solomon and hermitian codes,” Ph.D. dissertation, University
College Cork, University College Cork, Cork, Ireland, 2002.
[20] G. D. Forney, “On decoding bch codes,” IEEE Transactions
on Information Theory, vol. 11, pp. 393–403, Oct. 1965.
[21] S. L. Howard, C. Schlegel, and K. Iniewski, “Error control
coding in low-power wireless sensor networks: When is ecc
energy-efﬁcient?” EURASIP Journal on Wireless Communi-
cations and Networking, vol. 2006, 2, pp. 1–14, 2006.
348
International Journal on Advances in Software, vol 2 no 4, year 2009, http://www.iariajournals.org/software/

