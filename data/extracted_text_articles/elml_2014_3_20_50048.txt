On Benefits of Interactive Online Learning in Higher Distance Education 
Case Study in the Context of Programming Education 
 
Winfried Hering, Helga Huppertz, Bernd J. Krämer, 
Silvia Schreier 
Faculty of Mathematics and Computer Science 
FernUniversität in Hagen,  
Hagen, Germany 
Winfried.Hering@q-perior.com, 
{helga.huppertz|bernd.kraemer}@fernuni-hagen.de, 
silvia.schreier@kollee.de 
          Johannes Magenheim, Jonas Neugebauer 
Department of Didactics of Informatics 
University of Paderborn 
Paderborn, Germany 
jsm@upb.de, jonas.neugebauer@uni-paderborn.de
 
 
Abstract—The advent of the world-wide web has challenged 
traditional distance teaching to supplement custom textbooks, 
video broadcasting and limited personal interaction with 
computer-based learning technologies, multimedia systems and 
compter-mediated 
interaction 
between 
geographically 
dispersed students and teachers. In this paper, we report on a 
project that aims to identify potential for improving learning 
outcomes of distance learners on the subject object-oriented 
(OO) programming through the use of new media and online 
learning technologies. We performed a quantitative and 
qualitative study based on a competence model for OO 
modeling and comprehension with two groups of distance 
students: a control group using classical textbooks and an 
examination group using a standard learning environment 
enhanced by digital interactive tools and re-purposed learning 
materials. In addition, we tracked the behavior of the online 
study group and analyzed the log data with the help of SAS 
Business Analytics to detect conspicuous learning behaviors.    
Keywords-distance learning; online learning; web log 
mining; learning analytics; competence analysis 
I. 
 INTRODUCTION  
Traditionally, European distance teaching universities 
used prepackaged self-instructional correspondence courses 
that allowed their students to study at the time and location 
of their choice. Starting out in the early ‘90s, this teaching 
model was challenged by the advent of the worldwide web 
and, in succession, a rich world of digital learning and 
collaboration tools. This opened the opportunity to tailor 
course design to interactive online learning. As an 
intermediate 
step, 
we 
supplemented 
existing 
print 
courseware with interactive multimedia systems that allowed 
students to experiment with simulated or animated virtual 
worlds and get meaningful feedback on student-system 
interactions in real-time [1]. Asynchronous e-mail, chat 
tools, or text forums and synchronous webinars allowed us to 
narrow the distance between fellow students and teachers 
concerning social interaction possibilities.  
Several studies have compared online and face-to-face 
learning, partly with contradicting result [2], [3], [4]. There 
is, however, a paucity of original research dedicated to 
understanding 1) whether information and communication 
technology (ICT) is perceived beneficial for distance 
education, 2) what the impacts of the use of digital 
interactive learning and communication tools on the 
outcomes of distance students in higher education, such as 
grades and test scores, is and 3) how learning designs should 
be tailored to meet the needs of these students. 
A research project funded by the European Union [5] 
addressed the first question. It examined the impact of on 
learning in adult education, lifelong learning and, in 
particular, distance education. Research instrument was a 
series of randomized controlled trials using questionnaires 
and 
statistical 
analyses. 
The 
studies 
demonstrated 
conclusively that technology does, in fact, have a positive 
impact on learning. Major reasons include: technology 
facilitates easier access to material for those studying part-
time; online communication facilitates the interaction with 
teachers, learning technology supports the development of 
higher level thinking skills, such as synthesis and problem 
solving, and digital learning materials including multimedia 
and interactive elements can enhance learning.  
A first step towards answering the second and third 
question was recently undertaken in a joint project between 
FernUniversität and the University of Paderborn, Germany. 
The project set out to find answers to the following research 
questions: 
1. To what extent can the learning objectives of a 
course be achieved with traditional custom 
textbooks and asynchronous tutoring activities 
using email, text forums, and phone? 
2. Is there a significant difference in learning 
outcomes 
between 
students 
relying 
on 
traditional distance learning settings and online 
students? 
3. Do students prefer online learning technologies 
to traditional correspondence courseware? 
During summer semester 2012, we invited all 693 
students enrolled in the distance learning course “Object-
oriented Programming” for beginners to participate in a pre- 
and posttest evaluating the students’ modeling and 
comprehension competencies in the topic area. Further, we 
asked for a smaller group of volunteers who agreed to study 
a new interactive version of a course module and allow us to 
57
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-328-5
eLmL 2014 : The Sixth International Conference on Mobile, Hybrid, and On-line Learning

log their online behavior. The competencies analysis used 
the Modellierungskompetenz-Modell (modelling competen-
cies model, MoKoM) model that the researchers from 
Paderborn had developed in another research project that 
was financially supported by the German Science 
Foundation [6]. In a collaboration of computer scientists and 
psychologists, a theory based and empirically refined 
competency model for the domains of system application, 
system comprehension and system development was 
developed and tested by means of a competency test 
instrument [7]. 
In the following section, we present the project layout 
and sketch the research methods and tools used in the 
experiment and analysis of the data gathered by observation, 
competency testing, and surveys. Section III discusses the 
results of the competency, behavior, and the online learner 
satisfaction analysis. Section IV critically scrutinizes the 
validity of the results and outlines a modified design for a 
repetition of the experiment. 
 
II. 
SETTING THE SCENE 
 
Distance students own above average experience with 
self-directed learning. They are used to organize their 
learning freely but have limited time to participate in 
synchronous learning events, which requires the barriers for 
group learning actions to be kept low.  
To challenge students in the online group to perform 
more demanding learning activities, we redesigned both the 
instructional design and the content of one module of the 
course substantially. The selected course unit dealt with 
exceptions, testing and program documentation. We 
developed a number of interactive learning objects in Adobe 
Flash that allowed the students to experiment with alternative 
solutions of program designs, explore, modify and explain 
the behavior of given program solutions rather than just 
sketching a single program on paper and submitting it as 
their solution to a homework assignment. To provoke 
teamwork, we designed learning tasks that involve 2-4 
students playing different roles, such as a programmer or 
tester, and tasks whose solution was composed of several 
modules contributed by different students. 
A. Competence model for informatics modelling and system 
comprehension  
To define the important competencies for the considered 
domains, the MoKoM project derived a theoretically 
founded competency framework from relevant national and 
international curricula and syllabi. This framework was used 
as a basis to develop specific problem scenarios. They could 
be used to conduct qualitative interviews with experts in 
different fields of computer science education [8]. The 
analysis of the transcribed interviews led to an empirically 
refined competency model with six dimensions: K1 System 
Application, K2 System Comprehension, K3 System 
Development, K4 dealing with system complexity and K5 
Non-Cognitive Skills. 
For each competency described in these dimensions, a 
test item was developed for assessing it in large-scale 
competency tests. After some tests with smaller populations, 
the MoKoM project currently evaluates the results of an 
assessment of around 600 students. To deal with the large 
amount of tests, the instrument was split into 6 booklets with 
30 to 40 items that could be completed in roughly 90 minutes 
each. 
To use the instrument in the study at hand, we 
transformed the competency measurement instrument into an 
online version. Each item was adapted as close as possible 
into the online survey system LimeSurvey [9]. Due to the 
nature of the questions, some items could not be transformed 
properly (e.g., drawing a diagram). For each of these items, a 
decision had to be made, whether the associated competency 
was appropriate for the field of OOP and, if so, a new item 
had to be developed. As we wanted to exploit the breadth of 
the competency model and expected a high number of 
participants in the test, the partition into six booklets was 
kept for the online survey. 
To allow for an anonymous survey, an additional item 
was added to ask for a unique code. This code was generated 
individually for each student based on personal information.. 
This allowed for the association of pre- and post-test without 
revealing the students’ identity. 
B. Study Groups 
In a first step before the course started, we invited all 693 
students enrolled in the course to evaluate the online test. To 
raise the students’ interest in the study, we announced to give 
away three books in a raffle based on voluntarily provided 
email addresses. The 146 students who followed this request 
and worked on the test with different degrees of completion 
formed the study group for the competency test. 
Then we invited students who agreed to study the online 
course module and allowed us to track their behavior.  12 
volunteers formed the online student group whose behavior 
we analyzed and whose satisfaction with the e-learning 
version of the course we studied.  
At the end of the course, we asked the whole student 
population again to evaluate the post-test marking it with the 
same code they had used in the pretest.  
Finally, we aimed to differentiate between online and 
textbook students among those who participated in the 
obligatory written exam, which is held at the end of the 
course, in order to find possible differences in the learning 
outcomes of both groups. 199 students took the exam 
including 6 of the 12 online students. Both groups were truly 
comparable because they are all distance students enrolled in 
the same study program. In addition, the online students also 
have experience with textbook study materials from other 
course units of the test course and other distance courses in 
the curriculum. 
C. Digital Learning Environment and Learner Satisfaction 
Analysis  
The online course unit addressed exactly the same course 
topics as the textbook version but included more 
experimental learning components and a few cooperative 
58
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-328-5
eLmL 2014 : The Sixth International Conference on Mobile, Hybrid, and On-line Learning

tasks. Figure 1 depicts an interactive learning object, which 
is called crash lab. It allows students to explore the behavior 
of programming exceptions by pulling selected program 
statements from a pile underneath a program window into a 
code-frame provided in that window, compile the resulting 
code and run it if the compiler succeeds. Students are 
supposed to predict whether the different programs they 
build this way will fail or terminate successfully. The results 
of the test runs are collected in the two boxes labeled 
“aborted” and “regularly completed” at the bottom right.  
An example of a team problem is a simple game with a 
treasure being hidden in an area of 24 cells in which a player 
to find the treasure by moving in this area strategically. One 
student has to develop the program component controlling 
the game; the partner has to implement the behavior of the 
player. Some constraints are imposed on the behavior of both 
components, which may lead to program exceptions. Raising 
and handling exceptions properly are key learning objectives 
of this course unit. 
The online course was delivered through the learning 
management system Moodle and was supervised by the same 
faculty who also taught the textbook version. The installation 
of Moodle we used in the study was seamlessly connected to 
the learning object repository edu-sharing [10], [11]. This 
repository enabled the online students to share and 
collaboratively work on their contributions. The repository 
functionality allowed them to control which access rights to 
their personal workspace they wanted to grant selected peers. 
 
Figure 1: Crash lab supporting explorative programming experiences  
Besides the students’ online behavior, we also wanted to 
understand their satisfaction with the online course material 
and with the learning environment we provided. For this 
purpose, we designed an online questionnaire with 28 
questions addressing the following issues: preferred 
computer equipment, experiences with e-learning, usability 
of the online course and tool environment, subjective 
judgement of the discipline knowledge, communication and 
cooperation competencies gained in the online course, and a 
comparison of the effectiveness of the study process with 
traditional textbooks and the online study material. 
 
III. 
ANALYSIS OF RESULTS 
A. Competence Analysis 
To assess the competency gain of the students, they were 
asked to complete the online survey created from the 
MoKoM measurement instrument twice: Once at the start of 
the term and once at the end. The students were randomly 
partitioned into six groups, each having access to one of the 
six test booklets provided in a LimeSurvey installation at 
FernUniversität in Hagen, 
Unfortunately, the participation in the survey was not 
very good. Of the 693 students registered for the course, only 
57 started their pretest booklet and just 19 students in total 
finished more than 75% of all items. This number got even 
worse for the post-test with 30 started booklets and only 5 
finished surveys. At both tests intervals around 150 users 
visited the survey page but only one third and one fifth, 
respectively, started the first item at all. Since the 
participants who finished their booklet were spread across 
the six variants, the useable data for each test item is too low 
to get any meaningful results regarding the competencies of 
the students. Only one student completed both tests, what 
makes propositions about the competency gain through the 
course impossible. Even tendencies supporting or falsifying 
any of the hypotheses we started from are hard to state.  
To at least get a notion what to change for subsequent tests, 
the answering habits of the participants were examined. By 
counting the number of students who tried to solve an item 
over all 87 datasets it is easy to see that the completion rate 
drops to 60% after only four items (see Figure 2). The eighth 
item was completed by only 50%. 
 
Figure 2: Relative completion rate of survey items 
A reason for these results might have been the length of 
the competency test. Tailored to be conducted in a German 
classroom setting, where two successive lessons equals 90 
minutes, it seems the time a student is willing to spend on his 
own in answering the survey is considerably shorter. 
Examination of the response rates for online surveys even 
showed that the ideal length for an online survey is thirteen 
minutes or less [12]. This ideal timeframe is too short for a 
competency test, but without the constraints of a school 
environment, 90 minutes seem much too long. 
0%#
10%#
20%#
30%#
40%#
50%#
60%#
70%#
80%#
90%#
100%#
1# 2# 3# 4# 5# 6# 7# 8# 9# 10#11#12#13#14#15#16#17#18#19#20#21#22#23#24#25#26#27#28#29#30#
Rela1ve#comple1on#rate#
Itemnumber#
59
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-328-5
eLmL 2014 : The Sixth International Conference on Mobile, Hybrid, and On-line Learning

 
B. Behavior Analysis of Online Students 
It is obvious that the small number of cases (12) cannot 
provide statistically valid results. Even so, some insights 
gained are worth mentioning.  
The database for the behavior analysis of the online 
students was compiled from the log data provided by Moodle 
and edu-sharing and the log data captured by the Flash-based 
interactive learning objects. The latter allowed us to a) relate 
student results to assignments, b) see errors he or she trapped 
into, and c) find repetitions performed in the attempt to solve 
an assignment. As user identities for the login to Moodle and 
edu-sharing (single-sign-on), we used the personal code the 
students had defined for the pre-test. All log data were time-
stamped. These time-stamps helped use to integrate the data 
coming from different sources. The raw data were cleaned 
and integrated to a single database that was then analyzed 
with the help of the business analytics software SAS. SAS 
was particularly used for structure and usage mining. 
Structure mining relies on the links between information 
pages and links from within course pages to self-test 
examples, homework assignments, forum entries, and objects 
maintained in the repository and workspaces.  
The objective of structure mining is to identify recurring 
patterns of behavior, e.g., in the form of paths through the 
learning materials or repeated experiments with exercises 
and programming problems. These paths form a network that 
visualizes how students navigate through the course material 
and the learning environment. The open source software 
Gephi Data Laboratory [13] was used for network analysis 
and result plotting. Particular indexes of the network analysis 
are the weighted in- and out-degrees of course elements, 
which indicate the frequencies of visits. The master solutions 
to self-tests in the sections about exceptions and program 
testing and a quiz about program comments had the highest 
values: 35, 34 and 30, respectively, while the average was 
9.9. Another index is the connection intensity between 
elements. It records navigation steps leading from one to 
another element in the graph. A high index value between an 
information page and a self-test indicates multiple trials of 
this test and suggests a higher degree of difficulty of the 
problem exposed. This index may help the course author to 
vary the degree of difficulty of self-tests in a distance-
learning course as it is perceived by the students. The index 
“page rank” also identifies the master solutions for self-tests 
on program testing and exceptions as top candidates. A path 
analysis shows that students work through the first part of the 
online course mostly following the structure provided by the 
course author. In later parts, however, their behavior is more 
flexible.    
Usage mining provides useful descriptive statistics. This 
includes:  
• 
information about the number of page visits, 
• 
time spent on a page, exercise, or problem, 
usage depth or  
• 
typical entry and exit pages.  
The usage depth in this experiment tells us that, in the 
average, 35 course elements were touched in a single visit.  
The 
quantitative 
analysis 
described 
above 
was 
complemented by a qualitative analysis based on the log data 
captured from the interaction with Flash objects. For 
instance, five students worked on the explorative problem 
depicted in Figure 1 and all five predicted the behavior of the 
8 different program snippets correctly. This result suggests 
that the difficulty of the problem was too low. A test of the 
students’ ability to understand the semantics of exceptions, 
which was composed of 4 sub-tests, shows that the success 
rates diminish with each sub-test. This observation confirms 
the author’s intention to increase the difficulty of the sub-
tests step by step. An investigation of problems in which 
students had to write code reveals that one student performed 
significantly worse than his or her peers. His behavior seems 
to exhibit a trial-and-error strategy as opposed to the 
structure procedures the other students applied.  
C. Test Scores 
Originally, we wanted to relate the test scores of the 
exam to the competences test to investigate whether the self-
evaluation of the students correlated with the examination 
scores and whether we can find significant differences 
between textbook and online students. We also hoped to see 
whether an initial difference in competences in the topic area 
has a significant impact on the exam result. In addition, we 
were curious whether the course can even out such 
competency differences in the student group.  Further we 
were looking for differences in test scores between textbook 
and online students.  
Figure 3 shows a plot of the frequencies of scores for the 
199 students who took the exam. The best score (1.0) was 
achieved by 8 students, 57 students failed with a score higher 
than 4.0. The lower curve shows the frequencies for the six 
online students, which roughly follows the shape of the other 
curve with 4 students scoring between 1.3 and 3.0 and two 
students having failed.  
 
Figure 3: Scores of 199 students in the final exam 
Knowing that this interpretation is, indeed, daring, we 
hope to perform better in an ongoing second experiment with 
about 100 online students.   
60
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-328-5
eLmL 2014 : The Sixth International Conference on Mobile, Hybrid, and On-line Learning

D. Learner Satisfaction 
In order to analyse the online-students’ needs and their 
satisfaction with the online version of the course, we 
developed a web-based questionnaire that was also 
accessible via LimeSurvey. We adopted and enhanced the 
Technology Acceptance Model (TAM) [14], and the System 
Usability Scale (SUS) [15]. Both were already successfully 
applied to analyse e-learning environments [16], [17]. The 
questionnaire provides 28 Likert-scaled statements and 
covers 9 analytical dimensions. They are:  
• 
use of the learning environment’s technical 
functionality,  
• 
potential to convey expert knowledge,  
• 
potential to convey methodological skills,  
• 
usability of the learning environment,  
• 
patterns of usage,  
• 
types of exercised co-operative learning,  
• 
use of platform’s communication tools,  
• 
students’ motivation, and 
• 
students’ technical affinity. 
The questionnaire was delivered at the end of the term to 
those students, who attended the online-course and also 
agreed to participate in the behaviour-analysis study based 
on their log-files. Unfortunately, only six students 
responded. Thus, it does not make sense to deploy statistical 
methods of quantitative data analysis on this sample. 
Nevertheless, 
the 
students 
uttered 
some 
interesting 
statements, which might contribute to the comprehension of 
existing 
problems. 
The 
addressed 
categories 
were: 
importance of a good mentoring service, necessity to be 
aware of the system’s functionality, students’ personal 
situations and missing pressure to succeed lead online-
learners to drag behind the schedule of the course, this 
additionally 
creates 
barriers 
for 
co-operation 
and 
communication with fellow students. 
IV. 
CONCLUSION AND FUTURE WORK 
The underwhelming results of the first survey led us to 
conduct a second iteration with a slightly different concept 
during the winter term of 2013. To get more students to 
finish the test, it had to be possible to complete it within 60 
minutes. For this reason, instead of testing the complete 
range of competencies of the MoKoM model, a collection of 
items especially tailored to the requirements of the course at 
hand was selected. To choose the appropriate test items, the 
desired learning outcomes for the programming course were 
matched to the competency descriptions of the MoKoM 
model. This way, a selection of relevant test items could be 
accumulated and combined into one competency test. This 
also eliminated the need for several test booklets and will 
allow for a bigger set of data for each test item.  
Since the complexity of the item did not allow for a test 
with the suggested optimal length of thirteen minutes, an 
additional incentive had to be made to complete both tests. 
For this reason, we decided to offer bonus points up to 10% 
of the total number of credit points that can be achieved in 
the exam. Up to half of admissible bonus points will be 
given for participating in both pre- and posttest, the amount 
depending on the degree of completion of each test. The 
other half will be granted for participating in the online study 
group, depending on the degree of activity and collaboration. 
Those students who want to compensate their bonus points 
with the exam result have to give up anonymity against the 
examiner, but their personal data will not be accessible in the 
competency analysis. The bonus points can only be used in 
the exam offered immediately after the end of the course. 
This constraint renders us quite optimistic that the students 
who volunteered for the competency test and online behavior 
analysis will take the exam and thus give us the chance to 
compare the learning outcomes of a reliable sample.  
So far, the measures seem to be successful, since the pre-
test performed late September 2013 showed a tremendous 
increase in the number of participants, with around 180 
complete or nearly complete datasets. 
 
 
REFERENCES 
[1] B. J. Krämer, “New possibilities for distance learning,” IEEE 
Computer, vol. A247, April 1995, pp. 529–551  
[2] J. Ferguson and A. M. Tryjankowski, “Online versus face‐to‐
face learning: looking at modes of instruction in Master’s‐
level courses,” Journal of Further and Higher Education vol. 
33, no. 3, 2009, pp. 219-228  
[3] R. J. Smith and L. J. Palm, “Comparing learning outcomes 
between traditional and distance learning introduction to 
philosophy courses,” Discourse: Learning and teaching in 
philosophy and Religious Studies, vol. 6, no. 2, 2007, pp. 
205-226 
[4] B. Means, Y. Toyama, R. Murphy, M. Bakia, and K. Jones, 
“Evaluation of evidence-based practices in online learning: a 
meta-analysis and review of online learning studies,” U.S. 
Department 
of 
Education, 
2010, 
http://www2.ed.gov/rschstat/eval/tech/evidence-based-
practices/finalreport.pdf, last access: 18 Jan. 2014 
[5] F. Agrusti et al., “The impact of technologies on distance 
learning students,” Research Report 4/2008, FernUniversität 
in Hagen, 2008,  http://deposit.fernuni-hagen.de/2954/, last 
access: 18 Jan. 2014 
[6] B. Linck et al., “Empirical refinement of a theoretically 
derived competence model for informatics modelling and 
system comprehension,” Proc. IFIP Conference Addressing 
educational challenges: the role of ICT (AECRICT), 2-5 July 
2012  
[7] T. Rhode, “Development and testing of an instrument for 
measuring informatics modelling competences in a didactic 
context,” Universität Paderborn 2013 (in German) 
[8] B. Linck et al., “Competence model for informatics modelling 
and system comprehension,” IEEE Global Engineering 
Education Conference (EDUCON), Berlin 2013, pp. 85–93. 
[9] LimeSurvey, “Open source online survey system,” 2003, 
http://www.limesurvey.org, last access: 18 Jan. 2014 
[10] edu-sharing, “Open source repository network for learning 
content and educational knowledge,” http://edu-sharing.net/, 
2009, last access: 18 Jan. 2014 
[11] M. Klebl, B. J. Krämer, and A. Zobel, “From content to 
practice: sharing educational practice in edu-sharing,” British 
Journal of Educational Technology, vol. 41, no. 6, Nov. 2010, 
pp. 936-951 
61
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-328-5
eLmL 2014 : The Sixth International Conference on Mobile, Hybrid, and On-line Learning

[12] W. Fan and Z. Yan, “Factors affecting response rates of the 
web survey: a systematic review,” Computers in Human 
Behavior, vol. 26, no. 2, Feb. 2010, pp. 8–8 
[13] M. Bastian, S. Heymann, and M. Jacomy “Gephi: an open 
source software for exploring and manipulating networks,” 
Proc. International AAAI Conference on Weblogs and Social 
Media, San Jose, California, March 2009, pp. 361–362  
[14] F. Davis, “A technology acceptance model for empirically 
testing new end-user information systems - theory and 
results,” PhD thesis, Massachusetts Institute of Technology, 
1985 
[15] J. Brooke, “SUS - A quick and dirty usability scale,” 
http://www.usabilitynet.org/trump/documents/Suschapt.doc, 
last access: 18 Jan. 2014 
[16] C. Ong, J. Lai, and Y.  Wang. “Factors affecting engineers‘ 
acceptance of asynchronous e-learning systems in high-tech 
companies,” Information & Management, vol.  41. No. 6, 
2004, pp. 795-804 
[17] M. Zviran, C.  Glezer, and I. Avni, “User satisfaction from 
commercial web sites: the effect of design and use,” 
Information and Management ,  vol. 43, no. 2, 2006, pp. 157-
178. 
 
 
62
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-328-5
eLmL 2014 : The Sixth International Conference on Mobile, Hybrid, and On-line Learning

