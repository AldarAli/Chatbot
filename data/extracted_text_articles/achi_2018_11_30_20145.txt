Classifying Daily Activities regardless of
Wearable Motion Sensor Orientation
Aras Yurtman and Billur Barshan
Department of Electrical and Electronics Engineering
Bilkent University, 06800 Ankara, Turkey
Email: {yurtman, billur}@ee.bilkent.edu.tr
Abstract—Most studies on wearable sensing assume that each
sensor is correctly placed on the body, ﬁxed to a pre-determined
position at a pre-determined orientation. This is not practical
and feasible in many applications where elderly, disabled, or
injured people need to place the sensor units on their own,
especially for wireless and small sensor units. It is a considerable
improvement to make wearable systems robust against the
placement orientations of the sensors, and further, to allow the
sensors to be placed at any orientation. For this purpose, we
propose a transformation based on Singular Value Decomposition
(SVD) that removes the absolute orientation information from
the sensor data. We apply this transformation in the pre-
processing stage of the standard human activity recognition
scheme using multiple publicly available datasets, classiﬁers, and
cross-validation techniques, and achieve an average accuracy that
is only 7.56 % lower than the reference approach with ﬁxed sensor
orientations. The most common method that is an alternative to
the proposed transformation is taking the Euclidean norm of
3D data vectors, which obtains 13.50 % lower accuracy than the
reference approach. We show that randomly oriented sensors
cause a reduction of 21.21 % in the activity recognition accuracy
when no transformation is applied for orientation invariance;
hence, the standard system cannot handle incorrectly oriented
sensors. On the other hand, the proposed approach allows users
to place the sensor units at any orientation on their body with an
acceptable reduction in the accuracy, outperforming the common
Euclidean norm approach.
Keywords–Human
activity
recognition;
Wearable
sensing;
Orientation-invariant sensing; Motion sensors; Singular value
decomposition.
I.
INTRODUCTION
The exceptional development of mobile devices and their
sensing capabilities have enabled them to seamlessly obtain
information about people’s activities and behaviors. Human
activity recognition ﬁnds application in the human-computer
interaction, healthcare, surveillance, military, and entertainment
domains [1]. There are mainly two approaches in activity
recognition: wearable sensing and computer vision. As a result
of increased computational power and reduced size and weight
of mobile devices, wearable sensing has become applicable
to various scenarios in a less obtrusive manner, whereas the
vision-based approach requires external cameras that reduce
the mobility of the subject and raise privacy concerns [2].
In wearable sensing applications including human activity
recognition, it is often assumed that each wearable device that
contains sensors is placed at a pre-determined position on the
body at a pre-determined orientation. Wearable devices have
reduced in size and gained wireless communication capabili-
ties; hence, they have become more likely to be incorrectly
placed on the body. For instance, smart phones can be carried
in a pocket at different orientations, and it is obtrusive to
require the user to place his/her phone in his/her pocket
always at a ﬁxed orientation. Moreover, in some applications,
these sensors are used by elderly, disabled, injured people, or
children who may have difﬁculty in determining the orientation
of the sensors for correct placement. However, while there are
many studies on activity recognition using wearable motion
sensors, only a small fraction of them considers incorrectly
orientated sensors [3].
Inertial sensors (accelerometers and gyroscopes) and mag-
netometers are the common types of wearable motion sensors.
Each sensor is typically tri-axial, acquiring data on three mu-
tually perpendicular axes that are part of the device. When the
sensor is placed at the same position at a different orientation,
the acquired data have a representation in a new, rotated
coordinate frame. Wearable systems that assume correct sensor
placement are not robust against this change in general. We
propose a transformation that removes the dependency of the
data on the sensor axes while keeping most of the information
content of the data. In this approach, the transformed data are
invariant to the orientation at which the sensors are placed on
the body, which enables the users to place the sensor units at
any orientation at the pre-determined positions.
The rest of this paper is organized as follows: We summa-
rize the related work in Section II. The proposed method for
orientation invariance is described in Section III. Section IV
includes the description of the datasets, the activity recogni-
tion scheme, and the experimental results. In Section V, the
proposed method is discussed comparatively with the existing
approaches. We state our conclusions and directions for future
work in Section VI.
II.
RELATED WORK
The most common and the simplest approach in the liter-
ature to achieve robustness to incorrectly oriented sensors is
to take the Euclidean norm (that is, the magnitude) of the 3D
vectors acquired by tri-axial sensors. The Euclidean norm of
the acquired vectors does not depend on the orientation of the
sensor. References [4]–[7] use the magnitude in classiﬁcation,
whereas [1][8][9] append the magnitude to the tri-axial sensor
data as a fourth axis.
The second approach assumes that the gravity component
is dominant over the linear acceleration component in the
acceleration vectors during daily activities, and estimates this
direction by averaging the accelerometer data [10]–[13]. Then,
the tri-axial vectors are decomposed into vertical and horizontal
components [10]–[12]. Reference [13] additionally calculates
the forward-backward direction of the body as the principal
axis that is perpendicular to the vertical axis.
206
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-616-3
ACHI 2018 : The Eleventh International Conference on Advances in Computer-Human Interactions

In our earlier work [14], we proposed two alternative
transformations to remove the information regarding absolute
sensor orientation without making any assumptions about the
sensor conﬁgurations or usage scenario. The ﬁrst transforma-
tion extracts geometrical features from tri-axial sensor data,
whereas the second projects the sensor data onto three principal
axes in each time segment. Both transformations are shown to
signiﬁcantly improve the activity recognition accuracy when
the sensors are randomly oriented. Reference [15] applies
Principal Component Analysis (PCA) to the tri-axial data to
remove the information related to absolute sensor orientation.
Another approach is to estimate the sensor orientation
relative to the Earth frame, based on accelerometer, gyroscope,
and magnetometer data [16]. Then, the sensor readings can be
represented in the ﬁxed Earth frame, independent of the sensor
orientation.
Reference [17] classiﬁes the orientation of the sensor unit
among four pre-determined orientations based on the acquired
data while the unit is moving and rotates the sensor data
accordingly. Reference [18] assumes that incorrect placement
of a sensor unit only affects the class means in the feature
space, which may not be always true, and compensates for
this by using the expectation-maximization algorithm. In [19],
orientation invariance is achieved based on calibration postures
that the subjects need to perform after placing the sensor units.
Reference [20] develops a robust classiﬁcation methodology
against corruption in some portion of the data to handle
incorrect placement of some of the sensor units.
III.
PROPOSED TRANSFORMATION FOR
ORIENTATION INVARIANCE
The tri-axial sensor data are originally acquired in the
sensor coordinate frame in terms of the x, y, z axes that
depend on the orientation at which the sensor is ﬁxed on the
body. To achieve orientation invariance, we apply SVD to the
data to represent them in terms of the principal axes.
The sensor data acquired in a time segment can be repre-
sented as a matrix
A = [a1 a2 · · · aN]
(1)
where ai = [aix aiy aiz]T
(i = 1, . . . , N) is the acquired
data vector in 3D space at time sample i. We decompose A
into three matrices by SVD as
A = UΣVT
(2)
In the compact form, Σ is a 3×3 diagonal matrix containing the
singular values of A. The 3 × 3 and 3 × N matrices U and V
contain the left and right singular vectors in their columns,
respectively. Since A consists of real numbers, U and V have
orthonormal columns and satisfy UUT = UTU = I3×3 and
VTV = IN×N.
Placing the sensor unit at a different orientation is equiv-
alent to applying the same rotation to all the data vectors in
3D space, provided that the sensor unit is rigidly attached to
the body. Here, we assume that the sensor unit moves together
with the body part it is placed on, regardless of the orientation
at which it is ﬁxed on the body. This assumption is implicitly
made in almost all studies on orientation invariance because
otherwise the sensor unit would rotate independently of the
body and would not be able to capture substantial information
about the body motion.
A different orientation of the sensor causes each of the
acquired data vectors ai to be rotated by the same (unknown)
rotation matrix R as ˜ai = Rai, which can be represented as
the matrix product ˜A = RA. Using (2), the rotated data matrix
can be expressed as
˜A = R

TABLE I. ATTRIBUTES OF THE FIVE DATASETS.
dataset
D1 [22]
D2 [23]
D3 [24]
D4 [25]
D5 [26]
no. of subjects
8
4
30
14
15
no. of activities
19
5
6
12
7
activities
(*: stationary activity)
(1*) sitting, (2*) standing, (3*,4*) lying on back
and on right side, (5,6) ascending and descending
stairs, (7) standing still in an elevator, (8) moving
around in an elevator, (9) walking in a parking
lot, (10,11) walking on a treadmill in ﬂat and
15◦ inclined positions at a speed of 4 km/h,
(12) running on a treadmill at a speed of 8 km/h,
(13) exercising on a stepper, (14) exercising on a
cross trainer, (15,16) cycling on an exercise bike
in horizontal and vertical positions, (17) rowing,
(18) jumping, (19) playing basketball
(1) sitting down,
(2) standing up,
(3*) standing,
(4) walking,
(5*) sitting
(1) walking,
(2) ascending
stairs,
(3) descending
stairs,
(4*) sitting,
(5*) standing,
(6*) lying
(1) walking,
(2,3) walking left and
right, (4,5) ascending
and descending stairs,
(6) running forward,
(7) jumping, (8*) sitting,
(9*) standing,
(10*) sleeping,
(11,12) ascending and
descending in an
elevator
(1*) working at a
computer, (2) standing
up–walking–ascending/
descending stairs,
(3*) standing, (4) walking,
(5) ascending/descending
stairs, (6) walking and
talking with someone,
(7*) talking while
standing
no. of units
5
4
1
1
1
no. of axes per unit
9
3
6
6
3
unit positions
torso, right and left arm, right and left leg
waist, left thigh,
right ankle,
right upper arm
waist
front right hip
chest
sensor types
accelerometer, gyroscope, magnetometer
accelerometer
accelerometer,
gyroscope
accelerometer,
gyroscope
accelerometer
dataset duration (hr)
13
8
7
7
10
sampling rate (Hz)
25
8
50
100
52
no. of time segments
9,120
4,130
10,299
5,353
7,345
segment duration (s)
5
5
2.56
5
5
no. of features
(with no transformation)
1,170
276
234
156
78
IV.
EXPERIMENTAL METHODOLOGY AND RESULTS
To assess the performance of the proposed method, we
apply the standard activity recognition scheme on multiple
datasets, classiﬁers, and cross-validation techniques.
A. Datasets
We use ﬁve publicly available datasets, which have different
sensor conﬁgurations and have been acquired by different
research groups [22]–[26]. Brief information about the datasets
is provided in Table I.
B. Pre-Processing
The time-domain data are divided into segments of ﬁxed
duration (see Table I). Then, we apply one of the following
pre-processing methods:
• REF (Reference): No transformation is applied.
• ROT (Random Rotation): The tri-axial data acquired from
each sensor unit in each time segment are rotated by a dif-
ferent random rotation matrix generated from independently
and uniformly distributed roll, pitch, and yaw angles [27]
in the interval [0, 2π).
• NORM (Euclidean Norm): The Euclidean norm of each
3D data vector is taken. This is an existing approach for
orientation invariance [4]–[7] and included for comparison.
• SVDT (SVD-based Transformation): We apply the pro-
posed transformation deﬁned in Section III to each sensor
unit in each time segment separately. This method is robust
against placing each sensor unit at a different orientation
in each time segment.
C. Classiﬁcation
After transforming the time-domain data, the following
statistical features are extracted from each axis of each time
segment: minimum, maximum, mean, variance, skewness,
kurtosis, the coefﬁcients of the autocorrelation sequence for
the lag values of 5, 10, . . . , 50 samples, and the ﬁve largest
Discrete Fourier Transform (DFT) peaks (that are at least
11 samples apart) with the corresponding frequencies. Fewer
autocorrelation coefﬁcients and DFT peaks can be used in some
datasets if there are not sufﬁciently many time samples in a
segment. The number of features extracted from a single time
segment of the untransformed data is provided in the last row
of Table I for each dataset.
The features are normalized to the interval [0, 1] for each
subject in each dataset. Then, PCA is used to reduce the
number of features to 30.
Classiﬁcation is performed by four state-of-the-art classi-
ﬁers (see [28] for further information):
• Bayesian Decision Making (BDM): A multi-variate Gaus-
sian distribution is ﬁtted to the training feature vectors of
each class. For a test feature vector, the class that has the
highest class-conditional probability is selected.
• k-Nearest Neighbor (k-NN): The training phase consists
of the storage of the training vectors with their true labels. A
test vector is assigned the most common class label among
the k training vectors that are closest to it in terms of the
Euclidean distance. A suitable value of k is selected as 7.
• Support Vector Machines (SVM): The features are
mapped to a higher-dimensional space by using the Gaus-
sian Radial Basis Function (RBF). A binary classiﬁer is
trained for each distinct class pair to divide the feature
space into two regions by a hyperplane that maximizes the
margin. The classiﬁcation relies on the decision of the most
conﬁdent classiﬁer. The RBF and penalty parameters are
optimized as γ = 0.2 and C = 40, respectively, by a two-
level grid search [14].
• Artiﬁcial Neural Networks (ANN): Three layers of neu-
rons with a sigmoid output function are used. The number
of neurons in the input and output layers are 30 and as
many as the number of classes, respectively. The number
of neurons in the intermediate layer is selected as the
nearest integer to the average of the optimistic and pes-
simistic cases [14]. Coefﬁcients of the linear combinations
are randomly initialized in [0, 0.2] and determined by
the back-propagation algorithm with an adaptive stopping
criterion [14] and a learning rate of 0.3. In classiﬁcation, the
test vector is fed to the input and the class corresponding
to the maximum output is selected.
208
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-616-3
ACHI 2018 : The Eleventh International Conference on Advances in Computer-Human Interactions

Two cross-validation techniques are used to assess the
accuracy: P-fold (with P = 10) randomly divides the feature
vectors into P partitions and tests each partition by a classiﬁer
trained by the feature vectors in the remaining partitions. Leave-
1-subject-Out (L1O) considers each subject’s data as a partition
and follows the same approach as P-fold. L1O is more chal-
lenging and generalizable than P-fold because the training and
test vectors belong to different subjects in L1O.
D. Results
The accuracies for all the classiﬁers, pre-processing ap-
proaches, and datasets are shown in the bar chart in Figure 2.
A stick centered at the tip of each bar indicates plus/minus one
standard deviation in the accuracy over the cross-validation it-
erations. Figures 2(a)–(c) show the results for all the activities,
stationary activities, and non-stationary activities, respectively.
(Stationary activities are denoted by an asterisk (*) in Table I.)
Figure 3 shows the accuracy values averaged over the four
classiﬁers and the ﬁve datasets for each approach and cross-
validation technique separately for the the whole activity set,
stationary, and non-stationary activities.
As observed from Figures 2 and 3, the highest accuracy is
obtained by using the REF approach where the orientations of
the sensor units are ﬁxed and no transformation is applied to
the data. The accuracy considerably drops in the ROT approach
where the acquired data are rotated randomly to simulate
the placement of the sensor units at random orientations
and no transformation is applied for orientation invariance.
This shows that the standard activity recognition scheme is
not robust against incorrectly oriented sensors. Among the
two types of transformations for orientation invariance, the
proposed approach SVDT performs signiﬁcantly better than
the widely used approach NORM on the average. This is
especially observed for datasets D1 and D4 and for non-
stationary activities.
V.
DISCUSSION
We consider different approaches at the pre-processing
stage to observe the effects of incorrectly oriented sensors and
the transformations for orientation invariance, unlike most of
the existing studies. The sensor units are correctly placed on
the body in all the datasets that we use, and we synthetically
rotate the signals to simulate randomly oriented sensors. This
enables us to use the same data in both approaches so that
we can fairly compare them. If a new dataset were recorded
for randomly rotated sensors, it would have a different level
of difﬁculty in activity recognition because of the variations
that occur in the data. These variations can be observed from
the standard deviations over the cross-validation iterations in
Figure 2. Reference [29] investigates the variations in the
activity data within and between subjects in detail.
When we apply the existing or the proposed transformation
to the sensor data, we mathematically ensure that the trans-
formed data are not affected by the orientations at which the
sensors are ﬁxed to the body. In other words, the same data
can be obtained by transforming the original and randomly
rotated data (up to the signs of the axes in SVDT). Therefore,
we do not need to record a new dataset with different sensor
orientations to observe the effects of the orientation-invariant
transformations on the activity recognition accuracy.
In the ROT approach, we rotate the tri-axial sensor data
separately for each sensor unit in each time segment. Hence,
the training and test sets contain data corresponding to differ-
ent sensor orientations. This is advantageous for the system
because in the training phase, the classiﬁers may adapt to the
variation in the data by relying on (the linear combinations of)
the features that are more robust against the changes in sensor
orientations. Keeping the training data unchanged and rotating
only the test data may result in a higher accuracy compared to
the method we use.
In Section III, we allow the user to initially place the
sensors on his/her body at any orientation, but we assume that
the sensor rotates together with the body part on which it is
placed; that is, the sensor is rigidly attached to the body. This
assumption is required even for the simplest approach, NORM,
because otherwise the motion sensors would record different
signals when they rotate freely independent of the body
movements. In particular, the gravity vector acquired by the
accelerometer, the angular rate detected by the gyroscope, and
the magnetic ﬁeld of the Earth measured by the magnetometer
would be all unrelated to the body motion in this case.
A sudden change in the orientation of the sensor with
respect to the body corrupts the signals for a short time interval.
If the data are transformed by the NORM approach, the same,
short time interval will be corrupted in the transformed data.
If the proposed transformation SVDT is applied, different
principal vectors will be obtained, and the whole transformed
data will be affected. Since we transform each time segment
separately, only the corresponding segment(s) will be affected,
which corresponds to a short time interval because the seg-
ments have a duration of at most 5 seconds.
There is an important advantage of the method we propose
over most of the existing approaches: Our method only requires
a transformation to be added at the pre-processing stage,
without a need to modify the rest of the activity recognition
system. The input and output of the transformation are both tri-
axial and of the same form. The transformation does not change
the physical units of the acquired data and their dimensionality;
hence, does not require any modiﬁcations in the following steps
in the activity recognition paradigm.
VI.
CONCLUSIONS AND FUTURE WORK
In wearable sensing, it is mostly assumed that the sensors
are placed on the body at ﬁxed orientations, which is not
feasible in many applications such as monitoring of the elderly
or children. We show that incorrectly oriented wearable sensors
signiﬁcantly decrease the activity recognition accuracy, which
is consistently valid for multiple datasets, classiﬁers, and cross-
validation tests. We propose an SVD-based transformation to
represent the tri-axial data in terms of three principal axes to
remove the information of absolute sensor orientation. In most
cases, this method signiﬁcantly increases the accuracy when
the sensors are placed at random orientations, providing an
accuracy close to the reference approach with ﬁxed sensor
orientations in some cases. Our approach achieves a better
overall accuracy than the conventional Euclidean norm method
and can be integrated into most of the existing systems without
much effort.
As future work, the proposed method can be applied to
other applications of wearable sensing, such as fall detec-
tion [30] and physical therapy [31], and extended to handle the
incorrect positions of the sensor units in addition to incorrect
orientations.
209
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-616-3
ACHI 2018 : The Eleventh International Conference on Advances in Computer-Human Interactions

P-fold
99.01
98.64
99.11
98.79
84.07
68.13
85.63
78.27
89.92
85.56
92.20
89.17
98.27
97.38
98.32
96.74
BDM
 k-NN
SVM
ANN
dataset D1
0
50
100
L1O
77.09
87.36
90.36
89.67
60.67
51.82
61.58
54.18
61.82
66.69
74.73
68.49
73.07
79.44
81.74
78.72
0
50
100
97.94
97.36
99.06
98.38
82.17
76.24
89.87
82.17
97.24
96.80
99.22
98.23
95.71
95.42
97.58
97.21
BDM
 k-NN
SVM
ANN
dataset D2
0
50
100
83.83
78.92
87.58
89.59
68.61
56.74
71.78
56.27
74.83
80.30
87.85
89.14
72.46
73.39
84.01
79.78
0
50
100
93.71
93.05
97.77
96.69
60.03
56.27
67.66
57.26
75.30
75.00
83.99
77.26
75.82
74.85
83.76
75.72
BDM
 k-NN
SVM
ANN
dataset D3
0
50
100
88.72
89.47
92.19
91.03
59.09
54.35
63.61
51.17
58.40
69.77
80.29
75.35
69.15
68.94
71.97
62.51
0
50
100
90.81
91.65
93.17
90.40
67.88
60.86
73.90
67.86
63.07
72.18
72.17
68.22
81.37
82.64
86.35
80.91
BDM
 k-NN
SVM
ANN
dataset D4
0
50
100
72.12
66.09
79.38
76.99
60.49
53.04
59.92
44.40
27.73
47.49
52.55
41.53
63.00
63.83
71.67
67.04
0
50
100
57.74
76.90
80.35
78.83
46.71
56.83
63.41
61.42
46.82
70.81
68.22
58.03
56.76
71.25
73.47
74.11
BDM
 k-NN
SVM
ANN
dataset D5
0
50
100
accuracy (%)
42.77
45.27
47.17
42.35
42.64
50.50
53.19
38.10
34.52
50.94
51.04
34.32
43.83
50.42
50.06
46.32
0
50
100
accuracy (%)
(a) all activities
P-fold
99.64
99.69
100.00
99.69
71.93
57.97
72.86
65.57
72.76
64.58
76.93
70.26
98.59
98.23
98.85
96.56
0
50
100
L1O
55.57
85.31
83.39
84.48
49.06
39.01
46.72
33.85
13.65
34.84
36.61
30.42
47.66
66.04
62.97
55.99
0
50
100
98.60
97.42
99.67
99.30
79.72
69.74
89.91
79.32
97.53
97.91
99.59
99.13
95.20
94.60
98.07
97.80
0
50
100
87.43
83.03
94.63
96.88
65.50
51.32
69.21
47.51
77.68
91.34
95.28
96.44
71.40
74.53
90.31
84.17
0
50
100
89.40
88.77
96.01
94.69
41.09
39.29
48.52
34.75
61.15
61.22
74.50
62.91
57.34
55.24
70.41
56.06
0
50
100
83.15
87.10
87.78
87.16
39.11
38.77
44.01
33.15
33.23
60.53
72.04
63.88
46.87
48.21
50.91
33.51
0
50
100
91.13
95.50
97.43
93.62
38.50
45.23
60.70
46.92
39.49
60.26
64.24
62.13
67.39
72.75
79.92
68.93
0
50
100
75.57
74.01
84.31
72.47
42.12
39.67
46.23
15.25
2.90
49.37
48.50
29.16
56.86
55.05
63.32
39.89
0
50
100
45.09
67.68
74.48
68.70
32.02
45.74
50.98
48.66
37.29
61.90
56.05
43.79
43.34
61.96
64.79
63.31
0
50
100
accuracy (%)
29.31
36.65
35.17
35.46
28.66
40.78
40.96
35.60
29.29
40.95
39.50
34.19
30.43
42.00
39.75
38.63
0
50
100
accuracy (%)
(b) stationary activities
P-fold
98.85
98.36
98.88
98.56
87.31
70.83
89.03
81.65
94.50
91.15
96.28
94.21
98.18
97.15
98.18
96.79
0
50
100
L1O
82.83
87.90
92.22
91.06
63.76
55.24
65.54
59.60
74.67
75.18
84.89
78.64
79.85
83.01
86.75
84.78
0
50
100
94.57
95.41
96.65
94.63
74.62
75.19
81.83
76.91
94.29
92.71
97.82
94.54
93.79
94.15
94.29
93.49
0
50
100
60.68
58.88
57.55
61.01
62.19
59.36
62.86
59.76
55.81
50.66
57.62
60.63
54.93
53.74
53.09
55.06
0
50
100
98.53
97.72
99.78
98.85
82.99
75.84
89.98
82.66
91.93
90.84
94.83
92.67
97.98
98.16
99.58
98.48
0
50
100
94.92
91.83
97.18
95.28
82.54
72.11
86.24
70.66
87.72
80.53
90.07
87.25
95.82
93.21
96.11
95.53
0
50
100
87.62
86.46
88.06
85.29
78.48
63.65
74.30
73.44
70.55
75.38
73.95
70.36
84.03
82.41
84.94
81.15
0
50
100
68.34
59.35
74.09
75.37
65.22
55.71
63.50
60.28
45.47
49.76
55.84
50.72
63.31
64.11
70.62
74.82
0
50
100
46.99
34.91
32.84
32.08
36.88
22.65
22.62
27.29
28.04
31.09
30.79
31.06
45.04
33.23
30.04
31.57
0
50
100
accuracy (%)
33.69
18.11
23.97
19.62
28.57
21.21
22.13
11.08
23.47
19.97
24.23
8.62
32.69
19.65
23.70
21.77
0
50
100
accuracy (%)
(c) non-stationary activities
REF (reference)
ROT (random rotation)
NORM (Euclidean norm)
SVDT (SVD-based transformation)
Figure 2. Activity recognition accuracy of each classiﬁer for each pre-processing approach for each dataset. The horizontal stick centered at the tip of each bar
indicates plus/minus one standard deviation in the accuracy over the cross-validation iterations.
210
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-616-3
ACHI 2018 : The Eleventh International Conference on Advances in Computer-Human Interactions

P-fold
91.47
69.33
78.97
84.68
0
50
100
accuracy (%)
L1O
75.90
55.61
61.39
67.57
0
50
100
accuracy (%)
(a) all activities
P-fold
89.83
55.97
68.18
76.97
0
50
100
accuracy (%)
L1O
72.94
42.32
48.99
54.92
0
50
100
accuracy (%)
(b) stationary activities
P-fold
83.25
68.41
76.85
81.63
0
50
100
accuracy (%)
L1O
67.19
56.38
58.09
65.13
0
50
100
accuracy (%)
(c) non-stationary activities
REF (reference)
ROT (random rotation)
NORM (Euclidean norm)
SVD (SVD-based transformation)
Figure 3. Activity recognition accuracy averaged over the classiﬁers and the datasets for each pre-processing approach. The horizontal stick centered at the tip
of each bar indicates plus/minus one standard deviation in the accuracy over the classiﬁers and the datasets.
REFERENCES
[1]
M. Shoaib, S. Bosch, ¨O. D. ˙Incel, H. Scholten, and J. M. Havinga,
“Fusion of smartphone motion sensors for physical activity recognition,”
Sensors, vol. 14, no. 6, pp. 10146–10176, June 2014.
[2]
O. D. Lara and M. A. Labrador, “A survey on human activity recognition
using wearable sensors,” IEEE Communications Surveys and Tutorials,
vol. 15, no. 3, pp. 1192–1209, Third Quarter, 2013.
[3]
J. Morales and D. Akopian, “Physical activity recognition by smart-
phones, a survey,” Biocybernetics and Biomedical Engineering, vol. 37,
no. 3, pp. 388–400, May 2017.
[4]
K. Kunze and P. Lukowicz, “Sensor placement variations in wearable
activity recognition,” IEEE Pervasive Computing, vol. 13, no. 4, pp. 32–
41, October–December 2014.
[5]
S. Bhattacharya, P. Nurmi, N. Hammerla, and T. Pl¨otz, “Using unlabeled
data in a sparse-coding framework for human activity recognition,”
Pervasive and Mobile Computing, vol. 15, pp. 242–262, December 2014.
[6]
S. Reddy et al., “Using mobile phones to determine transportation
modes,” ACM Transactions on Sensor Networks, vol. 6, no. 2, article
no. 13, February 2010.
[7]
P.
Siirtola
and
J.
R¨oning,
“Recognizing
human
activities
user-
independently on smartphones based on accelerometer data,” Interna-
tional Journal of Interactive Multimedia and Artiﬁcial Intelligence:
Special Issue on Distributed Computing and Artiﬁcial Intelligence,
vol. 1, no. 5, pp. 38–45, June 2012.
[8]
L. Sun, D. Zhang, B. Li, B. Guo, and S. Li, “Activity recognition
on an accelerometer embedded mobile phone with varying positions
and orientations,” Proceedings of the 7th International Conference on
Ubiquitous Intelligence and Computing, 26–29 October 2010, Xi’an,
China, Lecture Notes in Computer Science, vol. 6406, no. 1, pp. 548–
562, Springer, Berlin, Heidelberg, Germany, 2010.
[9]
M. Janidarmian, A. R. Fekr, K. Radecka, and Z. Zilic, “A comprehensive
analysis on wearable acceleration sensors in human activity recognition,”
Sensors, vol. 17, no. 3, article no. 529, March 2017.
[10]
Y. Yang, “Toward physical activity diary: motion recognition using
simple acceleration features with mobile phones,” Proceedings of the
1st International Workshop on Interactive Multimedia for Consumer
Electronics, pp. 1–10, 23 October 2009, Beijing, China.
[11]
H. Lu et al., “The Jigsaw continuous sensing engine for mobile phone
applications,” Proceedings of the 8th ACM Conference on Embedded
Networked Sensor Systems, pp. 71–84, 3–5 November 2010, Z¨urich,
Switzerland.
[12]
N. Wang, S. J. Redmond, E. Ambikairajah, B. G. Celler, and N. H.
Lovell, “Can triaxial accelerometry accurately recognize inclined walk-
ing terrains?” IEEE Transactions on Biomedical Engineering, vol. 57,
no. 10, pp. 2506–2516, October 2010.
[13]
A. Henpraserttae, S. Thiemjarus, and S. Marukatat, “Accurate activity
recognition using a mobile phone regardless of device orientation and
location,” Proceedings of the International Conference on Body Sensor
Networks, pp. 41–46, 23–25 May 2011, Dallas, TX, U.S.A.
[14]
A. Yurtman and B. Barshan, “Activity recognition invariant to sensor
orientation with wearable motion sensors,” Sensors, vol. 17, no. 8, article
no. 1838, August 2017.
[15]
J. Morales, D. Akopian, and S. Agaian, “Human activity recognition
by smartphones regardless of device orientation,” Proceedings of SPIE-
IS&T Electronic Imaging: Photonics: Mobile Devices and Multimedia:
Enabling Technologies, Algorithms, and Applications, R. Creutzburg
and D. Akopian (eds.), SPIE vol. 9030, pp. 90300I-1–90300I-12,
18 February 2014, San Francisco, CA, U.S.A.
[16]
Y. E. Ustev, ¨O. D. ˙Incel, and C. Ersoy, “User, device and orientation
independent human activity recognition on mobile phones: challenges
and a proposal,” Proceedings of the ACM Conference on Pervasive and
Ubiquitous Computing, pp. 1427–1436, 8–12 September 2013, Z¨urich,
Switzerland.
[17]
S. Thiemjarus, “A device-orientation independent method for activity
recognition,” Proceedings of the International Conference on Body
Sensor Networks, pp. 19–23, 7–9 June 2010, Biopolis, Singapore.
[18]
R. Chavarriaga, H. Bayati, and J. del R. Mill´an, “Unsupervised adap-
tation for acceleration-based activity recognition: robustness to sensor
displacement and rotation,” Personal and Ubiquitous Computing, vol. 17,
no. 3, pp. 479–490, June 2011.
[19]
M. Jiang, H. Shang, Z. Wang, H. Li, and Y. Wang, “A method to deal
with installation errors of wearable accelerometers for human activity
recognition,” Physiological Measurement, vol. 32, no. 3, pp. 347–358,
February 2011.
[20]
O. Banos, M. A. Toth, M. Damas, H. Pomares, and I. Rojas, “Dealing
with the effects of sensor displacement in wearable activity recognition,”
Sensors, vol. 14, no. 6, pp. 9995–10023, June 2014.
[21]
L. De Lathauwer, B. De Moor, and J. Vandewalle, “A multilinear
singular value decomposition,” SIAM Journal on Matrix Analysis and
Applications, vol. 21, no. 4, pp. 1253–1278, April 2000.
[22]
K. Altun and B. Barshan, “Daily and Sports Activities Dataset,” UCI
Machine Learning Repository, University of California, Irvine, School
of Information and Computer Sciences, 2013. Available from: http:
//archive.ics.uci.edu/ml/datasets/Daily+and+Sports+Activities, retrieved:
January 2018.
[23]
W. Ugulino et al., “Wearable Computing: Classiﬁcation of Body
Postures
and
Movements
(PUC-Rio)
Data
Set,”
UCI
Machine
Learning
Repository,
University
of
California,
Irvine,
School
of
Information
and
Computer
Sciences,
2013.
Available
from:
https://archive.ics.uci.edu/ml/datasets/Wearable+Computing%3A+
Classiﬁcation+of+Body+Postures+and+Movements+(PUC-Rio),
retrieved: January 2018.
[24]
J.
L.
Reyes-Oritz,
D.
Anguita,
A.
Ghio,
L.
Oneto,
and
X.
Parra,
“Human
Activity
Recognition
Using
Smartphones
Data
Set,”
UCI
Machine
Learning
Repository,
University
of
Califor-
nia, Irvine, School of Information and Computer Sciences, 2012.
Available from: https://archive.ics.uci.edu/ml/datasets/Human+Activity+
Recognition+Using+Smartphones, retrieved: January 2018.
[25]
M. Zhang and A. A. Sawchuk, “USC-HAD: a daily activity dataset
for ubiquitous activity recognition using wearable sensors,” ACM Inter-
national Conference on Ubiquitous Computing Workshop on Situation,
Activity, and Goal Awareness, pp. 1036–1043, 5–8 September 2012,
Pittsburgh, PA, U.S.A.
[26]
P. Casale, O. Pujol, and P. Redeva, “Human activity recognition from
accelerometer data using a wearable device,” Proceedings of the Iberian
Conference on Pattern Recognition and Image Analysis, 8–10 June 2011,
Gran Canaria, Spain, Pattern Recognition and Image Analysis, Lecture
Notes in Computer Science, vol. 6669, pp. 289–296, Springer, Berlin,
Heidelberg, Germany, 2011.
[27]
B. Kastner, Space Mathematics: Math Problems Based on Space Sci-
ence, Dover Books on Aeronautical Engineering, Courier Corporation,
Washington, D.C., WA, U.S.A., 2013.
[28]
R. O. Duda, P. E. Hart, and D. G. Stork, Pattern Classiﬁcation, John
Wiley & Sons, New York, NY, U.S.A., 2000.
[29]
A. Yurtman and B. Barshan, “Investigating inter-subject and inter-
activity variations in activity recognition using wearable motion sensors,”
The Computer Journal, vol. 59, no. 9, pp. 1345-1362, September 2016.
[30]
A. T. ¨Ozdemir and B. Barshan, “Detecting falls with wearable sensors
using machine learning techniques,” Sensors, vol. 14, no. 6, pp. 10691–
10708, June 2014.
[31]
A. Yurtman and B. Barshan, “Automated evaluation of physical therapy
exercises using multi-template dynamic time warping on wearable
sensor signals,” Computer Methods and Programs in Biomedicine,
vol. 117, no. 2, pp. 189–207, November 2014.
211
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-616-3
ACHI 2018 : The Eleventh International Conference on Advances in Computer-Human Interactions

