 
Following a Robot using a Haptic Interface without Visual Feedback  
 
Ayan Ghosh 
Materials and Engineering Research Institute, 
Sheffield Hallam University. 
Sheffield, United Kingdom 
ayan.ghosh@student.shu.ac.uk 
 
Peter Jones 
The Department of Humanities, 
Sheffield Hallam University. 
Sheffield, United Kingdom 
 p.e.jones@shu.ac.uk 
 
Lyuba Alboul, Jacques Penders 
Materials and Engineering Research Institute, 
Sheffield Hallam University. 
Sheffield, United Kingdom 
{l.alboul, j.penders} @shu.ac.uk 
 
Heath Reed 
Art and Design Research Group, 
Sheffield Hallam University. 
Sheffield, United Kingdom 
h.reed@shu.ac.uk
Abstract— Search and rescue operations are often undertaken 
in smoke filled and noisy environments in which rescue teams 
must rely on haptic feedback for navigation and safe exit. In 
this paper, we discuss designing and evaluating a haptic 
interface to enable a human being to follow a robot through an 
environment with no-visibility. We first briefly analyse the task 
at hand and discuss the considerations that have led to our 
current interface design. The second part of the paper 
describes our testing procedure and the results of our first 
informal tests. Based on these results we discuss future 
improvements of our design. 
 
Keywords-human robot interaction; haptic interface; support 
for no-visibility/visually impaired 
 
I. 
 INTRODUCTION 
   In this paper, we discuss designing an interface to 
enable a human being to follow a robot (as shown in Figure 
1). A vital pre-condition for successful human-robot 
cooperation in such circumstances is that the human trusts 
and has confidence in the robot. Trust and confidence are 
complex matters, which we have explored in more detail in 
[14]. In this paper, we focus on designing interfaces for 
following a robot and make a first attempt to evaluate the 
designs. 
A. No-visibility 
 Being guided along an unknown path without visual 
feedback poses several challenges to a human being, in 
particular if the guide is a robot. 
 Contrary to popular prejudice, search and rescue 
operations are undertaken only when the ground is relatively 
easily passable [13]; the major problem however, is that the 
environment is smoke-filled and noisy. Rescue teams 
therefore must rely on haptic feedback for exploration, 
navigation and safe exit. However, because of the lack of 
visual (and auditory) feedback, humans get easily 
disorientated and may get lost. Robots with a range of 
sensors on board might be helpful for such conditions. In 
addition to search and rescue, there are everyday situations 
where vision and audition are problematic, for instance, a 
visually impaired person trying to navigate a busy street. 
Though robots are very promising, the issue of being guided 
by a robot is largely open and has not received much 
attention yet. 
 
 
Figure 1.  The Handle. 
Young et al. [18] describe walking a robot using a dog-
leash. They note that leading a robot consists of a delicate 
interplay between the human leader and the robot requiring 
ongoing communication and interaction. This includes (for 
both the robot and the human) monitoring the other’s 
movement direction and speed [18]. The dog-leash is used 
in conditions of good visibility and a relatively low level of 
environmental noise. The monitoring heavily relies on 
visual and aural feedback i.e., the eyes and ears of the 
human.  
 However, lacking visual and aural feedback hampers 
orientation and causes significant stress for rescue workers 
as well as for the visually impaired; in addition it constitutes 
a significant obstacle when aiming to cater for trust and 
confidence. Nevertheless, psychological research has 
demonstrated, contrary to early assumptions and common 
prejudice, ‘the presence of a comparable set of spatial 
abilities in people without vision as can be found in those 
with vision’ [5].  Bremner and Cowle [1][15] note: the 
147
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-325-4
ACHI 2014 : The Seventh International Conference on Advances in Computer-Human Interactions

 
senses touch, proprioception, vision, and occasionally 
audition, ‘convey information about the environment and 
body in different neural codes and reference frames’. 
Research has also highlighted the extraordinary speed and 
sensitivity of the haptic sense [8].  This gives enough 
reasons to explore how to make better use of the haptic 
sense. Eventually, a well-designed haptic interface suitable 
for guidance in no-visibility conditions might also be useful 
in normal conditions and may free the visual sense and 
related mental resources so that they can be used for other 
tasks. 
B. Navigation and following 
 Leading a robot is far from a simple physical locomotion 
problem [18]. However, making a robot lead a person raises 
considerable additional issues, concerning the degree of 
autonomy granted to the robot and the type and extent of 
control exerted by the human. Based on our analysis of the 
interaction between a visually impaired person and a guide 
dog we distinguish between locomotion guidance and 
navigation. While the visually impaired human handler 
determines global navigation (i.e., final destination and en-
route decision points) the guide dog provides locomotion 
guidance between these decision points; as it can be seen in 
Figure 2. Locomotion guidance is effected through a simple 
haptic interface between dog and handler - that is a rigid 
handle held by handler and attached to the dog's harness.  
 
 
Figure 2.  Handling a guide dog/robot; task analysis[14]. 
 Inspired by this, the current paper has the focus on 
locomotion guidance or simply following a robot in a safe 
manner. However, we leave the questions on confidence and 
trust for future. 
 The paper is organised as follows: after a brief literature 
(Section II) review, we discuss in Section III, the design 
presumptions and considerations, which led to the 
implementation of the final interface (shown in Figure 1). In 
Section IV, we describe our preliminary and informal test 
trials. We finish with a discussion on the open issues. 
II. 
LITERATURE OVERVIEW 
 Literature on experiences of human subjects with 
human-robot interaction in low-visibility is rather sparse. 
The Guardians project [13] pioneered a group of 
autonomous mobile robots assisting a human rescue worker 
operating within close range. Trials were held with fire 
fighters and it became clear that the subjects by no means 
were prepared to give up their procedural routine and the 
feel of security provided: they simply ignored instructions 
that contradicted their routines. 
 There are several works on robotic assistance to the 
visual impaired. Tachi et al. [16] developed a guide-dog 
robot for the visually impaired, which leads the person. The 
robot tracks the follower using active sonar, and the 
follower wears a stereo headset, which provides coded aural 
feedback to notify whether the follower is straying from the 
path. There is no means to communicate to the robot, and 
the follower must learn the new aural-feedback code: the 
robot serves as a mobile beacon that communicates with the 
headset. 
 Allan Melvin et al., [12] developed a robot to replace a 
guide dog; however the paper does not extensively report 
trials with users. The GuideCane [17] is a cane like device 
running on unpowered wheels, it uses Ultra Sound to detect 
obstacles. The follower has to push the GuideCane - it has 
no powered wheels- however it has a steering mechanism 
that can be operated by the follower or operate 
autonomously. In autonomous mode, when detecting an 
obstacle the wheels are steering away to avoid the obstacle. 
The GuideCane has been tested with 10 subjects three of 
whom were blind and cane users, the other seven were 
sighted but blindfolded. Basic conclusion: ‘walking with the 
GuideCane was very intuitive and required little conscious 
effort’, unfortunately nothing more is reported on the 
subjects' experience.  
 The robotic shopping trolley developed by Kulyukin 
[4][11] is also aimed at the visual impaired. This trolley 
guides the (blind) shopper - who is holding the trolley 
handle - along the aisles into the vicinity of the desired 
product. The locomotion guidance is fully robot driven but 
restricted to navigating the aisles; the emphasis is on 
instructing the shopper how to grab the product using voice 
instructions. 
 In the current paper, we restrict the use of a haptic 
interface (no aural or visual feedback) to locomotion 
guidance only. By simplifying the task, we are able to take 
the first step towards evaluating the subject's performance, 
while following the robot. The future aim is to combine the 
observed performance of the subjects with assessing their 
confidence in technology.   
III. 
ROBOTIC GUIDE 
A. Design presumptions 
 Our final aim is to design a system and interface that 
allows skilled and successful guidance, enhancing human 
trust and confidence. We expect that a key dimension of the 
skillset of the human follower is the ability to 'read' the 
whole situation in relation to the relevant programme of 
action [6][7].  The aim is for transparent technology; 
technology that is so well fitted that it becomes almost 
invisible in use’ [2]. In contrast, an ‘opaque technology’ is 
‘one that keeps tripping the user up, and remains the focus of 
attention even during routine problem-solving activity’ [2]. 
148
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-325-4
ACHI 2014 : The Seventh International Conference on Advances in Computer-Human Interactions

 
The classic illustration of ‘transparent technology’ in this 
sense, and of particular relevance to our own study, was the 
use of a cane by a blind person (or ‘cane traveler’) for 
navigational purposes [2]. 
B. Mechanical interface: design considerations and history 
 A first step towards this aim is to build an interface that 
will lead the follower along a safe path. The safest path for 
the follower is a path that the robot already has traversed; 
thus the follower should follow the trail of the robot exactly. 
Hence our experiments, reported below, look at the 
following behaviour of the follower in terms of the ability to 
closely match the live path of the robot. 
 Obviously, in order to be able to follow the robot, the 
follower needs to know where the robot is relative to his/her 
current position and orientation. Initially our project looked 
at three distinct interfaces: a wirelessly connecting device for 
instance a Nintendo Wii, a short rope/rein or leash and a stiff 
handle. A major problem for any wireless device lies in how 
to indicate the position of the robot with respect to the 
follower. A rope does indicate the direction of the robot but 
only when there is no slack. Young et al. [18] use a spring-
loaded retractable leash design (popular with dogs), which 
keeps the leash taut; the retracting mechanism however 
obscures the length of the leash and thus the distance 
between the robot and the follower is not known. Our final 
choice has been for a stiff handle via which the position 
(direction and distance) of the robot is immediately clear to 
the follower. 
C. Interaction with a Stiff interface: 
 We tried a stick held in one hand mounted on a disc 
with unpowered omni-directional wheels (as presented in 
Figure 3). Basically, the disc would be set into motion by 
the person holding the stick. The omni-directional wheels 
made the disc easy manoeuvrable in any direction (on the 
floor). However, when holding the stick blind folded, a lack 
of accuracy in sensing the direction has been noticed; 
several subjects immediately put their second hand on the 
stick to compensate. Our observation of a lack of accuracy 
of a one handed hold is in line with experiences in using a 
white cane. Visually impaired people using a white cane do 
hold the cane in one hand but they also apply a special grip 
(for instance stretched the index finger) and/or keep the 
elbow touching the body. We note that manipulating our 
disc is not as easily as handling a white cane.  
 From this we concluded that a crutch like design of the 
handle, in which the stick is fixed on the lower arm, is 
preferred. 
 
Figure 3.  Hand held stick on a disc with omni-directional wheels. 
D. Implementing the handle (stiff Rein) on the robot 
 Based on these conclusions, a simple crutch-like 
prototype with a ball-free mechanism at the base (as 
presented in Figure 4) was developed to enable some initial 
experimentation. The pilot studies have revealed that, there 
have been instances such as:  
 
                
 
Figure 4.  Ball-free mechanism at the Base. 
 
The follower did not feel safe following the robot 
(as presented in Figure 5). Obviously we can judge 
the path of the follower as safe when it closely 
matches the path of the robot. 
 
The follower lost track of the orientation (heading) 
of the robot, though its position was clear. As a 
consequence, the follower did not feel comfortable 
following the robot at the turns. The handle 
delivered an abrupt tug to the follower at the point 
of the turns.                       
          
 
Figure 5.  Unsafe path, the follower gets deviated too much off the course. 
 These findings led to the design of a third prototype to 
ensure safety, comfort and rigidity. The prototype consists 
of a mechanical feedback spring system at the base, as 
presented in Figure 6. The spring system allows rotation of 
the handle on the horizontal plane. When the spring system 
has zero tension, the handle is aligned with the center line of 
the robot. When the handle is being rotated, the spring 
system induces tension on the handle, which increases with 
the rotation angle. The system also comes with a pin 
enabling to nullify the action of the springs, giving us the 
option to carry out a comparative study between a flexible 
joint and a fixed joint. Thus, this handle provides two 
testing options: 
 
The handle is attached in a fixed joint (rigid): 
meaning the handle is fixed at base using the pin. 
149
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-325-4
ACHI 2014 : The Seventh International Conference on Advances in Computer-Human Interactions

 
 
The handle is attached with a flexible joint 
(spring): meaning the handle can rotate in the 
horizontal plane, and rotation induces tension on 
the handle. 
 
 
Figure 6.  Handle with spring system. 
E.  Robot and sensors 
 The handle has been mounted on a Pioneer-3AT 4-
wheel robot. In the experiments reported below, the robot 
was autonomously navigating fixed trajectories while being 
supervised by an operator. The operator was able to 
stop/start the robot remotely using a developed Java 
application [9]. The robot operated with a linear speed of 
0.6m/s and the angular speed was set at 0.5 rad/s (at the 
turns). 
 At all time, the walking pattern of the follower was 
being observed and the degree of displacement of the 
follower with respect to the center line of the robot was 
being recorded using a Hokuyo Laser Range Finder, which 
was fixed exactly at the middle of robot's rear bumper. Data 
collection proceeded at a speed of 10Hz or 10 observations 
per second. The positions of the robot at every instance of 
time were measured by odometry sensors. The data was sent 
to the operator's workstation using a Lantronix 802.11g 
WiPort modem. 
IV. 
ROBOTIC EVALUATION GUIDELINES 
 In 
designing 
and 
interpreting 
our 
preliminary 
experimental studies, we were guided by the theoretical 
perspective of developing the robot guide as ‘transparent 
technology’ [2] [3]. And, the primary evaluation purpose 
was to test usability: whether a person could easily follow 
the robot. 
A. Testing Protocol  
 We studied the effect of two different settings of the 
stiff interface on the following behaviour of right-handed 
participants.  On each of the trials, the subjects were asked 
to use the stiff handle in one of the following modes: 
 
The handle attached in a fixed joint (rigid)  
 
The handle attached with a flexible joint (spring) 
 The overall aim of the study is to evaluate the use of an 
autonomous robot guide. However, autonomous behaviour 
can occur in many variants; for our study, we confined the 
robot to five pre-programmed repeatable behaviours. Thus, 
the robot was made to move autonomously in one of the 
following pre-programmed trajectories below: 
 
path A: Straight line (approximately 8 meters). 
 
path B: Straight line (approximately 5 meters) + 
sharp turn (right/left) + straight line (approximately 
3 meters).  
 
path C: Straight line (approximately 5 meters) + 
gentle 
turn 
(right/left) 
+ 
straight 
line 
(approximately 3 meters). 
 When the robot moves in a straight line, the set linear 
speed is inspired by the normal walking speed of a person. 
However, for setting the robot's angular speed we do not 
have an intuition; therefore we designed a smooth turn 
(close to 45 degrees) and a sharp turn (close to 90 degrees).   
 Our preliminary and informal tests were carried out 
with team members (four) as subjects; each of them 
performing 8 trials for each of the paths A, B and C, with 
different handle settings. Subjects were blindfolded and 
asked to put headphones on. Before the commencement of 
each trial, the handle was attached to the subject's forearm 
and a gentle pat was the pre-arranged haptic signal from the 
experimenter, used to indicate the start of each trial. For 
each trial we monitored the following: 
 
the position coordinates (odometry sensors) of the 
robot in the experimental space, at a frequency of 
10 Hz . 
 
the degree of displacement of the subject from the 
trajectory of the  robot. 
 The data collected were used to examine the spatial 
correspondence of the robot's path and the follower's path.  
B. Experimental results 
Robot following path A: 
 Our first trial with each subject aimed to observe how 
the person follows the robot. The handle is mounted in the 
middle of the robot, while the crutch like part of the handle 
is attached to the right fore-arm of the follower (right-
handed) thereby making him/her stand about 15-20 cm left 
of the center line of the robot (as presented in Figure 7). In 
the figures below, we show reconstructions of the paths of 
the robot and the follower across several trials. The 
reconstruction is based on the data collected (10 Hz) on 
board of the robot. The movements (straight/left/right) of 
the robot and follower are shown in the diagrams.  
The robot is around a meter (length of the handle) in 
front of the follower. So while the robot starts at time t0 at 
position (0,0) the follower is at time t0 at position (-1,0). 
Figure 8 shows a reconstruction of the straight path (path 
150
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-325-4
ACHI 2014 : The Seventh International Conference on Advances in Computer-Human Interactions

 
A). When the path is straight, there is no impact of handle 
settings (fixed or sprung joint) on the following behaviour: 
the follower follows the robot, slightly (15-20 cm) off the 
robot's centre. 
 
 
Figure 7.  Position of the follower at the start. 
 
 
Figure 8.  Reconstruction of the paths of robot and follower moving in a 
straight-line. 
Robot following path B: 
 Figure 9 and Figure 10 show a reconstruction of the 
paths while the robot takes a sharp turn to the left. It is 
visible across the trials that there is a very obvious 
difference between the follower's experience with fixed 
joint (Figure 9) and the sprung joint (Figure 10) and the 
impact of these two different handle settings on the 
follower's following behaviour. When the joint is fixed as in 
Figure 9 the follower is forced across the centre line of the 
robot. The follower gets deviated about 0.5 m of the life 
path of the robot. With the flexible joint this effect is rather 
minimal and there is a higher degree of matching of paths 
(as Figure 10 shows).  
 
 
Figure 9.  Reconstruction of the paths of robot and follower with sharp turn 
(fixed-joint). 
 
Figure 10.  Reconstruction of the paths of robot and follower with sharp 
turn (sprung-joint). 
Robot following path C: 
 Figure 11 shows the reconstruction of the paths of both 
robot and follower, while the robot takes a gentle turn. In 
this case, when the robot turns, there is also a clear, but 
smoother deviation of the follower's path from the path of 
the robot.  
 
 
Figure 11.  Reconstruction of the paths of robot and follower with gentle 
turn (fixed-joint). 
Turning right versus turning left 
 It became evident from our experiments that there is an 
acute difference in the following behaviour when the robot 
is turning right and when the robot is turning left. On right 
turns, the follower's path deviates considerably more from 
path of the robot (at the point of turn) than on left turns, 
compare Figure 9 with Figure 13.  
The follower is holding the handle in the right hand; 
when the robot is taking a right turn, the crutch like handle 
pushes the follower's arm towards the body of the follower. 
This is forcing the follower to step out; at the same time the 
initial 'inertia' of the follower causes slippage of the robot 
meaning that Figure 13 and Figure 14 also include a 
slippage error. 
 
 
Figure 12.  The body posture of a person during left (left) and right (right) 
turn. 
151
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-325-4
ACHI 2014 : The Seventh International Conference on Advances in Computer-Human Interactions

 
 
Figure 13.  Reconstruction of the paths of robot and follower with sharp 
right turn (sprung-joint). 
 Contrarily, during a left turn the arm has much more 
freedom for movement and the following behaviour looks 
more comfortable. Figure 12 shows the body postures of a 
person when the robot starts turning right and left. These 
effects are persistent during gentle turns as well (as shown 
in Figure 14).  
 
 
Figure 14.  Reconstruction of the paths of robot and follower with gentle 
right turn (sprung-joint). 
V.  DISCUSSION 
 The findings of the experimental trials raise a number of 
issues about the design of the handle and user experience 
that deserve further investigation. First of all, it seems clear 
that when the handle is attached with a flexible joint 
(spring) the follower's path better matches the path of the 
robot; there is only little displacement of the human 
follower from the robot's trail. For right turns, deviations 
start very abrupt, but remain smaller with the sprung-joint. 
In the turns the follower is exerting some force on the robot 
and this causes the robot to slip and maybe slide. The 
reconstructed paths in figures 9-14 are based on odometry 
data and will contain some error, nevertheless the overall 
patterns can be recognised in the videos taken.  
 The flexible handle setting allows for a build-up of 
tension within the spring mechanism in real time, meaning 
that the forces on the subject accumulate gradually, thereby 
causing a delay between the start of the robot's turn and the 
follower reacting to it (the start of the subject's turn). That 
delay makes for a smoother turn and one that is more 
accurate spatially, however, it leaves open how immediately 
and accurately the follower is alerted of the movements of 
the robot through the haptic interface. 
 In terms of the subjective experience of the follower, 
our initial anecdotal evidence suggests that the flexible 
handle setting affords a smoother and more comfortable 
guided experience, although the firmer and more abrupt tug 
delivered by the inflexible handle may give the handler a 
keener awareness of spatial orientation and location.  
 Future experiments will have more formal layout and 
will include questionnaires in order to capture the subjective 
experiences. Also, we will have to compare right and left 
handed subjects in order to confirm our intuition that on a 
left turn a left handed person is also forced to step out and 
mirrors the pattern of a right turn by a right handed person.    
 Future work will concentrate on refining the objective 
and subjective measures of path correspondence and 
examine to what extent following can be seen as a learnable 
skill, with the handle becoming 'transparent technology' and 
helping in 'human-technology symbiosis' [10]. 
VI. CONCLUSION 
 In this paper, we have presented a haptic interface 
attached to an autonomous robot for locomotion guidance. 
We have reported on a small scale experimental study of 
different settings of the interface. We have learned that the 
feedback spring mechanism at the base of the interface 
created a quite different feel to the task of following the 
robot without any visual and audio feedback, giving more 
safety and comfort.  
ACKNOWLEDGMENT 
 This research was supported by the UK Engineering and 
Physical Sciences Research Council (EPSRC) grant no. 
EP/I028765/1. 
 The authors also would like to thank Alireza Janani for 
providing measurements for human and robot paths.  
 
REFERENCES  
[1] A. J. Bremner, and D. Cowie, "Developmental Origins of the 
Hand in the Mind," and "The Role of the Hand in the Development 
of the Mind" (in [15], 2013). 
[2] A. Clark, "Natural Born Cyborgs:Mind,Technologies and The 
future of Human Intelligence," Oxford: Oxford University Press, 
2003. 
[3] A Clark, "Supersizing the Mind: Embodiment, Action, and 
Cognitive Extension," Oxford: Oxford University Press, 2011. 
[4] C. P. Gharpure, and V. A. Kulyukin, "Robot-assisted shopping 
for the blind: issues in spatial cognition and product selection," 
Intelligent Service Robotics, 2008, Volume 1, Number 3, 237-251, 
DOI: 10.1007/s11370-008-0020-9. 
[5] R.G. Golledge, R. L. Klatzky, and  J. M. Loomis,  "Cognitive 
mapping and wayfinding by adults without vision," in J Portugali 
(ed.) The Construction of Cognitive Maps, Kluwer Academic 
Publishers: 215-246, 1996. 
[6] R Harris, "Signs, Language and Communication," London: 
Routledge, 1996. 
[7] R Harris, "The integrational conception of the sign", in 
Integrationist Notes and Papers 2006-2008, Sandy:Bright Pen, 61-
81, 2009. 
[8] M. A. Heller, W. Schiff, and L. Erlbaum, "The Psychology of 
Touch," Inc. 1991. 
[9]  A. Janani, A. Holloway, H. Reed, and J. Penders, "Design of a 
Mechanical Impedance Filter for Remote Haptic Feedback in Low-
Visibility Environment," TAROS, 2013. 
152
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-325-4
ACHI 2014 : The Seventh International Conference on Advances in Computer-Human Interactions

 
[10] P. Jones, A. Ghosh, J. Penders, and H. Read, "Towards human 
technology symbiosis in the haptic mode,".In: International 
Conference on Communication, Media, Technology and Design, 
Famagusta, North Cyprus, 2-4 May 2013. 307-312, 2013. 
[11] V. Kulyukin and A. Kutiyanawala, "Accessible shopping 
systems for blind and visually impaired individuals: Design 
requirements and the state of the art," The Open Rehabilitation 
Journal, 2010. 
[12] A. Melvin, A. Prabu, B. Nagarajan, R. Bukhari, and Illia, 
"ROVI: a robot for visually impaired for collision- free 
navigation," Proceedings of the International Conference on Man-
Machine Systems (ICoMMS 2009). 
[13] J. Penders, L. Alboul, U. Witkowski, A. Naghsh, J. Saez-
Pons, S. Herrechtsmeier, and M. El-Habbal, "A robot swarm 
assisting a human firefighter," Advanced Robotics, 25:93-117, 
2011. 
[14] J. Penders, P. Jones, A. Ranasinghe, and T. Nanayakara,  
"Enhancing trust and confidence in human robot interaction," In: 
UKRE, Sheffield, 25-3-2013. 
[15] Z. Radman, "The Hand, an Organ of the Mind," MIT Press, 
2013. 
[16] S. Tachi, R. W. Mann, and D. Rowell, “Quantitative 
comparison of alternative sensory displays for mobility aids for the 
blind,” IEEE Trans. Biomedical Engineering, vol. 30, no. 9, pp. 
571–577, Sep. 1983. 
[17] I. Ulrich, and J. Borenstein, "The GuideCane-applying mobile 
robot 
technologies 
to 
assist 
the 
visually 
impaired,"  
Systems, Man and Cybernetics, Part A: Systems and Humans, 
IEEE Transactions on  Issue Date: Mar 2001 Volume: 31, Issue:2 
On page(s): 131 - 136 ISSN: 1083-4427. 
[18] J. E.Young, Y. Kamiyama, J. Reichenbach, T. Igarashi, and  
E. Sharlin, "How to Walk a Robot: A Dog-Leash Human-Robot 
Interface," In: International Symposium on Robot and Human 
Interactive Communication, RO-MAN, pp. 376–382 (2011).
 
 
 
153
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-325-4
ACHI 2014 : The Seventh International Conference on Advances in Computer-Human Interactions

