Improving Online Interactive Modules:  
An Iterative Design Model 
Vanessa Slinger-Friedman and Lynn M. Patterson 
Department of Geography & Anthropology 
Kennesaw State University 
Kennesaw, United States of America 
vslinger@kennesaw.edu and lpatters@kennesaw.edu 
 
Abstract— With the promotion of online and interactive 
learning, instructors may be tempted to include arbitrarily the 
latest technologies into their modules without considering the 
ramifications on students and their learning processes.  
Instead, we propose that online and interactive modules should 
be carefully designed using multiple levels of review to ensure 
clarity of instruction, ease of use of interactive components, 
and engagement of the students with the material and any 
related activities.  This paper documents the process to design 
and improve an online module using an iterative process 
involving instructional designers, student pilot tests, and a 
focus group. Feedback from these constituents enabled 
instructors to optimize instructional design to maximize 
learning 
opportunities 
and 
achievement 
in 
online 
environments.  
Keywords—iterative design process; student focus group; 
interactive online modules; cognitive learning;  human 
geography  
 
I. 
 INTRODUCTION  
Relatively recent changes in the focus of curriculum 
preparation from instructor-centered to learner-centered has 
put more attention on the deliberate design of courses and 
the development of content and assignments. In online 
courses, new technologies increasingly are being introduced 
to provide students with content and provide opportunities 
for students to interact with that content. However, 
designing new learning environments is challenging, and the 
center of attention tends to be on the delivery of new content 
through the technology rather than assessment of the impact 
of these new learning technologies on student learning [1]. 
Additionally, assessment of courses is often inadequate, 
poorly timed, and limited in the effect it has on course or 
module modification and improvement. While considered 
important, feedback on module design before it is delivered 
to students within the structure of a formal class is 
uncommon [2].  
Using a cognitive theory framework, an online interactive 
learning module was developed for an introductory human 
geography course and as part of the development of a 
completely online textbook.  The cognitive theory 
framework supports a multimedia design of educational 
materials [3][4][5] through which students can engage in 
meaningful learning when they actively process material 
through “selecting relevant words and pictures, organizing 
them into coherent pictorial and verbal models, and 
integrating them with each other and appropriate prior 
knowledge” [5]. This module is one of several developed 
that includes imagery, custom videos, readings, discussions, 
animations, interactive exercises, and assessments.  In the 
module the integration of theory and applications takes 
place through activities in which theories and ideas are 
applied for use in practical situations to answer real-world 
geographic questions, bringing the course material “alive” 
for students.  
The purpose of this paper is to document and analyze the 
iterative 
design 
process 
for 
the 
development 
and 
improvement of this online interactive human geography 
module.  This iterative design process involved the input 
and feedback from Instructional designers (IDs), student 
pilot testers, and a student focus group. In the design 
process and improvement of the module, we obtained 
feedback in the three areas of (1) engagement, (2) clarity 
and ease of use of module elements, and (3) the ability of 
the module elements to assist in meeting learning outcomes. 
This paper begins with the literature associated with a 
learner-centered approach utilizing a multimedia design, and 
the use of assessment and feedback to design and then 
improve modules that have a learner-centered focus. 
Specific examples of courses that have used an iterative 
design process or involved feedback from instructional 
designers and students in the course design process are 
included.  Then, the design of the interactive module, based 
on a process of student-centered learning, created for the 
introductory human geography course is detailed. Next, we 
outline the use of an iterative design process using IDs, 
student pilot testers, and a student focus group that provided 
the basis for module redesign. We present the results and 
lessons learned from this iterative process. Finally, we 
conclude with the broader implications of this research on 
optimizing instructional design to maximize learning 
opportunities.  
II. 
LITERATURE 
Traditional curriculum preparation has conventionally 
focused on the instructor rather than the learner. That is, the 
instructor prepared the material for delivery and expected 
students to absorb the material through lectures, readings 
 
137
International Journal on Advances in Life Sciences, vol 5 no 3 & 4, year 2013, http://www.iariajournals.org/life_sciences/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

and written exams.  In recent years a paradigm shift has 
moved the emphasis from teaching to learning and to a 
student-centered curriculum with a greater emphasis on 
meeting learning outcomes and the ability for students to 
demonstrate skills and competencies within courses and 
modules. In the world of online courses this means that 
contemporary online learning development is moving away 
from courses with “pages of electronic text, to more 
deliberately planned learning designs, learning tasks, and 
processes structured in deliberate ways” [6].  
Correspondingly, there is more of a focus on how 
learners learn and the design of effective learning 
environments based on best practices.  The learner-centered 
model calls for active student participation and the use of 
multidimensional products to develop deeper understanding 
by students [7]. As technology has developed and become a 
more integral part of the distance learning environment, it 
has impacted the delivery of content, learning tasks, and 
assignments [8]. The ways by which information is 
presented and also the way in which students interact with 
that material are important.  Furthermore, the medium 
employed can motivate and engage students as active and 
collaborative learners rather than just providing information 
to them. Multimedia instruction rather than “flat resources,” 
such as static text documents, have been identified as an 
important element of high-level interactive engagement and 
student satisfaction [9][10].  
The design of the online interactive module for this study 
is predicated on Mayer’s research on cognitive theory-based 
assumptions regarding the way that people learn from 
words, pictures, and active processing of material (what 
Mayer considers the two elements of the “Dual Channel 
Assumption and the Active processing assumption) in 
computer-based multimedia presentations results in deeper 
understanding in learners [5]. This concept of knowledge 
transmission is based on a constructivist point of view 
where knowledge is constructed by the learner through 
activity [8]. This construction has led to the development of 
“new learning environments” or what Martens et al. [8] call 
“constructivist e-learning environments” in which activities 
are created to challenge students and provide them with 
realistic contexts so that students become intrinsically 
motivated to explore and control their own learning process.     
Despite the shift in the teaching/learning paradigm and 
the rise of assurance of learning outcome assessments, there 
is a lack of available texts and other material to guide 
instructors involved in module design/redesign, and a lack 
of attention given to and results in terms of course 
improvement derived from the type of student feedback 
currently 
elicited 
[11][12]. 
Course-based 
student 
assessment, now commonly found at most accredited higher 
education institutions, is created to encourage instructors to 
examine their roles as course creators and to articulate their 
goals and objectives. The process of collecting student 
feedback through formal assessment of individual teachers 
and courses is widespread [12]. While there are other 
purposes for formal assessments, a significant function of 
this feedback is to provide instructors with information 
about their teaching, with the intention that they will use 
this feedback to improve their courses and enhance the 
effectiveness of their teaching. Most commonly, these 
student evaluation surveys take the form of automatically 
scanned standard questionnaires with questions like, ‘The 
instructor is knowledgeable about the course material’ and 
‘The instructor inspires interest in the course and course 
content’, using a five-point scale ranging from ‘strongly 
agree’ to ‘strongly disagree’. Customarily, course-based 
student assessment is done at the end of a semester, leaving 
no time for modification to be made to a module or course 
for that cohort of students or even the incoming cohort of 
students. Furthermore, research has shown that the test-
retest reliability of students’ evaluations is high, indicating 
that the performance of the teachers is not improving with 
experience, perhaps as a result of teachers and institutions 
not taking the student feedback sufficiently seriously [12]. 
While student evaluations and ratings of an instructor and 
course might be reliable, they do not in themselves lead to 
any improvement in the quality of teaching and the 
effectiveness of course content and course design [13]. 
Formative assessment is a well-recognized element of basic 
Web site design and publication, yet it has received limited 
attention in the literature on online module development 
[14]. 
In line with the cognitive theory for learning, course or 
module evaluation based on criteria that is co-operatively 
developed and focused on obtaining information about the 
quality and effectiveness of the module should be most 
constructive in course redesign. Additionally, some scholars 
believe evaluation should not be just a retrospective process, 
but it should be an integral part of module development, 
informing instructors before, during, and after the process, 
e.g., [2].  
Designing and improving new learning environments is 
challenging. Much of the available research shows an 
emphasis on delivery of these new learning environments 
rather than on analysis, evaluation and process of 
development, e.g., [15]. Module authors need to consider 
design elements including perceived applicability, content 
organization and interaction, ease of use, and the potential 
for learner engagement [14].  
Designers of new online learning environments rarely 
gain knowledge of how students will perceive the module 
and associated tasks before they are delivered to the 
students. Greenberg [16] asserts that quality assessments 
should be taking place during the design of the course and 
include the course creators. Some rare examples of this can 
be seen in Kingston et al. [17] and Lederman [18]. Kingston 
et al. [17] utilized mobile technologies and virtual fieldtrips 
to teach physical geography.  Students who had taken the 
old module and completed the new module were given 
questionnaires and then participated in a focus group to 
investigate the effectiveness of the new technologies. 
138
International Journal on Advances in Life Sciences, vol 5 no 3 & 4, year 2013, http://www.iariajournals.org/life_sciences/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

Improvements were made to the material based on student 
feedback. Lederman [18] also suggests that focus groups 
can be very useful for pre-testing educational materials as 
they “provide an opportunity for extensive commentary, 
unrestrained by the limits of a survey questionnaire or the 
student-teacher relationship which may affect course 
evaluations at the end of a class” ([18], page 126). Skye et 
al. [14] concluded that multiple methods of data collection 
could be used to provide information to module developers 
to improve modules and to address module ease of use, 
navigation and content. Based on the precedent set by these 
examples, in designing our module we used an iterative 
process that required design advice and inspection by IDs, 
student pilot tests, a student focus group, and refining of the 
module prior to incorporating the module into classes. 
III. 
MODULE DESIGN 
The interactive multi-media module design uses the 
concepts 
of 
space, 
place, 
and 
human-environment 
interaction to study the process of hydraulic fracturing 
(otherwise known as fracking) from a geographic 
perspective.  The module requires approximately 30 to 45 
minutes for completion. Hover texts and graphics provide 
the module with text explanations. Using a web-based 
format, the module lists the learning objectives and begins 
with a short reading of one to two paragraphs in length that 
provides an overview of the applied topic.  Next, a three 
minute narrated animation illustrates the concept of 
fracking.  This is followed by a short video, which discusses 
the geographic implications of the topic. Finally, a series of 
interactive exercises allows the student to explore the topic 
using geographic tools (e.g., visual examination, verbal 
descriptions, digital mapping, cognitive perceptions, and 
mathematical modeling). For several of the module 
elements described above, an interactive textbox appears to 
the right where the student is encouraged to take notes.    
Different components in the module require different 
intensities of interactivity. For example the animation and 
videos require students to click to start or more forward. The 
multimedia uses animation, voiceover, and video to engage 
students in a different way than simply reading the content. 
Other activities have a higher intensity of interactivity. The 
clickable maps require students to be actively engaged in 
thinking about, manipulating, and actively participating in 
the learning through completing the exercise. An example of 
this is when students have to use a calculator embedded in 
the module to find the population change in Williston, North 
Dakota and Rifle, Colorado. Likewise, they have to click on 
the markers that show fracking violations in Pennsylvania 
and make active decisions about what information to use to 
contribute to the discussion on the blog tied to this activity. 
The results of the interactive exercises are shared with the 
instructor and, in some cases, other students. The interactive 
exercises were developed using publically available 
software such as micromob, Scribblar, Google Earth and 
ArcGIS ™ (Figs. 1 and 2). 
 
 
Figure 1. Interactive exercise using Google Earth 
 
 
139
International Journal on Advances in Life Sciences, vol 5 no 3 & 4, year 2013, http://www.iariajournals.org/life_sciences/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

  
 
Figure 2. Interactive exercise using ARCGIS 
 
 
IV. 
METHODOLOGY FOR ITERATIVE DESIGN 
DEVELOPMENT AND IMPROVEMENT OF AN ONLINE MODULE  
In an iterative design process to develop and improve an 
online interactive human geography module, we engaged 
instructional designers, student pilot testers, and a student 
focus group.  We focused on three areas for module 
assessment and improvement: (1) engagement, (2) clarity 
and ease of use of module elements, and (3) the ability of 
the module elements to assist in meeting learning outcomes. 
We also asked our IDs, student pilot testers, and focus group 
students to suggest general improvements to the module.  
The online interactive module was created and evaluated 
in four stages (Fig. 3). At the beginning of the module’s 
design, the instructors met with IDs from the University to 
gain knowledge about software (with a focus on free 
software) and design. After the module was designed the 
instructors met once again with the IDs to test for 
functionality of the module.  Next, the module was pilot 
tested with three student volunteers.  These students 
provided feedback on the module design and functionality. 
Finally, a group of 17 students participated in a large-scale 
pilot.  As a group and simultaneously, each student 
completed the module in advance of the focus group 
interview.    
The focus group students provided feedback on their 
engagement with the module, the clarity and ease of use of 
the different module elements, and how they thought the 
module elements assisted them in achieving the learning 
outcomes stated at the outset of the module (Fig. 4).  Both 
members of the research team were present – one to serve as 
moderator and the other as a note taker who recorded 
speakers, comments and significant non-verbal behavior 
[19].    
To address a potential a repercussion of focus groups, we 
had two mechanisms to minimize groupthink [20].  First, 
students each filled out a short questionnaire at the 
completion of the module.  The questionnaire allowed us to 
obtain individual feedback that may not have come out in 
the group discussion but that might have been vital to 
improving the e-learning modules.  Second, we asked the 
focus group members to jot down notes during the group 
interview. In the event students did not get a chance to share 
their comments, these notes were collected at the end.  
 
140
International Journal on Advances in Life Sciences, vol 5 no 3 & 4, year 2013, http://www.iariajournals.org/life_sciences/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

Introductions 
Facilitator introduces members of the research team and each of the 
group members introduce themselves.  The facilitator provides the 
background and ground rules (confidential and anonymous reporting, 
honest opinions, etc.).  The facilitator will inform the group that we 
would like to collect notes made by the participants during the session 
to ensure we collected as much feedback as possible, if the 
participants are willing. 
Issues and Discussion Questions (Semi-structured) 
Overall Impressions 
• 
Please share with us overall how you felt about the modules? 
• 
What did you like about the modules?  What didn’t you like 
about the modules? 
Engagement 
• 
What about the material (videos, photos, readings) did you find 
the most engaging? 
• 
How did the interactive exercises affect your interest in the 
content? 
• 
Did any of the material or exercises make you want to learn 
more about the topic?  If so, which and how? 
Clarity and Ease of Use of Elements  
• 
What concepts or parts of the module were the most clear? The 
least clear? 
• 
What aspect of the interactive exercises did you find the 
clearest/easiest?  What aspects were unclear/more difficult? 
Learning 
• 
Overall, how useful did you find the exercises? 
• 
How did the interactive exercises assist you in understanding 
course content?  In applying course content? 
• 
How did the interactive exercises challenge you? 
Improvements 
• 
What improvements could we make to improve the elements of 
the modules? 
Summary of what we have heard   
• 
Have we missed anything? 
Collect notes (to review later). 
   
 
 
Figure 3. The Four Stages of Module Creation and Evaluation
 
 
 
Figure 4. Focus Group Questions 
 
We reviewed the results of the surveys and interviews, 
coded the data and created categories to allow trends to 
emerge and to be able to develop summary statements that 
capture the essence of the responses [18][21].  The results of 
the coding offer two outcomes.  First, the student responses 
helped us identify which of the interactive exercises had 
greater perceived value to students and which of the module 
elements need to be discarded or modified due to their 
inability to engage students and help them meet the stated 
learning outcomes for the module. Second, areas of 
confusion and lack of clarity were identified. The modules 
were then revised to address weaknesses. 
V. 
FEEDBACK AND DISCUSSION 
Given the quality and content of the feedback from the 
three audiences, we concur with Greenberg [16] that 
assessment provided during the design of the course that 
include course creators can result in a quality outcome.   
Specifically, the feedback obtained for all design elements 
(content, clarity, ease of use, and engagement) were vital to 
ensuring the desired learning outcome could be achieved 
using these interactive exercises [14]. 
The iterative process allowed us to obtain feedback from 
different audiences and revise the module accordingly before 
we finalized it for in-class implementation.  In the first three 
phases of feedback, we used the responses to modify the 
interactive exercises for clarity and ease of use.  In the focus 
group stage, we not only focused on clarity and ease of use 
but also on engagement and ability to meet learning 
outcomes.     In addition to these three areas, the focus group 
also provided feedback through general likes/dislikes, 
technical comments, and recommendations for the module.  
The students even recommended additional content they 
would like to see included.   
The IDs’ feedback from the pre-tests was mostly technical 
in nature and was intended to improve the student 
experience.  For example: 
•  Outline module content 
•  Sketch out potential 
interactives 
Instructional Designer 
Brainstorming 
•  Review suggested 
technologies 
•  Insert interactives 
Instructional Designer 
Testing 
•  Adjust interactives 
based on feedback 
•  Prepare for student 
pilot testing 
Student Pilot Testing 
and Feedback 
•  Review feedback 
•  Edit interactives based 
on feedback 
•  Prepare for focus group 
Focus Group Testing 
and Feedback 
•  Review feedback 
•  Edit interactives based 
on feedback 
•  Prepare for in-class 
implementation 
In-Class 
Implementation of 
Modules 
141
International Journal on Advances in Life Sciences, vol 5 no 3 & 4, year 2013, http://www.iariajournals.org/life_sciences/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

• 
You might want to include an example of the word 
popper and explicitly tell students when they see 
blue text in following pages to hover over the word 
for additional information. 
• 
You may want to add a comment above the 
animation, telling students to let each slide load 
before they try to watch it so there are no buffering 
issues. 
• 
You may want to direct students to click "finish" 
after they take their notes so this is saved and ready 
to be submitted to you. 
• 
I had issues getting the Google Earth plug in to 
download on my computer and ended up having to 
manually install it and restart the browser. You 
may want to take a second on the introduction page 
to tell students to download the plug in. This way 
they can have it installed and ready as they move 
through the lesson.  
• 
You might want to expand the pixel dimensions of 
the calculator tool so students don't have to scroll 
in this box. 
• 
I was able to comment on the blog, but once I 
submitted my comment I got kicked out of 
Softchalk to the actual blog page. I am trying to 
find alternative tools to help you accomplish the 
same thing. One option might be a tool called 
MicroMob (free, but students would have to 
register for an account).  
 
The three student volunteers, who completed the pre-test, 
offered similar feedback but this time as representative users.  
Some comments from the pre-testers were: 
• 
I tested the Module, but couldn't open Scribblar on 
my computer, but I think it was only a problem with 
my computer not the module itself.  
• 
I was very impressed with the map interface, it 
worked well in conjunction with the activity.  
• 
It was extremely helpful in learning about fracking 
and fracking sites when I was able to visualize what 
was talked about in the video from the lesson. 
• 
On the cognitive map I was a little unsure what to 
do with the second drawing tool? So...I didn't really 
mess with it. 
• 
I really like the blog section I thought it was cool 
but I wasn't entirely sure what to do. I think it's 
meant to be interactive with other students 
commenting on each others post, right? 
• 
The map with the shale plays was a little difficult to 
read 
• 
The layout might work better if you kind of 
separated the directions from the text? Or changed 
the lettering so it's easier to distinguish from the 
info? 
• 
I really liked that you guys came at this from 
several different angles. I REALLY think that 
reinforces the information. Especially with the 
drawing. 
• 
I also liked that you weren't exactly for or against 
fracking you gave different perspectives which 
allowed me (the student) to make up their own 
mind on where they stood. Unfortunately, I was 
already against fracking to begin with but had I 
never heard of if I'd be able to empathize with 
multiple sides. 
• 
I like that I had to take notes during the YouTube 
video and that it gave me the option to print. 
• 
I think it's important to have a basic understanding 
of the processes you're learning about so the intro 
video was very helpful and kind of set the 
foundation 
for 
the 
info 
that 
followed. 
 
The focus group students provided a different set of 
insights, including their overall impression of the module, 
including likes and dislikes, their perceptions on clarity and 
ease of use, engagement with the material and their general 
perceptions of how the interactive components helped them 
to meet the learning objectives. 
In general, the students comments related to likes and 
dislikes were distinctly separate.  The students liked the 
interactivity and the way the interactive components made 
them “think about the material” and “made it clearer.”  The 
dislikes mostly centered on technical issues such as wanting 
to break up the text and having to sign up for so many things 
(from free software).   
Per the survey, the students indicated they would like to 
have more interactive exercises in their courses with 88% of 
students in agreement.  The responses from both the survey 
and the interviews provided additional insight as to why the 
students would be interested:  
• 
Because learning with visual and hand on exercise 
is much better than just reading a textbook. 
• 
It helped get info across in a way a lecture might 
not- the added interactivity reinforced it by 
requiring me to put forth the lesson I had just 
learned.  
• 
More and more learning is occurring online, but 
there is only so much one can take from online 
classes.  Interactive courses like this may increase 
the level of participation.   
• 
…Interactive material like this is great, especially 
for learning complex material in a brief snapshot, 
which this achieves remarkably. 
• 
It takes learning to another level…drawing a map 
uses a different part of your brain and it forces you 
to kind of think critically about the subject matter. 
 
From the survey, students overwhelmingly believed that 
the interactive exercises helped them to learn more (Fig. 5).  
They also indicated that the interactive activities helped them 
to describe the process of fracking (Fig. 6) and the activities 
helped them to use the geographic tools to explore fracking 
from a geographic perspective (Fig. 7). They were less 
certain, but still positive that the interactive exercises helped 
them to apply the geographic perspectives to fracking (Fig. 
8).
142
International Journal on Advances in Life Sciences, vol 5 no 3 & 4, year 2013, http://www.iariajournals.org/life_sciences/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

 
Figure 5. Overall, the interactive exercises ______. 
 
 
 
Figure 7. The interactive exercises helped me to use the 
geographic tools (visual verbal, cognitive, mathematical, 
digital) 
in 
exploring 
fracking 
from 
a 
geographic 
perspective). 
 
 
 
 
Figure 6.  The interactive exercises helped me to describe the 
process of fracking. 
 
 
Figure 8. The interactive exercises helped me to apply the 
geographic perspectives (space, place, human-environment 
interaction) to fracking. 
The feedback from the focus group interview was similar.  
When asked “if the material had just been presented as text, 
would you have learned as much?”, the response was a 
resounding ‘no!’.  Opinions on the level of interactivity and 
how it helped students meet learning objectives was mixed.  
A few students responded that the lower intensity interactive 
really (video and animation) were instrumental in helping 
them learn the material.  
Additional comments regarding the ability of the 
interactive exercises to assist students to meet learning 
outcomes included:  
• 
Use of maps and aerial photography helped to 
contextualize this information. I can see where 
fracking occurs and what impacts on environment 
and population it has.   
• 
First it [the module and its interactive activities] 
explained what space and place meant, then gave an 
example of each.  Then the human-environment 
interaction.  
 
The students rated the clarity of the interactive exercises 
in categories of Clear, Somewhat Clear and Unclear (Fig. 9) 
with the visual and verbal components offering the most 
clarity.  For the mathematical and cognitive components, less 
than 50% of respondents were confident in how to utilize the 
interactive activities.  In reviewing the comments associated 
with these activities, we realized that technical issues and 
perception caused a problem for the students.  For instance, 
there was no mechanism to store the answers for the 
population growth calculation so when students had to 
answer questions, they had forgotten the specific numbers.  
Additionally, the calculator did not appear in one of the 
Internet browsers on campus, which limited the students’ 
ability to fully utilize the module.  While Scribblar™, the 
drawing software, allowed students to be working 
simultaneously on their cognitive maps, it did not allow for 
real-time edits.  Students also requested the ability to return 
to view and discuss each other’s maps.  This feedback helped 
us to hone in on the elements in the module that had the least 
clear instruction and/or software to use. 
 
88% 
12% 
Helped me learn more 
Made no difference to 
how I learned 
Were detrimental to 
my learning process 
47% 
47% 
6% 
Strongly Agree 
Agree 
Neither Agree nor 
disagree 
Disagree 
Strongly Disagree 
47% 
47% 
6% 
Strongly Agree 
Agree 
Neither Agree 
nor disagree 
Disagree 
Strongly 
Disagree 
53% 
23% 
24% 
Strongly Agree 
Agree 
Neither Agree nor 
disagree 
Disagree 
Strongly Disagree 
143
International Journal on Advances in Life Sciences, vol 5 no 3 & 4, year 2013, http://www.iariajournals.org/life_sciences/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

 
Figure 9. Student rating in terms of Clarity  
 
The students also rated the interactive activities in terms 
of ease of use (Fig. 10).  The mathematical and cognitive 
maps exercises rated more difficult than the others.   
 
The scribbler page was time consuming and 
a bit frustrating, but not bad.  For the math 
exercise, it would have been much easier had 
the chart and the calculator been on the same 
section of the page so that the user doesn’t 
have 
to 
scroll 
up 
and 
down 
while 
remembering the numbers.   
 
The response to the math difficulty appeared to be 
technical while the cognitive interactive exercise may have 
been a combination of both technical frustration and 
conceptual reach as it is often a difficult concept to grasp.   
Furthermore, students had the most trouble with the free 
software program used in this section. We recognize this is 
the case and have modified that section of the module to 
improve clarity in both instruction and ease of use by 
changing the software used to capture the cognitive maps 
drawn by the students.  We also provided a textbox for 
students to type in the results of their calculations for easy 
reference when they are answering the follow-up questions. 
 
 
Figure 10. Students’ rating of interactive activities in terms 
of Ease of Use. 
 
The students’ reactions to the multi-media interactive 
components and how it inspired creativity, was consistent 
with the notion that students have a higher satisfaction and 
engagement rate using these technologies [9, 10].   The 
students repeatedly referenced the animation, the news video 
and the cognitive map exercise.   The animation and video 
seemed to be most helpful in describing the process and 
providing additional background, while the cognitive map 
drawing exercise was seen as helpful because it inspired 
creativity.   The students also found value in the quizzes and 
calculator based exercise.   
Regarding the Google Earth™ interactive, the students 
discussed how they explored and looked at other areas 
outside of the ones that they were directed to study to see 
how fracking was evident on the landscape.  For the GIS-
based exercise, the students also commented on some other 
data that could be incorporated to give additional insight in 
the impact of fracking to the region.  These types of 
comments indicate that the interactive module did create a 
constructivist e-learning environment where students were 
motivated to explore the subject matter further on their own 
[8].   
One comment we found particularly insightful was the 
recognition that the various components and levels of 
intensity of the interactives reinforced learning by touching 
on various learning styles.   
 
I think it hit on those components more so 
than you could in a normal classroom.  My 
whole life teachers have told us there are 
various learning styles.  Some people are 
audio, others are visual and some are 
Kinesthetic learners.  This module kind of hit 
on all three of those so just in case you 
cannot keep your focus to listen to a seven 
minute video, you have a map to draw to 
reinforce the concepts you may have not 
totally gotten from just listening.   
 
Further evidence of the students’ engagement was how 
they provided us with multiple suggestions on additions to 
the material and interactive content.  The students were 
engaged in the material to the degree that they began 
thinking of other ways to present and expand on the content 
covered in the module.    
Finally, in both the focus group survey and the discussion, 
students provided excellent technical recommendations to 
improve the modules.  Notation of these specific items were 
made and we have edited the module to integrate these 
suggestions to the best of our ability.   
VI. 
LIMITATIONS AND RECOMMENDATIONS 
We recognize that the development of the module and the 
focus group testing had limitations.  First, we were 
dependent upon the use of free software in developing the 
interactive modules. This was cumbersome for the students 
and the instructors as we had to test out a variety of software 
packages to find the “right” match. Since the modules in the 
textbook will have many of the same interactive activities, 
once we have resolved the software issue and/or have 
0% 
20% 
40% 
60% 
80% 
100% 
Least Clear 
Somewhat Clear 
Clear 
0% 
20% 
40% 
60% 
80% 
100% 
Difficult 
Moderate 
Easy 
144
International Journal on Advances in Life Sciences, vol 5 no 3 & 4, year 2013, http://www.iariajournals.org/life_sciences/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

software developed specifically for this purpose, this 
limitation should be resolved.  
Second, the students were asked to complete the module 
as part of a combined and timed class session.  Because of 
this, student feedback to each other in the cognitive 
mapping interactive and the ArcGIS™ blogging exercise 
was limited and the full scope of the interactivity between 
students was virtually non-existent.  We recommend 
spacing the focus group participation over a series of days 
so that the interactivity can be fully explored.    
VII. CONCLUSION  
The iterative process of gathering formative assessment 
feedback was essential to the improvement of the module.  
We suspect that often instructors assume that as long as the 
material is presented, the students should be able to navigate 
and succeed given the intent.  A severe limitation of online 
content is that instructors do not gain immediate feedback 
from students unless there is a problem.  And then, it is 
often too late for the instructors to efficiently address the 
problem without slowing the pace of the class down and 
confusing and frustrating the students.  The idea of 
obtaining feedback prior to implementation is not 
innovative or unique; instead it is the exception rather than 
the rule, e.g., [14][2].  
Through the iterative review process, we gained 
knowledge through the various audiences that would not 
have been attained through simple implementation of module 
in class.  The IDs provided multiple levels of feedback. In 
the initial brainstorming session, they offered ideas on 
technologies and software that might be useful for the 
interactive components.  Utilizing the IDs as a resource was 
key as it opened up possibilities we had not considered and 
their expertise in this arena offered insight into what works 
for students. They also tested the technologies for us.  From 
an end-user perspective, the pilot student group provided us 
with feedback for functionality and learning. The focus 
group enabled us to see how a broader sample of multiple 
learners experienced the modules [18]. 
 
Educational delivery models for college courses have 
changed. 
Contemporary 
educational 
delivery 
models 
include online and distance education; however, there has 
been a gap in the assessment of these learning technologies 
of their impact on student learning [22]. In the development 
of learning modules for students in online courses there is 
room for an iterative design process whereby advice from 
IDs and feedback from students can and should be taken 
into consideration. 
Ongoing assessment of a course can allow faculty to 
systematically incorporate feedback from all involved in the 
teaching and learning process, adding to, replacing, 
correcting, and improving an ever-growing body of learning 
materials and best practices. In each redesign of a module 
or, on a large scale, course shifts can be made towards 
making it more active and learner-centered [23]. 
Educational 
research 
of 
this 
nature 
tackles 
the 
fundamental question of how to optimize instructional 
design to maximize learning opportunities and achievement 
in online and distance learning environments [24]. Thus, by 
enlisting instructional designers and students in curriculum 
development, we expect to improve the module content and 
interactive activities by directing revision based on their 
feedback. While the overall assessment by the IDs, pilot 
testers and focus group students indicated that interactive 
activities in the module were perceived favorably by the 
students and likely required only tweaking for clarity and 
ease of use, an ongoing consideration should be whether or 
not the interactive activities are helpful in delivering content 
and developing desired skill sets.  As such, research on 
perceived and actual learning outcomes should be 
conducted.   
 
ACKNOWLEDGMENT 
We wish to thank Tommy and Beth Holder for their 
support through the KSU Foundation Holder Award, the 
Center for Teaching and Learning at Kennesaw State 
University for providing technical assistance, and the 
College of Humanities and Social Sciences Office of 
Distance Education for research and travel funding.  
 
REFERENCES 
[1] 
V. Slinger-Friedman and L. M. Patterson, “Student Learning and 
Student Perceptions of Learning from Interactive Modules,” Proc. 
Fifth International Conference on Mobile, Hybrid, and On-line 
Learning (eL&mL 2013). Held 23 February – 1 March, 2013, pp. 1-5.  
http://www.thinkmind.org/index.php?view=article&articleid=elml_20
13_1_10_50018. Last accessed 12/9/2013. 
[2] 
R. Donnelly and M. Fitzmaurice, Designing Modules for Learning, 
Dublin Institute of Technology, Ireland 2005, n.p. Available from 
http://www.aishe.org/readings/2005-1/donnelly-fitzmaurice-
Designing_Modules_for_Learning.html. Last accessed 12/9/2013. 
[3] 
B.L. Black, H. Heatwole, and H. Meeks, Using multimedia in 
interactive learning objects to meet emerging academic challenges in 
Learning Objects: Theory, Praxis, Issues, and Trends, Koohang, A. & 
Harman, K. Eds. Santa Rosa, California: Informing Science Press, 
2007, pp. 209 – 257. 
[4] 
M. Maag, “The Effectiveness of an interactive multimedia learning 
tool on nursing students’ math knowledge and self-efficacy,” CIN: 
Computers, Informatics, Nursing, vol. 22, no. 1, January-February 
2004, pp. 26-33. 
[5] 
R.E. Mayer, “Cognitive theory and the design of multimedia 
instruction: An example of the two-way street between cognition and 
instruction,” New Directions for Teaching and Learning, vol. 2002, 
no. 89, 2002, pp. 55-71. 
[6] 
R. Oliver, Learning objects: supporting flexible delivery of flexible 
learning in Meeting at the crossroads: Proceedings of ASCILITE 
2001 G. Kennedy, M. Keppell, C. McNaught & T. Petrovic, Eds. , pp 
453-460. Melbourne: The University of Melbourne. 
[7] 
R. Frye, G.R. McKinney, and J.E. Trimble, Tools & Techniques for 
Course Improvement: Handbook for Course Review and Assessment 
of 
Student 
Learning 
2007. 
Available 
at 
http://www.wwu.edu/depts/vpue/assessment/documents/course_hand
book.pdf. Last accessed 12/9/2013. 
[8] 
R. Martens, T. Bastiaens, and P.A. Kirschner, “New learning design 
in distance education: The impact on student perception and 
motivation,” Distance Education, vol. 28, no. 1, May 2007, pp. 81-93. 
145
International Journal on Advances in Life Sciences, vol 5 no 3 & 4, year 2013, http://www.iariajournals.org/life_sciences/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

[9] 
M. Murray, J. Pérez, D. Geist, A. Hedrick, “Student interaction with 
online course content: Build it and they might come,” Journal of 
Information Technology Education: Research, vol. 11, 2012, pp. 125 
– 140. 
[10] D. Jensen, B. Self, D. Rhymer, J.Wood, and M. Bowe, “A rocky 
journey toward effective assessment of visualization modules for 
learning enhancement in engineering mechanics,” Educational 
Technology & Society vol. 5, no. 3, 2002, pp. 150-162.  
[11] P.T. Knight, Being a Teacher in Higher Education. Maidenhead, UK: 
Society for Research in Higher Education and the Open University 
Press, 2002. 
[12] J.T.E. Richardson, “Instruments for obtaining student feedback: a 
review of the literature,” Assessment & Evaluation in Higher 
Education 30(4), pp. 387-415. 
[13] D. Kember, D.Y.P. Leung, and K.P. Kwan, “Does the use of student 
feedback questionnaires improve the overall quality of teaching?” 
Assessment and Evaluation in Higher Education, 27, 2002, pp. 411–
425. 
[14] E.P. Skye, L.A. Wimsatt, T.A. Master-Hunter, and A.B. Locke., 
“Developing online learning modules in a family medicine 
residency,” Fam Med vol. 43, no. 3, 2011, pp. 185-192. 
[15] J.J.G. van Merrienboer and R. Martens, “Computer-based tools for 
instructional design,” Educational Technology, Research and 
Development, vol. 50, 2002, pp. 5-9. 
[16] G. Greenberg, “Conceptions of quality in course design for web-
supported education,” Proc. of the 26th Annual Conference on 
Distance Teaching & Learning. Aug. 2010, Madison, WI. Available 
from http://www.uwex.edu/disted/conference/Resource_ 
library/proceedings/28667_10.pdf. Last accessed 12/9/2013.   
[17] D.G. Kingston, W.J. Eastwood, P.I. Jones, R. Johnson, S. Marshall, 
and D.M. Marshall, “Experiences of using mobile technologies and 
virtual fieldtrips in Physical Geography: implications for hydrology 
education,” Hydrology and Earth System Sciences, vol. 10, May 
2012, pp. 1281-1286. 
[18] L.C. Lederman, “Assessing the educational effectiveness:  The focus 
group interview as a technique for data collection,” Communication 
Education, vol. 38, 1990, pp. 117-127. 
[19] P.S. Kidd and M. B. Parshall, “Getting the focus and the group: 
Enhancing analytical rigor in focus group research,” Qualitative 
Health Research, vol. 10, May 2000, pp. 293-308.   
[20] L. Janis, Victims of Groupthink: a psychological study of foreign-
policy decisions and fiascoes. Boston: Houghton Mifflin, 1972. 
[21] M. A. Carey, “Comment:  Concerns in the analysis of focus group 
Data,” Qualitative Health Research, vol. 5, no. 4, Nov. 1995, pp. 487-
495. 
[22] J. O’Malley, “Students perceptions of distance learning, online 
learning and the traditional classroom,” Online Journal of Distance 
Learning Administration, vol. 2, no. 4, Winter 1999, pp. 1-9.  
[23] National Center for Academic Transformation, Five principles of 
successful course redesign, 2005, pp. 1 – 10. Available from 
http://www.thencat.org/R2R/R2R%20PDFs/SuccCrsRed.pdf. 
Last 
accessed 12/9/2013. 
[24] S.D. Johnson, S.R. Aragon, N. Shaik, and N. Palma-Rivas, 
“Comparative analysis of learner satisfaction and learning outcomes 
in online and face-to-face learning environments,” Journal of 
Interactive Learning Research, vol. 11, no. 1, 2000, pp. 29-49. 
 
 
 
 
 
 
 
 
 
 
 
146
International Journal on Advances in Life Sciences, vol 5 no 3 & 4, year 2013, http://www.iariajournals.org/life_sciences/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

