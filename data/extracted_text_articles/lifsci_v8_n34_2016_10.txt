277
International Journal on Advances in Life Sciences, vol 8 no 3 & 4, year 2016, http://www.iariajournals.org/life_sciences/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
KINECT-Based Auscultation Practice System 
 
Yoshitoshi Murata, Kazuhiro Yoshida 
Faculty of Software and Information Science 
Iwate Prefectural University 
Takizawa, Japan 
e-Mail: y-murata@iwate-pu.ac.jp, kyoshida@ipu-
office.iwate-pu.ac.jp 
Natsuko Miura, Yoshihito Endo 
Faculty of Nursing 
Iwate Prefectural University 
Takizawa, Japan 
e-Mail: natsuko@iwate-pu.ac.jp, y-endo@iwate-pu.ac.jp
 
Abstract— Students in medical and nursing schools have to 
practice auscultation. Students usually learn disease sounds, 
correct points and order for locating a stethoscope on a body in 
the practice. Humanoid simulators have been widely 
introduced to practice auscultation. They are effective to learn 
disease sounds. However, most humanoids cannot detect 
whether or not a stethoscope is located on a body and which 
part of body a stethoscope is placed on. And also, since they 
are too expensive, the number of them is not enough for the 
number of students in a class. In this paper, we propose a low-
cost and high performance system for the practice of 
auscultation. In this system, students themselves play the role 
of a patient, instead of a humanoid, and stethoscope locations 
on the body are measured with KINECT. Also, appropriate 
disease sounds including normal ones can be assigned at some 
points on the upper body. Practicing students hear such 
disease sounds, synchronized with the movement of breathing, 
through earphones when a stethoscope is placed on the 
assigned points. Movements of the upper body from breathing 
can also be detected by KINECT. Experimental results with a 
prototype showed that our system could perfectly detect 
stethoscope locations on a body, except for a few lower points, 
and it could also detect respiratory changes on the front body. 
The results of a questionnaire for nursing students and 
practicing nurses showed that our proposed system was useful 
for them to learn auscultation. 
Keywords-simulator; auscultation; nursing; KINECT. 
I. 
 INTRODUCTION 
Generally, practicing auscultation is a required subject 
for students in medical and nursing schools. Students usually 
learn disease sounds, correct points and order for locating a 
stethoscope on the body in the practice. We proposed a new 
auscultation practice system to learn auscultation techniques 
effectively [1]. Humanoid-type simulators [2][3][4][5] have 
been widely introduced into medical and nursing schools, 
and several reports have found that such simulators improve 
auscultation skills [6]. These humanoid simulators are 
effective to learn disease sounds; and they enable 
determining correct stethoscope locations by marking these 
points on a mannequin. However, it is impossible to detect 
whether or not a stethoscope is actually placed correctly on a 
mannequin. Moreover, correct locations vary among patients 
according to body size. Cardionics provides a hybrid 
simulator in which a student plays the role of a patient 
instead of a mannequin to solve such problems [7]. In this 
hybrid simulator, patches are attached on a body to identify 
correct attachment points. 
In either type of simulators, students have difficulty 
practicing auscultation by themselves. Teachers have to 
support students during practice. Moreover, both types of 
simulators cannot indicate the learning progress for each 
student using scores. 
In our simulator, students themselves are the practice 
subjects instead of a humanoid model, and the location of a 
stethoscope can be detected with KINECT, which is a line of 
motion sensing input devices made by Microsoft [8]. The 
correct locations are normalized with respect to the positions 
of both shoulder joints and both hip joints for each student 
playing the role of patient. Therefore, our proposed simulator 
can both show correct locations on a body and can detect 
whether or not a stethoscope is placed on correct points 
without patches regardless of the change in body size.  
In addition, most existing simulators cannot simulate the 
timing of breathing or the synchronized forward and 
backward movements of the upper body. However, our 
simulator can detect these forward and backward movements 
of the front body and provide exhalation and inhalation 
sounds synchronized with those movements.  
We have developed a prototype system and evaluated it 
experimentally. The results showed that our system could 
perfectly detect stethoscope placement on a body at seven of 
ten points. Our system could not always detect a stethoscope 
because three lower points were easily shadowed from 
KINECT by the T-shirt worn by a student acting as a patient. 
Also, our system could detect changes of the front body in 
breathing. 
The aforementioned mean that students could learn 
auscultation practice by themselves using our system and 
that our system could indicate the learning progress for each 
student through their increases in score during the KINECT 
simulation. 
Moreover, we found that all students could set up our 
system by themselves. The results of a questionnaire for 
nursing students and practicing nurses showed that our 
proposed system was useful for them to learn auscultation. 
After introducing related works in Section II, we describe 
the concepts and features of our system in Section III. The 
key technologies of our simulator and the evaluation results 
are described in Section IV. An application for a teacher to 
lecture about auscultation was developed. It is introduced in 
Section V. The questionnaires for students who set up and 

278
International Journal on Advances in Life Sciences, vol 8 no 3 & 4, year 2016, http://www.iariajournals.org/life_sciences/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
used our system and for practicing nurses are shown in 
Section VI. The key points are summarized in Section VII. 
II. 
RELATED WORKS 
Many kinds of patient simulators have been developed 
and provided as medical and nursing training tools 
[2][3][4][5][7][9]. Because we propose a new type of 
simulator for practicing auscultation, we first discuss existing 
auscultation simulators, which are divided into three groups: 
the humanoid model type, the virtual reality type and the 
hybrid type. 
A. Humanoid model type 
Kyoto Kagaku Co., Ltd. provides the Lung Sound 
Auscultation Trainer (LSAT) [2], shown in Figure 1, for 
respiratory auscultation. There are several small speakers 
inside a mannequin. Disease sounds are recorded from real 
patients. This simulator also works for cardiac auscultation 
by changing from respiratory sounds to cardiac sounds. 
Sakamoto Model Corporation provides the Sakamoto 
auscultation simulator [4]. This simulator also works for both 
respiratory and cardiac auscultation. Sakamoto provides a 
transparent cover for this simulator, as shown in Figure 2, to 
illustrate correct stethoscope locations.  
 
 
Figure 1. Lung Sound Auscultation Trainer (LSAT) 
by Kyoko Kagaku 
 
 
 
Figure 2. Transparent chest cover, by Sakamoto Model, to illustrate 
correct stethoscope locations 
 
Although the above two simulators are focused on the 
upper body, they simulate disease sounds, not the motion of 
the upper body. And also, they cannot detect where a 
stethoscope is placed on. Each price is too expensive to buy a 
few of them. 
On the other hand, the SimMan® 3G [4] by Laerdal is an 
advanced 
patient 
simulator 
that 
can 
simulate 
the 
characteristics of a real patient, including the blood pressure, 
heart beat, chest motion, and so on. It is too expensive, 
however, for a general nursing school to buy. 
B. Virtual reality type 
Zadow, et al. developed the SimMed system for medical 
education [9]. By using an interactive multi-touch tabletop to 
display a simulated patient, as shown in Figure 3, they have 
created an immersive environment that supports a large 
variety of learning scenarios. The simulated patient can show 
skin changes and be animated to show realistic bodily and 
facial movements. By its nature, the setup allows scenarios 
to be repeated easily and to be changed and configured 
dynamically. 
SimMed is substantially lower in cost than a full-scale 
humanoid simulator. It has many functions, however, and is 
still too expensive for most nursing schools. Moreover, while 
students can touch the virtual patient on a display, they 
cannot physically feel the motion of the virtual patient. 
 
 
Figure 3. The SimMed system 
 
C. Hybrid type 
Cardionics 
provides 
the 
SimScope 
WiFi 
Hybrid 
Simulator™ [8]. As shown in Figure 4, a specific 
stethoscope has a Bluetooth earphone mounted to hear 
disease sounds from a PC, and patches are attached on a 
body to decide the locations for auscultation practices. 
Therefore, a person who attaches patches on a body has to 
know the correct locations for each practice. Students have 
difficulty practicing auscultation by themselves. 
 
 
 
Figure 4. SimScope WiFi™ The Hybrid SimulatorTM by Cardionics 
 
III. 
SYSTEM CONCEPT  
As a matter of course, the introduction cost is adequate 
for the number of students in a nursing school. Among the 

279
International Journal on Advances in Life Sciences, vol 8 no 3 & 4, year 2016, http://www.iariajournals.org/life_sciences/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
nursing skills that students have to learn are the recognition 
of different sounds between different kinds of diseases and 
the knowledge about placing correct points and order for 
locating a stethoscope on a body. In the case of respiratory 
auscultation, students have to listen to respiratory sounds for 
more than one cycle. Moreover, the learning progress for 
every student is consolidated on a cloud server. Therefore, an 
auscultation practice system requires the following issues: 
· 
Low system cost. 
· 
Simulating real disease sounds at different points on 
the body. 
· 
Showing correct points for locating a stethoscope 
on an operation display. 
· 
Judging whether or not a stethoscope is located on 
shown points. 
· 
Judging whether or not a stethoscope is fixed on a 
body for more than one respiratory cycle. 
· 
Logs that students practice are stored on a cloud 
server. 
Our practice system comprises a cloud server, terminal 
equipment, and a PC for a teacher. The terminal equipment 
and PC for a teacher are connected to the cloud server 
through the Internet as shown in Figure 5. With the terminal 
equipment in our system, shown in Figure 6, the students 
themselves can act as patients instead of having mannequins 
act as patients—much like in Cardionics’s hybrid 
simulator—to achieve low cost. The stethoscope locations 
and forward and backward movement of a body during 
breathing are measured with KINECT. The students can hear 
disease sounds generated by a PC through earphones. The 
sound volume for each point is different for locating a 
stethoscope, as with a real patient. Therefore, a specific 
stethoscope and patches are not needed. These also enable 
low system cost. 
Log data for a student practicing with terminal equipment 
are sent to the cloud server and stored. Such stored data are 
managed in the cloud server. A teacher can access the cloud 
server and can look at a list of learning progress reports for 
each student. 
 
 
 
Figure 5. System configuration of our proposed system 
 
 
 
Figure 6. Terminal equipment in our proposed system 
IV. 
DETECTION TECHNOLOGIES 
The following capabilities are necessary to implement 
our auscultation practice tool: 
· 
Tracing a stethoscope. 
· 
Detecting a stethoscope’s location. 
· 
Detecting whether or not a stethoscope is placed 
properly on a body. 
· 
Automatically adjusting correct points for locating a 
stethoscope on a body according to body size. 
Also, detecting the inhalation and expiration during 
respiration is desired. 
Because KINECT automatically generates position data 
for a stethoscope while it is traced, we examined each of 
these four issues except the second one. 
A. Tracing  a stethoscope 
Three candidate tracing methods are shape tracing, color 
tracing and the AR marker. Since a stethoscope is held by 
hand, the shape of a stethoscope as viewed through a video 
camera changes over time. And, size of AR marker on a 
stethoscope would be too small to detect it with KINECT.  
Therefore, we chose the color tracing, rather than shape 
tracing or AR marker. The process of color tracing is shown 
schematically in Figure 7. Video data from the BGR (Blue, 
Green, and Red) 32 output of KINECT is converted to HSV 
(Hue, Saturation, and Value) color data. First, the traced 
object, i.e., a stethoscope, is pointed, and its hue histogram is 
generated and stored. Then, masking data are generated for 
each frame from the HSV data, the minimum saturation, and 
the maximum and minimum brightness.  
The video data for practice is also converted to HSV data, 
and target areas are separated with the masking data. Noise, 
including the hue data of the target area, is reduced by a 
median filter. The output data from the median filter is then 
traced by the cvCamShift function of Open CV [10]. Since 
cvCamShift sometimes outputs incorrect data, the hue 
histogram for the output data is repeatedly compared with 
the pre-stored hue histogram. When these histograms are 
equivalent, the color tracing is successful. 
We used an experiment to select a target color from 
among seven choices: “red”, “green”, “light blue”, “yellow”, 
“yellow-green”, “pink”, and “orange”. We used KINECT v1 
and examined whether it could detect only a target color. We 
show the resulting data for the top three colors in Figure 8. 
“Yellow-green” had the best performance, with no portions 

280
International Journal on Advances in Life Sciences, vol 8 no 3 & 4, year 2016, http://www.iariajournals.org/life_sciences/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
having the target color except the target. “Light blue” also 
performed well, but since the color of a stethoscope tube is 
“light blue”, that was detected. In the case of “yellow”, dots 
of the same color appeared in the bottom-left region of the 
image. The other colors exhibited more dots of the same 
color or had a smaller target size. Hence, we chose “yellow-
green” as the target color for our experiments. 
The aforementioned processing enables obtaining the x0 
and y0 axis of a stethoscope on a video image window of 
KINECT. The size of the video image window and the size 
of the 3D window in KINECT are the same. Therefore, the 
length z0 between KINECT and the location (x0, y0) can be 
obtained to assign (x0, y0) to the 3D window as shown in 
Figure 9. This means that the location (x0, y0, z0) can always 
be obtained. 
 
 
 
 
Figure 7. Process of color tracing 
 
 
 
 
Figure 8. Experiment for deciding a target color 
 
The KINECT camera should be tilted horizontally 
because the z0 is the shortest length between the flat surface 
of a KINECT camera and the flat surface going through the 
measuring point parallel to the surface of the KINECT 
camera. Also, we recommend the KINECT camera be 
positioned at about shoulder height. 
 
 
 
Figure 9. Relationship between video image window and 3D window in 
KINECT 
 
B. Detecting whether a stethoscope is placed on a body  
At this time, we think determining whether or not a 
stethoscope is placed exactly on a body is unnecessary, so 
we do not use any sensors on the stethoscope. Instead, we 
estimate the stethoscope is located on a body, where a 
stethoscope is placed and fixed within distance S from a 
body surface for T seconds, as shown in Figure 10. In this 
figure, Lbs is the length between the shoulder and a KINECT, 
and Ld is the threshold length to determine a stethoscope 
placed on the body. We used S = 10 cm, and T = 0.3 second 
to achieve balance between certainty and fast recognition. If 
stricter detection were required, we could change these 
parameters. Before measuring length Lst between a 
stethoscope and a KINECT, we determine whether or not the 
stethoscope is within the outline of a body by using a pre-
installed program on a KINECT to get an outline. 

281
International Journal on Advances in Life Sciences, vol 8 no 3 & 4, year 2016, http://www.iariajournals.org/life_sciences/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
 
Figure 10. Scheme to estimate whether a stethoscope is placed on the body 
 
We experimentally examined whether or not this method 
was useful. We started to develop a prototype system by the 
KINECT v1. However, as described in the next sub-section, 
the KINECT v1 could not detect the respiration well. 
Therefore, we redevelop the system by the KINECT v2. The 
data for the front body were measured by the KINECT v1, 
and the data for the back body were measure by the KINECT 
v2. A student acting as a patient wore a white T-shirt with 
dots marking correct stethoscope locations; a student acting 
as a nurse placed a stethoscope on these marked dots. We 
marked 10 dots on a front of T-shirt and 12 dots on the back 
as shown in Figure 11. These points are decided on the 
auscultation practice manual. The participants were five male 
students and five female students for the front body, and 
three male students and two female students for the back 
body. Since we think that there is not difference between 
male and female for the back body, we reduce the number of 
patients. The experimental results are listed in Table I. 
The system sometimes missed when the stethoscope was 
placed at certain points (points 8, 9, 10) of the front body. As 
seen from Figure 6, a stethoscope placed on one of these 
points would sometimes be shadowed from KINECT, 
especially for women, because these points were below the 
breasts. The other hand, there are not big sagging on a T-
shirt in the back body as shown in Figure 12. Therefore, the 
system detected a stethoscope was placed on a T-shirt 
perfectly for every participant. 
These are possible solutions to improve detection rate for 
the front body: 
- Switch from a T-shirt to clothing with a tighter fit. 
- Attach some dimensional markers to the stethoscope. 
 
   
 
                       (a) Front view                                 (b) Back view 
Figure 11. T-shirt with dots marking stethoscope locations 
 
Figure 12. Scene of experiment for the back body 
 
TABLE I. COUNT OF DETECTING A STETHOSCOPE PLACED ON A BODY 
(1) Front body 
 
* Measured by KINECT v1 
 
(2) 
Back body 
 
* Measured by KINECT v2 
 
C. Detecting the inhalation and expiration in respiration  
Burba et al. used chest motion to detect breathing [11]. 
They found that detecting the motion of the front body was 
possible, but they did not describe detecting respiration from 
the back body. However, practicing auscultation makes it 
desirable to detect the inhalation and expiration from the 
back body if feasible.  
In our previous study, we found a KINECT v1 could 
detect the inhalation and expiration from the front body. 
However, mistakes sometimes occurred in detecting the 
inhalation and expiration for the front body, and the output 
data for some measurement points had extraordinary values. 
In this section, first, we describe the difference between 
KINECT v1 and v2 in detecting respiration for the front 
body. Then, detecting respiration for the back body is 
described. 
 
1)  Difference between KINECT v1 and v2 in detecting 
respiration for the front body 
Since there are two main types of breathing: chest 
respiration and abdominal respiration, we select six points 

282
International Journal on Advances in Life Sciences, vol 8 no 3 & 4, year 2016, http://www.iariajournals.org/life_sciences/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
for measuring movement of the chest and abdomen in this 
experiment, as shown in Figure 13. Since the stethoscope 
location is not always fixed, we do not consider it in this 
experiment. The upper three measuring points are the inner 
junctions of five vertical lines equally dividing the space 
between both shoulders into four regions and a horizontal 
line halfway between the height of the center of the spine 
and the average height of both shoulders. The lower three 
measuring points are the inner junctions of the above-
mentioned vertical lines and a horizontal line through the hip 
center. We adopt the same direction of more than 4 points as 
the resultant direction.  
 
 
Figure 13. Measuring points for breathing motion 
 
We designed our system to detect the changes from 
expiration to inhalation and vice versa as quickly as possible. 
Since there were small but rapid changes in the output data, 
we used a moving average of 30 samples, with a sampling 
period of 10 ms. We then have specified that when the 
sampling data continued to increase 3 times, the breathing 
mode was expiration; and when the sampling data continued 
to decrease 3 times, the breathing mode was inhalation.  
We measured the number of inhalation and expiration in 
breathing and their periods with our proposed system, and 
compared the results with other data obtained by participants 
keying the up and down arrow keys on a keyboard.  
The experimental results were shown in Table II. We 
measured them for two KINECTs that are K-1 and K-2. Both 
of them were the same model. At first, we measured for 
many participants with K-1. Our system counted more and 
more breaths than did keying for most participants. Data 
shown in Table II were some of them. In addition, the output 
data for some measuring points had extraordinary values, as 
shown in Figure 14 (2). We changed K-1 to K-2 to clear the 
reason of this problem. Our system with K-2 could count 
more accurately than that with K-1. However, our system 
counted fewer breaths than did keying for participant D. And, 
the extraordinary values were sometimes measured with K-2, 
too. As shown in Figure 14 (1), our system detected changes 
in breathing with a delay of about 1 sec. relative to keying 
when breathings were correctly detected. The measured 
delay is bigger than we expected. 
TABLE II. NUMBER OF BREATHS IN KINECT V1 
Inhalation
Expiration
Inhalation
Expiration
A 
12
12
16
16
B
10
10
39
39
C
14
14
15
15
C
11
11
11
11
D
10
10
7
7
E
15
15
15
15
K-2
Participant
Keying
Proposed system
K-1
 
 
We think this false detection is derived from the 
aforementioned extraordinary output data. Also, we estimate 
the extraordinary output would be derived from a sagging T-
shirt and the depth measuring algorithm, which is the 
random pattern scheme [8]. The random pattern algorithm 
would be affected from fluctuations of sagging T-shirts as 
movement from breathing. 
We re-programmed the proposed system to KINECT v2; 
its depth measuring algorithm is the time of flight (TOF) [8]. 
We experimentally examined the false-detection rate and 
detection delay for the re-programmed system. The 
experimental results were shown in Table III and Table IV. 
The participants were eight male students and four female 
students. They did not wear our prepared small white T-shirt, 
but their own clothes. One false detection occurred in about 
ten breaths for three participants, even though they wore a 
relatively loose shirt or sweater. 
 
 
Figure 14. (1) Respiratory period, and (2) change in distance between 
KINECT v1 and measuring points 
 

283
International Journal on Advances in Life Sciences, vol 8 no 3 & 4, year 2016, http://www.iariajournals.org/life_sciences/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Table III. Number of Breaths and detection delay in KINECT v2 
 
 
TABLE IV. AVERAGE NUMBER OF BREATHS AND DETECTION DELAY IN 
TABLE III 
 
 
Table IV showed the average of the false detection rate 
and detection delay. We think the false detection rate was 
low enough for the auscultation practicing system. However, 
some measured detection delays were longer than 1 second. 
We have to improve the detection scheme for inhalation and 
expiration so that shorter detection delays occur and a high 
detection rate is possible. The other hand, since the 
proposed system can start to measure movement of upper 
body just after recognized a student playing a patient, it is 
possible to avoid this problem by detecting respiration cycle 
before a stethoscope placing on a body.  
 
 
 
Figure 15. (1) Respiratory period, and (2) change in distance between 
KINECT v2 and measuring points 
As for the high detection rate, KINECT v2 did not find 
any extraordinary values for every participant, as shown in 
Figure 15.  Because Figure 15 (2) is an example of 
abdominal respiration, the measured values for every 
abdominal measurement point varied widely and clearly. No 
extraordinary values are evident in this figure. 
We could not clarify the reason for the extraordinary 
outputs found by KINECT v1 from these experiments. 
However, we will develop our system using KINECT v2 in 
the future. 
 
2) Detecting respiration for the back body 
We tried to utilize the previously described program to 
detect the inhalation and expiration during respiration for 
the back body. We noticed that the detection results for the 
back body were opposite to those for the front body in pre-
experiment. When a participant breathed in, our system 
recognized it as expiration. Also, we noticed that the motion 
of the back body was different from that of the front body. 
Hence, we measured the motions of both shoulders in 
addition to the previously described six points for 13 
participants. Five of them were female.   
There were many participants who beyond our 
expectation. The measured data values for only two of the 
thirteen participants varied clearly and widely enough to 
detect the inhalation and expiration in both the chest and 
abdomen. Also, we could not get enough data to detect them 
for 
other 
participants. 
Most 
clearly 
changed 
data 
corresponded to the breathing shown in Figure 16, and less 
than optimal data are shown in Figure 17 and Figure 18.   
We noted the following issues from these measured 
data: 
- 
All measured data on the back chest changed to less 
than the data on the abdomen. 
- 
In the case of abdominal respiration, the shoulders 
almost never move up and down. 
Measurement points on which data clearly changed 
depended on the person. For example, for both Participant A 
in Figure 16 and B in Figure 17, the shoulders moved up 
and down for chest respiration. The other hand, the 
shoulders of Participant C in Figure 18 did not move. 
However, we do not think that this issue is serious 
problem. The reason we would like to detect motion of 
upper body derived from respiration is that we think a 
student playing a nurse feels unnatural when sounds of 
expiration or inhalation do not synchronize with movement 
of the upper body. When there is not any movement of a 
back body, there is not any synchronization between 
respiration sounds and motion of back body. Therefore, a 
student playing a nurse would not care for the replay timing 
of respiration sounds, when a back body did not move. In 
prototype system, when the system detects motion of back 
body, respiration sounds are replayed to synchronize with 
movement of a back body: when the system cannot detect 
them, respiration sounds are automatically replayed with 
respiration cycle measured for a front body. 

284
International Journal on Advances in Life Sciences, vol 8 no 3 & 4, year 2016, http://www.iariajournals.org/life_sciences/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
 
(1) Chest respiration (Back body) 
 
(2) Chest respiration (Shoulder) 
 
(3) Abdominal respiration (Back body) 
 
(4) Abdominal respiration (Shoulder) 
 
Figure 16. Example of clearly motion in the back body 
Participant A, Sex: Male, Cloth: Prepared T-shirt 
 
 
(1) Chest respiration (Back body) 
 
(2) Chest respiration (Shoulder) 
 
(3) Abdominal respiration (Back body) 
 
(4) Abdominal respiration (Shoulder) 
Figure 17. Example of unclearly motion in the back body (1) 
Participant B, Sex: Female, Cloth: Her own loose shirt 
 
(1) Chest respiration (Back body) 
 
(2) Chest respiration (shoulder) 

285
International Journal on Advances in Life Sciences, vol 8 no 3 & 4, year 2016, http://www.iariajournals.org/life_sciences/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
 
(3) Abdominal respiration (Back body) 
 
(4) Abdominal respiration (Shoulder) 
 
Figure 18. Example of unclearly motion in the back body (2) 
Participant C, Sex: Male, Cloth: Prepared T- shirt 
 
D. Automatically adjusting according to body size 
The correct points for placing a stethoscope depend on 
the size of the body; they differ a little between men and 
women. Also, their X and Y values vary according to the 
distance between a student playing a patient and a KINECT. 
Therefore, we estimate correct positions for placing a 
stethoscope with respect to the positions of both shoulder 
joints and both hip joints. 
Two sets of correct position data and the positions of 
both shoulders and both hips are measured and stored as 
standard man and woman data. Since the origin of the output 
of KINECT’s depth camera is at the upper left, it is difficult 
to compare body sizes among people. Therefore, for each 
person we reset the origin to the junction of a horizontal line 
connecting the average right and left hip heights and a 
vertical line passing through the midpoint between the right 
and left hip heights. Here, we assume that a person is sitting 
upright, and that the human body is a little unsymmetrical. 
The result of resetting the origin for each person is illustrated 
in Figure 19.  
The estimated left-side (XLE, YLE) and right-side (XRE, 
YRE) locations are calculated with the following equations by 
using the above stored standard locations and the measured 
positions of both shoulders and both hips: 
 
XLE = XLS * Xmls/Xsls 
, 
 
 
(1) 
YLE = YLS * Ymls/Ysls 
, 
 
 
(2) 
XRE = - XRS * Xmrs/Xsrs , 
 
 
(3) 
YRE = YRS * Ymrs/Ysrs 
, 
 
 
(4) 
where 
(XLS, YLS) is the left-side standard location, 
(Xsls, Ysls) is the standard left shoulder position,  
(Xsrs, Ysrs) is the standard right shoulder positon,  
(Xpls, Ypls) is the measured left shoulder position of the patient, 
and 
(Xprs, Yprs) is the measured right shoulder positon of the 
patient.  
 
 
Figure 19. Illustration of adjusting locations for body size 
 
We measured the ten points marked on a T-shirt and 
skeleton data for three men and three women to validate the 
proposed automatically adjusting algorithm. Each man wore 
the same T-shirt like that in Figure 11.  
Since the T-shirt was relatively small, it should have 
closely fit each person, and we predicted the marked points 
should have adjusted to each person correctly. After 
selecting one man and one woman each as the standard, we 
estimated correct points a stethoscope placing by using the 
above equations and the data for the standard person.  
TABLE V. COMPARISON BETWEEN MEASURED AND ESTIMATED 
LOCATIONS 
X
Y
X
Y
X
Y
X
Y
measured
28
136
46
157
53
167
52
157
estimated
33
142
42
154
35
164
36
155
measured
28
118
43
140
55
144
53
133
estimated
32
127
40
137
45
140
46
131
measured
27
94
44
118
52
123
50
107
estimated
31
107
39
116
44
120
45
113
measured
30
51
43
78
55
68
50
59
estimated
33
72
41
77
56
68
57
64
measured
40
36
49
59
62
52
65
54
estimated
38
57
48
62
63
55
65
52
Female-1
Location
2
4
Female-2
6
8
10
Male-1
Male-2
 

286
International Journal on Advances in Life Sciences, vol 8 no 3 & 4, year 2016, http://www.iariajournals.org/life_sciences/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Since there was not big difference between left-side data 
and right-side data, we showed only the left-side data in 
Table V. The location relationships between measured and 
estimated correct points for a female and male participant are 
shown in Figure 19. In these figures, shoulder points and 
correct points a stethoscope placing for a standard person, 
and shoulder point and estimated and measured correct 
points a stethoscope placing for other participants are 
presented. 
 
 
(1) Female-1 
 
(2) Male-1 
Figure 20. Example of measured and estimated correct points 
 
Data unit in Figure 20 was not the millimeter, but the 
pixel. In these figures, 10 pixels corresponded roughly to 4 
cm. Differences between estimated and measured positions 
depend on participants and measuring points. Maximum 
difference was about 8 cm. In case of Figure 20 (1), shoulder 
and hip positions of Female-1 were the same as those of a 
standard participant. Therefore, estimated correct positions 
of Female-1 were the same as those of a standard participant. 
However, locations of measured points were different from 
location of estimated ones in Y axis. The reason for this 
difference must be that each person did not wear the T-shirt 
symmetrically in the horizontal direction. In case of Figure 
20 (2), differences between estimated and measured Y values 
were bigger for lower points, because the T-shirt was less 
elastic in the vertical direction.  
We think the above experiment does not appropriate to 
evaluate the proposed estimation scheme for adjusting 
locations to different body sizes after experimenting. We try 
to devise an appropriate scheme to evaluate this technology. 
 
V. 
APPLICATION 
As we described already, every necessary technology to 
implement a practical auscultation practice system has been 
achieved. Therefore, we developed a simple application for 
a teacher to explain practicing auscultation for the lungs. 
Our co-authors in the nursing course showed the 
following functions: 
- 
Dividing the upper body: The lungs and the abdomen 
were divided, and the right and left lungs were divided 
up and down at the pit of the stomach level. 
- 
Assigning sounds: An appropriate sound was assigned 
to each divided area for teaching content. 
  
We developed two programs: 
- 
Sound assigning program: This program assigns a 
sound to each area, dividing the upper body into five 
areas. 
- 
Auscultation teaching program: This program detects 
on which area a stethoscope should be placed, and it 
replays a sound assigned to that area. 
   
Figure 21 (1) shows the operation window to assign a 
sound file to each divided area. The front upper body was 
divided into five areas. The upper three points are the 
location on both shoulders and the bottom of the throat that 
was obtained from the Microsoft skeleton program. The pit 
of the stomach was also obtained from the Microsoft 
skeleton program. The abdomen is the isosceles right 
triangle whose top is the pit of the stomach. First, a teacher 
selects an area. Then, he or she selects a sound by clicking 
the right radio buttons according to the taught content. 
Disease sounds are managed using the operation window 
shown in Figure 21 (2). Because the disease sounds were 
different for the expiration and inhalation, both of them 
were prepared. Prepared sound files were assigned to 
expiration and inhalation for each disease. 
Figure 22 shows an example window of the developed 
teaching program. When a stethoscope is placed on various 
areas, the area on which the stethoscope is placed changes 
yellow. 
 

287
International Journal on Advances in Life Sciences, vol 8 no 3 & 4, year 2016, http://www.iariajournals.org/life_sciences/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
 
(1) Assigning sound 
 
(2) Management of sounds 
Figure 21. Operation window to assign a sound to divided body area 
 
 
 
 
Figure 22. Operation window of the auscultation teaching program 
 
VI. 
EVALUATION 
We think our system is useful for not only students in 
medical and nursing schools but also for practicing nurses to 
continue their learning. Hence, we asked nursing students 
and practicing nurses to test whether or not our system was 
useful for them. Unfortunately, we did not ask medical 
students. For this questionnaire, we used the first prototype 
for which a student heard one sound when a stethoscope 
was placed on anyplace of the upper body, not the second 
prototype written in Section V. 
A. Nursing course student 
We asked six nursing students who took physical 
assessment including practicing auscultation. The questions 
asked of them and the results are as follows; 
Q1: Could you set up this system by yourself? 
Yes: 6, No: 0 
Q2: Did you hear both a normal sound and a disease  
sound when you placed a stethoscope on the body? 
Yes: 5, No: 1 
Q3: Did you hear sounds prior to placing a stethoscope 
on the body? 
Yes: 5, No: 1 
Q4: Which simulator is more useful to practice 
auscultation, the 
existing mannequin or our system? 
Equal: 2, Our system: 4, Mannequin: 0 
Q5: Would you like to use this system to practice  
auscultation? 
Yes: 6, No: 0 
 
Their comments were as follows: 
(1) Disease sounds can be learned in addition to 
learning communication with patients. 
(2) Imagining a practical patient is easy. 
(3) Learning is possible while practicing auscultation 
naturally. 
(4) I’m very interested in this system, because it could 
utilize a wide range of applications. 
(5) I feel the timing of the replayed sounds was a little 
different from that in real situations. 
(6) I feel the replayed sounds were a little different 
from those in real life, because I could not hear the 
heartbeats. 
 
On the basis of the answers for Q3 and comments for (5), 
we redesigned the replay timing of sounds so that they could 
be heard naturally. 
 
B. Practicing nurse 
We asked the following questions to 50 practicing 
nurses. Their age composition is shown in Figure 23. Most 
of them had several years of experience. 
 

288
International Journal on Advances in Life Sciences, vol 8 no 3 & 4, year 2016, http://www.iariajournals.org/life_sciences/
2016, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
 
Figure 23. Age composition of questionnaires to active nurses 
 
Q1: Are you interested in this system? 
 
Q2: Does our system imitate real clinical situations? 
 
Q3: Is it useful for a student to learn auscultation? 
 
Q4: Is it useful for a practicing nurse to learn 
 auscultation? 
 
Most of them evaluated our system highly for both 
nursing students and practicing nurses to learn auscultation. 
Also, most of them thought our system imitates real clinical 
situations. 
VII. CONCLUSION 
We proposed a new auscultation practice system for 
medical and nursing students. In this system, students 
themselves play the role of a patient instead of a humanoid 
model, and the locations for stethoscope placement on the 
body are measured with KINECT. Therefore, this practice 
system has low cost. In addition, the system can judge 
whether or not stethoscope locations are correct. Practicing 
students hear disease sounds, synchronized with the 
movement of breathing, through earphones or a speaker. 
We developed a prototype system and evaluated it 
experimentally. The results showed that our system could 
perfectly detect stethoscope placement on a body, except for 
three lower points, and that it could detect respiratory 
changes. Also, it perfectly detected the inhalation and 
expiration during respiration for the front body. However, 
the detection delay for respiratory changes was slightly 
larger than expected. We found that most people breathed 
without any movement on their back body.  
We asked nursing students and practicing nurses to test 
whether or not our system was useful for them. The results 
of a questionnaire showed that our system was useful for 
them to learn auscultation. However, some of them felt the 
replay timing of sounds was a little different from that in 
real situations.  
We think that it is possible to solve or avoid such 
problems; and plan to provide production service in a future. 
REFERENCES 
[1] Y. Murata, N. Miura, and Y. Endo, “Proposal for A KINECT-Based 
Auscultation Practice System,” in Proc. eTELEMED, 2016, pp. 86-91. 
[2] Patient simulators for nurse & nursing care training, Kyoto Kagaku 
Co., Ltd.,  
https://www.kyotokagaku.com/products/list02.html#cate_head01 
[retrieved: Decomber, 2016] 
[3] Lung Sound Auscultation Trainer "LSAT", Kyoto Kagaku Co., Ltd., 
https://www.kyotokagaku.com/products/detail02/m81-s.html 
[retrieved: December, 2016] 
[4] Sakamoto Model Corporation, Sakamoto auscultation simulator,  
http://www.sakamoto-model.com/product/emergency/m164/  
[retrieved: December, 2016] 
[5] Laerdal, SimMan 3, http://www.laerdal.com/us/SimMan3G 
[retrieved: December, 2016]. 
[6] J. Butter, W. C. McGaghie, E. R. Cohen, M. E. Kaye, and D. B. 
Wayne, "Simulation-Based Mastery Learning Improves Cardiac 
Auscultation Skills in Medical Students,” Springer, Journal of 
General Internal Medicine, Volume 25, Issue 8, 2010, pp. 780-785. 
[7] SimScope WiFi (The Hybrid simulator), Cardionics  
http://www.cardionics.com/simscope  [retrieved: December, 2016]. 
[8] Meet Kinect for Windows, https://developer.microsoft.com/en-
us/windows/kinect, [retrieved: March, 2016] 
[9] U. von Zadow, S. Buron, T. Harms, F. Behringer, K. Sostmann, R. 
Dachselt, “SimMed: Combining Simulation and Interactive Tabletops 
for Medical Education,” in Proc. CHI '13, Proceedings of the SIGCHI 
Conference on Human Factors in Computing Systems, April 2013, pp. 
1409-1478. 
[10] Open CV, http://opencv.org/  [retrieved: December, 2016] 
[11] N. Burba, M. Bolas, D. M. Krum, and E. A. Suma, “Unobtrusive 
Measurement of Subtle Nonverbal Behaviors with the Microsoft 
Kinect,” IEEE, Virtual Reality Short Papers and Posters (VRW), pp. 
1-4, 2012. 

