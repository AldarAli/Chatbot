Image Retrieval System Based on Combination of Color, Texture and Shape 
Features 
 
Atoany N. Fierro-Radilla, Mariko Nakano-Miyatake, 
Karina Perez-Daniel, Hector Perez-Meana  
Escuela Superior de Ingeniería Mecánica y Eléctrica UC 
Instituto Politécnico Nacional 
Mexico City, Mexico 
Email: mnakano@ipn.mx, afierror@hotmail.com.  
Francisco Garcia-Ugalde, Manuel Cedillo-Hernandez 
Facultad de Ingeniería, Universidad Nacional Autonoma de 
Mexico UNAM 
Mexico City, Mexico 
Email: fgugalde@gmail.com, mcedillohdz@hotmail.com  
 
 
 
Abstract—In the Content-Based Image Retrieval (CBIR) 
system, an effectiveness of the visual descriptors, such as color, 
texture and shape, determines a good retrieval performance. 
Recently, more than two types of visual descriptors are 
combined to improve the performance of CBIR. The 
combination method used to combine different types of visual 
descriptors also plays an important role to obtain a good 
performance.  However, we are aware that researchers have 
not paid sufficient attention to the combination methods, so in 
this paper, we focused on combination of schemes of three 
types of visual features to obtain higher improvement of the 
retrieval performance.  Firstly, several visual descriptors that 
belong to three types of visual features (color, texture and 
shaper) are analyzed individually to select a better descriptor 
from each category. Second, the several combination methods 
are analyzed to determine a best combination method. The 
performance of the proposed scheme is compared with some 
CBIR systems in which more than two different types of 
descriptors are combined.  
Keywords-CBIR system; color descriptor; texture descriptor; 
shape descriptor; combination methd of descriptors 
I. 
 INTRODUCTION 
Multimedia data is very common in daily activities, 
because nowadays it is easy and inexpensive to take pictures 
and videos. Digital images and videos are not only important 
in common activities, but also in areas such as medicine, 
biology, astronomy, commerce, tourism, etc. Due to the 
importance of multimedia information, the facility of sharing 
these data trough high-speed Internet connections and the 
high storage capacities, the size of databases has been 
increasing considerably. As a consequence of this situation, 
an efficient classification, indexing and retrieval of digital 
images stored in a huge database have been challengeable 
tasks. Therefore, the Content-Based Image Retrieval (CBIR) 
has become an urgent research topic because the traditional 
retrieval method, that is manual and subjective process, has 
become time consuming operation with ambiguity results in 
a huge database.  
The CBIR systems describe multimedia content using 
visual features, such as color, texture, shape, etc. These 
features are low-level visual features, which describe images 
making the information retrieval be fast, objective and 
automatic.  In general, color is one of the most dominant and 
distinguishable features in describing image [1]. Therefore, 
until now, several color-based descriptors have been 
proposed in the literature [2]-[6]. The Histogram Intersection 
(HI) [2], the Color Correlogram (CC) [3], the Dominant 
Color Descriptor (DCD) [4], the Color Layout Descriptor 
(CLD) [5] and the Color Structure Descriptor (CSD) [6] are 
widely used as color-based descriptors in the CBIR. To 
improve the retrieval performance, Atoany et al. [7] 
proposed the Dominant Color Correlogram Descriptor 
(DCCD), which optimizes the CC using only eight dominant 
colors. Also, some texture-based descriptors have been 
proposed in the literature to describe the image using the 
texture patterns [1][8]-[12]. The steerable filters [1], The 
Edge Histogram Descriptor (EHD) [8], the Texture 
Browsing Descriptor [9], the co-occurrence matrix-based 
descriptors [10][11] and Local Binary Pattern (LBP) [12] are 
some of the most widely used texture-based descriptors. The 
shape feature is another important factor that can be used to 
identify objects and classify the image context. As the shape-
based descriptors, the Fourier Descriptor (FD) [13], the 
moment-based descriptors, such as Pseudo-Zernike Moments 
(PZM) [1] and Polar Harmonic Transforms (PHT) [14], and 
Pyramidal Histogram of Oriented Gradients (PHOG) [15] 
have been used in the CBIR systems.   
There are some algorithms that do not use the above 
mentioned visual features to characterize the images and 
retrieve desired images. The Scale-Invariant Feature 
Transform (SIFT) and Speed Up Robust Feature (SURF) are 
examples of these types of algorithm.  In the CBIR system, 
the SIFT and the SURF obtain some robust interest points, 
and using these points together with their neighborhood 
regions, the relevant images are retrieved [16][17].  
Recently, the learning-based approaches, such as bag-of-
visual-word [18] and the deep learning [19] are used to 
solve the practical problem in the CBIR.  
In order to improve the image retrieval performance, it is 
necessary to combine more than one visual feature. Several 
combination or fusion methods are proposed in the literature 
[20]-[23]. The simplest method is concatenation of two or 
three descriptors to generate one descriptor with large 
number of elements [20], while in [21], the descriptors 
related to color, texture and shape features are combined to 
generate a single completed binary region descriptor 
37
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-400-8
MMEDIA 2015 : The Seventh International Conferences on Advances in Multimedia

(CBRD); in this case the combination is not simple 
concatenation.  In another combination scheme, different 
types of descriptors are applied to image database in the 
cascade or the parallel manner [22][23].  In [22], firstly, 
color-based descriptor is applied to retrieve a sub-set of 
relevant images, and then, shape-based descriptor is applied 
to images belonging to the previously retrieved sub-set only.  
In the parallel structure of [23], several sub-sets of relevant 
images are extracted independently using different types of 
descriptor, and then, the decision stage makes a final set of 
the relevant images from all extracted sub-sets of images.      
In [1][24]-[27], the visual features are lineally combined 
to improve the retrieval performance, in which the similarity 
of each feature is independently computed using a distance 
metric. And then, an adequate weight for each feature is 
defined to generate a weighted lineal combination of the 
similarity scores of the different features.  This combination 
scheme provides us a construction of a flexible CBIR 
scheme depending on the application.  For example in the 
medical image retrieval system, in which the texture feature 
may be more relevant than other features, the weight value 
assigned to texture feature can be more important than color 
and shape features.  In [24] and [25], the color feature and 
the texture feature are combined. In [24], the CLD and the 
Texture Browsing Descriptor are used, while in [25], the 
EHD and several color-based descriptors, such as the CLD 
and the CSD, are used. In [26], the DCCD as color feature 
and the PHOG as shape feature are combined lineally. In [1] 
and [27], three features are used. In [1], the DCD, the 
steerable filter and the PZM are used as color, texture and 
shape features, respectively. The color histogram and 
moments as color feature, texture feature based on Gabor 
filter and the PZM as shape feature are combined lineally in 
[27].  
In this paper, we propose a CBIR system, in which color, 
texture and shape-based descriptors are obtained and three 
distances between query image and database image using 
these three descriptors are calculated. And then, three 
distances are lineally combined using adequate weights. 
Firstly, we analyzed individually several visual descriptors, 
which belong to one of three types of descriptors in order to 
select the most adequate descriptor from each category.  As 
color descriptor, we select the DCCD, which was proposed 
by Atoany et al. [7]. As texture-based descriptor, we selected 
the directional local motif XoR patterns (DLMXoRP) [11] 
and, finally, the shape feature is extracted using the PHOG 
descriptor [15]. Next, the weights for three features are 
adapted, varying their values, to improve the retrieval 
performance. The proposed scheme was evaluated using 
some common metrics used in the CBIR systems such as 
Average Normalized Modified Retrieval Rank (ANMRR), 
Average Retrieval Rate (ARR) and Average Retrieval 
Precision (ARP), and it was compared with some CBIR 
systems which combined two or more visual features. 
The rest of this paper is organized as follows: In Section 
II, we explain the color-based descriptor used in our CBIR 
system, and its performance is compared with other color 
descriptors.  In Section III, we present the texture-based 
descriptor used in our CBIR system together with the 
performance of this descriptor, and in Section IV, the 
selected shape-based descriptor and its performance are 
provided.  In Section V, we provide an analysis of the 
combination methods of the selected three types of 
descriptors and the global performance of the proposed 
CBIR system. Finally, in Section VI, we conclude this work. 
II. 
COLOR-BASED DESCRIPTOR 
Color descriptors are divided into two categories, i) 
global color descriptors take into account the whole image in 
order to extract color information, this process does not 
include pre-processing or image segmentation; ii) local color 
descriptors extract spatial information on how pixels are 
distributed in certain region, and this is done using pre-
processing or image segmentation [28].  Several color-based 
descriptors were proposed in literature, and some of them 
were adopted by the MPEG-7.  
In the proposed CBIR system, we selected DCCD [7] due 
to its better performance and compact representation.  
A. Dominant Color Correlogram Descriptor (DCCD) 
First, the image is converted from RGB to HSV color 
space, because this color space presents more similarity to 
the human color perception. Then, the HSV image is 
quantized [4], in order to reduce the computational cost. This 
color quantization is done as: 
                𝐻 =
{
 
 
 
 
 
 
 
 
 
 0 𝑖𝑓 ℎ       ∈ [316,20)     
1 𝑖𝑓 ℎ       ∈ [20,40)       
2 𝑖𝑓 ℎ        ∈ [40,75)        
3 𝑖𝑓 ℎ        ∈ [75,155)     
4 𝑖𝑓 ℎ        ∈ [155,190)  
5 𝑖𝑓 ℎ       ∈ [190,270)
6 𝑖𝑓 ℎ       ∈ [270,295)
7 𝑖𝑓 ℎ       ∈ [295,316)
 
                    







]1,7.0
(
     if   
,2
]
7.0,2.0
(
if   
  ,1
]
2.0,0
[
    if   
,0
s
s
s
S








]1,7.0
(
    if   
,2
]
7.0,2.0
(
if   
  ,1
]
2.0,0
[
     if   
,0
v
v
v
V

We only consider the 8 more representative hues (red, 
orange, yellow, green, blue, dark blue, purple, violet), and 
three levels for saturation (S) and value (V). It is important to 
mention that Human Visual System (HVS) is irregular, that 
is the reason why we are using this method of color 
quantification. The dominant colors are determined from the 
quantized image with 72 colors, which is given by:  
                       𝐹 = {{𝑐𝑖, 𝑃𝑖}, 𝑖 = 1, … , 𝑀}                     
where M<72 is the numbers of dominant colors of a 
quantized image, ci is i-th dominant color with three 
components (H,S,V) and Pi is the percentage of the 
dominant color ci.  Firstly, the percentages Qj, j=0,..., 71, of 
all existent colors are calculated, and then, M colors ci, 
38
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-400-8
MMEDIA 2015 : The Seventh International Conferences on Advances in Multimedia

i=0,…, M-1 with the first M largest percentage are extracted 
as dominant colors,  in this paper, we use M=8 dominant 
colors. Then, the percentages of each dominant color ci  is 
adjusted as 
                                𝑃𝑖 =
𝑝𝑖
̅̅̅
∑
𝑀−1 𝑄𝑗
𝑗=0
                                     
𝑖 = 0,1, … , 𝑀 − 1,    𝑗 = 0,1, … ,71
where 𝑝̅𝑖 = 𝑄𝑖 if ci is a dominant color, otherwise 𝑝𝑖̅ = 0. 
Once dominant colors are obtained, the correlation of pair 
pixels of the same dominant color is calculated using color 
correlogram [3], and it is defines as:
    𝛾𝑐𝑖𝑐𝑖
 
(𝐼) ≜ 𝑃𝑟𝑝1∈𝐼𝑐𝑖,𝑝2∈𝐼𝑐𝑖[𝑝2 ∈ 𝐼𝑐𝑖||𝑝1 − 𝑝2| = 1]          
where 𝛾𝑐𝑖𝑐𝑖(𝐼), is the probability of finding a pixel p1 of 
color ci away from another pixel p2 of the same color. 
Obtaining this correlogram for all dominant color 𝑐𝑖  with 
𝑖 = 0, 1, … , 𝑀 − 1, we get the DCCD, which is given by 
                                    𝐷𝐶𝐶𝐷 = {𝑐𝑖, 𝐶𝐶𝑖}                          
where 𝐶𝐶𝑖 is the color correlogram of i-th dominant color 𝑐𝑖. 
B. Performace comparison of color-based descriptors 
The performance of the DCCD is compared with other 
conventional color-based descriptors. Table I shows 
experimental results of the DCCD together with six color-
based descriptors using the Database 2, composed by 1000 
Corel images, divided into 10 categories with 100 ground 
truth images per category. From the Table I, the DCCD and 
the conventional CC descriptors show better performance, 
although DCCD is 8 times more compact than the CC [3]. 
TABLE I.  
COMPARISON RESULTS OF TEXTURE-BASED DESCRIPTORS 
 
Taking into account the good performance and 
compactness of the DCCD, we select it as color-based 
descriptor. 
III. 
TEXTURE-BASED DESCRIPTOR 
Texture is an important property for characterization and 
recognition of image [8][10]. We analyzed the performance 
of several texture-based descriptors in order to select the 
most efficient one.  
A. Directional Local Motif  Xor Pattern (DLMXoRP)   
The DLMXoRP [11] is one of the high-performance 
texture-based descriptors, in which an input image is 
divided into 3x3 overlapping blocks and a vector at a 
specific direction is extracted as shown by Figure 1.  
 
 
In Figure 2 we can observe that a number of motif 
(1,2…,7) is assigned depending on the relation between the 
three pixel values of the extracted vector as follows [11]: 
 
 
In order to extract the texture features, the following 
equations are used 
               𝐷𝐿𝑀𝑋𝑜𝑅𝑁,𝑅 = ∑
𝑁−1 𝑇𝑖𝑚(𝑝𝑖, 𝑝𝑐) × 2𝑖
𝑖=0
                
where: 
                                 𝑇𝑖𝑚 = {1
𝑝𝑖 ≠ 𝑝𝑐
0
𝑝𝑖 = 𝑝𝑐                            
And 𝑁 is the number of neighbor pixels, 𝑅 is the radio 
of the 3x3 overlapping block, 𝑝𝑐 is the central pixel and 𝑝𝑖 , 
i=0,.. 7, are the neighbor pixels. Using this information, a 
histogram is computed by: 
 
 
      𝐻𝐷𝐿𝑀𝑋𝑜𝑅𝑝
𝜃
(𝑙) = ∑
∑
𝑓2(𝐷𝐿𝑀𝑋𝑜𝑅𝑃𝜃(𝑖, 𝑗), 𝑙)
𝑁2
𝑗=1
𝑁1
𝑖=1
     
𝑙 ∈ [0, 2𝑁 − 1] 
Method 
ANMRR 
ARR 
𝜶 
𝟐 
ARR &  
ARP 
𝜶 
𝟏 
ARR 
𝜶 
𝟏
𝟐 
 
ARP 
𝜶 
𝟏
𝟒 
 
DCCD 
0.3086 
0.7590 
0.5960 
0.7560 
0.8840 
CC 
0.3228 
0.7200 
0.5870 
0.7620 
0.8880 
IH 
0.3174 
0.7610 
0.5760 
0.7380 
0.8640 
DCD 
0.3384 
0.7420 
0.5590 
0.6920 
0.8480 
LBA 
0.3478 
0.7320 
0.5500 
0.7040 
0.8000 
CLD 
0.3194 
0.7620 
0.5740 
0.7280 
0.8360 
CSD 
0.4431 
0.6190 
0.4630 
0.6200 
0.7680 
 
Figure 1.  3-element vector extraction [11] 
 
 
                             Figure 2.  Seven motif asigment [11] 
 
39
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-400-8
MMEDIA 2015 : The Seventh International Conferences on Advances in Multimedia

where 𝜃 = [0°, 45°, 90°, 135°] and: 
                                𝑓2(𝑥, 𝑦) = {1
𝑥 = 𝑦
0
𝑥 ≠ 𝑦                        
Finally, the obtained histograms are concatenated as:  
𝐻𝐷𝐿𝑀𝑋𝑜𝑅𝑃 =
        [𝐻𝐷𝐿𝑀𝑋𝑜𝑅𝑃
0°
, 𝐻𝐷𝐿𝑀𝑋𝑜𝑅𝑃
45°
, 𝐻𝐷𝐿𝑀𝑋𝑜𝑅𝑃
90°
, 𝐻𝐷𝐿𝑀𝑋𝑜𝑅𝑃
135°
]           (11) 
B. Performance comparison of texture-based descriptors 
To select the most efficient texture-based descriptor, we 
carried out a performance comparison of several texture-
based descriptors, such as MCM [10] and DLMXoRP [11]. 
Vipparthi and Kagar [11] compared their DLMXoRP with 
several LBP-based descriptors, showing superiority of this 
descriptor.  We also compared Steerable Filters [1] and the 
EHD [8]. In this evaluation, we used some metrics 
commonly used in image retrieval evaluation, such as 
ANMMR, ARR and ARP, using the Corel Dataset 1k.  The 
evaluation results of four texture-based descriptors are 
shown in the Table II. 
TABLE II.  
COMPARISON RESULTS OF TEXTURE-BASED DESCRIPTORS 
Metric 
Steerable 
Filter 
MCM 
DLMXoRP 
EHD 
 
ANMRR 
 
0.5144 
0.5404 
0.4460 
0.5401 
ARR 
𝛼 = 2 
0.5820 
0.5410 
0.6140 
0.5550 
ARR 
𝛼 = 1 
0.4010 
0.3810 
0.4680 
0.3790 
ARP 
𝛼 = 1 
0.4010 
0.3810 
0.4680 
0.3790 
ARP 
𝛼 = 0.5 
0.4620 
0.4660 
0.5960 
0.4400 
ARP 
𝛼 = 0.25 
0.5120 
0.5560 
0.6720 
0.5000 
 
As shown in the Table II, the DLMXoRP provides a 
better performance in the CBIR task, therefore we selected 
this descriptor as the texture-based descriptor in our 
proposed CBIR system.  
IV. 
SHAPE-BASED DESCRIPTOR 
Shape is known to play an important role in human 
recognition and perception, providing a powerful clue to 
object identity [1]. 
 
Shape-based descriptors can be categorized into two 
classes [13]: i) contour-based descriptors, which use the 
boundary information only, ignoring important information 
in the interior of the objects, ii) region-based descriptors, 
which use both, boundary and the interior information of 
objects. 
A. Pyramid Histogram of Oriented Gradients (PHOG) 
In the proposed CBIR system, we selected the PHOG 
descriptor [15], which extracts boundary information from 
the object.  
 
 
 
 
 
 
 
 
 
 
 
This descriptor extracts the edges from a gray-scale 
image using Canny edge detector, as shown in Figure 3. 
Then, the image is divided into several blocks (Figure 4) in 
hierarchical manner to generate several pyramid levels, and 
in each pyramid level, a histogram of oriented gradients is 
computed.  Finally, the PHOG descriptor is obtained 
concatenating all these histograms [15]. 
B. Performance comparison of shape-based descriptors 
 
To select the most powerful shape-based descriptor, 
some conventional shape-based descriptors, such as PZM 
 
a) 
 
b) 
Figure 3. Edge information extraction. a) RGB image, b)edge 
information 
 
Figure 4. Segmentation of the edge image 
40
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-400-8
MMEDIA 2015 : The Seventh International Conferences on Advances in Multimedia

[1], two representations of the PHT, which are Polar 
Complex Exponential Transform (PCET) and Polar Cosine 
Transform (PCT) [14], and the PHOG, are evaluated. The 
comparison results are shown by Table III. 
 
TABLE III.  
COMPARISON RESULTS OF SHAPE-BASED DESCRIPTORS 
Metric 
PZM 
PCET 
PCT 
PHOG 
 
ANMRR 
 
0.7177 
0.7212 
0.7066 
0.5825 
ARR 
𝛼 = 2 
0.3808 
0.3642 
0.3554 
0.4462 
ARR 
𝛼 = 1 
0.2000 
0.2231 
0.2423 
0.3538 
ARP 
𝛼 = 1 
0.2000 
0.2331 
0.2423 
0.3538 
ARP 
𝛼 = 0.5 
0.2308 
0.2615 
0.2538 
0.4692 
ARP 
𝛼 = 0.25 
0.3692 
0.3692 
0.3692 
0.6308 
 
From the Table III, we can observe that the PHOG 
outperforms other shape-based descriptors, so we decided to 
incorporate it as the shape-based descriptor in our CBIR 
system. 
V. EXPERIMENTAL RESULTS 
The most adequate three descriptors, which are the 
DCCD as color-based descriptor, the DLMXoRP as texture-
based descriptor and the PHOG as shape-based descriptor, 
were selected through the performance comparison in the 
CBIR system, we analyzed several combination methods of 
these three descriptors. The combination of the three visual 
descriptors is done using a weighted linear combination 
given by 
 
 
( , )
( , )
( , )
( , )
I Q
S
I Q
S
I Q
S
S I Q
shape
s
tex
t
c color






       (12) 
 
where I and Q  are an image extracted from database and a 
given query image, respectively, and  
)
( ,
Scolor I Q
, 
)
( ,
Stex I Q
 
and 
, )
( Q
Sshape I
 are individual scores of image I respect to 
the query image Q in color, texture and shape aspects, 
respectively, and   S(I,Q)  is the global score of I  respect to 
Q.  The weight values:  ωc, ωt and ωs present the grades of 
importance of each visual feature, and ωc+ωt +ωs=1 must be 
satisfied. 
To determine the most adequate combination of three 
weight values, the performance of proposed CBIR system is 
evaluated varying these three values using Corel Dataset 1K, 
which are shown by Table IV.  In Figure 5, we present the 
image retrieval behavior for each class at a specific weight 
combination. From Table IV and Figure 5, we can observe 
that using combination number 7, which presents 𝜔𝐶 =
0.2, 𝜔𝑇 = 0.3, 𝜔𝑆 = 0.5 , the performance of the image 
retrieval task is improved.  
 
 
TABLE IV.  
CBIR PERFORMANCE WITH DIFFERENT COMBINATIONS OF 
THREE WEIGHT VALUES 
Weights 
ANMRR 
ARR 
𝜶 
𝟐 
ARR &  
ARP 
𝜶 
𝟏 
ARR 
𝜶 
𝟏
𝟐 
 
ARP 
𝜶 
𝟏
𝟒 
 
𝜔𝐶 = 0.3 
𝜔𝑇 = 0.3 
𝜔𝑆 = 0.3 
0.3387 
0.7520 
0.5610 
0.6980 
0.7880 
𝜔𝐶 = 0.5 
𝜔𝑇 = 0.3 
𝜔𝑆 = 0.2 
0.3510 
0.7390 
0.5470 
0.6820 
0.7600 
𝜔𝐶 = 0.5 
𝜔𝑇 = 0.2 
𝜔𝑆 = 0.3 
0.3584 
0.7290 
0.5410 
0.6700 
0.7520 
𝜔𝐶 = 0.2 
𝜔𝑇 = 0.5 
𝜔𝑆 = 0.3 
0.3353 
0.7360 
0.5660 
0.7200 
0.8200 
𝜔𝐶 = 0.3 
𝜔𝑇 = 0.5 
𝜔𝑆 = 0.2 
0.3346 
0.7390 
0.5670 
0.7040 
0.8120 
𝜔𝐶 = 0.3 
𝜔𝑇 = 0.2 
𝜔𝑆 = 0.5 
0.3421 
0.7550 
0.5570 
0.6780 
0.7800 
𝜔𝐶 = 0.2 
𝜔𝑇 = 0.3 
𝜔𝑆 = 0.5 
0.3306 
0.7600 
0.5760 
0.7060 
0.8080 
𝜔𝐶 = 0.4 
𝜔𝑇 = 0.4 
𝜔𝑆 = 0.2 
0.3413 
0.7490 
0.5570 
0.6960 
0.7760 
𝜔𝐶 = 0.2 
𝜔𝑇 = 0.4 
𝜔𝑆 = 0.4 
0.3321 
0.7410 
0.5690 
0.7180 
0.8160 
𝜔𝐶 = 0.4 
𝜔𝑇 = 0.2 
𝜔𝑆 = 0.4 
0.3511 
0.7390 
0.5490 
0.6840 
0.7600 
 
The proposed CBIR system with optimum weight values, 
obtained through above mentioned observation, is evaluated 
comparing with other CBIR schemes [1][26]. Both CBIR 
schemes use more than two visual descriptors, in [1], the 
DCD based on LBA algorithm is used as color-based 
descriptor, the PZM and Steerable filter are used as shape-
based descriptor and texture-based descriptor, respectively. 
While in [26], the DCCD and the PHOG are used as color-
based and shape-based descriptors, respectively. The 
comparison results are shown by Table V. 
TABLE V.  
PERFORMANCE COMPARISON 
Metric 
LBA+PZM 
+ 
Steerable Filters 
[1] 
DCCD +PHOG 
[26] 
Proposed 
 
ANMRR 
 
0.3672 
0.2698 
0.1821 
ARR 
𝛼 = 2 
0.6750 
0.7800 
0.8560 
ARR 
𝛼 = 1 
0.5420 
0.6550 
0.7370 
ARP 
𝛼 = 1 
0.5420 
0.6550 
0.7370 
ARP 
𝛼 = 0.5 
0.7020 
0.8120 
0.8940 
ARP 
𝛼 = 0.25 
0.8400 
0.9320 
0.9577 
41
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-400-8
MMEDIA 2015 : The Seventh International Conferences on Advances in Multimedia

The comparisons show that the proposed CBIR scheme, 
which combines color, texture and shape based descriptors 
with optimum weight values, provides much better 
performance compared with other CBIR systems previously 
proposed.  
VI. CONCLUSIONS 
In this paper, we analyzed several visual descriptors that 
belong to color-based, texture-based and shape-based 
descriptors. Through the performance analysis of color-based 
descriptors in the CBIR task, we determined that the DCCD 
is the most efficient color-based descriptor from its retrieval 
performance and compact representation. As texture-based 
descriptor, we selected the DLMXoRP considering its higher 
performance compared with the conventional texture-based 
descriptors, while the PHOG descriptor shows much better 
performance compared with other shape-based descriptors, 
such as the PZM and the PHT descriptors, so this descriptor 
is selected as shape-based descriptor in the proposed CBIR. 
The three descriptors are combined lineally, and the 
weight values are determined after exhaustive evaluations of 
proposed CBIR system using Corel Dataset 1k.  The 
determined weight values are 𝜔𝐶 = 0.2, 𝜔𝑇 = 0.3, 𝜔𝑆 = 0.5, 
respectively, which means that the shape feature is most 
important compared with other two features to retrieve 
desired  images respect to a given query image. The 
comparison results show that the proposed CBIR scheme 
outperforms considerably other CBIR schemes.  
The optimum weight values are varied depending on the 
given query image, so adaptive process according to a given 
query image to determine optimum weight values can be 
used, which is our feature work.  
ACKNOWLEDGEMENT 
We thank to the National Council of Science and 
Technology of Mexico (CONACyT) for the financial 
support during the realization of this research. 
REFERENCES 
[1] X. Y. Wang, Y. J. Yu, and H. Y. Yang, “An effective image 
retrieval scheme using color, texture and shape features,” 
Computer Standards & Interfaces, Vol 33, Mar. 2010, pp.59-
68, doi:10.1016/j.csi.2010.03.004 
[2] D. Zhang and G. Lu, “Evaluation of similarity measurement 
for image retrieval”, International Conference on Neural 
Networks and Signal Processing, Dec. 2003, pp. 928-931, 
ISBN: 0-7803-7702-8. 
[3] J. Huang, S. R. Kumar, M. Mitra, W. J. Zhu, and R. Zabih, 
“Image indexing using color correlograms”, International 
Conference on Computer Vision and Pattern Recognition, 
Jun. 1997, pp. 762-768, ISBN: 0.8186-7822-4. 
[4] H. Shao, Y. Wu, W. Cui, and J. Zhang, “Image retrieval based 
on MPEG-7 dominant color descriptor”, International 
Conference for Young Computer Scientist, Nov. 2008, pp. 
753-757, ISBN: 978-0-7695-3398-8. 
[5] E. Kasutani and A. Yamada, “The MPEG-7 color layout 
descriptor: a compact image feature description for high-
speed 
image/video 
segment 
retrieval”, 
International 
Conference on Image Processing, Oct. 2001, pp. 674-677, 
ISBN: 0-7803-6725-1. 
[6] K. M. Wong, L. M. Po, and K. W. Cheung, “Dominant color 
structure descriptor for image retrieval”, IEEE International 
Conference on Image Processing, Sept. 2007, pp. 365-368, 
ISBN: 978-1-4244-1437-6. 
[7] F. R. Atoany, P. D. Karina, M. N. Mariko, and B. Jenny, 
“Dominant color correlogram descriptor for content-based 
image retrieval”, International Conference on Image Vision 
and Computing (ICIVC 2014), Sept. 2014. 
[8] D. K. Park, Y. S. Jeon, and C. S. Won, “Efficient use of local 
edge histogram descriptor”, ETRI, vol. 24, Feb. 2002, pp. 23-
30, doi:10.1145/357744. 
[9] B. S. Manjunath and W. Y. Ma, “Texture features for 
browsing and retrieval of image data”, IEEE Transactions on 
Pattern Analysis and Machine Intelligence, vol. 18, no. 8, 
Aug. 1996, pp. 837-842, doi: 10.1109/34.531803. 
[10] N. Jhanwar, S. Chaudhuri, G. Seetharaman, and B. 
Zavidovique, “Content-based image retrieval using motif 
cooccurrence matrix”, Image and Vision Computing, vol. 22, 
Mar. 
2004, 
pp. 
1211-1220, 
doi: 
10.1016/j.imavis.2004.03.026. 
 
Figure 5. Average precision usgin Corel Dataset 1k 
 
42
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-400-8
MMEDIA 2015 : The Seventh International Conferences on Advances in Multimedia

[11] S. K. Vipparthi and S. K. Nagar, “Expert image retrieval 
system using directional local motif XoR patterns”, Expert 
System with Applications, vol. 41, Dec. 2014, pp. 8016-8026,  
doi: 10.1016/j.eswa.2014.07.001. 
[12] O. Timo, P. Matti, and H. David, “A comparative study of 
texture measures with classifcation based on featured 
distributions”, Pattern Recognition, vol. 29, no. 1, Jan. 1996, 
pp. 51-59, doi: 10.1016/0031-3203(95)00067-4. 
[13] D. Zhang and G. Lu, “Evaluation of MPEG-7 shape 
descriptors against other shape descriptors”, Multimedia 
Systems, vol. 9, no. 1, Jul. 2003, pp. 15-30, doi: 
10.1007/s00530-002-0075-y. 
[14] P. T. Yap, X. Jiang, and A. C. Kot, “Two dimensional polar 
harmonic transforms for invariant  image 
representation”, 
IEEE Transsactions on Pattern Analysis and Machine 
Intelligence, vol. 32, Jul. 2010,  pp. 1259-1270, , doi: 
10.1109/TPAMI.2009.119. 
[15] Y. Bai, L. Guo, L. Jin, and Q. Huang, “A novel feature 
extraction method using pyramid histogram of orientation 
gradients 
for 
smile 
recognition”, 
IEEE 
International 
Conference on Image Processing, Nov. 2009, pp. 3305-3308, 
ISBN: 978-1-4244-5455-0. 
[16] W. Xu, J. Wu, X. Liu, L. Zhu, and G. Shi, “Application of 
image SIFT features to the context of CBIR”, International 
Conference on Computer Science and Software Engineering, 
2008, pp. 552-555, ISBN: 978-0-7695-3336-0. 
[17] C. H. Manuel, G. U. Francisco, C. H. Antonio, N. M- Mariko, 
and P. M. Hector, “Mexican archaeological image retrieval 
based on object matching and a local descriptor”, 
International Conference on Computer Communications and 
Informatics (ICCCI 2015), Jan. 2015, ISBN: 978-1-4799-
6805-3. 
[18] Y. Jun, G. J. Yu, G. H. Alexander, and W. N. Chong, 
“Evaluating bag-of-visual-words representationsin scene 
classification”, International Workshop on Multimedia 
Information 
Retrieval, 
2007, 
pp197-206, 
doi: 
10.1145/1290082.1290111. 
[19] W. Ji, W. Dayong, C. H. H. Steven, W. Pengcheng, Z. Jianke, 
Z. Yongdong, and L. Jintao, “Deep learning for content-based 
image retrieval: a comprenhensive study”, International 
Conference on Multimedia, 2014, pp. 157-166, ISBN: 978-1-
4503-3063-3. 
[20] V. L. Milind, B. Praveen, and J. Pritesh, “An effective 
content-based image retrieval using color, texture and shape 
feature”, Intelligent Computing, Networking, and Informatics, 
vol. 243, Dec. 2013, pp. 1163-1170, ISBN: 978-81.322-1664-
3. 
[21] S. Nishant  and T. Vipin, “Region based image retrieval using 
integrated color, texture and shape features”, Information 
Systems Design and Intelligent Applications, vol. 340, Jan. 
2015, pp. 309-316, ISBN: 978-81-322-2246-0. 
[22] B. G. Prasad, S. K. Gupta, and K. K. Biswas, “Color and 
shape index for region-based image retrieval”, vol. 2059, 
May. 2001, pp. 716-725. ISBN: 978-3-540-42120-7. 
[23] F. Pawel and F. Dariusz, “Strategies of shape and color 
fusions for content based image retrieval”, Computer 
Recognition Systems 2, vol. 45, 2007, pp. 3-10, ISBN: 978-3-
540-75174-8. 
[24] H. A. Jalab, “Image retrieval system based on color layout 
descriptor and gabor filters” International Conference on 
Open Systems (ICOS201), Sept. 2011, pp. 32-36, ISBN: 978-
1-61284-931-7. 
[25] M. Bleschke, R. Madonski, and R. Rudnicki, “Image retrieval 
system based on combined MPEG-7 texture and colour 
descriptors”, International Conference Mixed Design of 
Integrated Circuits and Systems, Jun. 2009, pp. 635-639, 
ISBN: 978-1-4244-4798-5. 
[26] F. R. Atoany, P. D. Karina, N. M. Mariko, P. M. Héctor 
Pérez, and B. P. Jenny, “An effective visual descriptor based 
on color and shape features for image retrieval”, Mexican 
International Conference on Artificial Intelligence (MICAI 
2014), Nov. 2014, pp. 336-348, ISSN: 0302-9743, ISBN: 
978-3-319-13646-2. 
[27] S. Ch. Ryszard, A. Tomasz, and Ch. Michal, “Integrated 
color, texture and shape information for content-based image 
retrieval”, Pattern Analysis and Applications, vol. 10, Apr. 
2007, pp. 333-343. ISSN: 1433-7541. 
[28] A. Talib, M. Mahmuddin, H. Husni, and L. E. George, “A 
weighted dominant color descriptor for content-based image 
retrieval”, Journal of Visual Communication & Image 
Representation, vol. 24, Jan. 2013, pp. 345-360, doi: 
10.1016/j.jcvir.2013.01.007. 
[29] N. Jhanwar, S. Chaudhuri, G. Seetharaman, and B. 
Zavidovique, “Content-based image retrieval using motif 
cooccurrence matrix”, Image and Vision Computing, vol. 22, 
Mar. 
2004, 
pp. 
1211-1220, 
doi: 
10.1016/j.imavis.2004.03.026. 
 
 
 
 
 
 
43
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-400-8
MMEDIA 2015 : The Seventh International Conferences on Advances in Multimedia

