Building Bridges Between Elderly and TV Application Developers  
José Coelho, Carlos Duarte, Pedro Feiteira, Daniel Costa and David Costa 
LaSIGE, Department of Informatics 
University of Lisbon 
Lisbon, PT 
{jcoelho, cad, pfeiteira}@di.fc.ul.pt, {thewisher, dcosta}@lasige.di.fc.ul.pt 
 
Abstract- The development of new digital TV systems and the 
design practices adopted in the development of new TV based 
applications often isolate elderly and disabled users. By 
considering them as users with special needs and not taking 
their problems into account during the design phase of an 
application, developers are creating new accessibility problems 
or just keeping bad old habits. In this paper, we describe a 
novel adaptive accessibility approach on how to develop 
accessible TV applications, by making use of multimodal 
interaction techniques and without requiring too much effort 
from the developers. By putting user-centered design 
techniques in practice, and supporting the use of multimodal 
interfaces with several input and output devices, we confront 
users, developers and manufactures with new interaction and 
design paradigms. From their evaluation, new techniques are 
created capable of helping in the development of accessible TV 
applications. 
Keywords–multimodal; adaptation; developers; elderly. 
I. 
 INTRODUCTION 
Ageing is certainly an obstacle to adequate human-
computer interaction, mostly because of physical and 
cognitive impairments. Traditional computational systems 
only provide keyboard and mouse interaction to users. This 
makes impossible, for example, for users with severe motor 
impairments to interact in any manner (at least effectively). 
Also, as recent developments are responsible for new 
television (TV) systems and applications, unimodal 
interaction is still being favored without accessibility 
concerns, excluding persons whom suffer from an 
impairment of the sensory channel needed to interact. This 
situation brings social exclusion and e-exclusion to the 
Human-Computer Interaction (HCI) world and new TV 
platforms, as it seriously restricts actions and information 
access to users with impairments (like the elderly), providing 
means of interaction exclusively for the so called “normal 
users”. However, multimodality can resolve this issue by 
offering the possibility of presenting content in many ways 
(audio, visual, haptic), and in the most suitable way to each 
user’s characteristics. Also, by offering users the possibility 
to use the inputs more adequate to them (or the context of 
interaction), in a single or combined manner, multimodal 
interaction can improve interaction efficiency and, more 
importantly, accessibility.  
Multimodal interfaces aim to provide a more natural and 
transparent way of interaction with users. They have been 
able to enhance human-computer interaction (HCI) in many 
numbers of ways, including: User satisfaction: studies 
revealed that people favor multiple-action modalities for 
virtual object manipulation tasks [14]; Oviatt [17] has also 
shown that about 95% of users prefer multimodal interaction 
over unimodal interaction; Robustness and Accuracy: “using 
a number of modes can increase the vocabulary of symbols 
available to the user” leading to an increased accessibility 
[15]. Oviatt stated that multiple inputs have a great potential 
to improve information and systems accessibility, because by 
complementing each other, they can yield a “highly 
synergistic blend in which the strengths of each mode are 
capitalized upon and used to overcome weaknesses in the 
other” 
[18]; Efficiency 
and 
Reliability: 
Multimodal 
interfaces are more efficient than unimodal interfaces, 
because they can in fact speed up tasks completion by 10% 
and improve error handling and reliability [16]; Adaptivity: 
Multimodal interfaces also oﬀer an increase in flexibility and 
adaptivity in interaction because of the ability to switch 
among diﬀerent modes of input, to whichever is more 
convenient or accessible to a user [15]. However, Vitense 
[20] illustrates the need of additional research in multimodal 
interaction, especially involving elderly people. This paper 
tries to extend this knowledge. 
Also, the majority of current approaches to the 
development of multimodal or adaptive systems, either 
addresses speciﬁc technical problems, or is dedicated to 
speciﬁc modalities. The technical problems dealt with 
include multimodal fusion [10], presentation planning [10], 
content selection [12], multimodal disambiguation [18], 
dialogue structures [3], or input management [9]. Platforms 
that combine speciﬁc modalities are in most cases dedicated 
to speech and gesture [19], speech and face recognition [11] 
or vision and haptics [13]. Even though the work done in 
tackling technical problems is of fundamental importance to 
the development of adaptive and multimodal interfaces, it is 
of a very particular nature, and not suited for a more general 
interface description. Also, frameworks supporting the 
development of interfaces for various devices exist; however, 
they do not consider the specificities of multimodal 
interaction in its design [5][6]; or they focus only on the use 
of the same modality in different devices [1]; or they ignore 
the possibility of adapting the components properties and 
features in run-time placing the burden on the designer [4]. 
In general, they do not consider in there architectures the 
introduction of modalities, and how they can be explored to 
achieve the goals of Universal Access. 
In the following, we first explain how European funded 
project GUIDE [7], aims to adapt interaction and UI 
presentation to fit each user’s characteristics and level of 
expertise. Also, resulting from specific user trials and 
discussions with developers, we also show how it makes use 
53
Copyright (c) IARIA, 2012.     ISBN: 978-1-61208-177-9
ACHI 2012 : The Fifth International Conference on Advances in Computer-Human Interactions

of a User Initialization Application to know and instruct its 
users, and how it supports adaptation by providing 
developers with solutions for UI modification, and tools for 
helping in the development of new user-centered and 
accessible applications. All this attending to user needs and 
differences, at the same time as it takes into consideration the 
developer’s interests.  
II. 
CHARACTERISTICS OF GUIDE PROJECT 
A. End-Users and Goals 
GUIDE [7] aims to achieve the necessary balance 
between developing multimodal adaptive applications for 
elderly and disabled users, and preserving TV and Set-Top 
Box(STB) developers/manufacturers design methodologies 
and efforts. Consequently, there are clearly two different 
end-users of this project: elderly and impaired users and 
developers of TV based applications. Creating a bridge 
between these two, we have also the STB manufacturers who 
dictate the rules about which type and which characteristics 
of applications can be used on a TV based environment. 
Firstly, for elderly and users with impairments, GUIDE has 
the goal of providing new ways of interacting with a TV, by 
applying multimodal interaction, supporting the use of 
different devices as well as different combinations of input 
and output techniques, and adaptation to each application’s 
UI an each user’s way of interaction. In other words, elderly 
or impaired users who are having difficulties interacting with 
modern TV systems because of their complexity, will be able 
to interact in a more intuitive way, using alternative 
modalities in a single or combined fashion, while each 
interface characteristics will also be adapted to fit user’s 
characteristics automatically. For all this, GUIDE has as a 
clear defined target environment, a STB connected to a TV 
in user’s home (and closed) environment. Secondly because 
developers tend to have no concerns about accessibility 
when designing TV applications, GUIDE has to be capable 
of reducing development effort in a radical manner. For that 
end, GUIDE wants to create a toolbox for accessible 
applications and UI design, shifting the design principle from 
a conventional user-centered design process to a GUIDE-
assisted design and development process. Through all this, 
GUIDE also wants to ensure that developers (and also 
manufacturers) 
can 
maintain 
the 
control 
over 
the 
modifications made on their own applications UI. Meaning, 
the adaptation provided by the system for adapting interfaces 
to user characteristics must have boundaries that cannot be 
crossed. And these boundaries are defined by the developers. 
B. Multimodal Interfaces and Devices 
Input modalities to be supported in GUIDE are based in 
the more natural ways of communication for humans: speech 
and pointing (and gestures). Complementary to these 
modalities, and given the TV based environment, the 
framework should support the usage of remote controls and 
other devices capable of providing haptic input or feedback. 
As a result, GUIDE incorporates four main types of UI 
components: visual sensing and gesture interpretation; audio; 
remote control; haptic interfaces and a multi-touch tablet. In 
what concerns the output modalities, the framework should 
consider and integrate the following output components: 
video rendering equipment (TV); audio rendering equipment 
(Speakers); tablet supporting a subset of video and audio 
rendering and remote control supporting a subset of audio 
rendering and vibration feedback. A tablet may also be used 
to clone the TV screen or complement information displayed 
on the TV screen but essentially is used as a secondary 
display. The main user interface should be able to generate 
various configurable visual elements such as text (e.g., 
subtitles or information data), buttons for navigation 
purpose, images and video (e.g., video conference or media 
content). Additionally also a 3D avatar is generated and 
expected to play a major role for elderly acceptance and 
adoption of the GUIDE system, being able to perform non-
verbal expressions like facial expressions and gestures and 
giving the system a more human like communication ability. 
In order for the UI to be adapted to the user’s needs, 
these elements are necessarily highly configurable and 
scalable (vector-based). Size, font, location, and color are 
some attributes needed to maintain adaptability. These 
graphical elements enable the system communication with 
the users by illustrating, answering, suggesting, advising, 
helping or supporting them through their navigation. Also, 
both input and output modalities can be used in a combined 
manner to enrich interaction and reach every type of user. 
C. Discussion: What GUIDE needs to know 
For reaching its goals, GUIDE has to define a framework 
structure and collect information by asking and testing its 
end-users. So, the following questions have to be answered: 
What components the GUIDE framework has to have? What 
are the main preferences and typical behavior of elderly 
users when interacting with the system, and how to collect 
these preferences? How to perform automatic UI adaptation? 
How to help developers and manufactures in design process? 
III. 
LEARNING FROM END USERS 
To get answers to the questions above, we firstly derive 
end user requirements from results obtained through a 
quantitative and qualitative analysis of data recorded in 
comprehensive user trials [8]. Secondly, we organized focus 
group sessions with developers and used an online survey as 
qualitative 
research 
tools 
in 
gathering 
additional 
requirements from developers and STB platform providers 
A. Initial User Trials 
The GUIDE project pursues a User Centered Design 
(UCD) process, taking into account that one of the main 
principles that characterize UCD is iterative design. 
According to this principle the system is designed, modified 
and repeatedly tested. This iterative cycle allows the 
designers to think in the product design and include the 
changes needed depending on the users’ feedback. Following 
this approach, an initial study to elicit user requirements has 
been carried out. 
 
 
54
Copyright (c) IARIA, 2012.     ISBN: 978-1-61208-177-9
ACHI 2012 : The Fifth International Conference on Advances in Computer-Human Interactions

1) Main Objectives 
Additionally to the identification of viable usage methods 
(gestures, command languages) of novel traditional UI 
paradigms for the different impairments in the target groups 
via user studies in realistic user scenarios, this user trials also 
have the goal to generate quantitative and qualitative user 
data in order to establish and construct a generic user model. 
This user model will provide data representations for each 
user and will constitute the first step for adaptation in 
GUIDE, and will “virtualize” user impairments to try to 
capture the amount of knowledge needed for application 
design.  
2) Organization and Setting 
The initial user studies carried out can be divided in two 
different categories; one survey session and one technical 
trials session. While the aim of the survey was to collect 
qualitative information about application acceptance, user 
habits and modalities of interaction, the objective of the 
technical trials was to gather both quantitative and qualitative 
data and observe the interaction between the elderly and the 
system, performing simple tasks in the context of TV 
interaction. In [8] you can find an extensive description of 
the tests performed.  
B. Developer Focus Groups and Survey 
The GUIDE system is not exclusively focused on elderly 
users, but also centered in developers of TV based 
applications and manufacturers of STBs. For this reason, 
major discussions regarding subjects like adaptation, elderly 
user’s interaction, type of applications, and developers 
requirements for making possible the GUIDE ideas, has 
taken place in this evaluation, by performing both focus 
group with these end-users and by launching an online 
survey with the same user target. 
1) Main Objectives 
The general goal is to explore and understand the 
common practice among developers working on STBs. Thus 
the first objective is to gain data about current tools and APIs 
used in Set top box/connected TV platforms and to 
investigate how accessibility is currently perceived and 
applied in the industry. Secondly, exploring developer 
knowledge to identify which tools would developers need to 
efficiently integrate GUIDE-enabled accessibility features 
into their applications. Additionally, stimulate new ideas 
through discussions and to identify new relationships 
between objects embodying GUIDE concepts and objects 
embodying common practice. And finally, inform STB 
application development community about GUIDE. 
2) Organization and setting 
Developer Focus Groups: Two focus group sessions were 
carried out with connected TV platform providers and 
developers of applications and user interfaces deployed on 
STBs in a natural and interactive focus group setting. The 
sessions were conducted by two moderators (for ensuring 
progress and topic coverage) and each focus group session 
had between six and eight participants and lasted between 
120 and 150 minutes. Sessions were initiated with 
presentations of scripts containing development and use 
cases that cover different aspects of the GUIDE project and 
its concepts. Presentations of each development case script 
lasted 10 minutes and were followed by 30 minutes of 
interactive brainstorming, and discussions. 
Developer Online Survey: A questionnaire was designed 
to investigate how accessibility is currently perceived and 
applied in the industry. In addition, the survey was used as a 
medium to let respondents vote on the most important 
features of the envisaged GUIDE framework and toolbox. 
Both survey and focus group  were composed by the 
following participant types: STB test developers, STB 
experts in Innovative part, Flash application developers, 
HTML developers, middleware STB developers, architects 
in STB platforms, GUI developers for STB, project 
managers for STB projects, managers in Innovative projects 
for STB, product and marketing managers, research 
community, 
and 
standardization 
bodies 
and 
related 
organizations. In total, 81 participants from 16 countries, and 
30 companies all over the world, participated.  
C. Results and Conclusions 
From the realization of both initial user-trials and 
developers focus group (and online survey), we now 
summarize qualitative results which will work as starting 
points for the next section of this paper:  
1) User Survey Results 
The large numbers of variables contained in the data set 
were submitted to a two-stage process of analysis where 
correlations were made and a k-mean cluster analysis [2] was 
performed, reducing the results to only significant data. 
Resulting from this, 3 user profiles capable of discriminating 
differences between users were created. These profiles were 
formed by combining and grouping all modalities 
simultaneously such that a specific grouping may represent 
capability on users perceptual, cognitive and motor 
capability ranges. The main differences noticed were the 
following measures: capability to read perfectly from close 
and distant vision; capability of seeing at night, and color 
perception; capability to hear sounds of different frequencies 
and to distinguish conversations in a noisy background; 
cognitive impairments; and mobility diagnosis like muscular 
weakness and tremors. 
2) Technical User Trials Results 
Big, centered and well-spaced buttons were preferred by 
users because they are easier to see and select (and elderly 
users typically have some kind of visual and motor 
impairments). Additionally, users prefer medium sized fonts 
and medium volumes for audio, but users with impairments 
tend to prefer bigger fonts and higher volumes. However, 
more than based on user abilities or preferences, both visual 
and audio elements configuration, depends on the interaction 
context and must be at all times modifiable and repeatable by 
the user. All the preferences described regarding visual 
components, reflect the low efficiency (lot of time needed for 
each selection) and accuracy (wrong target when selecting) 
registered when interacting with any type of pointing in these 
tests.  
Users clearly preferred gestures easier to make (swipe 
and pinch), and have no problem whatsoever interacting by 
gestures. It was also evident that alternative ways of 
55
Copyright (c) IARIA, 2012.     ISBN: 978-1-61208-177-9
ACHI 2012 : The Fifth International Conference on Advances in Computer-Human Interactions

interacting with the TV (speech and finger pointing) are 
preferred to the traditional way. Also, training makes any 
type of modality more efficient as the user learns what is 
required to perform each interaction. However, any type of 
interaction should not be imposed on the users, but be 
available as an intuitive option for interacting with the TV. 
Additionally, when not used by the user intuition, modalities 
of interaction should be explained to the user before he or 
she starts using the system.  
Every user is able to interact multimodally with the 
system and combine speech and pointing, even when they 
prefer only one modality. Users exhibited different 
multimodal interaction patterns during the trials and there is 
no specific interaction pattern for each user (a user can speak 
first and point afterwards, and in the next interaction do the 
opposite). Users can also change the way they interact 
depending on the type of feedback given while interacting. 
Regarding user preferences in input and output modalities, 
there are clear differences between what users say they 
prefer, and what users really ask for when interacting. In 
fact, 100% of the users want multimodal output every time 
information is presented to them, because every user who 
said to prefer only one type of feedback admitted differently 
when in specific interaction contexts. The same happened 
concerning input modalities, with almost half of the users 
admitting, when confronted with practical tasks, that they 
were wrong when they said to prefer only one modality.  
The results obtained in these trials enforce the need of a 
multimodal system and also the need for adaptation, as we 
can see in a more detailed fashion in [8].  
3) Developer Focus Groups and Survey Results 
Developers agree that if users are involved in every 
development phase of the applications (or in the maximum 
phases possible), the resulting UI will be more usable. It was 
concluded that for elderly people UIs should be maintained 
clear and simple, however without giving the impression that 
it has been designed for someone with impairments (not 
leaving the feeling of a “system for seniors”). Additionally, 
costs are the current major reason for reduced application of 
user-centered design in the industry (followed by time and 
lack of awareness). As the current most important device on 
interaction with STBs, the remote control must continue to 
have a central role in the interaction, and should only be 
relegated to a secondary role if that is a result of each user 
interaction preferences.  Gesture control and speech input are 
recognized 
as 
secondary 
technologies. 
In 
general, 
participants agree that automatic adaptation of user interfaces 
can help elderly users to access ICT services. However 
GUIDE adaptation mechanism should never change interface 
aspects unless it is mandatory for specific user interaction. 
Also, radical changes in the UI must be avoid so that the user 
feels he/she is in control and not get lost in the interface. If a 
radical change is indispensable the UI must inform the user 
of the proposed changes. Identified as the main obstacle to 
UI adaptation is the fact that elderly users present too many 
differences between each other. Therefore, for adaptation to 
fit each user, GUIDE has to first find a way to know his or 
her impairments, preferences or characteristics. This 
“discovery” will have to occur the first time the user interacts 
with the system, and will have to be short, not too much 
intrusive and entertaining to the user. The most important 
conclusion debated in this subject is the one saying GUIDE 
should support UI mark-up as interface between application 
and GUIDE adaptation. This way, developers will be 
allowed to keep tools and development environments and 
without too much additional effort, take a first step to 
accessible design. Web developers mostly use HTML editors 
as the most important tools in Web & TV development. 
However, having to learn new development processes will 
drive developers away from the GUIDE framework. So, 
developers should not be required to develop taken into 
consideration specificities of the multimodal operations but 
have a clear specification of how such devices interact with 
the framework. As it was already described in UI adaptation 
results, identification of UI components should be made 
using only mark-up language, however applications coded 
using dynamic HTML (through JavaScript) must continue to 
be able to change, remove or insert elements in the currently 
rendering page. Meaning, all changes in application 
presentation will need to be identified at run-time. For most 
participants connected TV platforms and STBs will be most 
relevant platforms in the future. Also, Web-based application 
environments will become more important for Web & TV. 
Manufactures stated increasing STBs capabilities cannot 
raise its price to much, or development will be more difficult 
and costly. Developers also pointed out GUIDE system must 
consider situations where multiple users are using the TV 
and services.  
IV. 
MULTIMODAL APPLICATION DEVELOPMENT 
From the results and implications reported in the 
previous section of this paper, we now derive GUIDE project 
solutions for giving answers to the same questions raised in 
the beginning of this paper.  
A. Multimodal and Adaptive Framework 
We now give an overview of the GUIDE framework [7] 
following an interaction cycle, starting from the user input 
and going through the construction of the system’s output to 
be presented to the user. 
 A user provides input through multiple devices and 
modalities which can be used simultaneously. The signals 
from recognition based modalities are processed by 
interpreter modules (e.g., a series of points from the motion 
sensor go through a gesture recognition engine in order to 
detect gestures). The signals from pointing modalities go 
through input adaptation modules (e.g., in order to smooth 
tremors from the user’s hand). Both interpreter and 
adaptation modules base their decisions on knowledge stored 
in the user proﬁle, thus improving the efficiency of noise 
reduction in the input signals. Then, the multimodal fusion 
module receives, analyses and combines these multiple 
streams (outputs of input interpreters and input adaptation 
modules, or raw data that did not go through any of these) 
into a single interpretation of the user command based on the 
user, context and application models. This interpretation is 
sent to the dialogue manager who decides which will be the 
application’s response, basing its decision on knowledge 
56
Copyright (c) IARIA, 2012.     ISBN: 978-1-61208-177-9
ACHI 2012 : The Fifth International Conference on Advances in Computer-Human Interactions

about the current application state and the possible actions 
that can be performed on the application in that state. The 
dialogue manager decision is fed to the multimodal ﬁssion 
module, which is responsible for rendering a presentation in 
accordance to which output to present (derived from the 
application itself and the application model), the user 
abilities (accessed through the user model) and the 
interaction context (made available through the context 
model). The ﬁssion module takes all this information and 
prepares the content to render, selects the appropriate output 
channels and handles the synchronization, both in time and 
space, between channels when rendering. This rendering is 
then perceived by the user, which reacts to it, and starts a 
new cycle by providing some new input. 
B. User Initialization Application 
In both technical user-trials and focus groups, it is the 
necessity of knowing every user characteristics, preferences 
and impairments from the first time he or she interacts with 
the system. This is mandatory because of the user’s 
differences and the necessity of adapting both UI 
components and interaction to fit each user, as well as the 
necessity of instructing the user about every possibility of 
interaction in order to reach the maximum efficiency when 
using the system. GUIDE adaptation begins through a User 
Initialization Application (UIA) that allows for the 
acquisition of primary assumptions about the user. So, 
knowing that each user model contain assumptions about 
interesting characteristics of user subgroups, after “going 
trough” the UIA, a user is assigned to a user model as certain 
preconditions are met. From that moment on, and for any 
GUIDE application the user interacts with, the system is 
“initially” adapted to him/her. It’s relevant to say that the 
UIA is presented to the user as a simple step-by-step 
configuration of a “general” interface. In each step, different 
types of contents and different contexts of interaction are 
presented, so the user can test different components and 
parameters, and the system learns the user characteristics, 
from his impairments to his preferences. Addressing the 
results from the developer focus groups, every UIA run as to 
be short in time, intuitive and transparent to the user and also 
serve as a “tutorial” for learning every modality of 
interaction available in the system. Additionally, the user 
must be recognized (facial or voice patterns) by the system 
so that the information provided can be stored in a profile 
and loaded every time the user interacts with the system.   
C. Simulation of User Impairments 
As developers need tools for helping saving time and cost 
in the development of inclusive TV base applications, 
GUIDE offers a simulator [2] which will allow the developer 
to perform accessibility tests based on virtual users, saving 
much time in comparison to tests with real users. So, 
evaluation as a typically expensive step in user centered 
design is supported in GUIDE by a simulation functionality 
allowing to illustrate to developers how users with typical 
impairment profiles will perceive or may interact with an 
application. The simulator can show how certain visual and 
strength impairments influence the way a user perceives and 
visualizes a certain UI (e.g., how an elderly color blind user 
sees a specific UI), and also what are the effects of those 
impairments in the user interaction (e.g., predicting cursor 
paths on the screen or task completion times). This simulator 
can be characterized as a tool for helping developers to take 
adaptation into consideration at design time. 
D. Filtering 
As verified by the inefficient and erroneous use of 
pointing interaction when performing selections in the user 
trials, elderly users potentially have a wide range of 
impairments that hinder their ability to communicate their 
intentions to an application. In some cases these impairments 
can be severe, and significantly affect the speed and 
accuracy. This leads to an inefficient or even undesirable 
interaction with an application. The use of cursor smoothing 
techniques in GUIDE consists in processing the raw user 
input to obtain a filtered input (Input Adaptation Module 
described in the framework). This requires the usage of 
efficient statistical signal processing schemes to estimate the 
user’s intended operations in real time. Basically it consists 
in the application of corrective forces and forcing relatively 
smooth paths in a cursor interaction as well as assigning 
attraction fields to UI elements. Therefore, the following 
graphical UI filters can help improving pointing interaction 
within the GUIDE project:  
Exponential averaging: this modification calculates the 
cursor position pi as pi = αxi + (1-α)pi-1, where xi is the user 
input, pi-1 is the previous cursor position and α ϵ [0,1] is a 
parameter determining how strong the user input influences 
the cursor position. This method produces smooth cursor 
traces but has the drawback that it can produce a delay 
between user’s intended position and the actual position;  
Damping: This method introduces a quadratic force that 
opposes the velocity of the cursor preventing sudden changes 
in directory or speed when interacting; Gravity well: This 
method warps the cursor space, generating attractive basins 
to ease the selection of visual targets. This simplifies 
pointing interaction selection forcing the selection of buttons 
or UI elements that are more close to the location where the 
user is pointing. 
Considering the different user characteristics and 
impairments, and the different UI element configuration, the 
existence of these filters make possible that motor impaired 
users can more easily interact with pointing and also makes 
possible the use of small and less spaced buttons in 
applications UIs avoiding errors in selection caused by the 
proximity of the buttons. All because pointing interaction 
accuracy is raised. 
E. Semantic Programming and Run-Time Adaptation:  
The specification of TV based applications in GUIDE 
will be based on Web-based languages like HTML, CSS and 
JavaScript because of their wide acceptance among 
developers and compliance with STB specifications. 
However, in GUIDE exists the additional side-condition of 
specifying multimodal applications that needs to be merged 
with these web-based specification languages. This is made 
by specifying additional information about how an 
57
Copyright (c) IARIA, 2012.     ISBN: 978-1-61208-177-9
ACHI 2012 : The Fifth International Conference on Advances in Computer-Human Interactions

application is supposed to adapt in different modalities. For 
this semantic annotations are added to the HTML code, 
based on the WAI-ARIA draft specification of the 
W3C.Only by providing this type of supplementary 
information it is possible for the system to create an abstract 
representation of the application. Then, using an automatic 
application transformation module the system converts the 
annotated 
application 
description 
into 
a 
modality-
independent application representation, the Application 
Model described in the framework. Subsequently, and 
depending on the user interacting and on the level of control 
defined by the application developer, adaptation of UI 
components is performed. Developers can create their 
applications and UIs in an established manner, and GUIDE 
automatically adapts the UI to the user. This avoids having to 
design 
many 
user 
interface 
templates 
for 
various 
heterogeneous user groups. Therefore, GUIDE provides the 
application developers with three possible levels of adaptive 
control: Augmentation: presentation and interaction options 
taken by the developer are not subject of change. Instead, if 
the user model suggests that the presentation is insufficient 
for the user abilities, the presentation is augmented in 
different modalities (for example supplementing a visual 
interface with sound feedback). The multimodal fission 
mechanism 
renders 
the 
application 
output 
directly, 
augmenting or not the rendered presentation depending on 
the user model; Adjustment: application rendering is 
adjusted to the abilities of the user (for example adjusting 
components of a visual interface to fit user characteristics, 
like raising font size or button size). The rendering changes 
can be achieved through CSS manipulation. Adjustment can 
be combined with augmentation; Replacement: application 
content is replaced by a new version of content. In this level 
the application hands over completely the rendering 
responsibilities to the multimodal fission component, ant the 
UI is rendered by transformation of application content, 
layout configuration, as well as presentation styles. 
Augmentation and adjustment can also be performed.  
V. 
CONCLUSIONS 
The results obtained in the technical user-trials about the 
existence of disparity between what modalities users say they 
need, and what modalities they ask for when using the 
system, favors multimodality almost every time. This only 
helps to prove that the use of several input and output 
modalities is indispensable in the development of 
multimodal 
TV 
based 
applications 
for 
all. 
Also 
indispensable, are the components identified in the GUIDE 
framework, and the combined use of semantic programming 
and run-time adaptation mechanisms to fit UI components to 
each user characteristics. Additionally, the use of a simulator 
of user impairments can help developers understand at 
design-time how certain UI templates and components are 
perceived by different users with different impairments, 
preventing 
user 
exclusion 
and 
making 
accessible 
applications easier to design 
 
ACKNOWLEDGMENT 
The research leading to these results has received funding 
from the European Union’s Seventh Framework Programme 
(FP7/2007-2013) under grant agreement nº 248893. 
REFERENCES 
[1] Balme, L., Demeure, A., Barralon, and N., Coutaz, J. & 
Calvary, G.. CAMELEON-RT: A Software Architecture 
Reference Model for Distributed, Migratable, and Plastic User 
Interfaces. In EUSAI’2004  
[2] Biswas, P., Robinson, P., and Langdon, P.: Designing 
inclusive interfaces through user modelling and simulation. 
International Journal of Human Computer Interaction. 
[3] Blechschmitt,E., and Strodecke,C.: An architecture to provide 
adaptive, synchronized and multimodal human computer 
interaction. In MULTIMEDIA ’02, NY, USA, pp. 287-290.  
[4] Bouchet, J., and Nigay, L.: ICARE: a component-based 
approach for the design and development of multimodal 
interfaces. In CHI ’04, NY, USA, pp. 1325-1328.  
[5] Calvary, G., Coutaz, J., and Thevenin, D.: A unifying 
reference framework for the development of plastic user 
interfaces. In EHCI ’01, London. UK, pp. 173—192. 
[6] Calvary, G., Coutaz, J., Thevenin, D., Limbourg, Q., 
Souchon, N.,Bouillon, L., Florins, M., and Vanderdonckt, J. 
(2002). Plasticity of user interfaces: A revised reference 
framework. In TAMODIA ’02, Bucharest, pp. 127-134. 
[7] Coelho, J., and Duarte, C.: The Contribution of Multimodal 
Adaptation Techniques to the GUIDE Interface. HCII2011, 
Orlando, Florida, USA, pp. 337-346. 
[8] Coelho, J., and Duarte, C., Biswas, P., Langdon, P. 
Developing Accessible TV Applications, Proceedings of 
ASSETS 2011, pp. 131-138. 
[9] Dragicevic, P., and Fekete, J.D.: The input conﬁgurator 
toolkit: towards high input adaptability in interactive 
applications. In AVI ’04, ACM Press, NY, USA, pp. 244-247. 
[10] Elting, C., Rapp, S., Mohler, G., and Strube, M.:  Architecture 
and implementation of multimodal plug and play. In ICMI 
’03, ACM Press, NY, USA, pp. 93-100. 
[11] Garg, A., Pavlovi´ c, V., and Rehg, J. (2003). Boosted 
learning in dynamic bayesian networks for multimodal 
speaker detection. Proceedings of  IEEE 91, pp. 1355–1369. 
[12] Gotz, D., and Mayer-Patel, K.: A general framework for 
multidimensional adaptation. In MULTIMEDIA’04, NY, 
USA, pp. 612-619. 
[13] Harders, M., and Szekely, G. (2003). Enhancing human-
computer interaction in medical segmentation. Proceedings of 
IEEE, 91, pp. 1430–1442.  
[14] Martin, J.C., Julia, L., and Cheyer, A.:. A theoretical 
framework for multimodal user studies. In CMC98, Tilbur, 
Netherlands, pp. 104-110. 
[15] Oakley, I., Brewster, S. A., and Gray, P. D.: Solving multi-
target haptic problems in menu interaction. CHI’01, Seattle, 
USA, pp. 357-358. 
[16] Oviatt S. L. Multimodal interactive maps: Designing for 
human performance. Human-Computer Interaction, 1997, pp. 
93-129 
[17] Oviatt, S. L., DeAngeli, A., and Kuhn, K. Integration and 
synchronization of input modes during multimodal human-
computer interaction. CHI '97, New York, USA, pp. 415-422.  
[18] Oviatt, S.L.: Mutual Disambiguation of Recognition Errors in 
a Multimodal Architecture. CHI’99, Pittsburgh, USA, pp. 
576-583.  
[19] Sharma, R., Yeasin, M., Krahnstoever, N., Rauschert, ICai, 
G., Brewer, I., Maceachren, A.M., and Sengupta, K. (2003). 
58
Copyright (c) IARIA, 2012.     ISBN: 978-1-61208-177-9
ACHI 2012 : The Fifth International Conference on Advances in Computer-Human Interactions

Speech-gesture driven multimodal interfaces for crisis 
management. Proceedings of IEEE, 91, pp. 1327-1354  
[20] Vitense, H. S., Jacko, J. A., and Emery, V. K. Multimodal 
feedback: An assessment of performance and mental 
workload. Ergonomics 46,  2003, pp. 66-87. 
59
Copyright (c) IARIA, 2012.     ISBN: 978-1-61208-177-9
ACHI 2012 : The Fifth International Conference on Advances in Computer-Human Interactions

