Towards Validating a Toolkit of Bilingual Interprofessional Healthcare Education 
Assessment Tools: Data  
 
Colla J. MacDonald, Douglas Archibald, David 
Trumpower, Betty Cragg, Wilma Jelley 
University of Ottawa 
Ottawa, Ontario, Canada 
cjmacdon@uottawa.ca, darch080@uottawa.ca, 
david.trumpower@uottawa.ca, bcragg@uottawa.ca, 
wjelley@uottawa.ca 
Lynn Casimiro 
Montfort Hospital 
Ottawa, Ontario, Canada 
lynncasimiro@montfort.on.ca 
 
Abstract— There is a need to create validated evaluation 
strategies for interprofessional education (IPE) in order to 
continue to improve IPE education, gather evidence of learning 
and transfer knowledge to the point of care. In this paper we 
present the findings from our data collection(to date) over an 
18 month period in a variety of IPE courses and program 
(university and college credit courses in nursing, medicine, 
various healthcare courses, and continuing education courses 
for healthcare professionals in four countries). The two 
quantitative tools used in both English and French were 
validated and are reported in this paper.  
Keywords- 
interprofessional 
education; 
assessment; 
evaluation instruments 
I. 
 INTRODUCTION 
The purpose of this project (funded by a Health Force 
Ontario Research Grant) was to design, develop, pilot, refine 
and validate a toolkit of qualitative and quantitative 
assessment tools to assess interprofessional education (IPE). 
The four qualitative and quantitative evaluation tools used in 
both English and French included an evaluation survey based 
on the W(e)Learn framework, a survey to access changes in 
attitudes and behaviour and a team and learner exemplar and 
companion contract [1]. The process of developing our tools 
has been published elsewhere [2]. Descriptions of the 
instruments can be found in Table 1. 
TABLE I.  
TOOLKIT OF EVALUATION INSTRUMENT 
Assessment Tool 
Purpose 
ICCAS 
– 
Interprofessional 
Collaborative 
Competencies 
Attainment Survey 
This quantitative survey has been designed to 
document learner’s perceptions of changes in 
their attitudes and behaviors with regard to IPC 
competencies as a result of IPE. Learners reflect 
back after completing the IPE experience and 
identify (in hindsight) where they perceive they 
were before and after the learning experience [2] 
 
W(e)Learn 
Assessment 
 
This quantitative instrument is designed to align 
with 
the 
W(e)Learn 
framework  
http://www.ennovativesolution.com/WeLearn/ 
Learners rate their experiences in an IPE program 
using the dimensions of IPE identified by the 
W(e)Learn framework (content, media, service, 
structure and outcomes) [2].  
 
Learner Contract 
 
This qualitative tool has been created to help 
individual learners develop the core competencies 
needed to collaborate effectively with other 
healthcare 
professionals. 
Specifically, 
the 
contract will: (a) document how learners plan to 
develop their knowledge, skills and activities 
associated with IPC; (b) assist learners in 
identifying and documenting learning outcomes 
associated with the learning activities. 
This tool will facilitate learner’s planning, 
monitoring, 
and 
assessment 
of 
their 
IPE 
experience [2]. 
Learner 
Contract 
Exemplar 
The Learner Contract exemplar provides ideas, 
appropriate language and suggestions for how to 
plan and implement strategies to facilitate and 
assess IPE activities. Facilitators may choose to 
share this tool with learners to guide them in their 
IPC planning or to use some or all of it to support 
their teaching [2]. 
Team Contract 
This qualitative tool has been created to help 
teams work together effectively. Specifically, the 
contract will: (a) document how teams plan to 
develop 
knowledge, 
skills 
and 
activities 
associated with (IPC);(b) assist teams in 
identifying and documenting learning outcomes 
associated with the learning activities. 
This tool will facilitate team’s in planning, 
monitoring, and assessing their IPE experience 
[2]. 
Team 
Contract 
Exemplar  
The Team Contract exemplar provides ideas, 
appropriate language and suggestions for how to 
plan and implement strategies to facilitate and 
assess IPE activities. Facilitators may choose to 
share this tool with teams to guide them in their 
IPC planning or to use some or all of it to support 
their teaching [2]. 
 
This study is unique because (a) all instruments are 
aligned with a Canadian, nationally validated set of IPE core 
competencies and an IPE framework (W(e)Learn), (b) the 
post/post design of our quantitative ICCAS tool, we believe, 
will be sensitive to changes in IPE attitude and behaviour, 
and (c) the learner contract is a teaching/learning tool to help 
teams or individual learners plan their learning and becomes 
a qualitative self-reflection assessment tool after learning. 
In this paper, we present the findings (to date) from our 
18 month validating process in a variety of IPE courses and 
programs (university and college credit courses in nursing, 
medicine, various healthcare courses, and continuing 
94
eL&mL 2011 : The Third International Conference on Mobile, Hybrid, and On-line Learning
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-120-5

education courses for healthcare professionals in Canada, 
and New Zealand. Data collection and analysis will continue 
over the next six months. The two quantitative tools used in 
both English and French were validated and are reported in 
this paper. Data collection is still in process for the learner 
and team contracts.  
II. 
METHODOLOGY 
A letter of invitation was emailed to potential participants 
met at various IPE and healthcare conferences who 
expressed an interest in participating in the validation 
process. Letters were sent to various IPE educators in 
Canada, the United States, England and New Zealand 
between November 2009 and June 2010. A copy of the letter 
can be found in the Appendix. Through word of mouth and 
additional conference presentations, growing interest to 
participate in the study emerged. Response to the invitation 
to participate in the study was enthusiastic as many programs 
revealed that they need and want assessment instruments for 
their IPE projects. 
The four instruments in both French and English were 
emailed to programs in October 2009 with a list of criteria 
that needed to be met in order to be involved in the study. 
The criteria included completion of a brief (approximately 10 
minute) survey providing a course description and feedback 
on the instruments. Programs were requested to provide an 
MS Excel template for each assessment instrument 
completed by learners within the programs to assist with 
validation. Ethics approval was obtained and confidentiality 
of the participants was ensured. 
For their participation in the project, programs were 
promised an electronic version of the four final instruments 
(in either French or English or both), with a synopsis of the 
psychometric properties. Programs also received a small 
stipend ($300.00 Canadian) to be used as needed (e.g., to pay 
a research assistant to input data). 
Data started to emerge in February 2010. The project 
manager was in contact with the participating program 
coordinators via telephone and email. On several occasion 
the participating cites were visited by the project manager to 
discuss the instruments with instructors. Table 2 shows the 
participating programs to date. 
Results of the validation process of the two quantitative 
instruments are reported in the next section of this paper. The 
validation 
process 
included 
calculating 
the 
internal 
consistency of the resulting scales on the planning and 
assessment tools with Cronbach’s alphas [3]. Any scales 
with poor internal consistency (alphas < .7) were then 
subjected to item analysis. Item-total correlations were 
computed between individual item responses and respective 
total scale scores. Items with low item-total correlations were 
discarded from their respective scale. This process was then 
repeated in an iterative fashion until all scales displayed 
adequate internal consistency. 
Construct validity of the tools was assessed with 
confirmatory factor analyses. Criterion related validity was 
assessed by comparing scores on the various subscales of the 
survey with other related measures. Divergent validity 
evidence was sought by computing correlations between 
scores on the quantitative tools and non-related demographic 
variables, such as education level. 
TABLE 1: PARTICIPATING PROGRAMS TO DATE 
IPE Program 
N 
Country 
Jeux Interprofessionnels Francophones 
2010, à la Cité Collégiale 
29 
Canada 
IPE Program, Northern Ontario School 
of Medicine 
15 
Canada 
IPE showcase, à la Cité Collégiale 
58 
Canada 
Hybrid IPE course, Algonquin College 
60 
Canada 
IPE project with Nursing and Library 
students, Algonquin College 
74 
Canada 
Transdisciplinary Community Health 
Program – University of Ottawa 
22 
Canada 
Rural Interprofessional Clinical 
Education Project – University of 
Ottawa 
6 
Canada 
IPE program, Faculty of Health and 
Environmental  Sciences, Auckland 
University of Technology 
     945 
New Zealand 
IPE project – Elisabeth Bruyere 
Research Institute and the University of 
Ottawa 
25 
Canada 
Clinique universitaire 
interprofessionnelle de réadaptation 
Université d'Ottawa 
        7 
Canada 
en formation interprofessionalisme 
Hôpital Montfort 
        5 
Canada 
III. 
DATA ANALYSES 
A. Internal Consistency 
The internal consistency of resulting scales on the 
planning and assessment tools assessed with Cronbach's 
alphas. The reliability analysis showed that all items on the 
W(e)Learn instrument are very highly correlated. The 
reliability analysis also showed that all items on the ICCAS 
instrument are very highly correlated. 
Items of the W(e)Learn were grouped under the 
constructs of structure, content, service, and outcomes. All 
items had Cronbach’s alphas over .90 with the exception of 
the five items under the service construct which were 
approaching .90 (r=.88). Items of the ICCAS were grouped 
under the constructs of the competencies for IPC: 
communication; collaboration; roles and responsibilities; 
collaborative patient/family centred approach; conflict 
management/resolution; and team functioning. Cronbach’s 
alphas for all items were above .90, ranging from r=.90 to 
r=.93 
Item-total 
correlations 
were 
computed 
between 
individual item responses and respective total scale scores. 
Items with low item-total correlations were to be discarded 
from their respective scale. This process would be repeated 
in an iterative fashion until all scales display adequate 
internal consistency. However, it was determined that if any 
item were deleted on the W(e)Learn , the Cronbach’s alphas 
of the remaining items would not increase. Likewise, if any 
95
eL&mL 2011 : The Third International Conference on Mobile, Hybrid, and On-line Learning
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-120-5

item were deleted on the ICCAS, the Cronbach’s alphas of 
the remaining items would not increase. Thus, no items on 
the W(e)Learn and ICCAS were deleted. 
B. Construct Validity 
Construct validity of the tools was assessed with 
confirmatory factor analyses. It was expected that items 
would load strongly onto the factors in accordance with the 
model variables. Further, it was expected that items from the 
same subscale will load onto the same factors. 
Confirmatory factor analyses were conducted for both the 
W(e)Learn and ICCAS assessment instruments. In fact, two 
factor analyses were conducted for the ICCAS; for the 
"before" items and then the "after" items. Items of the 
W(e)Learn instrument loaded onto three factors. As 
expected, the factor analyses for the ICCAS showed items 
for each sub-scale loading onto six factors with some 
overlap. 
For the W(e)Learn, factors did not correspond perfectly 
with the 4 subscales. This was not entirely surprising, as the 
theoretical model upon which the W(e)Learn is based 
proposes that both Content and Structure are dependent upon 
various aspects of Structure. Results suggest a possible 
regrouping of some Structure items with Content (those 
related to collaborative pedagogical strategies), some 
Structure items with Service (those related to facilitation 
strategies). Items for the Outcomes subscale, as well as items 
for the Structure, Content, and Service subscales that relate 
to practical/authentic IPE content and resources loaded onto 
factor 1 (factor loadings > .50). Items for the Structure and 
Content subscales that relate to collaboration and teamwork 
loaded onto factor 2 (factor loadings > .50), whereas items 
for the Structure and Service subscales that relate to 
facilitation loaded onto factor 3 (factor loadings > .50). 
Regarding the ICCAS, results suggest that, before IP 
training, individuals tend to focus more solely on their 
interactions with the patient/family in planning care, whereas 
following training, individuals better understand the 
interactions with other IP professions in effective team 
functioning. 
For the "before" results of the ICCAS, items related to 
team functioning and collaborative patient/family-centred 
approaches loaded on factor 1 (factor loadings > .70); 
communication items loaded onto factor 2 (factor loadings > 
.56); conflict resolution/management items loaded onto 
factor 3 (factor loadings > .73); collaboration items loaded 
onto factor 4 (factor loadings > .73); and, roles and 
responsibilities items seemed to load onto factors 5 and 
6(factor loadings > .65). 
For the "after" results of the ICCAS, items related to 
collaborative patient/family-centred approaches loaded on 
factor 1 (factor loadings > .70); communication items loaded 
onto 
factor 
2 
(factor 
loadings 
> 
.53); 
conflict 
resolution/management items loaded onto factor 3 (factor 
loadings > .63) with team functioning items also showing 
relatively high factor loadings (.56 -.59); collaboration items 
loaded onto factor 4 (factor loadings > .60); roles and 
responsibilities items seemed to load onto factor 5 (factor 
loadings > .40); and some roles and responsibilities and 
communication items loading onto factor 6 (factor loadings 
> .40). 
Correlations were computed between subscales of the 
ICCAS and a related instrument, the IEPS, using responses 
from after training. 
C. Criterion Related Validity 
Criterion related validity was assessed by comparing 
scores on the various subscales of the survey with other 
related measures. Factors 2 and 4 of the IEPS, measure 
understanding the need for interdisciplinary cooperation and 
understanding the value of other professions.  
It may be hypothesized that individuals who score higher 
on 
the 
Communication, 
Collaboration, 
Roles 
and 
Responsibilities, 
Conflict 
Management, 
and 
Team 
Functioning subscales of the ICCAS would be more likely to 
see the value in and need for cooperation with team members 
from other professions. This hypothesis was partially 
supported, as Factor 2 of the IEPS was significantly 
positively correlated with all scales of the ICCAS. However, 
factor 4 of the IEPS did not correlate significantly with 
subscales of the ICCAS. 
D. Divergent Validity 
Divergent validity evidence was sought by computing 
correlations between scores on the quantitative tools and 
non-related demographic variables, such as education level. 
Correlations were computed between subscales of the 
ICCAS and a related instrument, the IEPS, using responses 
from after training. 
Factors 1 and 3 of the IEPS measure perceptions of the 
competence and cooperation within one's profession but not 
one's own ability. It may therefore be hypothesized that 
individual's scores on the subscales of the ICCAS should not 
be related to the IEPS. This hypothesis was partially 
supported, as neither Factors 1 nor 3 of the IEPS was 
significantly positively correlated with any scales of the 
ICCAS. 
IV. 
CONCLUSIONS AND FUTURE WORK 
Validating instruments and processes will increase the 
likelihood that IPE experiences are planned and delivered 
effectively and increase justification and accountability for 
healthcare educational experiences and clinical practice. 
In summary, assessment is an important, and often 
overlooked, element in designing an IPE initiative. These 
instruments are intended to make the assessment process 
easier and more effective. Although these instruments were 
designed with interprofessional healthcare teams in mind, we 
feel the validated instruments could readily be transferable to 
a variety of interdisciplinary tasks and settings such as social 
work and human services education. Next steps include 
collecting more data over the next year, conducting the 
validation analyses again, and making any refinements to the 
instruments if necessary.  
REFERENCES 
[1] 
C.J. MacDonald, E.J. Stodel, T-L. Thompson,  and L. Casimiro, 
“W(e)Learn: 
A 
framework 
for 
interprofessional 
education,” 
96
eL&mL 2011 : The Third International Conference on Mobile, Hybrid, and On-line Learning
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-120-5

International Journal of Electronic Healthcare, vol. 5, issue 1, 2009,  
pp. 33-47.  
[2] 
C.J. MacDonald, D. Archibald, D. Trumpower, E. Cragg, L. 
Casimiro, and W. Jelley, “Quality standards for interprofessional 
healthcare education: Designing a toolkit of bilingual assessment 
instruments,” Journal of Research in Interprofessional Practice and 
Education, vol. 1, issue 3, 2010, pp. 1-13. 
[3] 
L.J. Cronbach, “Coefficient alpha and the internal structure of tests,” 
Psychometrika, vol. 16, issue 3, 1951, pp. 297–334.  
 
 
APPENDIX 
 
Dear (type name), 
Thank you for agreeing to participate in the validation study of our 
interprofessional education (IPE) assessment instruments.  Enclosed you 
will find the IPE instruments, as well as a brief survey about your IPE 
program(s), a copy of our ethics approval letter, and an Excel file to be 
used for entering your data.  A brief description of each enclosure is 
included at the end of this letter. 
Your participation in the validation study involves the following steps: 
1) 
Begin using the IPE instruments in your IPE program(s) at any time.  
You may choose to use the instruments in one program or in multiple 
programs.  As well, you may choose to use all of the instruments or 
just a subset of them. 
2) 
Enter the data that you collect with the IPE instruments into the 
enclosed Excel template.  If you are using the instruments in multiple, 
distinct programs, please save the data from each program in a 
separate Excel file, and name each file using a unique label (e.g., If 
you are using the instruments in 2 different courses, you might name 
the Excel files with the associated course code: EDU5191.xls and 
EDU7395.xls). 
3) 
Complete the Program Description and Instrument Feedback 
Survey.  If you are using the instruments in multiple, distinct 
programs, please complete a separate survey for each, and be 
sure to include the program labels that you used for naming the 
associated Excel files. 
4) 
Return the Excel data file(s) and the completed Program 
Description and Instrument Feedback Survey(s) to: 
When we receive your data, we will send you a $300.00 CDN stipend for 
your participation.  If you use at least 3 of the IPE instruments (the ICCAS, 
W(e)Learn, and either the Team or Learner Contract) in your program we 
will send an additional $300 CDN stipend.  In order to receive the stipend 
and be included in the initial validation report, your data must be received 
before September 1st, 2010.  After analyzing the data, we will send you our 
initial report on the psychometric properties of the instruments as well as 
revised instruments for your use. 
If you have any questions about the process or about the instruments, we 
encourage you to contact us at. Again, we sincerely thank you for your 
participation. 
 
 
 
 
 
97
eL&mL 2011 : The Third International Conference on Mobile, Hybrid, and On-line Learning
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-120-5

