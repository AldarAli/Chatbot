Practical Application of the Data Preprocessing Method for Kohonen Neural
Networks in Pattern Recognition Tasks
El Khatir Haimoudi(1), Loubna Cherrat(2)
(1) Plury-disciplinary Laboratory
FP of Larache, Abdelmalek Essaâdi University
Larache, Morocco
e-mail: helkhatir@gmail.com
Otman Abdoun(1), Mostafa Ezziyyani(2)
(2)Mathematics and Application Laboratory
FST of Tangier, Abdelmalek Essaâdi University
Tangier, Morocco
e-mail: cherrat@gmail.com, m.ezziyyani@fstt.ac.ma
Abstract — Self-Organizing Map (SOM) is a very effective
solution for solving pattern recognition problems. However,
some ambiguities appear during learning process with the
existence of linear patterns in the learning data, in this case,
the learning process lasts for a long time and the network
produces irrelevant results. The work provides the resolution
of the detected problem and the application of the SOM for the
pattern recognition. To achieve our objective and minimize the
learning time, a SOM improved model has been developed.
This model uses a special block able to filter the input data and
reduce the size of the learning multitude. The presented
experimental test results in this work show that the improved
model exceeds the standard model in terms of the recognition
results accuracy and the learning time. The results obtained in
this work encouraged us to think about using the improved
model
to
develop
a
smart
approach
(SmartMaps)
of
Geographic Information Systems (GIS).
Keywords- Pattern recognitions; Artificial Neural Network;
self-organizing map; preliminary processing of input vectors;
Data visualising; principal component analysis; power iteration
algorithm.
I.
INTRODUCTION
The
new
information
technologies
offer
great
opportunities
for
human
activity
in
different
areas.
However, the important element of their evolution is not
only the extent increase of computer technology’s capacity,
but also its intellectualization by the creation of new
intelligent systems in the form of software or hardware
models. These systems must be equipped with intellectual
abilities comparable to those of humans. Their use is to
solve very complex problems for classical information
systems, such as the recognition, diagnosis and prediction.
Recently systems based on Artificial Neural Networks
(ANN) are widely used to create these systems [1]-[3]. The
essential advantage of ANN is a functional similarity to
biological neural networks and the universality for solving a
wide range of tasks. There are a variety of architecture and
learning methods for different ANN models. Currently the
models based in competitive learning algorithms, like Self-
Organizing Maps (SOM) and counter-propagation network
are widely used in pattern recognition tasks [4]-[8]. An
important and useful feature of SOM is the ability to
visualize multi-parameter objects in a one-dimensional or
two-dimensional space [10].
However, tests show that the use of SOM as it stands,
does not give relevant results, as the learning algorithm
requires normalizing input data. The consequence of this
operation is the loss of some information about the initial
lengths of objects, and the ratio between the absolute values
of input object components. In this case and with the
existence of linear patterns, the learning process takes a long
time and SOM produce irrelevant results [11]. In this work,
we realized a new model of the SOM which provides the
introduction of a preprocessing block and data optimization.
Pre-treatment process is based on a method that combines
two
well-known
and
approved
algorithms:
Analysis
Principal Components and Iterated Power. The both map
models (standard and improved) are applied to solve a task
of pattern recognition; the task objective is to visualize
geographical
information
for
the
African
continent
countries. In this work we present also the results of this
practical application, and the detailed analysis of their
comparison.
The article consists of five sections. In the introduction,
we show the importance of intelligent systems, their
application area and new means for their development. We
also describe the purpose of the work, the solved problem
and the future perspective. The second section comprises the
description of the pattern recognition task, citing the classical
and modern methods used to solve this problem. So we give
details of the learning algorithm of the SOM, and the
principles of its application in this domain, including the
ambiguities detected in this model of ANN and possible
solutions. In the third section we present the algorithm of
preliminary data processing and optimization, with argument
and explanation of different steps of its implementation and
the benefits obtained from its application with SOM. The
fourth part provides the practical application of the two
38
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-477-0
IMMM 2016 : The Sixth International Conference on Advances in Information Mining and Management (includes DATASETS 2016)

models for the recognition of the African continent countries.
In this section we described: the approach we have followed
to solve this task, the means and tools provided by the
application developed for its use, as well as the results
obtained and details of their analysis. As a conclusion, we
mention the important moments concerning the problems
encountered
during
the
SOM
applications
in
pattern
recognition, the contribution of the proposed solution and
our perceptive.
II.
THE APPLICATION FEATURES OF NEURAL NETWORKS
IN PATTERN RECOGNITION TASKS
The task of pattern recognition can be considered as a
combination of two related subtasks: classification and
clustering. The classification task is to determine the
belonging of the input pattern to one of predefined classes
[11]. This classification type is used for the recognition of
handwritten texts, the lyrics and ECG signals. During
clustering, the learning algorithm is based only on the input
data without desired output. In this case, the learning
process will try to identify the similarity between patterns,
and similar objects will be brought to the same category
(cluster); the proximity is often understood in the sense of
the Euclidean metric [12] [13]. This problem occurs during
the extraction of data, the study of their properties and
compression. Therefore, two paradigms are identified in the
problems of pattern recognition: recognition supervised
based on the classification technique and unsupervised
recognition where we use the clustering technique.
The classical model is based on supervised recognition
methods; these are the probabilistic methods, in particular,
the method based on Bayes formula, adapted for manual
calculations [14]. The solution rules can be derived as
probabilistic identification parameters of belonging of an
object to a particular class (Bayesian method), or as a simple
analytical function (discriminate analysis method). These
methods have certain limitations, such as absence of
reliability, because they are based only on the linear rules
[15].
The modern recognition methods as neural networks
cannot be used without computers. These systems are able
to elaborate the classification and clustering rules, and to be
used to develop intelligent systems for a wide use.
A. The pattern recognition process with the self-organizing
map of Kohonen
Artificial neural networks are widely used for pattern
recognition; these systems use specific algorithms for
classification and clustering of multi-parameter objects
(events, situations, processes). Currently, there are several
ANN paradigms that are used in this task. However, the
models
which
are
mainly
used
are
the
ones
using
competitive learning methods. In particular, we can cite the
SOM [6] and the counter propagation network [7].
The Kohonen network model uses the competitive
learning method. This process brings together similar
objects in same cluster
by reserving the topological
relationships in input data [16] [17]. During learning, the
neurons compete, and for each group of similar objects, a
single winner neuron is defined. The fixed neurons represent
the centers of clusters. The metric used in this operation is
the
Euclidean
distance
between
the
synaptic
weights
vectors, and the input objects vectors.
The learning procedure begins with the normalization of
input data and synaptic weights to reduce the learning time
[11]. This operation is based on the following algebraic
formula:
∑
=
−
=
1
0
2
/
n
j
j
i
i
x
x
x
(1)
Where: xi – the input object component or the vector of
synaptic weights;
n – The number of variables in the vector x.
The
main
learning
algorithm
passes
successively
through a series of iterations, and it relies only on the input
data. During the learning process, it attempts to define for
each group of similar objects a specific neuron qualified as
winner. At the end of this procedure the topologically
adjacent neurons, respond to similar input vectors.
To fix the winner’s neurons, we use the metric of the
Euclidean distance [5] see formula below:
o
k
o
k
∀
−
≤
−
x
w
x
: w
(2)
Subsequently the algorithm performs a correction of
synaptic
weights
to
gradually
minimize
the
distance
between the winning neurons and the input objects. For this
correction we use the following formula [6]:
[
( ])
( ) ( , )
( )
)1
(
w t
y
t h d t
w t
t
w
ij
i
i
ij
ij
−
⋅
+
=
+
α
(3)
where: yi - the value of the output neuron i;
wij (t) and wij (t +1): the synaptic weights during t and (t +
1) iterations.
αi(t): learning rate, this coefficient can have a value
between 0 and 1, and it is calculated using the
following equation:
tei
i
α 0
α
=
(4)
where: i is the iteration number;
t is the iteration rate.
h(d,
t)
:
neighborhood
function,
it
is
written
according to the formula below:






≥
<
=
−
( )
,0
( )
,
, )
(
( ) )
2
(
t
d
t
d
e
d t
h
t
d
δ
δ
δ
(5)
)
(
0
( )
µ
δ
δ
t
e
t
−
=
(6) 
39
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-477-0
IMMM 2016 : The Sixth International Conference on Advances in Information Mining and Management (includes DATASETS 2016)

where: d is the distance between the winner neuron and an
x neuron.
)
(
log
10 δ0
µ
n
=
(7)
where: δ0 is Constant.
n is Iteration rate.
The learning process will be continued until the
stabilization of the SOM, and the results will be presented
as a grid of neurons in a two dimensional space.
However,
the
application
of
the
SOM
can
give
irrelevant results due to the problem of linear dependence
[12]. To avoid these constraints we offer the use of an
enhanced map model that can well classify data even with
the presence of linear patterns. The new model included a
pretreatment method and data optimization.
III.
THE DATA PRETREATMENT METHODS BASED ON A
GEOMETRIC APPROACH
The idea of the proposed method is to use a specific
block of data preprocessing. The processing operation uses
an algorithm based on two typical methods of data analysis:
Principal Component Analysis (PCA) and Iterated Power
(IP) [24] [32]. This combination allows filtering data to
reduce the dimension of the data table and saving the most
informative parameters in each multitude vectors. The new
contribution of this block is the elimination of regularity
between the vectors components and disappearance of the
linear dependence problem, which could prevent this type
of ANN to provide accurate and relevant results.
Initially, we assume that the learning data table is
composed of n rows and p columns; see Figure 1.
Figure 1. Learning data table
In the first step, the algorithm calculates the vector of
main point g. This point is the center of the points cloud in a
space F. See the formula below:
)
,...,
(
1
p
t
x
g = x
(8)
j
i
n
i
j
x
n
x
∑
=
=1
1
(9) 
At the base of the vector g is calculated the data centered
matrix, which is written in terms of X as the following way:
tg
X
Y
−1
=
(10) 
where: gt is the transposed of g, and the term centered
signifies that the means of the variables
are zero.
The centered data matrix Y is used in this step for
calculating the variance-covariance matrix V, which is
written as a function of Y as follows:
n Y Y
V
t
= 1
(11)
where: Yt is the transposed of Y.
The V matrix is presented as follows:












=
p
p
p
S
S
S
S
S
V
.
.
.
.
.
.
.
.
.
.
.
1
21
1
1
where: SKL is the covariance of the variables k and l, and Sk
is the variance of the variable k.
In the last step, in order to develop the correlation
matrix R we must calculate the two diagonal matrices
as a function of V as follows:
)
(
1
2
1/
Diag V
D
S =
(12) 
)
(
1
/ 2
1
/
1
S
S
Diag D
D
=
(13) 
The
matrix
R
is
composed
of
linear
correlation
coefficients between the variables p. It summarizes and
shows the structure of linear dependencies between these
variables. The matrix is symmetric, and the component
values of its diagonal equal 1. R calculates as a function of
V as follows:
SVD S
D
R
1/
1/
=
(14)
where: D1/s is a diagonal matrix, its diagonal is composed
by the values
.
Now it is the time to apply the iterative power method to
search the eigenvectors [32]. These vectors are the rows of
the final matrix of input objects M.
40
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-477-0
IMMM 2016 : The Sixth International Conference on Advances in Information Mining and Management (includes DATASETS 2016)













=
nl
n
l
x
x
x
x
M
.
.
.
.
.
.
.
.
.
.
.
.
1
1
11
The figure below presents the proposed algorithm flowchart.
Figure 2. The proposed algorithm flowchart
In this algorithm, the first seven steps allows to calculate
the correlation matrix R using the ACP method. This matrix
will be used by the iterated power method to search the
eigenvalues and the eigenvectors. The last two steps allows
to develop the reduced final matrix based on the IP
algorithm.
The new matrix calculated by using the proposed method
will present the data source for learning the SOM. The
results are displayed and interpreted using grids of neurons
in two-dimensional space. The functional structure of the
proposed model is shown below in Figure 3.
Figure 3. The functional structure of the proposed SOM model
According to the proposed algorithm diagram, and the
functional structure schema of the proposed model, we can
summarize the learning process of this neural network
system in the following steps:
• The treatment of the initial data by the principal
component
analysis
blocks
PCAB,
to
obtain
the
correlation matrix R.
• At the base of the correlation matrix, the block of the
iterated power IP seeks for eigenvectors that constitute
the rows of the resulting matrix M of input objects.
• The NB blocks perform the normalization of data matrix
M.
• The last step provides the phase of the network learning
based on the pattern data calculated in the precedents
steps.
IV.
OBJECTIVE AND RESEARCH BASE
The objective of research is to establish the advantages
and
disadvantages
of
the
preprocessing
method
of
realizations based on the geometric approach in practical
pattern recognition problems. The research base is the
software model of the SOM developed by us using the
VP.net programming platform. The application includes both
learning algorithms: standard and improved, and it is able to
visualize the results as neural grids in a two-dimensional
space. The interpretation of learning outcomes is based on
the distribution of winner neurons on the map, and the
definition of the cluster which they belong.
A. Application tests and results
To
study
and
analyze
the
proposed
method
of
preliminary data processing for the SOM, we use a typical
problem of pattern recognition: the Recognition of the
African Continent Countries [33]. The objective of this task
is to test the implementation of two SOM models (standard
and modified). The recognition will be performed using the
SOM instruments and tools, including the possibility of
classification and clustering, as well as to view the data on
the neurons grid in a two-dimensional space. The application
begins with the learning step to prepare the knowledge base
necessary for its operation; this database should generate
relevant results. For this typical neural network, the learning
results evaluation is done by using the maps which visualize
the classes and clusters objects (Country). After correct
learning, the SOM can be used to build an intelligent Atlas
map that is able to give the necessary information about the
continent countries. The learning set is composed on 52
vectors, where each one corresponds to a country. Each
vector
is
characterized
by
20
parameters
(geographic
location, language, area, religion, color and flags elements,
etc); see Table I.
The developed software works in two modes, and
supports both models: standard and improved. The learning
results are interpreted by using the two maps (Class map and
Clusters map) and textual data. The maps are drawn as
rectangles grids, of dimension (N × N), corresponding to the
number of output layer neurons. The top left rectangle
presents the first neuron. For a better interpretation of the
learning results, we use a coloring system where the colored
rectangles represent the winner neurons. By click on every
41
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-477-0
IMMM 2016 : The Sixth International Conference on Advances in Information Mining and Management (includes DATASETS 2016)

rectangle the application displays the related information.
The same principle is used with the map clusters; see Figure
4.
TABLE I. THE DESCRIPTIVE DATA OF THE AFRICAN CONTINENT
COUNTRIES
Figure 4. Graphic software interface
B. Test results analysis
In this section, we will try to interpret and analyze the
learning results, in order to reveal the advantages and
disadvantages of each model over the other. We use two
specific metrics to compare the studied models: The first one
is the percentage of recognition which defines a relationship
between the number of winner neurons and the total number
of input learning vectors. The second metric represents the
learning time.
The research results presented in Figures 5 and 6 show
that the standard model has defined 49 winner neurons for
the 52 input objects, that present a recognition percentage
equivalent to 94.23%, and a learning time that reaches
204660 MS. But the improved model has defined 52 winner
neurons for the 52 individuals that present a recognition
percentage reaches 100%, and a learning time not exceeding
105964 MS.
These results affirm that the new model exceeds the
standard model at the level of the recognition relevance and
the learning time. So we can say that with the proposed
method
of
data
pretreatment,
the
map
possess
new
opportunities and able to give good results even with the
existence of linear dependency in the learning data.
Figure 5. Learning Result recognition of African countries (modified model)
Figure 6. Learning result recognition of African countries (modified model)
The visual analysis of maps shows that for standard
model the most of the winner neurons and their clusters are
concentrated in the lower left half of the maps, but for the
improved model these elements are well dispersed over the
map
surface.
This
improvement
in
the
topological
presentation of the results for both types of maps (winners
42
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-477-0
IMMM 2016 : The Sixth International Conference on Advances in Information Mining and Management (includes DATASETS 2016)

and Clusters) is explained by the change in the learning data
structure, including the relationship between objects. This
modification is performed by using data pretreatment
process.
To show the impact of the input data size on the learning
time, and the contribution of the method used in the
improved model, the data set has been distributed to groups
containing different numbers of individuals going from 5
until 52; see Table 2.
The data in Table II shows that the data pretreatment
method has reduced the individual lengths, from 20 to 10
components for each individual. This decrease has allowed
to the improved model reduce the learning time compared to
the standard model.
TABLE
II.
LEARNING RESULTS FOR ALL INDIVIDUAL
GROUPS
Individual
numbers
Standard model
Improved model
Component
numbers
winner
numbers
Iteration rate
Learning time
in MS
Component
numbers
Winner
numbers
iteration rate
Learning time
in MS
5
100
5
487
8076
50
5
495
5384
10
200
10
612
21446
100
10
633
13880
15
300
15
675
37671
150
15
720
24283
20
400
20
787
52239
200
20
774
32367
25
500
25
784
65214
250
25
767
43168
30
600
30
832
66254
300
30
816
56204
35
700
35
861
106513
350
35
890
68892
40
800
40
877
112596
400
40
900
84489
45
900
45
977
148312
450
45
908
92963
52
1040
49
1000
204660
520
52
987
105964
Figure 7. The graphical presentation of the learning results for both SOM
models.
The graph in Figure 7 above shows the relationship
between the learning time spent by both models, and the
number of individuals employed. The graph curves show that
the learning time for both models is increasing in parallel
with the augmentation of the learning multitude size. Thus, it
is observed that the learning process takes less time for the
improved model than the standard model. And the difference
of learning time between both models is enlarged with
growth of the pattern multitudes size.
To sum up, the improved model exceeds the standard by
three parameters: The relevance of the results of the
recognition, learning speed and the dispersion the winner
neurons on the map. The first parameter is justified by the
recognition
percentage,
which
reached
100%
for
the
improved model, but only 94.23% for the standard. The
second parameter is justified by the learning time that
decreases twice using the improved model compared to the
standard model. And the latter parameter is justified by
equitable dispersion of winner neurons and the clusters on
maps (classes and Clusters). So, the results show that the
improved model has solved this task better than the standard
model.
V.
CONCLUSION AND FUTURE WORK
In this work we have tried to improve and implement a
type of neural networks in a task of object recognition, called
the Self-Organizing Map (SOM). In this work we have
justified the choice of the used paradigm, and demonstrated
that its direct application does not provide good results. So
our objectives were determining the ambiguities and the
means of their eliminations. For the first objective, via a
theoretical study and experimental tests we have defined the
problem that prevents the correct learning of network. To
achieve the second objective, we proposed and approved a
data pretreatment method, at the basis of which we have
developed a new functional structure for the improved model
of SOM. The results of the tests show that: The SOM is a
reliable and intelligent tool for solving the recognition
problems, and the method of preliminary processing of the
input data enriches the SOM with new competences. Finally,
the improved model exceeds the standard model in the
accuracy of the results and the learning time. The obtained
results encourage us to improve and apply the ANN in the
various domains of human activities. In future work, firstly
we will apply the new SOM model on the GIS, and then, the
proposed method will be used in order to improve another
ANN paradigm.
REFERENCES
[1]
Deng, Geng and M.C. Ferris, “Neuro-dynamic programming for
fractionated radiotherapy planning,“ Springer Optimization and Its
Applications 2008. p. 47–70.
[2]
M. Roman, Balabin and I. Ekaterina. Lomakina, “Neural network
approach to quantum-chemistry data: Accurate prediction of density
functional theory energies“, J. Chem. Phys 2009. p. 131 (7).
[3]
J.H. Frenster, “Neural Networks for Pattern Recognition in Medical
Diagnosis“,
Annual
International
Conference
in
the
IEEE
Engineering in Medicine and Biology Society 1990. vol. 12, N°. 3. p.
1423-1424.
[4]
Shah-Hosseini, “Binary Tree Time Adaptive Self-Organizing Map“,
Neurocomputing May 2011. 74 (11). p. 1823–1839.
[5]
T. Kohonen, Honkela and Timo, “Kohonen network“, Scholarpedia
2011. Retrieved 2012-09-24.
[6]
T Kohonen, Self-organizing maps, 2nd ed., Springer Verlag, 1997,
pp. 448.
T
i
m
e
i
n
m
s
Individual Numbers
Model
Standard
Model
improved
43
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-477-0
IMMM 2016 : The Sixth International Conference on Advances in Information Mining and Management (includes DATASETS 2016)

[7]
R. Hecht-Nielsen, “Counterpropagation networks Proceedings of the
IEEE First International Conference on Neural Networks“, San Diego
1987. vol.2. pp. 19-32.
[8]
R. Hecht-Nielsen. Applications of counter-propagation networks.
Neural Networks 1988. N.1. pp. 131-139.
[9]
E.V. Gubler. Computational methods of analysis and recognition of
the pathological processes Leningrad: Medicina 1978. pp. 296.
[10] Ultsch and Alfred, “U*-Matrix: A tool to visualize clusters in high
dimensional data“, Department of Computer Science, University of
Marburg 2003. Technical Report N.36.pp. 1-12.
[11] Michel Volle, “Analyse des données“, Economica 4e édition 1997.
[12] Yu. N. Tolstov, “Basics of multidimensional scaling“, M. KDU 2006.
pp. 160 .
[13] Joe. Kim, “factorial, discriminate and cluster analysis“, Edition Ozon
2012. pp. 216.
[14] Hooper and Martyn, “Richard Price, Bayes theorem, and God“,
Significance 10 (1) 2013. pp.36–39.
[15] I.P Gaydyshev. Analysis and data processing: a Special reference
book. St. Petersburg: Peter 2001. pp. 752 .
[16] J. A. Freeman and D. Skapma, “Neural Networks, Algorithms, and
programming
technique“,
Addison-Wesley
publishing
company
1992.
[17] B.Krose and P. van der Smagt, “ An introduction to neural networks“,
The University of Amsterdam. – 1996. – pp. 135.
[18] P D. Wasserman, “Neural Computing: Theory and Practice“, ANZA
Research. 1989. pp. 64.
[19] R.Christian and S.A.Yvan, “Mathématiques et technologie“, Springer
Science+ Bisness 2008. pp. 431.
[20] M. Mc. Cord Nelson. W.T. Illingworth, “A practical guide to neural
nets“, Addison-Wesley Publishing ompany 1991.
[21] E. Davalo and P. Naïm, “Des Réseaux de neurones“, Edition Eyrolles
1993.
[22] A. Deweze, “L’accès en ligne aux bases documentaire“, Collection
MASSON 1983.
[23] J. A. Farrel and A. N. Michel, “ Associative memory via artificial
neural networks“, IEEE control system magazine 1990.
[24] Jérôme Pagès. Analyse factorielle multiple avec R. EDP sciences
Paris, 2013. pp. 253.
[25] C.Guinchat
and
Y.Skouse,
“Guide
pratique
des
techniques
documentaires“, Vol 1, 2. EDICEF 1989.
[26] D. O. Hebb The organization of behavior J. Wiley and Sons NY.
1949:
[27] J Herault and C Jutten, “Réseaux de neurones et traitement de
signal“,. Edition HERMES 1994.
[28] Mirkes and M. Evgeny, “Principal Component Analysis and Self-
Organizing Maps“, Applet University of Leicester 2011.
[29] M. M Glybovets and A. V Olecko, “Artificial Intelligence“, K
Publishing house "KM Academy" 2002. pp. 366.
[30] M. Ezziyyani, E.Haimoudi and H. Fakhori, “Toward a New Approach
to Improve the Classification Accuracy of the Kohonen’s Self-
Organizing Map During Learning Process“, Proc. Technical Program
of International Conference on Advanced Information Technology,
Services and Systems (AIT2S 15), 16-17 Dec. 2015, Settat Morocco
[31] I.P Gaydyshev, “Analysis and data processing“, A Special reference
book. St. Petersburg: Peter 2001. pp. 752.
[32] Catherine Bolley, “Analyse numerique“, Ecole d'ingenieur. Nantes
France 2012. pp.97.
[33] Collins Gem Guide to Flags. Collins Publishers. 1986.
44
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-477-0
IMMM 2016 : The Sixth International Conference on Advances in Information Mining and Management (includes DATASETS 2016)

