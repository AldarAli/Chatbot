A Distributed Algorithm for In-Network Adaptive Estimation Using Incremental
Aggregated Gradient
Wael Bazzi
Electrical Engineering Department
American University in Dubai
Dubai, UAE
wbazzi@aud.edu
Amir Rastegarnia, Azam Khalili
Department of Electrical Engineering
Malayer University
Malayer, 65719-95863, Iran
{a_rastegar,a.khalili}@ieee.org
Saeid Sanei
Department of Computing
University of Surrey
Surrey, GU2 7XH, U.K.
s.sanei@surrey.ac.uk
Abstract—In this paper, we consider the distributed estimation
problem where a set of nodes is deployed to estimate a parameter
of interest when the statistical model (or information) for the
underlying processes is not available, or it varies in time.
Such a scenario appears in many real-world applications e.g.
in sensor networks. The estimation problem can be expressed
mathematically as the minimization of a cost function, which
is the sum of continuously differentiable local cost functions.
The paper aims to develop an iterative, fully distributed and
adaptive solution for the optimization problem. Similar to the
existing Incremental Least Mean-Squares (ILMS) algorithm,
in the proposed algorithm we use steepest-descent method to
generate an approximation of the descent direction at every
node. However, unlike the ILMS, the proposed algorithm uses
the aggregate gradient at each node which is the average of
the previously computed gradients by other nodes. The resultant
algorithm which is called Incremental Aggregated Gradient-LMS
(IAG-LMS) outperforms the ILMS algorithm in terms of the
steady-state error. Moreover, its stability bound (in terms of the
step-size parameter) is also wider than the ILMS algorithm. We
present numerical simulations to support the mentioned claims
and illustrate the results.
Keywords— Adaptive networks; incremental; least mean square;
estimation.
I. INTRODUCTION
Consider the following unconstrained optimization problem
minimize
w
f g(w) =
N

k=1
fk(w),
w ∈ RM
(1)
where every fk : RM → R is a continuously differentiable scalar
function on RM (M ∈ Z ≥ 1). The motivation for considering
optimization models of the form in (1) stems from practical problems
that arise in a variety of applications such as sensor networks, preci-
sion agriculture, environment monitoring, disaster relief management,
smart spaces, target localization, neural networks training, and a
number of medical applications [1],[2]. As an example, in sensor
networks, each fk(w) corresponds to the data collected by the kth
sensor in the network. There are already several useful strategies for
estimation over distributed networks such as consensus strategies and
adaptive networks. In the consensus strategy [3],[4],[5], each node
estimates the source signal (or parameter of interest) and shares the
information with its neighbors. As the number of iterations increases,
the local estimates, computed by the nodes may converge to the
same ﬁnal estimate. The initial implementations suggested for the
consensus strategy relied on the use of two time-scales [6]: one
for the collection of measurements and another to iterate over the
collected data to attain agreement before the process is repeated. The
main problem in the mentioned methods is inability of the network
to undertake continuous learning and optimization [7]. Therefore,
consensus-based algorithms that rely on two time-scales can not be
used in the problems that this paper seeks to provide a solution to
them.
Single time-scale consensus strategies, on the other hand, have
also been developed in the literature [4],[5],[8],[9]. Although this
kind of consensus algorithms can deliver continuous learning, as
it is shown in [10], they suffer from stability problems. Indeed,
consensus networks can become unstable even if all of the nodes
in the network are stable. This problem, along with the need for con-
tinuous learning in distributed networks, motivated the development
of adaptive networks [11],[12]. An adaptive network is a collection of
agents (nodes) that collaborate with each other through in-network
local processing rules in order to estimate and track the parame-
ters of interest [12]. The two major classes of adaptive networks
are incremental strategy [13],[14],[15],[16] and diffusion strategy
[17],[18],[19]. In the incremental mode, a cyclic path through the
network is required, and nodes communicate with neighbors within
this path, while in diffusion mode, each node can communicates with
all of its neighboring nodes. Although incremental networks are less
robust to node and link failures, they do offer excellent estimation
performance since every node uses data from the entire network to
update the local estimate of the desired parameter.
In
the
available
incremental-based
adaptive
networks
such
as [13],[15],[16],[20],[21],[22],[23],[24] the estimation problem is
solved by starting from steepest-descent solution, splitting the global
cost function into N local cost functions (N is the number of nodes),
and applying a suitable local stochastic approximations (such as
LMS-type learning rule) at every node. The resultant algorithms are
able to handle the unavailability or variation of statistical information.
However, they use only one of the N components (local cost
functions) in order to generate an approximate descent direction
[25]. Thus, in this paper we employ the aggregate gradient at each
node which is the average of the N previously computed gradients.
The resultant algorithm outperforms the existing incremental LMS
(ILMS) algorithm in terms of the steady-state error. Moreover, the
stability bound (in terms of the step-size parameter) is also wider
than of the ILMS algorithm. We also present some simulation results
to support the mentioned claims.
Notation: We adopt boldface letters for random quantities. The
symbol ∗ denotes conjugation for scalars and Hermitian transpose
for matrices. y = ﬂoor(x) rounds the elements of x to the nearest
integers less than or equal to x. Moreover, (a)N denotes a modulo N
with representative classes 1, 2, · · · , N. We use the notation ∥x∥2
A =
142
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-347-6
ICWMC 2014 : The Tenth International Conference on Wireless and Mobile Communications

k
Fig. 1. A distributed network with incremental cooperation among the nodes.
The neighbor of node k is deﬁned as its preceding node in Hamilton cycle.
x∗Ax for the weighted square norm of x.
II. PROBLEM STATEMENT
To further explain the problem that this paper seeks to provide a
solution to it, let us consider a set of nodes N = {1, 2, · · · , N}
that are distributed over a domain in space. Each node k ∈ N
communicates with its neighbors (denoted by Nk). The set of
neighbors for node k for incremental cooperation mode is shown
in Fig. 1. At time instant i, node k records the scalar measurement
dk(i) and 1 × M regression data uk,i. The purpose of the network
is to solve the following optimization problem at every node in the
network
minimize
w
N

k=1
E

|dk(i) − uk,iw|2
(2)
Comparing (1) with (2) reveals that
fk(w) = E

|dk(i) − uk,iw|2
Remark 1. In many practical applications, we can assume a linear
regression model between the measurements {dk(i)} and {uk,i}.
According to this model, we have
dk(i) = uk,iwo + vk(i)
(3)
where wo
∈ RM is an unknown parameter and vk(i) is the
observation noise term with variance σ2
v,k. Based on the intended
application, the vector wo may represent different physical quantities,
e.g. location of a target and parameter of an auto-regressive (AR)
model.
The optimal solution for (2) is wo and can be expressed in terms of
the statistics of recorded data {dk(i), uk,i} via the following normal
equation [13],[14]
 N

k=1
Ru,k

wo =
N

k=1
pdu,k
(4)
where
Ru,k = E[u∗
k,iuk,i],
pdu,k = E[dk(i)u∗
k,i]
(5)
Remark 2. The unknown vector wo in the linear model (3) is the
same as the optimal solution in (4).
This solution needs the statistical information {pdu,k, Ru,k}k∈N
to be available at every node, which are not available in many
applications or they may vary in time. In the next section, we
introduce our proposed distributed estimation algorithm which relies
on the incremental aggregated gradient method.
III. PROPOSED ALGORITHM
To solve the unconstrained problem in (1), we start with the
standard steepest-descent method. Let wg
i be the global estimate for
wo at iteration i. Then, using the standard steepest-descent method
we have
wg
i = wg
i−1 − μ

∇wf g(wg
i−1)
∗
(6)
where μ > 0 is a step-size parameter and ∇wf g represents the
complex gradient of the f g(wg). The required gradient in (6) is
given by

∇wf g(wg
i−1)
∗ =
N

k=1
(Ru,kwg
i−1 − pdu,k)
(7)
Substituting (7) into (6) yields the steepest-descent solution as
wg
i = wg
i−1 + μ
N

k=1
(pdu,k − Ru,kwg
i−1)
(8)
As we mentioned before, this is not a practical solution as it needs
the statistical information {pdu,k, Ru,k}k∈N . This issue can be
addressed by replacing the statistical averages by time averages
assuming that the process is ergodic, i.e.
Ru,k ≈ u∗
k,iuk,i,
pdu,k ≈ dk(i)u∗
k,i
(9)
Replacing (9) in (8) yields
wg
i = wg
i−1 + μ
N

k=1
u∗
k,i(dk(i) − uk,iwg
i−1)
(10)
The above recursive equation can be implemented in a distributed
manner by splitting the update equation into N steps and resorting
to the incremental cooperation which is established among the nodes.
By doing the above steps, the ILMS algorithm is obtained which can
be written as follows
	
w1,i ← wN,i−1
wk,i = wk−1,i + μk u∗
k,i(dk(i) − uk,iwk−1,i)
(11)
where wk,i is a local estimate of wo at node k and time (iteration)
i. Note that we have
wN,i = wg
i
It is shown in [11], [13] that as i → ∞, we have wk,i → wo in the
mean, for every node k, and for an appropriately chosen set of step
sizes {μk : k = 1, . . . , N}.
It is clear from (6) that the ILMS algorithm uses only the available
information at node k in order to generate an approximate for the
steepest-descent direction. We can improve the approximate direction
if we use the average of the N previously computed gradients [25].
Thus, we use the concept of aggregated gradient to develop our
proposed algorithm. To begin with, let us deﬁne the sequence of
{xj}, j = 1, 2, · · · as
xj ≜ wk,i,
j = (i − 1) × N + k
(12)
Equivalently, we have
k = (j)N,
and i = ﬂoor(j/N) + 1
(13)
Using (12), we can deﬁne the aggregated gradient at iteration i as
gk,i =

 N−1

ℓ=0
∇wf(k−ℓ)N

xk−ℓ
∗
(14)
Note the right hand side of (14) is a function of i since according to
(12) xk−ℓ can be expressed in terms of wk,i.
143
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-347-6
ICWMC 2014 : The Tenth International Conference on Wireless and Mobile Communications

Remark 3. To calculate the aggregated gradient in (14) we need N
initial points x1, x2, · · · , xN which according to (12) can be selected
as
x1 = w1,1,
x2 = w2,1,
· · ·
xN = wN,1
That is why we have deﬁned gk,i for i ≥ 2. Possible initialization
strategies include setting x1 = x2 = · · · = xN.
Now, we can modify the standard steepest-descent (14) using (6)
as follows
xj = xj−1 − μ 1
N gk−1,i,
for j ≥ N + 1
(15)
or in terms of wk,i as
wk,i = wk−1,i − μ 1
N gk−1,i
(16)
where we add the factor 1/N in order to make gk,i comparable to
the one used in the standard incremental gradient method [25]. We
can also rewrite (14) in an equivalent form as
gk,i = gk−1,i −

∇wf(k)N (xk−L)
∗ +

∇wf(k)N (xk)
∗
(17)
As we will discuss later, this form enables us to implement equations
(15) and (17) in a distributed manner.
Remark 4. Note that for i ≥ 2 we can merge the equations (16)
and (17) to obtain an equivalent update equation as
wk,i = wk−1,i − μ 1
N

 N−1

ℓ=0
∇wf(k−ℓ)N

wk′,i′∗
(18)
where
k′ = (k − 1 − ℓ)N,
and i′ = ﬂoor((k − 1 − ℓ)/N) + 1.
Although the update equation given by (16) is a distributed solution
for (1), however, it is not an adaptive solution because we still need
to use the second-order moments {pdu,k, Ru,k} to evaluate (16). To
obtain an adaptive solution, we replace the mentioned moments with
the local instantaneous approximations as
 ∇wfk(w)
∗ = −(u∗
k,i(dk(i) − uk,iw))
(19)
where we used ∇wfk(w) to denote the approximate gradient. Using
(19), we can rewrite
wk,i = wk−1,i − μ 1
N gk−1,i
(20a)
gk,i = gk−1,i −
 ∇f(k)N (xk−L)
∗ +
 ∇f(k)N (xk)
∗
(20b)
We can implement (20) in a fully distributed manner as follows: at
iteration i, upon receiving x(i−1)×N+k−1 = wk−1,i and gk−1,i from
node k−1, node k updates the local estimate wk,i and the aggregate
estimate gk,i according to (20) and sends them to the next node k+1.
The pseudo code of the proposed algorithm (IAG-LMS algorithm) is
shown in Algorithm. 1.
Algorithm 1: The pseudo code of the IAG-LMS algorithm.
Initialize x1, x2, · · · , xN = w1,1, w2,1, · · · , wN,1 = 0
foreach i = 1, 2, · · · do
foreach k = 1, 2, ..., N · · · do
receive wk−1,i and gk−1,i from node k − 1
update wk,i according to (20a)
update gk,i according to (20b)
send wk,i and gk,i to the next node (node k + 1).
end
end
2
4
6
8
10
12
14
6.5
7
7.5
8
8.5
node k
Tr(Ru,k)
2
4
6
8
10
12
14
0
0.05
0.1
node k
σ2
v,k
Fig. 2.
Node proﬁle Tr(Ru,k) (up) and σ2
v,k (down).
IV. SIMULATION RESULTS
In this section, we present some simulation results to evaluate the
performance of the proposed algorithm. To this end, we assume a
network with N = 15 nodes, where the nodes are connected via
a ring topology. The regressors uk,i are generated as independent
realizations of a Gaussian distribution with a covariance matrix Ru,k
where the eigenvalue spread for every Ru,k is 2. The measurement
data dk(i) at each node k is generated by using the data model (3)
where the parameter wo is chosen to be [1 1 1 1 1]T . The observation
noise vk(i) in (3) is drawn from a Gaussian distribution with variance
σ2
v,k ∈ (0, 0.1). Fig. 2 shows the node proﬁle including the trace of
every covariance matrix Ru,k and σ2
v,k.
We consider the mean-square deviation as the performance metric
which is deﬁned at node k as
MSDk = E

∥wo − wk,i∥2
(21)
To evaluate the performance of ILMS and the proposed algorithm,
we use network MSD that is given by
MSD = 1
N
N

k=1
MSDk
(22)
Fig. 3 shows the learning curve, in terms of network MSD, for the
ILMS algorithm and the proposed algorithm. For both ILMS and the
proposed algorithm we select μ = 0.1. The results are obtained by
averaging over 100 independent runs. We observe that the proposed
algorithm outperforms the ILMS algorithm in terms of the steady-
state error at the expense of a slight increase in the computations
(due to the need for updating gk,i) and communications (due to the
need for sending gk,i). In Fig. 4, we have plotted the trajectory of
the proposed algorithm. The dash straight line corresponds to the
wo(1) = 1. It is seen that the proposed algorithm rapidly converges
to the desired parameter.
When we increase the step-size value, we observe that the ILMS
algorithm diverges and can not provide an acceptable estimate for
wo. This behavior does not occur for the proposed algorithm as it is
shown in Fig. 5, where we have plotted the network MSD for both
algorithms for a bigger step-size μ = 0.25). Fig. 6 shows steady-state
values of the network MSD for both algorithms in terms of the step-
size value. The steady-state values are obtained by averaging over 100
runs 50 time samples after the convergence of algorithms. We observe
that the stability range for the proposed algorithm is wider than the
ILMS algorithm, which leads to a more robust implementation.
144
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-347-6
ICWMC 2014 : The Tenth International Conference on Wireless and Mobile Communications

0
200
400
600
800
1000
−25
−20
−15
−10
−5
0
5
10
iteration
MSD (dB)
ILMS
IAG−LMS
Fig. 3.
Learning curve for ILMS and the proposed algorithm in terms of
network MSD (μ = 0.1).
0
200
400
600
800
1000
0
0.2
0.4
0.6
0.8
1
1.2
1.4
iteration
Magnitude
580
600
620
0.98
1
1.02
wo(1)
IAG-LMS
Fig. 4.
The trajectory of the proposed algorithm. The dash straight line
corresponds to wo(1) = 1, (μ = 0.1).
V. CONCLUSIONS AND FUTURE WORK
In this paper, we proposed an incremental adaptive network based
on the aggregated gradient method. The main advantages of the
proposed algorithm, (in comparison with the existing ILMS algo-
rithm) are its steady-state performance error and its wider stability
range. However, the cost of this improvement is a slight increase in
computation and communication per integration per node. In addition,
the convergence rate for the proposed algorithm is slower than ILMS
algorithm, which in turn affects its tracking performance spatially
in fast-changing environment. We presented some simulation results
to support our claims in this paper. This work can be extended by
considering the effects of noisy links, which is a more practical
assumption than the ideal links. Moreover, we can further improve
the performance of the proposed algorithm with tuning the step-size
parameter at every node according to its measurement quality. Our
future work will address the mentioned issues.
REFERENCES
[1] N. Bulusu and S. Jha, Wireless Sensor Networks: A Systems Perspective.
Artech House, 2005.
0
200
400
600
800
1000
−50
0
50
100
150
200
250
300
350
iteration
MSD (dB)
ILMS
IAG−LMS
Fig. 5.
Learning curve for ILMS and the proposed algorithm in terms of
network MSD (μ = 0.25).
0.02
0.04
0.06
0.08
0.1
0.12
0.14
−40
−20
0
20
40
60
80
100
120
140
160
μ
MSD (dB)
ILMS
IAG−LMS
Fig. 6.
Steady-state performances of the ILMS algorithm and the proposed
algorithm in terms of the step-size parameter.
[2] I. F. Akyildiz, T. Melodia, and K. R. Chowdury, “Wireless multimedia
sensor networks: A survey,” IEEE Wireless Commun. Mag., vol. 14,
no. 6, pp. 32–39, 2007.
[3] S. Boyd, A. Ghosh, B. Prabhakar, and D. Shah, “Randomized gossip
algorithms,” IEEE/ACM Trans. Netw., vol. 14, no. SI, pp. 2508–2530,
June 2006.
[4] I. Schizas, G. Mateos, and G. Giannakis, “Distributed LMS for
consensus-based in-network adaptive processing,” Signal Processing,
IEEE Transactions on, vol. 57, no. 6, pp. 2365–2382, 2009.
[5] S. Kar and J. M. F. Moura, “Convergence rate analysis of distributed
gossip (linear parameter) estimation: Fundamental limits and tradeoffs,”
Selected Topics in Signal Processing, IEEE Journal of, vol. 5, no. 4, pp.
674–690, 2011.
[6] L. Xiao, S. Boyd, and S. Lai, “A space-time diffusion scheme for peer-
to-peer least-squares estimation,” in Information Processing in Sensor
Networks, 2006. IPSN 2006. The Fifth International Conference on,
2006, pp. 168–176.
[7] J. Chen and A. H. Sayed, “Diffusion adaptation strategies for distributed
optimization and learning over networks,” IEEE Trans. Signal Process.,
vol. 60, no. 8, pp. 4289–4305, August 2012.
[8] A. Nedic and A. Ozdaglar, Cooperative distributed multi-agent opti-
mization, Y. Eldar and D. Palomar, Eds.
Cambridge, U. K.: Cambridge
Univ. Press, 2009.
145
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-347-6
ICWMC 2014 : The Tenth International Conference on Wireless and Mobile Communications

[9] J. Tsitsiklis, D. Bertsekas, and M. Athans, “Distributed asynchronous de-
terministic and stochastic gradient optimization algorithms,” Automatic
Control, IEEE Transactions on, vol. 31, no. 9, pp. 803–812, 1986.
[10] S.-Y. Tu and A. Sayed, “Diffusion strategies outperform consensus
strategies for distributed estimation over adaptive networks,” Signal
Processing, IEEE Transactions on, vol. 60, no. 12, pp. 6217–6234, 2012.
[11] A. H. Sayed, “Diffusion adaptation over networks,” in E-Reference
Signal Processing, R. Chellapa and S. Theodoridis, Eds.
Elsevier,
2013, to appear.
[12] A. H. Sayed, S.-Y. Tu, J. Chen, X. Zhao, and Z. Towﬁc, “Diffusion
strategies for adaptation and learning over networks,” IEEE Signal
Processing Magazine, vol. 30, no. 3, pp. 155–171, May 2013.
[13] C. G. Lopes and A. H. Sayed, “Incremental adaptive strategies over
distributed networks,” IEEE Trans. Signal Processing, vol. 55, no. 8,
pp. 4064–4077, August 2007.
[14] A. H. Sayed and C. G. Lopes, “Distributed recursive least-squares
strategies over adaptive networks,” in Proc. Asilomar Conf. Signals,
Systems, Computers, Monterey, CA, October 2006, pp. 233–237.
[15] N. Takahashi and I. Yamada, “Incremental adaptive ﬁltering over dis-
tributed networks using parallel projection onto hyperslabs,” IEICE
Technical Report, vol. 108, pp. 17–22, 2008.
[16] F. Cattivelli and A. H. Sayed, “Analysis of spatial and incremental LMS
processing for distributed estimation,” IEEE Trans. on Signal Process.,
vol. 59, no. 4, pp. 1465–1480, April 2011.
[17] C. G. Lopes and A. H. Sayed, “Diffusion least-mean squares over
adaptive networks: Formulation and performance analysis,” IEEE Trans.
on Signal Process., vol. 56, no. 7, pp. 3122–3136, July 2008.
[18] F. S. Cattivelli, C. G. Lopes, and A. H. Sayed, “Diffusion recursive
least-squares for distributed estimation over adaptive networks,” IEEE
Trans. on Signal Process., vol. 56, no. 5, pp. 1865–1877, May 2008.
[19] F. S. Cattivelli and A. H. Sayed, “Multilevel diffusion adaptive net-
works,” in Proc. IEEE Int. Conf. Acoustics, Speech, Signal Processing,
Taipei, Taiwan, April 2009.
[20] C. G. Lopes and A. H. Sayed, “Distributed processing over adaptive
networks,” in Proc. Adaptive Sensor Array Processing Workshop.
Lex-
ington, MAMIT Lincoln Lab., Lexington, MA: MIT Lincoln Lab, June
2006.
[21] L. Li, J. A. Chambers, C. G. Lopes, and A. H. Sayed, “Distributed
estimation over an adaptive incremental network based on the afﬁne
projection algorithm,” IEEE Trans. Signal Process., vol. 58, no. 1, pp.
151–164, January 2010.
[22] C. G. Lopes and A. H. Sayed, “Randomized incremental protocols over
adaptive networks,” in Proc. IEEE Int. Conf. Acoustics, Speech, Signal
Processing (ICASSP), Dallas, TX, March 2010, pp. 3514–3517.
[23] A. Khalili, M. A. Tinati, and A. Rastegarnia, “Amplify-and-forward
scheme in incremental lms adaptive network with noisy links: Minimum
transmission power design,” AEU - International Journal of Electronics
and Communications, vol. 66, no. 3, pp. 262 – 265, 2012.
[24] A. Rastegarnia and A. Khalili, “Incorporating observation quality infor-
mation into the incremental lms adaptive networks,” Arabian Journal
for Science and Engineering, vol. 39, no. 2, pp. 987–995, 2014.
[25] D. Blatt, A. Hero, and H. Gauchman, “A convergent incremental gradient
method with a constant step size,” SIAM Journal on Optimization,
vol. 18, no. 1, pp. 29–51, 2007.
146
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-347-6
ICWMC 2014 : The Tenth International Conference on Wireless and Mobile Communications

