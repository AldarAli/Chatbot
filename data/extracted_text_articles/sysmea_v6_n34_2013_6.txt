Static Preprocessing for Automated Structural Testing of Simulink Models
Benjamin Wilmes
Berlin Institute of Technology
Daimler Center for Automotive IT Innovations (DCAITI)
Berlin, Germany
E-Mail: benjamin.wilmes@dcaiti.com
Abstract—A feasible automation of testing software models
would be of great beneﬁt to industry, given the advantages
of early testing as part of an efﬁcient quality assurance
process. Despite search-based testing having been applied with
promising results to automate structural test data generation
for Simulink models, the approach lacks efﬁciency. This paper
features three static-analysis-based preprocessing techniques
which are carried out prior to an automated test data search, to
mitigate this efﬁciency problem. The ﬁrst technique identiﬁes
unsatisﬁable coverage goals by analyzing the ranges of model
internal signals and excludes them from the search. The second
preprocessing technique aims at reducing the search space
by analyzing which model inputs actually require stimulation
in order to reach a certain model state. A third technique
sequences the coverage-goal-related search processes in order
to maximize collateral coverage and reduce the size of the
generated test suite. These additional techniques are able to
make the search-based approach considerably more efﬁcient,
as results of a case study with our search-based testing tool
TASMO, applied to industrial Simulink models, reveal.
Keywords-Search-Based Testing, Static Preprocessing, Auto-
motive Industry, Simulink.
I. INTRODUCTION
First of all, note that this paper is an extended version of
a previous publication [1]. It contains further details on the
topic, additional illustrations and an extended case study.
In many of today’s application areas, the creation of
embedded controller software relies on model-based design
paradigms. Various industries, such as the automotive in-
dustry, use Matlab Simulink (SL) [2] as the standard tool
to create and simulate dynamic models along with a code
generator, for instance TargetLink (TL) [3], in order to
automatically derive software code from such models.
As they are normally the ﬁrst executable artifacts within
software development processes, SL models play an im-
portant role in testing theory. Industrial testing practice,
however, usually focuses on higher-level development arti-
facts, like testing integrated software or systems as a whole.
This discrepancy has both traditional and practical reasons.
On one hand, current testing processes still require further
adaptation to the model-based paradigm. On the other hand,
companies are pressed for time in product development and
must deal with an increasing demand for innovation. This
can lead to a disregard for low-level tests and model tests in
particular. Yet focusing too one-sided on tests of higher-level
software or system artifacts poses the risk that faults may be
found late in the process, which can lead to increased costs,
that some faults can hardly be discovered on higher levels,
or that certain functionality is not tested at all.
Thus, automating the testing of software models is highly
desirable in industrial practice, particularly with regard to
what is normally the most time-consuming testing activity:
the selection of adequate test cases in the form of model
input values (test data). Search-based testing is a dynamic
approach to automating this task. It transforms the test
data ﬁnding problem into an optimization problem and
utilizes meta-heuristic search techniques like evolutionary
algorithms to solve it. Search-based testing [4] has been
studied widely in the past and has also been applied success-
fully for testing industrial-sized software systems [5]. Both
structural (white-box) and functional (black-box) testing can
be automated with the search-based approach.
Zhan and Clark [6], as well as Windisch [7], applied
search-based testing to structural test data generation for
SL models. The work of Windisch not only supports
Stateﬂow (SF) diagrams (which are used fairly often in
SL models), but also makes use of an advanced signal
generation approach in order to generate realistic test data.
While his approach has led to promising results in general,
outperforming commercial tools in terms of effectiveness, it
lacks efﬁciency when applied to larger models. Furthermore,
it shows difﬁculties targeting Boolean states and tackling
complex dependencies within models [8].
The work presented in this paper is a ﬁrst step toward
overcoming some of these shortcomings by exploiting static
model analysis techniques before the search process actually
starts. These techniques will therefore be referred to as static
preprocessing techniques. Our scope is test data generation
for TL-compliant Simulink models. Our primary aim is
to improve the efﬁciency of the approach by Windisch.
While our work is targeted at SL models, we believe that
the presented concepts are generally portable to similar
data-ﬂow diagram types and, at least conceptually, even to
structural testing of code.
This paper is structured as follows: Section II introduces
search-based testing and its application to structural test-
ing of SL models. Section III presents our approach to
supporting the search-based technique by integrating three
310
International Journal on Advances in Systems and Measurements, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/systems_and_measurements/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

preprocessing techniques. In detail, we present a signal range
analysis (Section III-A) which captures range information
of internal model signals and, in this way, allows partial
detection of unreachable model states. We then propose
a signal dependency analysis for the purpose of search
space reduction (Section III-B). Our third contribution is
a sequencing approach which derives an order in which
coverage goals of a structural test are processed by the search
(Section III-C). Insight into our tool prototype and a case
study are provided in Section IV and V. An overview of
related work in the ﬁeld of structural test data generation
for SL models is provided in Section VI, followed by our
conclusions in Section VII.
II. BACKGROUND
A. Search-Based Structural Testing
Initiated in the 1970s by Miller and Spooner [9] and
revived by Korel in the 1990s [10], search-based testing [4]
and its application to industrial cases has been extensively
studied in the last decade.
The general idea of the search-based approach is pretty
simple: a test data ﬁnding problem (which surely differs in
its nature depending on the kind of testing) is transformed
into an optimization problem by deﬁning a cost function,
called ﬁtness function. This function rates any test data
generated by the deployed search algorithm - usually based
on information gained from executing the test object with
it. The rating must express, in as much detail as possible,
how far the test data is from being the desired test data.
An iteratively working search algorithm uses these ﬁtness
ratings to distinguish good test data from bad, and based
on this, generates new test data in each iterative cycle. This
fully automated procedure continues until test data satisfying
the search goal(s) has been found, that is, if a ﬁtness rating
has reached a certain threshold or until a predeﬁned number
of algorithm iterations have been performed. Various search
algorithms have been used in the past. Due to their strength
in handling diverse search spaces, evolutionary algorithms,
like genetic algorithms, were often preferred [11].
Applied to functional testing, the search-based approach
is generally utilized to search for violations of a requirement.
In this case, a sophisticated ﬁtness function needs to be
designed manually when following the standard approach.
However, when applied to structural test data generation,
ﬁtness functions can be derived completely automatically
from the inner structure of the program to be tested.
Structural testing is commonly aimed at deriving test data
based on the internal structural elements of the test object,
e.g., creating a set of test data which executes all statements
of a code function, or all paths in the corresponding control
ﬂow graph. Industrial standards like ISO 26262 even demand
the consideration of coverage metrics when performing low-
level tests. Search-based testing can automate this task for
various coverage criteria (like branch or condition coverage)
by treating each structural element requiring coverage as
a separate search goal, called a coverage goal (CG/CGs
in plural). Each CG is accompanied by a speciﬁc ﬁtness
function. Wegener et al. [12] recommend composing the
ﬁtness function of the following two metrics: approach level
(positive integer value) and branch distance (real value from
0 to 1). Given a test data’s execution path in the control
ﬂow graph of the test object’s code, the approach level
describes the smallest number of branch nodes between
the structural element to be covered and any covered path
element. To create a more detailed and differing rating of
generated test cases, the branch distance reﬂects how far the
test object’s execution has been from taking the opposite
decision at the covered branch node, which is the closest
to the structural element to be covered. This approach is
suitable for structural testing of program code, like C or
Java code.
B. Application to Dynamic Systems
As model-based development is now established in the au-
tomotive industry and practitioners have noticed opportuni-
ties to test earlier, Windisch [7] as well as Zhan and Clark [6]
have transferred the idea of search-based structural testing
from code to model level. For SL models, structural coverage
criteria similar to the ones known from code testing exist
and are commonly accepted in practice. Before addressing
the challenges of applying search-based test data generation
to SL models, we give a brief introduction to SL. SL is
a graphical data-ﬂow language for specifying the behavior
of dynamic systems. Syntactically, a SL model consists of
functional blocks and lines connecting them, while most of
the blocks are equipped with one or more input ports as well
as output ports. The semantics of such a model results from
the composed functionalities of the involved block types,
e.g., sum blocks, relational blocks or delay functions. In
addition, event-driven or state-based functionalities can be
realized within SL models using SF blocks. A SF block
contains an editable Statechart-like automaton.
When applying search-based structural testing to SL mod-
els, two fundamental differences compared to its application
on code level arise. First, SL models describe time and
state dependent processes. Inputs and outputs of SL models,
as well as block-connecting lines, are in fact signals. In
order to enable reaching all system states, an execution with
input sequences (signals) instead of single input values is
required. Such complex test data can only be generated
with common search algorithms by compressing the data
structure, as done by Windisch [13]. His segment-based
signal generation approach also considers the necessity for
being able to specify the test data signals to be generated
(e.g., amplitude bounds and signal characteristic, like wave
or impulse form). Second, the aforementioned ﬁtness func-
tion approach cannot be fully adopted since SL models are
data ﬂow-oriented. There are no execution paths because
311
International Journal on Advances in Systems and Measurements, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/systems_and_measurements/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

CG 2
. . .
Real Test Data
(Input Signals)
Abstract Test Data 
(Signal Segments)
A:  5.0
W: *
T:  *
A:  4.0
W: 0.5
T:  st
A:  3.2
W: 1.2
T:  sin
Signal 
Generation
Evaluation of Generated Test Data
Model Execution 
& Signal Logging
Fitness 
Calculation
Distance to CG 
Condition/Formula
Coverage Goal 1
Search
Algorithm
0.56875
CG 3
Figure 1.
Automated search for test sequences, which fulﬁll coverage
goals derived from the model under test.
the execution of a SL model involves the execution of
every included block. Hence, a CG-related ﬁtness function
addresses only distances to the desired values of one or more
model internal signals. For CGs in SF diagrams however, a
bipartite ﬁtness approach is possible [8]. Regardless of this,
a ﬁtness function has to operate on a sequence (signal) of
distance values since distance calculations are done for every
time step of the model’s execution. Thus, the minimum value
of a ﬁtness signal is usually taken as ﬁnal ﬁtness value.
Figure 1 visualizes the overall work ﬂow of applying
search-based test data generation to structural testing of SL
models as described.
C. Deﬁciencies and Potential
Search-based structural testing has been applied success-
fully to real (proprietary) SL models originating from devel-
opment projects at Daimler, e.g., a model of a windscreen
wiper controller [8]. Compared to purely randomized test
data generation of similar complexity, the search-based ap-
proach results in signiﬁcantly higher model coverage. Even
in comparison with a commercial tool, the search-based
approach performs more effectively.
Despite promising results, the approach lacks efﬁciency.
In general, the overall runtime of the search processes for
achieving maximal model coverage increases with the size
of the model under test. Similar experiences have been made
with code-level search-based structural testing [14]. Since a
single automotive SL model is often hundreds of blocks in
size, and because a test data generation process of more than
a couple of hours is undesirable, improving efﬁciency of the
search-based approach is vital. The following shortcomings
of the search-based testing approach for SL, as proposed by
Windisch [8], were identiﬁed as contributors to efﬁciency
problems. They will be addressed by the work presented in
this paper.
1) Even if a model is implementing its desired functional-
ity entirely correctly, there are often model states that
are simply unreachable. A search for test data that
results in such model states cannot possibly succeed.
However, the search technique is not aware of this
and carries out a pointless and time-consuming search
process.
2) Narrowing the size of the search space, i.e., the space
of all possible input data, is crucial for how easy
or difﬁcult it is for a search to succeed. Industrial-
sized SL models usually have many inputs for which
suitable signals need to be found. The size of the
overall search space varies with the number of model
inputs. Reaching certain model states, however, is
often independent of the stimulation of some of the
model’s inputs.
3) Targeting coverage criteria implies having to reach
various CGs, between which, in fact, logical depen-
dancies exist. The coverage of a CG often implies the
coverage of other CGs (collateral coverage). In princi-
ple, each CG requires a separate search. Those search
processes, however, are carried out in an uncontrolled
order, regardless of how much collateral coverage they
might cause.
There are two further technical problems leading to a
lack of efﬁciency which are only partially addressed by the
work presented in this paper. First, the structural test data
generation is performed black-box-like, which means that
the model is fed with input values on one end while some
distances for calculating ﬁtness are measured at some other
point in the model. Any structural information between is not
considered, thus the search might be blind to complicated
dependencies in the model (cf. [15]). Second, when targeting
a Boolean state in a model, a suitable ﬁtness function is hard
to ﬁnd since a simple true or false rating inadequately leads a
search [8]. Zhan and Clark suggest a technique called tracing
and deducing [16], which mitigates this problem in certain
cases, but fails in instances where the Boolean problem
312
International Journal on Advances in Systems and Measurements, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/systems_and_measurements/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

             [0,1] : 1
            [0,4] : 0.5
           [5,20] : 0.2
1
2
3
1
z
[0,300]:1
[0,300]:1
Signal Length: 30 sec
Signal Resolution / Simulation Sample Time: 0.1
User-
Specification
of Model 
Inputs
+
>
CG:
==1
[0]
*
[0,80]
Loop Range Analysis
Semantic-Driven Range Propagation from Model Inputs to Outputs
400
Figure 2.
Example of how determining the ranges of a model’s internal signals based on input speciﬁcation and block semantics works.
cannot be traced back in the model to a non-Boolean one.
As a whole, we aim to improve the search-based approach
for structural testing of SL models so that it performs accept-
ably and reliably in industrial development environments. To
this end, we turn our attention to testing of TL-compliant
SL models since the code generator TL is widely used in
industrial practice. TL extends SL by offering additional
block types, but also makes restrictions on the usage of
certain SL constructs like block types. Nevertheless, it is
possible to adapt our ideas to pure SL usage.
III. STATIC PREPROCESSING
We distinguish between techniques which support search-
based structural testing (a) before the CG-related search pro-
cesses, (b) between the different search processes, (c) during
each search process, and (d) after the search processes are
done. In the following sections, three techniques belonging
to category (a) are presented. Apart from making use of an
input speciﬁcation and choice of coverage criteria provided
by the user, all three techniques are fully automatic.
A. Signal Interval Analysis
In structural testing practice, achieving 100% coverage is
often not possible. One reason lies in the semantic construc-
tions precluding certain states or signal values. It might also
be that a tester speciﬁes the test data to be generated in such
a way that it prevents certain CGs from being satisﬁable.
Also, SL models might be designed variably, e.g., contain a
constant block with a variable value. When such variability
is bound during execution, e.g., via conﬁguration ﬁle, certain
model states may be unreachable.
CGs referring to unreachable states worsen the over-
all runtime and undermine the efﬁciency of search-based
structural test data generation since time-consuming search
processes are carried out without any hope of ﬁnding desired
test data. Therefore, we propose two techniques contributing
to automatic identiﬁcation of unreachable CGs. The ﬁrst one
is an interval analysis, which determines the range within
which the values of every internal model signal are. If a
signal range is in conﬂict with the range or value required by
a CG, this CG is unsatisﬁable. We use interval analysis since
other approaches to detect infeasibility, such as constraint
solving or theorem proving [17], are currently not scalable
enough for the complex equations constituted by industrial-
sized SL models. The second technique is an analysis of
dependencies between CGs. Since this technique is mainly
used for another purpose, it is presented in Section III-C.
The code generator TL, as well as the latest version
of SL, are capable of analyzing signal ranges in order to
perform code optimizations and improve scaling or data type
selection, respectively. While those range analysis features
are limited (e.g., determining ranges of signals that are
involved in loops is not possible without user interaction)
our signal interval analysis (SIA) makes use of an input
signal speciﬁcation in order to overcome such limitations
and derive more precise ranges.
As mentioned in Section II-B, a tester who uses the
search-based approach for testing SL models, as outlined by
Windisch, is asked to specify the test data to be generated
ﬁrst. This involves establishing (a) the range boundaries
and step size of each model input, as well as deﬁning
(b) a common length (in seconds) and sample rate for all
input signals - sigLength, sigRes∈R+, with sigLength being
a multiple of sigRes. SIA starts with information (a) for
the model’s input signals and propagates the corresponding
signal ranges of the form [x, y]:q, where q (optional) is
the step size, through the whole model. For every model
313
International Journal on Advances in Systems and Measurements, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/systems_and_measurements/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

internal signal si, we keep a list of consecutive time in-
tervals (time phases) of the form [ta, tb], where ta, tb∈N0
and 0≤ta≤tb≤(sigLength/sigRes). Every time interval goes
along with an interval set I(si)[ta,tb] that contains the actual
ranges. We use interval sets instead of a single interval per
signal, or per time phase, in order to derive more accurate
range information - as suggested by Wang et al. [18].
The propagation technique processes all model blocks
in a predetermined order, which is equivalent to the block
execution order that SL calculates for running a simulation.
Each propagation step is based on the semantics of a block
and the ranges of its incoming signals. The result of such a
step are ranges (intervals) for the outgoing signals of the
block. In this context, we derived interval semantics for
each block type of TL-compliant SL models using basic
concepts of interval arithmetic [19]. This approach can also
be described as a form of abstract interpretation.
Example:
The
original
semantics
of
a
Sum
block
with two incoming signals s1 and s2 and the outgo-
ing signal s3 is s3,t
= s1,t+s2,t, where t is a time
step. Given I(s1)[t1,t2] = {[a1,1, b1,1], ..., [a1,n, b1,n]} and
I(s2)[t3,t4]
=
{[a2,1, b2,1], ..., [a2,m, b2,m]} with equal
or overlapping time intervals, basic interval arithmetic
is used to obtain the corresponding interval set for
s3: I(s3)[max(t1,t3),min(t2,t4)]
=
{[a1,1+a2,1, b1,1+b2,1],
..., [a1,1+a2,m, b1,1+b2,m], ..., [a1,n+a2,1, b1,n+b2,1], ...,
[a1,n+a2,m, b1,n+b2,m]}. One could, however, calculate
only one resulting interval with the overall minimum and
maximum boundary values of the intervals listed above in
I(s3). Since this would lead to a loss of precision, we avoid
this approach. In order to still keep interval sets small, we
developed an algorithm that merges intervals of the very
same set in a suitable way, where possible.
Figure 2 graphically depicts the overall procedure with
the aid of a simple example. Note that the model contains
a loop, initiated by a delay block with an initial value of
0. The standard propagation procedure would be unable to
continue here since ranges are not available for all incoming
signals of the sum block. A simple, yet imprecise solution
is to set the range of the sum block’s outgoing signal to the
minimum and maximum values of the signal’s data type. A
more precise solution, however, is to use the information (b)
of the signal speciﬁcation in order to run a loop analysis.
From length and sample rate, the number of loop iterations
is derivable. Starting with the initial value of the delay block
a static analysis of the loop iterations is performed, resulting
in time-related range information. In order to keep the ﬁnal
results clean and minimal, as mentioned before, each signal’s
ranges as well as the time phases of ranges are combined, if
possible, in each iteration of the loop analysis. Note that the
ranges in Figure 2 are displayed simpliﬁed and summarized,
omitting time intervals and the details of their interval sets.
Using range propagation and loop analysis in combina-
tion, SIA is capable of determining the ranges of all signals
contained in the model under test. In cases of blocks with
unknown semantics or unsupported blocks, the minimum
and maximum values of the outgoing signal’s data type are
used. In the end, the results of SIA are used to assess whether
each CG’s associated formula is unsatisﬁable - such as the
CG in Figure 2. In addition to unsatisﬁable CGs, SIA can
also help in identifying Boolean signals or discrete signals
with only a few possible different values. As described in
Section II-C, CGs related to such signals can be problematic
for the search-based approach.
B. Signal Dependency Analysis
By default, the search algorithm generates test data for all
of the model’s inputs when targeting a CG. However, there
are usually CGs whose satisfaction is, in fact, independent
of the stimulation of certain model inputs. By not taking this
into account, the search space is unnecessarily large, which
makes it more difﬁcult for the search to ﬁnd desired test
data. To raise efﬁciency, we include a signal dependency
analysis (SDA) to identify which model inputs each CG
actually depends on. McMinn et al. [20] investigated a
related approach, however, on code level. SDA is closely
related to a slicing approach for SL models developed
parallel to our work [21].
At code level, such analysis is usually done by capturing
the control dependence in a graph. SL models though, as
pointed out previously, are dominated by data dependencies.
We therefore analyze the dependency of CGs on input
signals by creating a signal dependency graph (a) based
on the syntax of the model and (b) reﬁned according to
the semantics of blocks. Focusing purely on syntax, the
following principle leads to a graph describing which signal
b the value of a model internal signal a depends on: Signal
a is dominated by a signal b if signal a is the outcome
of a model block which has signal b incoming - written
a→b. Some blocks with multiple outgoing signals however,
do not use every incoming signal in order to calculate the
value of a certain outgoing signal. In such cases, the signal
dependency graph is reﬁned by removing over-approximated
dependencies. Similar to SIA, SDA processes all model
blocks in a predetermined order for collecting dependency
relations.
In addition to the basic procedure of SDA as outlined
above, some modeling constructs available in SL require
special handling. The concept of vector signals, for example,
which makes a signal being a container for a number of
subordinate signals, requires tracing the dependency between
subordinate signals of different containing signals. At this
point, the semantics of blocks that process vector signals
is relevant. A Sum block which has two vector signals
as inputs, for example, performs a pair-wise addition of
vector elements at the same vector index. Consequently, the
resulting signal is also a vector. Each of its subordinate
signals, however, is dominated by only two subordinate
314
International Journal on Advances in Systems and Measurements, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/systems_and_measurements/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

Switch
CG: s4==1
1
2
3
4
5
0
<=
NOT
1.2
==
AND
s1
s2
s3
s4
1
b1
b2
Figure 3.
Illustration of how determining which model inputs a CG
depends on might narrow the search space.
signals of the block’s input signals. SDA’s block-speciﬁc
collecting of dependency relations considers such cases. A
further exception that requires special handling by SDA is
the concept of conditional subsystems in SL. Such subsys-
tems are only executed if their control signals activate them
or keep them active. Every signal inside of such a subsystem
thus also depends on the control signal. SDA recognizes
these situations when processing the Inport blocks within a
conditional subsystem and adds dependency relations to the
graph accordingly.
In order to ﬁnally determine which model inputs a cer-
tain CG depends on, the signal or signals which the CG
expression refers to are selected in the dependency graph.
By traversing the graph up to the input signals, the set of
relevant model inputs is built up. Within the subsequent
search process for this CG, signals are generated only for the
relevant inputs. In addition, operators of the applied search
algorithm which merge and modify generated solutions, such
as crossover and mutation operators in a genetic algorithm,
target only relevant input signals. In this way, depending
on the speciﬁc application case, the search space might be
reduced by several dimensions. For model execution, all
other (irrelevant) model inputs receive a random signal that
is consistent with the input’s speciﬁcation.
Figure 3 shows an example illustrating the beneﬁcial
potential of such an approach in the context of search-based
test data generation. SDA’s block analysis would build up a
dependency graph expressing that s3→s1, s3→s2 (gathered
by analyzing b1), and s4→s3 (gathered by analyzing b2).
These relations alone are sufﬁcient to discover that CG
s4==1 depends solely on input 5. Hence, varying signals
for input 1 to 4 during a search has no effect on satisfying
the targeted CG. If the search would focus on exploring the
search space of input 5, however, it would likely ﬁnd the
desired input data sooner.
C. Coverage Goal Sequencing
No matter if structural testing is performed in addition to
functional (black-box) testing or purely as white-box testing,
it is usually a set of CGs that constitutes the test objective.
Remember, that for each CG a separate search needs to
be run. In Windisch’s approach, those search processes are
executed in random order. Hence, correlations between CGs
are ignored. Given CGs with the expressions s<90, s<80
and s<70, for example, it is most likely more efﬁcient to
aim for reaching the goal s<70 ﬁrst because it satisﬁes all
other CGs at the same time. As this example indicates, the
execution order of the CG-related search processes affects
the efﬁciency of the whole structural test.
Other researchers in the search-based testing community
have noticed this shortcoming as well. Fraser and Arcuri [22]
advise focusing on the generation of whole test suites rather
than targeting single CGs. They recommend optimizing
multiple test suites instead of multiple test data and also
suggest rewarding smaller test suites with a better ﬁtness,
in case two or more test suites achieve the same coverage.
Harman et al. [23], in contrast, suggest a multi-objective
search in which each CG is still targeted individually but the
number of collateral (accidentally covered) goals is included
as a secondary objective. Though facing a similar problem,
our approach differs. We keep the focus on CGs themselves
since, considering the complexity of the optimization prob-
lems constituted by industrial SL models, they are often
difﬁcult to reach and we do not want to impede the search by
burdening it with additional goals or mixed ﬁtness values.
Instead, we propose a coverage goal sequencing (CGSeq)
approach that creates a reasonable order in which the various
CGs are pursued. Li et al. worked out a related approach
[24], however, it is outside of the search-based and SL
context. Ultimately, by maximizing collateral coverage, our
approach attempts to minimize the number of CGs that need
to be pursued. Not only is this expected to improve overall
efﬁciency, but the resulting test suite should also be smaller.
The procedure of CGSeq is summarized in Figure 4.
First of all, the model under test is analyzed and CGs
are derived for all SL/SF-relevant coverage criteria (see
overview by Windisch [8]). In preparation to analyzing
dependencies between CGs, we apply several harmonization
and simpliﬁcation steps to the CG expressions. Note that
results of SIA (Section III-A) are used for this task as well,
e.g., an expression s≥1 would be transformed to s=1 if s
is a Boolean signal.
Next, possible dependencies between CG expressions are
analyzed, resulting in a dependency graph. In this graph,
we treat the nodes as CG sets, in which equivalent CGs are
grouped - noted as CGN = {CGa, ..., CGn}. Note that
we omit the set braces in some of the following notations
if the set contains only one element. In order to limit
graph complexity, the dependency analysis considers only
315
International Journal on Advances in Systems and Measurements, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/systems_and_measurements/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

Coverage Goal Analysis
1. Collect all CGs of all available coverage criteria
CG 1
CG 2
. . .
2. Harmonize and simplify CG expressions, e.g.
s1>10 AND s1>4
s1>10
s2≥1
+ s2 in [0,1]:1
s2==1
Dependency Analysis
1. Logical dependance between CGs
2. Semantical dependance due to connecting blocks
s3==1
s3≠4 
    incl. creation of virtual CGs for bridging between CGs 
s4==1
OR
s4
s5
s5==1
Optimization Goal Sequencing
1. Reduction of dependancy graph according to user 
    selection of coverage criteria / single CGs
2. Derivation of  optimization goals  from graph
CG 1
CG 4
CG 3
+
certain CG 
combinations
single CGs
3. Sequencing of optimization goals based on:  
    a) min. number of boolean signals in expression(s)
    b) min. depth in model
    c) max. number of implied CGs (collateral coverage)
    d) min. number of relevant inputs
        (as determined by SDA)
Switch
>
-5
OR
[0;5]
s6==0
s7>=0 AND s7<=5
s8==1
s9==1
(as determined
by SIA)
Figure 4.
Main process steps to create an efﬁcient order for a set of
coverage goals which are processed separately by a search.
relations that are useful for assessing CG satisﬁability and
for the ﬁnal goal of sequencing the CGs. The following
relations
are
captured:
implication
(CGN1→CGN2),
equivalence
(CGN1∪CGN2),
NAND
(CGN1↑CGN2),
and XOR (CGN1⊕CGN2). For a more compact notation
of implications we also use, in certain cases, conjunctions
((CGN1∧...∧CGNn)→(CGNm∧...∧CGNz))
and
dis-
junctions
((CGN1∨...∨CGNn)→(CGNm∧...∧CGNz)).
Dependencies between CG expressions are analyzed both
from a logical and a semantical point of view.
Logical dependency addresses relations between CGs
due to the operators involved in their formulas, as well
as due to contained constants and the ranges of related
signals, if SIA has been carried out prior to CGSeq.
For example, the relation (s1>0)CG→(s1≥0)CG is rec-
ognized solely based on the involved operators. The rela-
tion (s1==5)CG→(s1≥0)CG, however, requires taking the
involved constants into account. Going one step further,
the relation (s1==3)CG→(s1>s2)CG would be detected if
max(I(s2))<3, i.e., the maximum upper boundary of any
interval in s2’s range set is less than the constant 3. An
extensive distinction of such cases has been worked out.
Semantical dependency means that for each two CGs
relating to incoming or outgoing signals of the same model
block, a block-speciﬁc analysis checks if a relation be-
tween the CGs exists. Just as within SIA and SDA, all
model blocks are processed in a predetermined order for
this analysis. Given an OR block with the incoming sig-
nals s1 and s2, and the outgoing signal s3, the analysis
for this block would detect, for example, the relations
(s1̸=0)CG → (s3==1)CG, (s2̸=0)CG → (s3==1)CG, and
((s1==0)CG ∧ (s2==0)CG) → (s3==0)CG. In certain
cases the block-speciﬁc analysis adds virtual CGs as a bridge
to other CGs in order to detect further dependencies. For this
purpose, as illustrated in Figure 4, signal range information
from SIA is also used in case of certain block types.
As a side effect, based on the captured dependencies,
further CGs might be detected as unsatisﬁable in the course
of CGSeq (cf. Section III-A). For example, if the graph
contains the relation CG1↔CG2 and SIA has discovered
that CG2 is unsatisﬁable, then CG1 must also be unsatis-
ﬁable. CGSeq performs an extended satisﬁability check by
propagating unsatisﬁability conclusions through the graph.
Now, back to the goal of sequencing the CGs. In the next
step, the user’s selection of coverage criteria or single CGs is
considered by minimizing the graph accordingly. Amongst
others, non-selected CGs implying selected CGs are kept.
Afterwards, the ﬁnal optimization goals are derived from
the graph in a two-fold way:
1) For each selected CG, called CGOriginal, an optimiza-
tion goal consisting of a single CG, called CGT arget,
is derived. Starting in the dependency graph at the
node of CGOriginal, the implication relations are
analyzed backwards to determine CGT arget, which
316
International Journal on Advances in Systems and Measurements, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/systems_and_measurements/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

Simulink
TASMO
File   Edit   …                   
TASMO
Input Ranges,
Coverage Criteria,
etc.
Search
Configuration
Static Preprocessing
Model Analysis and 
Transformations
Signal Interval Analysis
Signal Dependency Analysis
Model Instrumentation
Report and Test Data Export
Simulink
Hidden Instance 
for Test Data 
Evaluation
User Instance
1
2
3
1
2
3
Coverage Goal Sequencing
Update
Execution
Figure 5.
Overall workﬂow of the tool prototype for search-based test data generation for Simulink, including the static preprocessing workﬂow.
ultimately is representing CGOriginal. If multiple CGs
are suitable candidates for CGT arget, criteria such as
a lower model depth (minimum path length from any
model input to any signal involved in the CG) or
a lower rate of Boolean signals involved in the CG
expression are taken into account.
2) Certain combinations of CGs are derived to form
optimization goals as well. Remember that the de-
pendency graph can contain conjunctions. Each con-
junction serves as a start point for building up a
conjunction tree which contains implication relations
from the graph that are leading (directly or indirectly)
to the conjunction. Out of every branch or sub-branch
within this tree one suitable CG is selected. These
CGs constitute one or multiple new optimization goals,
depending on the depth of the tree. In this way, a
few suitable optimization goals with potentially high
collateral coverage are added.
Finally, the optimization goals are sequenced according
to several metrics, primarily by the number of (so far
unsatisﬁed) implied CGs, but also by their depth in the
model and the amount of Boolean signals involved in the
expressions - since such goals should be avoided given
the ﬁtness function construction problem (see Section II-C).
Note that the pursuing order of optimization goals is updated
after each search process ends, since an optimization goal’s
number of unsatisﬁed implied CGs might have changed.
IV. IMPLEMENTATION
The presented preprocessing techniques have been imple-
mented in the course of developing our prototypical tool
TASMO (Testing via Automated Search for Models) [25].
TASMO is mainly written in Java and closely integrated
with Matlab. As shown in Figure 5, the user can trigger the
automated test data generation process directly from within
Matlab when having a SL model opened. TASMO then
extracts model related information using Matlab’s API and
programming language m. After the Java-based component
of TASMO has been triggered, it builds up an internal
representation of the model under test and derives all CGs
for every supported coverage criteria from this representation
in advance. It then applies transformation and reduction
steps to the internal model representation in order to focus
on the relevant parts for structural test data generation. In
particular, all blocks and signals from the model’s Outports
to the rightmost CGs are removed since they are irrelevant
for the following analysis and preprocessing steps. Model
constructs like virtual subsystems or bus systems, which are
both semantically irrelevant, are ﬂattened.
The user is then asked to select one or more coverage
criteria, like decision coverage or condition coverage, or
certain CGs directly. Regarding this step, an insight into the
user interface of the tool is given in Figure 6. As indicated
earlier, the user is also asked to specify the test data to
be generated, e.g., the range of each model input. After all
required presettings have been made, the three presented
preprocessing techniques are run in the following order:
First, SIA determines the ranges of all model internal signals
in order to assess the satisﬁability of each CG. During
SIA, TASMO might also transform the internal model rep-
resentation. Both general and block-speciﬁc transformation
317
International Journal on Advances in Systems and Measurements, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/systems_and_measurements/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

Figure 6.
Automated testing of Simulink models with TASMO: Selection
of coverage criteria and coverage goals for the model under test.
criteria are checked. For example, if a signal’s ranges all
contain only a single constant value and this signal does not
originate from a Constant block, the source block of this
signal is replaced with a Constant block. At the same time,
a backwards directed removal process is initiated starting
with the block that has been replaced. All signals and blocks
preceding this block (directly or indirectly) are removed
if no CGs are referenced to it. In this way, the internal
model is kept as compact as possible for subsequent analysis
steps without altering its semantics. Another example: If
the incoming signal of an Abs block (absolute value) has
solely non-negative ranges, the block can be removed and
its former incoming signal can be directly connected to its
former outgoing signal.
After SIA, TASMO runs SDA in order to determine for
each CG which model inputs it depends on. Finally, CGSeq
is carried out to build up and sequence a list of optimization
goals. Before the search algorithm processes optimization
goal after optimization goal, a hidden Matlab instance for
simulating the model with generated test data is started.
A copy of the real model under test is loaded, initialized,
and instrumented. Afterwards, an automated search for each
optimization goal is performed, which generates test data
solely compliant with the signal speciﬁcation for all inputs
that are relevant for the CGs of the current optimization
goal. If an optimization goal, or rather its contained CGs, are
covered by accident during a search for another optimization
goal (collateral coverage), the goal is considered done.
When ﬁnished, TASMO generates a report and provides the
generated test data in a reusable format.
V. CASE STUDY
We investigated the effect of the three preprocessing
techniques SIA, SDA and CGSeq on structural test data
generation for industrial SL/TL models. Two SL/TL models
served as case examples, model A and B.
Model A originates from the development of an electric
vehicle’s propulsion strategy, contains 730 blocks, and has
Table I
CASE STUDY CONFIGURATIONS
Conﬁguration
SIA
SDA
CGSeq
C1
-
-
-
C2
x
-
-
C3
x
x
-
C4
x
-
x
C5
x
x
x
12 inputs. A total of 600 CGs were derived for the chosen
coverage criteria decision coverage and condition coverage.
Model B implements the functionality of a rear window
defroster. It contains 1861 blocks, has 16 inputs, and 1113
CGs were derived in total.
In order to analyze the effect of the presented techniques,
ﬁve different conﬁgurations were compared. Table I outlines
the characteristics of each conﬁguration. The conﬁgurations
differ in the use or non-use of the static preprocessing
techniques within the test data generation process. C2 is
compared with C1 to evaluate the effect of SIA, C3 is
compared with C2 to evaluate the effect of SDA, and C4 is
compared with C2 to evaluate the effect of CGSeq. C5 brings
all three static preprocessing techniques together to evaluate
the entire static preprocessing as a whole. In particular, only
C5 allows CGSeq to use the results of both SIA and SDA in
order to analyze dependencies between the coverage goals
and to derive an execution order for the search goals. Note
that SIA is also activated in C3 and C4, since processing
unsatisﬁable CGs would otherwise extend the runtime of
the case study unnecessarily.
For the search, we applied a special genetic algorithm
for generating signals, as presented by Windisch and Al
Moubayed [13] (see Section II-B). The algorithm settings
were chosen as listed in detail in their paper, except for the
following differences. In each search iteration, 20 individuals
(test data) were generated. A search was stopped when the
targeted optimization goal was reached, when 40 iterations
had been carried out, or if the search stagnated. Search
stagnation was indicated by 8 successive iterations in which
no better individual was found. Since the search algorithm
is subjected to a certain randomness, we carried out a total
of 30 test data generation runs for each conﬁguration (C1-
C5). The input signals to be generated for model A were
speciﬁed to have a length of 30 seconds and a sample
rate of 0.1 seconds. For model B, the signal length was
set to 20 seconds and the sample rate to 0.25 seconds.
While the signal lengths were chosen with respect to the
dynamic functional behavior of the models, the sample rates
are speciﬁed in each model’s properties. Range boundaries
for each model’s inputs (see Section II-B and III-A) were
taken directly from development documents.
The study was run on a PC with an Intel Core 2 Duo
processor (P8600, 2.4 GHz), 4 GB RAM, and Windows 7
318
International Journal on Advances in Systems and Measurements, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/systems_and_measurements/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

107
107
107
107
556
449
450
449
450
33
33
33
33
44
11
10
11
10
0 %
10 %
20 %
30 %
40 %
50 %
60 %
70 %
80 %
90 %
100 %
Configuration
C1
Configuration
C2
Configuration
C3
Configuration
C4
Configuration
C5
Identified
Always-
Satisified CGs
CGs
Covered By
Search
Identified
Unsatisfiable
CGs
Uncovered
CGs
(a) Achieved coverage
C1: 09:25:15
C2: 02:08:19
C3: 01:59:51
C4: 00:52:44
C5: 00:48:25
00:00:00
02:00:00
04:00:00
06:00:00
08:00:00
10:00:00
C1: 6094
C2: 1622
C3: 1568
C4: 565
C5: 546
0
1000
2000
3000
4000
5000
6000
7000
C1: 0,00016%
C2: 0,00061%
C3: 0,00063%
C4: 0,00176%
C5: 0,00182%
0,0000%
0,0005%
0,0010%
0,0015%
0,0020%
C1: 6,63
C2: 6,77
C3: 6,93
C4: 5,87
C5: 5,93
5
5,5
6
6,5
7
C1: 45,13
C2: 13,83
C3: 13,1
C4: 10,63
C5: 10,2
0
10
20
30
40
50
C1: 3,91%
C2: 16,79%
C3: 20,48%
C4: 70,26%
C5: 70,53%
0%
20%
40%
60%
80%
100%
(b) Total runtime of required searches (hours:minutes:seconds)
C1: 09:25:15
C2: 02:08:19
C3: 01:59:51
C4: 00:52:44
C5: 00:48:25
00:00:00
02:00:00
04:00:00
06:00:00
08:00:00
10:00:00
C1: 6094
C2: 1622
C3: 1568
C4: 565
C5: 546
0
1000
2000
3000
4000
5000
6000
7000
C1: 0,00016%
C2: 0,00061%
C3: 0,00063%
C4: 0,00176%
C5: 0,00182%
0,0000%
0,0005%
0,0010%
0,0015%
0,0020%
C1: 6,63
C2: 6,77
C3: 6,93
C4: 5,87
C5: 5,93
5
5,5
6
6,5
7
C1: 45,13
C2: 13,83
C3: 13,1
C4: 10,63
C5: 10,2
0
10
20
30
40
50
C1: 3,91%
C2: 16,79%
C3: 20,48%
C4: 70,26%
C5: 70,53%
0%
20%
40%
60%
80%
100%
(c) Number of generated test data
C1: 09:25:15
C2: 02:08:19
C3: 01:59:51
C4: 00:52:44
C5: 00:48:25
00:00:00
02:00:00
04:00:00
06:00:00
08:00:00
10:00:00
C1: 6094
C2: 1622
C3: 1568
C4: 565
C5: 546
0
1000
2000
3000
4000
5000
6000
7000
C1: 6.63
C2: 6.77
C3: 6.93
C4: 5.87
C5: 5.93
5
5.5
6
6.5
7
C1: 45.13
C2: 13.83
C3: 13.1
C4: 10.63
C5: 10.2
0
10
20
30
40
50
C1: 3.91%
C2: 16.79%
C3: 20.48%
C4: 70.26%
C5: 70.53%
0%
20%
40%
60%
80%
100%
(d) Size of ﬁnal test suite
C1: 09:25:15
C2: 02:08:19
C3: 01:59:51
C4: 00:52:44
C5: 00:48:25
00:00:00
02:00:00
04:00:00
06:00:00
08:00:00
10:00:00
C1: 6094
C2: 1622
C3: 1568
C4: 565
C5: 546
0
1000
2000
3000
4000
5000
6000
7000
C1: 6.63
C2: 6.77
C3: 6.93
C4: 5.87
C5: 5.93
5
5.5
6
6.5
7
C1: 45.13
C2: 13.83
C3: 13.1
C4: 10.63
C5: 10.2
0
10
20
30
40
50
C1: 3.91%
C2: 16.79%
C3: 20.48%
C4: 70.26%
C5: 70.53%
0%
20%
40%
60%
80%
100%
(e) Number of searches (targeted optimization goals)
C1: 09:25:15
C2: 02:08:19
C3: 01:59:51
C4: 00:52:44
C5: 00:48:25
00:00:00
02:00:00
04:00:00
06:00:00
08:00:00
10:00:00
C1: 6094
C2: 1622
C3: 1568
C4: 565
C5: 546
0
1000
2000
3000
4000
5000
6000
7000
C1: 6.63
C2: 6.77
C3: 6.93
C4: 5.87
C5: 5.93
5
5.5
6
6.5
7
C1: 45.13
C2: 13.83
C3: 13.1
C4: 10.63
C5: 10.2
0
10
20
30
40
50
C1: 3.91%
C2: 16.79%
C3: 20.48%
C4: 70.26%
C5: 70.53%
0%
20%
40%
60%
80%
100%
(f) Success rate for performed searches
Figure 7. Measured variables of test data generation for model A, averaged
over 30 test data generation runs.
1055
959
959
955
955
52
52
58
58
58
6
6
0
0
0 %
10 %
20 %
30 %
40 %
50 %
60 %
70 %
80 %
90 %
100 %
Configuration
C1
Configuration
C2
Configuration
C3
Configuration
C4
Configuration
C5
Identified
Always-
Satisified CGs
CGs
Covered By
Search
Identified
Unsatisfiable
CGs
Uncovered
CGs
(a) Achieved coverage
C1: 14:23:57
C2: 01:17:22
C3: 01:08:07
C4: 00:16:23
C5: 00:11:37
00:00:00
04:00:00
08:00:00
12:00:00
16:00:00
C1: 8126
C2: 891
C3: 864
C4: 110
C5: 98
0
2000
4000
6000
8000
C1: 0,00016%
C2: 0,00061%
C3: 0,00063%
C4: 0,00176%
C5: 0,00182%
0,0000%
0,0005%
0,0010%
0,0015%
0,0020%
C1: 9
C2: 8,3
C3: 7,6
C4: 7,3
C5: 7,5
6
7
8
9
10
C1: 59,87
C2: 9,13
C3: 8,03
C4: 13,57
C5: 13,9
0
10
20
30
40
50
60
70
C1: 3,12%
C2: 24,90%
C3: 32,12%
C4: 98,08%
C5: 98,53%
0%
20%
40%
60%
80%
100%
(b) Total runtime of required searches (hours:minutes:seconds)
C1: 14:23:57
C2: 01:17:22
C3: 01:08:07
C4: 00:16:23
C5: 00:11:37
00:00:00
04:00:00
08:00:00
12:00:00
16:00:00
C1: 8126
C2: 891
C3: 864
C4: 110
C5: 98
0
2000
4000
6000
8000
C1: 0,00016%
C2: 0,00061%
C3: 0,00063%
C4: 0,00176%
C5: 0,00182%
0,0000%
0,0005%
0,0010%
0,0015%
0,0020%
C1: 9
C2: 8,3
C3: 7,6
C4: 7,3
C5: 7,5
6
7
8
9
10
C1: 59,87
C2: 9,13
C3: 8,03
C4: 13,57
C5: 13,9
0
10
20
30
40
50
60
70
C1: 3,12%
C2: 24,90%
C3: 32,12%
C4: 98,08%
C5: 98,53%
0%
20%
40%
60%
80%
100%
(c) Number of generated test data
C1: 14:23:57
C2: 01:17:22
C3: 01:08:07
C4: 00:16:23
C5: 00:11:37
00:00:00
04:00:00
08:00:00
12:00:00
16:00:00
C1: 8126
C2: 891
C3: 864
C4: 110
C5: 98
0
2000
4000
6000
8000
C1: 9
C2: 8.3
C3: 7.6
C4: 7.3
C5: 7.5
6
7
8
9
10
C1: 59.87
C2: 9.13
C3: 8.03
C4: 13.57
C5: 13.9
0
10
20
30
40
50
60
70
C1: 3.12%
C2: 24.90%
C3: 32.12%
C4: 98.08%
C5: 98.53%
0%
20%
40%
60%
80%
100%
(d) Size of ﬁnal test suite
C1: 14:23:57
C2: 01:17:22
C3: 01:08:07
C4: 00:16:23
C5: 00:11:37
00:00:00
04:00:00
08:00:00
12:00:00
16:00:00
C1: 8126
C2: 891
C3: 864
C4: 110
C5: 98
0
2000
4000
6000
8000
C1: 9
C2: 8.3
C3: 7.6
C4: 7.3
C5: 7.5
6
7
8
9
10
C1: 59.87
C2: 9.13
C3: 8.03
C4: 13.57
C5: 13.9
0
10
20
30
40
50
60
70
C1: 3.12%
C2: 24.90%
C3: 32.12%
C4: 98.08%
C5: 98.53%
0%
20%
40%
60%
80%
100%
(e) Number of searches (targeted optimization goals)
C1: 14:23:57
C2: 01:17:22
C3: 01:08:07
C4: 00:16:23
C5: 00:11:37
00:00:00
04:00:00
08:00:00
12:00:00
16:00:00
C1: 8126
C2: 891
C3: 864
C4: 110
C5: 98
0
2000
4000
6000
8000
C1: 9
C2: 8.3
C3: 7.6
C4: 7.3
C5: 7.5
6
7
8
9
10
C1: 59.87
C2: 9.13
C3: 8.03
C4: 13.57
C5: 13.9
0
10
20
30
40
50
60
70
C1: 3.12%
C2: 24.90%
C3: 32.12%
C4: 98.08%
C5: 98.53%
0%
20%
40%
60%
80%
100%
(f) Success rate for performed searches
Figure 8. Measured variables of test data generation for model B, averaged
over 30 test data generation runs.
319
International Journal on Advances in Systems and Measurements, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/systems_and_measurements/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

64-bit. Our Java-based tool TASMO was run using version
7 of the Java Runtime Environment in connection with
Matlab/Simulink 2009b and TargetLink 3.1.
In the following sections, the results of the case study are
presented. First, we analyze the results with regard to each
preprocessing technique separately. Then, we assess the use
of our preprocessing techniques as a whole.
A. Analysis of SIA
As visible in Figure 7a for C2, SIA identiﬁed 33 unsatis-
ﬁable CGs for model A, of which 8 were identiﬁed by SIA’s
loop analysis. In addition, 69 CGs from a total of 107 CGs,
which are always satisﬁed independent of the chosen input
data, were identiﬁed by SIA. All those CGs were excluded
from subsequent search processes. As for model B, 58 CGs
are unsatisﬁable, of which 52 were identiﬁed by SIA (6 by
its loop analysis), as can be seen for C2 in Figure 8a. For
model B, SIA also identiﬁed 96 CGs from a total of 100 CGs
that are always satisﬁed. Note that the other always-satisﬁed
or unsatisﬁed CGs were identiﬁed by CGSeq.
In order to evaluate the effect of preprocessing the test
data search with SIA, C1 and C2 are analyzed. The model
coverage achieved by a conﬁguration is used to measure for
effectiveness of a conﬁguration. C1 was able to generate
test data for model A with a coverage of almost 93% (95%
for model B). While introducing SIA in C2 did not lead
to higher coverage, as can be seen in Figures 7a and 8a, it
certainly improved the efﬁciency of the automated test data
generation process. As the statistics for model A in Figure
7b show, the runtime of all performed searches together was
reduced by 77% (from 9:25 hours for C1 to 2:08 hours for
C2). For model B, the runtime was even reduced by 91%
(from 14:24 hours for C1 to 1:17 hours for C2, see Figure
8b). The runtime of SIA itself, which is not included in the
values displayed in Figures 7b and 8b, was about 5 minutes
for model A and 15 seconds for model B. In large part,
SIA’s runtime was caused by its loop analysis feature.
In C2, test data was only generated for CGs that were
not identiﬁed as unsatisﬁable. While the test data generation
process of C1 for model A was targeting about 45 CGs
directly by search (about 60 for model B), only 14 (9 for
model B) were targeted by C2 (see Figures 7e and 8e). The
number of searches that were carried out additionally by
C1 is approximately as high as the number of unsatisﬁable
CGs that SIA was able to identify. As a consequence of
C2 targeting less CGs, less test data was generated overall,
as displayed in Figures 7c and 8c. Since C1 performed
searches for unsatisﬁable CGs without any hope for success,
the portion of successfully tackled CGs is naturally higher
for C2 in comparison to C1 (see Figures 7f and 8f).
B. Analysis of SDA
Using SDA, the search processes of C3 and C5 were
ignoring irrelevant inputs during input data optimization.
According to the preprocessing analysis carried out by SDA,
on average, 4.18 of model A’s 12 inputs turned out to be
relevant for reaching one of the CGs (6.58 of 16 inputs
for model B). Considering only the CGs that were targeted
in C3, 6.48 of the 12 inputs of model A, on average, are
relevant (12.06 of 16 for model B). For C5, the average
value is 4.71 (model A) and 7.85 (model B), respectively.
These values reﬂect the effective search space reduction due
to the use of SDA.
Comparing the coverage results of C3 with those of C2,
as well as C5 with C4, the introduction of SDA to the
automated test data generation process for model A led to
slightly higher coverage (see Figure 7a). One coverage goal
that was only covered seldomly by the other conﬁgurations
was always covered with SDA being activated. This obser-
vation is backed up by the increased rate of successfully
ﬁnished searches for C3, as visible in Figure 7f. In case of
model B, C1 managed to cover as many CGs as C2.
The use of SDA reduced the overall runtime of the
searches by 7% for model A (from 2:08 hours for C2 to
1:59 hours for C3) and 12% for model B (from 1:17 hours
for C2 to 1:08 hours for C3), as visible in Figures 7b and
8b. For both models, the runtime of SDA itself was only
about one second. Due to focusing solely on relevant model
inputs, the searches for a few targeted CGs ﬁnished slightly
quicker. Accordingly, less test data was generated, as can be
seen when comparing C3 with C2 in Figures 7c and 8c.
C. Analysis of CGSeq
Note that during CGSeq, another 38 always-satisﬁed CGs
were identiﬁed for model A in addition to 69 such CGs
that were already identiﬁed by SIA (107 in total as shown
in Figure 7a). Likewise, 4 additional always-satisﬁed CGs
were identiﬁed by CGSeq for model B, resulting overall
in 100 such CGs (see Figure 8a). All these CGs were
considered as covered and were excluded from the test data
generation process since any test data in the generated test
suite would satisfy them. For model B, CGSeq even found
another 6 unsatisﬁable CGs, in addition to 52 unsatisﬁable
CGs identiﬁed by SIA (see Figure 8a).
Using CGSeq prior to the test data search (as done in C4)
did not increase the total model coverage (compared to C2),
as apparent in Figures 7a and 8a. However, the runtime of the
search was reduced by 59% for model A (from 2:08 hours
for C2 to 52 minutes for C4) and 79% for model B (from
1:17 hours for C2 to 16 minutes for C4) due to introducing
CGSeq in the test data generation process, as visible in
Figures 7b and 8b. Similar to SIA and SDA, CGSeq itself
did turn out to be reasonable in terms of computation time.
Running CGSeq took only about 16 seconds for model A
and 20 seconds for model B on average.
Due to targeting more promising (e.g., non-boolean) goals
ﬁrst, the portion of successfully tackled searches was higher
(see Figures 7f and 8f) and less test data had to be generated
320
International Journal on Advances in Systems and Measurements, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/systems_and_measurements/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

(see Figures 7c and 8c). For model A, less searches were
performed (compare C4 and C2 in Figure 7e) and the
automated test data generation process resulted in smaller
test suites (see Figure 7d) since CGSeq prioritizes goals
with potentially high collateral coverage. While the size of
the resulting test suite was also reduced when activating
CGSeq for model B (compare C4 and C2 in Figure 8d),
more searches were performed (see Figure 8e). This might
be surprising at ﬁrst sight. However, it demonstrates how the
test data generation process becomes more target-oriented
when using CGSeq. In C2, a lot of CGs that are difﬁcult to
approach for the search algorithm were targeted. Plenty of
test data was generated in the course of this and as a side
effect, many CGs were covered rather coincidentally.
D. Combined Analysis
In summary, all three preprocessing techniques have
demonstrated their usefulness to automated test data gen-
eration for SL/TL models. The case studies have shown
that SIA is able to raise the test data generation’s efﬁciency
signiﬁcantly, due to exclusion of unsatisﬁable CGs. SDA
pointed out its capability to raise both effectiveness and
efﬁciency by turning the focus of the search for input data
on relevant model inputs. CGSeq improved the efﬁciency
considerably by advising the test data search to preferably
target CGs for which a suitable ﬁtness function is derivable,
and which entail high collateral coverage. In addition, the
runtime of all three preprocessing techniques was vanish-
ingly low compared to the runtime of a subsequent test data
search. Thus, we conclude that their use comes without any
signiﬁcant negative side effects.
Running all three prepocessing techniques in combina-
tion, as done in C5, indicates that the advantages of each
technique also complement each other suitably. While in
C3, only SIA and SDA, and in C4, only SIA and CGSeq
were combined, the combination of all of them led to a
further reduction of the search runtime of 8% for model
A and 29% for model B (see Figures 7b and 8b). Even
though the improvement in terms of runtime, as well as
number of generated test data, performed searches, and ratio
of successful searches, is only small (see Figure 7), C5 came
out on top of all conﬁgurations for both effectiveness and
efﬁciency of the automated test data generation, in case of
model A as well as in the case of model B.
The case study also demonstrates the practicability and
feasibility of search-based test data generation for SL/TL
models, if extended by static preprocessing such as the
presented ones. In particular, even if no automated test
oracle is available to evaluate the generated test suite’s
conformance to the speciﬁcation, and the test suite thus
needs to be analyzed manually under a functional aspect, the
case study has shown that the applied test data generation
approach can lead to relatively small test suites, making
a manual analysis feasible. Note that applying test suite
minimization techniques might further reduce the size of
the obtained test suites.
Further improvements and extensions of the applied test
data generation technique might increase its performance to
industrial SL/TL models even more. We investigated, for
instance, why certain CGs of model A were not covered at
all during the case study. It turned out that 8 CGs (of 10
uncovered ones, see Figure 7a) are unsatisﬁable and neither
SIA nor CGSeq were able to identify this circumstance;
mainly due to block types in the chosen model for which the
preprocessing techniques did not offer a speciﬁc handling.
The two satisﬁable, yet uncovered CGs left, were found to
be difﬁcult to solve by the search-based algorithm. Further
guidance, for instance, by advanced ﬁtness functions, is
required to facilitate the search to reach such CGs. Nev-
ertheless, the work presented in this paper turned out to be
a big step forward towards industrial applicability of search-
based automation of structural testing for SL/TL models.
VI. RELATED WORK
Besides search-based testing, as introduced in Section II,
a few other approaches have been applied or proposed for
automating structural test data generation for SL models. In
general, the approaches can be classiﬁed as dynamic, static
or hybrid techniques. Dynamic techniques, such as search-
based test data generation, execute the test object during test
data generation. Static techniques however, analyze the test
object without executing it in order to generate test data.
Hybrid techniques usually contain characteristics of both
dynamic and static techniques. Our approach mainly utilizes
a dynamic technique, but could also be classﬁed as a hybrid
approach since it is combined with static techniques.
In the ﬁeld of structural test data generation for SL, the
following dynamic techniques have been applied: search-
based testing and random testing. Case studies of Windisch
have shown that search-based testing outperforms random
testing for structural testing of SL models [8]. As for static
techniques, the use of symbolic execution/constraint solving
and model checking has been reported in the SL context.
Gadkari et al. developed an approach called AutoMOTGen
[26], which involves the transformation of SL/SF models
into a representation of the high-level language SAL and
the use of a model checker for generating test data [27].
While the authors encountered promising results, they were
also faced with technical limitations of their approach,
particularly scalability issues. P˘as˘areanu et al. use symbolic
execution and constraint solving in order to generate test
data for SL models [28]. They also transform the model
ﬁrst - in their case, into Java code. The tool Symbolic
PathFinder is then used to generate test data. However, their
reports indicate that this approach suffers from scalability
issues. Satpathy et al. developed REDIRECT, which applies
concolic testing to the test data generation problem for SL
[29]. Similar to the work of Gadkari et al., they transform
321
International Journal on Advances in Systems and Measurements, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/systems_and_measurements/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

SL/SF models to SAL ﬁrst. Concolic testing extends test data
generation via symbolic execution and constraint solving by
random testing. REDIRECT could also be classiﬁed as a
hybrid approach. The authors document satisfying results,
however, the SL models used in their case studies focus
on SF diagrams and are much smaller than the ones used
in this work. Peranandam et al. went along a similar path
and combined random testing, constraint solving, model
checking, and heuristics in their test data generation tool
SmartTestGen for SL/SF models [30]. SmartTestGen uses
a classiﬁcation algorithm that statically estimates which of
the listed techniques is likely to cover the coverage goal
in question and therefore applies it for test data generation.
The authors conclude that a case-speciﬁc use of different
test data generation techniques is advantageous to master-
ing the complexity of industrial-sized SL models and the
heterogeneity of their test data generation problems.
Commercial tools for structural test data generation for
SL models are available as well. Reactis [31] from Reac-
tive Systems, T-VEC [32] from T-VEC Technologies, and
Simulink Design Veriﬁer [33] from The MathWorks are
common tools for this job. Only little is published about how
these tools generate test data. Windisch [8] asserted that all
these tools use randomized test data generation in some way.
While Reactis combines this basic technique with guided
simulations, T-VEC with symbolic execution and constraint
solving, and Simulink Design Veriﬁer with static analysis, the
details of their implemented techniques remain unknown.
While dynamic approaches might face efﬁciency issues at
times, as described for search-based testing in Section II-C,
or have difﬁculties covering certain structural goals, as in
the case of randomized test data generation, purely static
approaches seem to lack scalability. As demonstrated in this
paper, we believe in the potential of hybridization, i.e., the
extension of dynamic test data generation techniques with
static techniques. Since search-based test data generation is
basically advanced randomized test data generation, which is
used as a basic technique in various approaches and tools, we
continue following the search-based testing path. Note that
our prototypical tool TASMO is also applying different test
data generation strategies, such as constraint solving, when
a CG is suitable. This option was deactivated during the
case studies presented in this paper though. More generally,
the presented static preprocessing techniques are even usable
independent of the choice of test data generation technique.
Identifying unsatisﬁable CGs, focusing on relevant model
inputs, and generating an order of satisﬁable CGs for ef-
ﬁcient processing, is also advantageous when the test data
is generated by randomization, constraint solving, or hybrid
techniques.
VII. CONCLUSION AND FUTURE WORK
This paper introduces an approach to improving the per-
formance of search-based testing when applied to structural
testing of SL models. Three static techniques extend the
standard search-based approach by analyzing the model
under test before the search processes for each CG are run.
Unsatisﬁable CGs are partially identiﬁed and excluded from
the search. The search space is reduced in such a way that the
search focuses solely on relevant model inputs. The separate
search processes for each CG are sequenced in order to
maximize collateral coverage, minimize test suite size, and
shorten the overall search runtime.
A tool prototype that demonstrates the applicability of
the extended search-based approach in industry has been
developed. A case study with two industrial SL/TL models
from the automotive domain has been performed, demon-
strating how the presented preprocessing techniques improve
the search-based test data generation approach. In partic-
ular, efﬁciency was raised distinctly. The use of all three
techniques in combination led to a reduction of over 90%
in the runtime otherwise required for search-based test data
generation without the use of any of these techniques.
Despite satisfactory results, further adaption of the tool to
industrial requirements is required, e.g., improved support of
SL/TL blocks and SF diagrams by the presented static pre-
processing techniques. Our next main step is to work on an
expanded hybridization of the test data generation process.
Besides integrating constraint solving techniques, we plan on
using different types of search algorithms in combination,
while factoring more information collected during model
execution into the (hybridized) search algorithm. Finally, a
broader comparison of the approach and tool, in particular
with established commercial tools, is required.
REFERENCES
[1] B. Wilmes, “Automated structural testing of Simulink / Tar-
getLink models via search-based testing assisted by prior-
search static analysis,” in VALID 2012, The Fourth Inter-
national Conference on Advances in System Testing and
Validation Lifecycle, 2012, pp. 51–56.
[2] The Mathworks, “Matlab Simulink,” Last access: 2013-08-16.
[Online]. Available: http://www.mathworks.com
[3] dSpace, “Targetlink,” Last access: 2013-08-16. [Online].
Available: http://www.dspace.com
[4] P. McMinn, “Search-based software testing: Past, present and
future,” in IEEE 4th International Conference on Software
Testing, Veriﬁcation and Validation Workshops, ser. ICSTW
’11, 2011, pp. 153–163.
[5] B. Wilmes, A. Windisch, and F. Lindlar, “Suchbasierter Test
f¨ur den industriellen Einsatz,” in 4. Symposium Testen im
System- und Software Life-Cycle, 2011.
[6] Y. Zhan and J. A. Clark, “A search-based framework for
automatic testing of MATLAB/Simulink models,” Journal of
Systems and Software, vol. 81, no. 2, pp. 262–285, Feb. 2008.
322
International Journal on Advances in Systems and Measurements, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/systems_and_measurements/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

[7] A. Windisch, “Search-based testing of complex Simulink
models containing Stateﬂow diagrams,” in 31st International
Conference on Software Engineering, 2009, pp. 395–398.
[8] A. Windisch, “Suchbasierter Strukturtest f¨ur Simulink Mod-
elle,” Ph.D. dissertation, Berlin Institute of Technology, 2011.
[9] W. Miller and D. L. Spooner, “Automatic generation of
ﬂoating-point test data,” IEEE Transactions on Software En-
gineering, vol. 2, no. 3, pp. 223–226, May 1976.
[10] B. Korel, “Automated software test data generation,” IEEE
Transactions on Software Engineering, vol. 16, no. 8, pp.
870–879, Aug. 1990.
[11] M. Harman and P. McMinn, “A theoretical and empirical
study of search-based testing: Local, global, and hybrid
search,” IEEE Transactions on Software Engineering, vol. 36,
pp. 226–247, 2010.
[12] J. Wegener, A. Baresel, and H. Sthamer, “Evolutionary test
environment for automatic structural testing,” Information and
Software Technology, vol. 43, no. 14, pp. 841–854, 2001.
[13] A. Windisch and N. Al Moubayed, “Signal generation for
search-based testing of continuous systems,” in International
Conference on Software Testing, Veriﬁcation and Validation
Workshops, ser. ICSTW ’09, 2009, pp. 121–130.
[14] T. E. Vos, A. I. Baars, F. F. Lindlar, P. M. Kruse, A. Windisch,
and J. Wegener, “Industrial scaled automated structural testing
with the evolutionary testing tool,” in Proceedings of the
3rd International Conference on Software Testing, Veriﬁcation
and Validation, ser. ICST ’10. Washington, DC, USA: IEEE
Computer Society, 2010, pp. 175–184.
[15] P. McMinn, M. Harman, D. Binkley, and P. Tonella, “The
species per path approach to search based test data genera-
tion,” in Proceedings of the 2006 International Symposium on
Software Testing and Analysis (ISSTA), ser. ISSTA ’06. New
York, NY, USA: ACM, 2006, pp. 13–24.
[16] Y. Zhan and J. A. Clark, “The state problem for test
generation in Simulink,” in Proceedings of the 8th Annual
Conference on Genetic and Evolutionary Computation. New
York, NY, USA: ACM, 2006, pp. 1941–1948.
[17] A. Goldberg, T. C. Wang, and D. Zimmerman, “Applications
of feasible path analysis to program testing,” in Proceedings
of the International Symposium on Software Testing and
Analysis, ser. ISSTA ’94. New York, NY, USA: ACM, 1994,
pp. 80–94.
[18] Y. Wang, Y. Gong, J. Chen, Q. Xiao, and Z. Yang, “An
application of interval analysis in software static analysis,” in
Proceedings of the 2008 IEEE/IFIP International Conference
on Embedded and Ubiquitous Computing, ser. EUC ’08,
vol. 2.
Washington, DC, USA: IEEE Computer Society,
2008, pp. 367–372.
[19] R. E. Moore, Interval Analysis.
Prentice-Hall, 1966.
[20] P. McMinn, M. Harman, K. Lakhotia, Y. Hassoun, and J. We-
gener, “Input domain reduction through irrelevant variable
removal and its effect on local, global, and hybrid search-
based structural test data generation,” IEEE Transactions on
Software Engineering, vol. 38, pp. 453–477, 2012.
[21] R. Reicherdt and S. Glesner, “Slicing Matlab Simulink mod-
els,” in 34th International Conference on Software Engineer-
ing, 2012, pp. 551–561.
[22] G. Fraser and A. Arcuri, “Evolutionary generation of whole
test suites,” in 11th International Conference on Quality
Software, 2011, pp. 31–40.
[23] M. Harman, S. G. Kim, K. Lakhotia, P. McMinn, and S. Yoo,
“Optimizing for the number of tests generated in search
based test data generation with an application to the oracle
cost problem,” in 3rd International Conference on Software
Testing, Veriﬁcation, and Validation Workshops, ser. ICSTW
’10, 2010, pp. 182–191.
[24] J. J. Li, D. Weiss, and H. Yee, “Code-coverage guided prior-
itized test generation,” Information and Software Technology,
vol. 48, no. 12, pp. 1187–1198, 2006.
[25] B. Wilmes, “Toward a tool for search-based testing of
Simulink/TargetLink models,” in 4th Symposium on Search
Based Software Engineering (Fast Abstracts).
Fondazione
Bruno Kessler, 2012, pp. 49–54.
[26] A. A. Gadkari, A. Yeolekar, J. Suresh, S. Ramesh, S. Mohalik,
and K. C. Shashidhar, “AutoMOTGen: Automatic model
oriented test generator for embedded control systems,” in Pro-
ceedings of the 20th International Conference on Computer
Aided Veriﬁcation, 2008, pp. 204–208.
[27] A. A. Gadkari, S. Mohalik, K. Shashidhar, A. Yeolekar,
J. Suresh, and S. Ramesh, “Automatic generation of test-cases
using model checking for SL/SF models,” in Proceedings of
the 4th Model-Driven Engineering, Veriﬁcation and Valida-
tion Workshop, 2007, pp. 33–46.
[28] C. S. P˘as˘areanu et al., “Model based analysis and test
generation for ﬂight software,” in Proceedings of the 3rd IEEE
International Conference on Space Mission Challenges for
Information Technology, 2009, pp. 83–90.
[29] M. Satpathy, A. Yeolekar, and S. Ramesh, “Randomized di-
rected testing (REDIRECT) for Simulink/Stateﬂow models,”
in Proceedings of the 8th ACM International Conference on
Embedded Software, 2008, pp. 217–226.
[30] P. Peranandam, S. Raviram, M. Satpathy, A. Yeolekar,
A. Gadkari, and S. Ramesh, “An integrated test generation
tool for enhanced coverage of Simulink/Stateﬂow models,”
in Proceedings of the Conference on Design, Automation and
Test in Europe, 2012, pp. 308–311.
[31] Reactive
Systems,
“Reactis,”
Last
access:
2013-08-16.
[Online]. Available: http://www.reactive-systems.com
[32] T-VEC Technologies, “T-VEC Tester for Simulink and
Stateﬂow,” Last access: 2013-08-16. [Online]. Available:
http://www.t-vec.com/solutions/simulink.php
[33] The
Mathworks,
“Simulink
Design
Veriﬁer,”
Last
access:
2013-08-16.
[Online].
Available:
http://www.mathworks.de/products/sldesignveriﬁer
323
International Journal on Advances in Systems and Measurements, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/systems_and_measurements/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org

