477
International Journal on Advances in Intelligent Systems, vol 2 no 4, year 2009, http://www.iariajournals.org/intelligent_systems/
1
Model Transformations Given Policy Modiﬁcations
in Autonomic Management
Raphael M. Bahati and Michael A. Bauer
Department of Computer Science
The University of Western Ontario, London, ON N6A 5B7, CANADA
Email: {rbahati;bauer}@csd.uwo.ca
Abstract—This paper presents an approach for adapting a
model learned from the use of an active set of policies to
run-time policy modiﬁcations. The work leverages our most
resent efforts utilizing Reinforcement Learning methodologies to
facilitate dynamic use of policies within autonomic computing.
The use of policies in this context offers signiﬁcant beneﬁts to
autonomic systems in that it allows systems to determine how to
effectively meet the desired, and at times seemingly conﬂicting,
objectives under dynamically changing conditions. Contrary to
other approaches that make use of some form of learning
to model performance management, our approach enables the
model learnt from the use of an active set of policies to be reused
once those policies change. Since the learning mechanisms are
modelled from the structure of the policies, policy modiﬁcations
can be mapped onto the learnt model. Our analysis of the
policy modiﬁcations suggest that most of the learned model could
be reused, potentially accelerating the learning process. In this
paper, we provide formal deﬁnitions on the different kinds of
policy modiﬁcations that might occur as well as elaborate, with
detailed examples, on how such modiﬁcations could impact the
currently learned model.
Index Terms—Model Transformation, Model Adaptation, Re-
inforcement Learning, Autonomic Management, Policy-based
Management.
I. INTRODUCTION
Policy-based management has been proposed as a way
in which the behavior of systems and applications can be
adjusted at run-time rather than through re-engineering [1].
The concept of policy-based management has been adopted
by standard bodies such as the (Internet Engineering Task
Force (IETF) [2], the Distributed Management Task Force
(DMTF) [3], and the Object Management Group (OMG) [4].
Policy-based management has also attracted signiﬁcant interest
from a wide variety of efforts, ranging from academia [5] to
industry [6], which have resulted in a great deal of diversity
in terms of what policies are and how they should be used.
It is often common that policies are used to express required
or desired behavior of systems and applications. In the context
of autonomic computing, for example, policies can be input
to or embedded within the autonomic management elements
of the system to provide the kinds of directives which an
autonomic manager could make use of in order to meet
operational requirements. Thus, through the modiﬁcations of
the policies driving autonomic management, the behavior of
systems could be dynamically adjusted. Such directives could
come from the users of such systems or other autonomic
management elements within the computing environment.
While the use of policies has been the subject of con-
siderable research focus, very little work has been done on
evaluating the effectiveness of the policies when in use to
determine whether the performance objectives are, in fact,
being met. More often than not, policy use within autonomic
computing has typically focused on the speciﬁcation and use
“as is” within systems. It is often assumed, for example,
that once the policies are speciﬁed, the system would behave
as expected. Consequently, many of these approaches fail to
recognize the stochastic, dynamic, and heterogeneous nature
of today’s systems which necessitates the need for autonomic
systems to adapt not only to how they use policies, but
also to run-time policy modiﬁcations. This is essential in
order for systems to cope with not only changes to the
conﬁguration of the managed environment, but also changes
to the perceived quality of services, something that is often
time-, user-dependent, and application-speciﬁc.
To this end, we have proposed an adaptive policy-driven
autonomic management framework highlighting two key con-
tributions in the use of policies within autonomic computing:
(1) The ability to evaluate, through model-learning, the effec-
tiveness of the enabled policies based on past experience from
their use which, in turn, provide guidance to the autonomic
system on how to effectively use the policies [7]. (2) The
ability to adapt the model learned from the use of policies
to run-time policy modiﬁcations, essentially allowing most of
the learned model to be reused in the new environment [8].
The focus of this work is on the second of these approaches.
It expands on the strategies we initially proposed in [8] by
formally describing the types of changes to a set of policies
that could occur and how we map such changes to model
transformations. This is done within the context of the adaptive
policy-driven autonomic management framework in [9].
The rest of the paper is organized as follows. We begin
with an overview of how we model learning in Section II,
summarizing some of the key deﬁnitions we make use of in
this paper. We then present our approach to model adaptation
in Section III. The section begins with formal deﬁnitions of
the types of changes that might occur to a set of policies, and
then describe how such modiﬁcations are mapped to speciﬁc
cases of model transformations. We summarize the results of
the experiments in Section IV, describe some related work in
Section V, and conclude in Section VI.

478
International Journal on Advances in Intelligent Systems, vol 2 no 4, year 2009, http://www.iariajournals.org/intelligent_systems/
II. LEARNING BY REINFORCEMENT
This section summarizes key deﬁnitions outlining our ap-
proach to modelling Reinforcement Learning [10] in the con-
text of policy-driven autonomic management. A more detailed
account can be found in [7].
Deﬁnition 1: An expectation policy is deﬁned by the tuple
pi = ⟨C, A⟩ where:
• C is conjunctive conditions associated with policy pi
with each condition, cj ∈ C, deﬁned by the tuple cj =
⟨ID, metricName, operator, Γ⟩, where ID is a
unique identiﬁcation for the condition; metricName is
the name of the metric associated with the condition;
operator is the relational operator associated with the
condition; and Γ is the threshold of the condition.
• A is a set of actions associated with policy pi with
each action, aj ∈ A, deﬁned by the tuple aj = ⟨ID,
function, parameters, τ⟩, where ID is a unique
identiﬁcation for the action; function is the name of
the function that should be executed; parameters is
a set of function parameters; and τ is a set of tests
associated with the action.
A policy-driven autonomic management system is likely to
consist of multiple expectation policies, a subset of which may
be active (or enabled) at any given time, which brings us to
our next deﬁnition.
Deﬁnition 2: Suppose that P A denotes a set of all expecta-
tion policies such that pi ∈ P A where pi = ⟨C, A⟩. Let P be
a subset of expectation policies an autonomic manager uses to
make management decisions; i.e., P ⊆ P A. A policy system
corresponding to P is deﬁned by the tuple PS = ⟨P, WC⟩
where:
• WC = ⟨ci, ωi⟩ associates each policy condition, ci, with a
metric weight, ωi, such that, for all ci ∈ pm and cj ∈ pn,
ωi = ωj if ci.metricName = cj.metricName.
Essentially, a policy system is the set of active policies where
each condition occurring in some policy has an associated
weight. The metrics weights, which are speciﬁed manually in
our current implementation, provide a way of distinguishing
policy conditions based on the signiﬁcance of violating a
particular metric. In essence, WC provides a way of biasing
how the autonomic system responds to quality of service
violations. These deﬁnitions provide the fundamental structure
that is used to build our reinforcement learning model.
A. System States
To model system’s dynamics from the use of an active
set of policies, we make use of a mapping between the
enabled expectation policies and the managed system’s states
whose structure is derived from the metrics associated with
the enabled policy conditions.
Deﬁnition 3: A policy system PS = ⟨P, WC⟩ derives a set
of system metrics, mi ∈ M, such that, for each pj = ⟨Cj, Aj⟩
where pj ∈ P, M =
[
ci∈Cj
{ci.metricName}.
The set M is the set of all metrics occurring in any of the
active policies. For each metric in this set, there are a ﬁnite
number of threshold values to which the metric is compared;
there can be ordered to form regions:
Deﬁnition 4: A policy system PS = ⟨P, WC⟩ with metrics
set M derives a set of metric regions, rmi ∈ M P
R , for each
metric mi ∈ M, whose structure is deﬁned by the tuple rmi =
⟨αmi, σmi⟩, where:
• αmi = ⟨ID, metricName, ω⟩ corresponds to a unique
metric from among the metrics of the conditions of the
policies in P; such that, metricName is the name of
the metric and ω is the weight of the condition (see
Deﬁnition 2) associated with metric mi.
• σmi = {Γ1, Γ2, . . . , Γk} is a set of thresholds from the
conditions associated with metric mi such that, Γi <
Γj if i < j. As such, σmi derives a set of metric
regions which map the observed metric measurement
onto appropriate localities (i.e., intervals) as deﬁned by
the thresholds of the policy conditions associated with
metric mi, such that Rmi = {R1
mi, R2
mi, . . . , Rk+1
mi },
where R1
mi = (−∞, Γ1), R2
mi = (Γ1, Γ2), etc., and
Rk+1
mi
= (Γk, ∞).
To be precise, the actual boundaries of region Rj
mi are
determined by the operators of the policy conditions asso-
ciated with a given metric. For example, a system with two
conditions c1 = ⟨ID,mi,>,Γ1⟩ and c2 = ⟨ID,mi,≥,Γ2⟩
such that σmi = ⟨Γ1, Γ2⟩ would yield three regions in our
approach, namely: R1
mi = [−∞, Γ1), R2
mi = [Γ1, Γ2), and
R3
mi = [Γ2, ∞]. Thus, a metric measurement of, say, Γ1 would
fall into region R2
mi. This brings us to our next deﬁnition.
Deﬁnition 5: Given a set of metric-regions rmi ∈ M P
R for
each metric mi ∈ M such that rmi = ⟨αmi, σmi⟩, where
σmi derives a set of metric regions Rj
mi ∈ Rmi; we deﬁne
a weighting function, f(Rj
mi) → R, which assigns a numeric
value to the j-th region in Rmi such that, f(Rk
mi) > f(Rl
mi)
if k < l.
An example of such a mapping, which we make use of in
our current implementation, is deﬁned by Equation 1:
f(Rj
mi) = 100 − ( 100
n − 1)(j − 1)
(1)
where n is the total number of regions in Rmi. This function
assigns a numeric value between 100 and 0 for each metric’s
region in Rmi, starting from 100 for the most desirable region
and decrementing at equal intervals towards the opposite end
of the spectrum, whose region is assigned a value of 0. This
approach guarantees that the highest value is assigned to
the most desirable region (i.e., the region corresponding to
the highest quality of service), assuming, of course, that the
assumptions about the conditions of the expectation policies
hold (see Deﬁnition 1). That is, smaller metric values are
“more desirable” though, in general, this is not a necessary
requirement of the weighting function. The idea is that, regions
of greater “desirability”, i.e., preferred quality of service, are
assigned higher values. The key role of these regions is that
they partition the space of values that a metric can take on with
respect to the thresholds in conditions involving that metric.
We use these to deﬁne a state within our model.
Deﬁnition 6: A policy system PS = ⟨P, WC⟩ with metrics
M and metrics-regions M P
R derives a set of system states
2

479
International Journal on Advances in Intelligent Systems, vol 2 no 4, year 2009, http://www.iariajournals.org/intelligent_systems/
S such that each state si
∈ S is deﬁned by the tuple
si = ⟨µ, M(si), A(si)⟩, and where:
• µ is a type which classiﬁes a state as either “violation”
or “non-violation” depending, respectively, on whether or
not there are any policy violations as a result of visiting
state si.
• P(si) is a set of expectation policies that are violated
when the system is in state si such that, P(si) ∈ P. As
noted previously, a policy is said to be violated if all its
conditions evaluate to true when matched against viola-
tion notiﬁcations received within a management cycle.
• A(si) is a set of actions advocated by the expectation
policies in P(si) plus the γ-action; i.e., a0 which corre-
sponds to “do-nothing”.
• M(si) is a set of state metrics for each metric mj ∈ M,
rmj ∈ M P
R , rmj = ⟨αmj, σmj⟩, such that each state
metric si.mj ∈ M(si) is deﬁned as follows:
Deﬁnition 7: A
state
metric
si.mj
∈
M(si)
given
αmj
=
⟨ID, metricName, ω⟩ and σmj = ⟨Γ1, Γ2, . . . , Γk⟩
is deﬁned by the tuple si.mj = ⟨ID, ω, value, Rl
mj⟩ where:
• ID is an integer value that uniquely identiﬁes each metric
mi ∈ M.
• ω is the weight associated with metric mi.
• value is the observed metric measurement, or average
value when state s is visited multiple times.
• Rl
mj is the region corresponding to a region in σmj
in which the average metric measurement (i.e., value)
falls; i.e., if Rl
mj = (Γ1, Γ2), then Γ1 < si.mj.value <
Γ2. Thus, for each such region, f(Rl
mj) associates a value
as described by Equation 1.
Using this approach, each state can be uniquely identiﬁed
by the region occupied by each state metric based on the con-
ditions of the expectation policies and the value associated
with each metric. That is, for a set of policies involving n
metrics, each state would have n metrics {m1, m2, ..., mn}
each assigned a region whose intervals are derived from the
thresholds of the conditions associated with the metric. We
elaborate further on this in Section II-C.
B. System Transitions
Transitions are essentially determined by the actions taken
by the management system and labelled by a value determined
by the learning algorithm.
Deﬁnition 8: A state transition ti(sp, ap, sc) is a directed
edge corresponding to a transition originating from state sp
and ending on state sc as a result of taking action ap while in
state sp, and is labelled by ⟨λ, Qti(sp, ap)⟩, where:
• λ is the frequency (i.e., the number of times) through
which the transition occurs.
• Qti(sp, ap) is the action-value estimate associated with
taking action ap in state sp. In our current implemen-
tation, Qti(sp, ap) is computed using a one-step Q-
Learning [10], [11] algorithm.
It is worth pointing out that, non-deterministic transitions
are also possible. That is, taking the same action in the same
state during different time-steps may result in transitions to
different states. A change in the system’s state may also be due
to external factors other than the impact of the actions taken by
the autonomic manager. In a dynamic Web server environment,
for example, a transition may be a result of a request to a page
with a database-intensive query, which could potentially cause
a state transition. In this work, such transitions are referred
to as γ-transitions; the action responsible for γ-transitions is
denoted by a0 (i.e., γ-action) as illustrated in Table IV.
C. State-Transition Model
A state-transition model is then deﬁned for a set of active
expectation policies:
Deﬁnition 9: A state-transition model derived from the
policy system PS
=
⟨P, WC⟩ is deﬁned by the graph
GP = ⟨S, T⟩ where:
• S is a set of system states (see Section II-A) derived from
the metrics of the conditions of the enabled expectation
policies and where each state, si ∈ S, corresponds to a
vertex on the graph.
• T is a set of transitions (see Section II-B) where each
transition, ti
∈
T, corresponds to a directed edge on
the graph. A transition is determined when the autonomic
manager takes an action as a result of being in one state,
which may, or may not, result in a transition to another
state.
For illustration purposes, suppose that the policies of Fig-
ure 1 are the only enabled expectation policies. From the
conditions of Table I, six metrics are then formed as illustrated
in Table II. Metric m1, for instance, would correspond to
the “CPU:utilization” policy conditions. This metric is
mapped onto three regions based on the thresholds of the con-
ditions the metric is associated with; i.e., σm1 = {15.0, 85.0}.
This means that, “CPU:utilization” could fall into three
unique localities; its value could be less than 15.0% (i.e.,
region R1
m1) between 85.0% and 15.0% inclusive (i.e., region
R2
m1), or greater than 85.0% (i.e., region R3
m1). Each of the
regions is also assigned a numeric value between 0 and 100 as
described in Deﬁnition 5. It is worth pointing out that, for the
system with only the policies in Figure 1 enables, 144 (i.e.,
32 ×24) states are possible. This is because there are two state
metrics (i.e., m1 and m5) with three possible regions each (i.e.,
32 possible permutations) and four state metrics (i.e., m2, m3,
m4, and m6) with two possible regions each (i.e., 24 possible
permutations).
ci
WC(ci)
Policy Condition
1
1/8
CPU:utilization > 85.0
2
1/8
CPU:utilizationTREND > 0.0
3
1/8
CPU:utilization < 15.0
4
1/8
MEMORY:utilization > 40.0
5
1/8
MEMORY:utilizationTREND > 0.0
6
1/8
APACHE:responseTime > 2000.0
7
1/8
APACHE:responseTimeTREND > 0.0
8
1/8
APACHE:responseTime < 250.0
TABLE I
POLICY CONDITIONS SET FROM THE EXPECTATION POLICIES IN FIGURE 1.
Once the metrics structure has been constructed, the next
step in the creation of actual states involves mapping the Mon-
3

480
International Journal on Advances in Intelligent Systems, vol 2 no 4, year 2009, http://www.iariajournals.org/intelligent_systems/


[ 1 ]
expectation
policy {CPUViolation (PDP, PEP)}
i f (CPU: u t i l i z a t i o n > 8 5 . 0 ) &
(CPU: utilizationTREND > 0 . 0 )
then{ AdjustMaxClients (−25)
t e s t {newMaxClients > 49}
|
AdjustMaxKeepAliveRequests (−30)
t e s t {newMaxKeepAliveRequests > 1}
|
AdjustMaxBandwidth ( −128)
t e s t {newMaxBandwidth > 255}}
[ 2 ]
expectation
policy { RESPViolation (PDP, PEP)}
i f (APACHE: responseTime > 2000.0) &
(APACHE: responseTimeTREND > 0 . 0 )
then{ AdjustMaxClients (+25)
t e s t {newMaxClients < 151} &
t e s t { IdleWorkers
< 5}
|
AdjustMaxKeepAliveRequests (−30)
t e s t {newMaxKeepAliveRequests > 1}
|
AdjustMaxBandwidth ( −128)
t e s t {newMaxBandwidth > 255}}
[ 3 ]
expectation
policy {CPUandRESPViol . ( PDP, PEP)}
i f (CPU: u t i l i z a t i o n > 8 5 . 0 ) &
(CPU: utilizationTREND > 0 . 0 ) &
(APACHE: responseTime > 2000.0)
then{ AdjustMaxKeepAliveRequests (−30)
t e s t {newMaxKeepAliveRequests > 1}
|
AdjustMaxBandwidth ( −128)
t e s t {newMaxBandwidth > 255}}
[ 4 ]
expectation
policy {MEMORYViolation (PDP, PEP)}
i f (MEMORY: u t i l i z a t i o n > 4 0 . 0 ) &
(MEMORY: utilizationTREND > 0 . 0 )
then{ AdjustMaxClients (−25)
t e s t {newMaxClients > 49}}
[ 5 ]
expectation
policy {SERVERnormal (PDP, PEP)}
i f (CPU: u t i l i z a t i o n < 1 5 . 0 ) &
(APACHE: responseTime < 250.0)
then{ AdjustMaxClients (−25)
t e s t {newMaxClients > 49} &
t e s t { IdleWorkers > 25}
|
AdjustMaxKeepAliveRequests (+30)
t e s t {newMaxKeepAliveRequests < 95}}



Fig. 1.
Sample policy system where P = {p1, p2, p3, p4, p5}.
itor events collected during a single management interval onto
appropriate state metrics and regions. Suppose, for example,
that during a single management interval, the management
system receives the events in Figure 2. By mapping the events
using the metrics structure in Table II, a new state is then
created as illustrated in Table III; lets call it state s∗. For a
management system with only the policies of Figure 1 enabled,
being in state s∗ would mean the violation of policy p1; i.e..
P(s∗) = {p1}. Thus, in addition to the action “a0 : γ-
action” which corresponds to “doing nothing”, A(s∗) would
consist of a set of unique actions from the actions of the
policies in P(s∗) as illustrated in Table IV.
CPU:utilization = 90.0
CPU:utilizationTREND = 1.0
MEMORY:utilization = 32.0
MEMORY:utilizationTREND = -2.0
APACHE:responseTime = 1500.0
APACHE:responseTimeTREND = -1.0
Fig. 2. A set of Monitor events collected during a single management interval.
In the sections that follow, we describe our approach for
adapting the state-transition model to run-time policy mod-
iﬁcations (denoted by △[P]) which we formally deﬁne in
Section III-A.
III. ADAPTING TO POLICY CHANGES
Figure 3 describes our approach to model-adaptation. The
left-part of the diagram (i.e., prior to △[P]) depicts the
Reinforcement Learning mechanisms described in Section II.
whereby the agent learns the model of the environment based
on past experience in the use of an active set of policies, P.
In this approach, the conditions of the policies deﬁne system
states while the actions of the policies deﬁne possible causes
of transitions between states. That is, at each time step, the
agent takes action ai ∈ A(s) where A(s) is a set of actions
advocated by the policies that are violated when the system
is in state s. This, in turn, causes a transition as depicted
in the diagram. Thus, the model is continuously updated
based on the experience garnered from the agent’s interaction
with its environment; i.e., based on the states and transitions
encountered. The right-part of the diagram (i.e., after △[P]
occurs) depicts the mapping (using the transformation function
Ψ) from the current system’s model (i.e., GP
n = ⟨S, T⟩) to a
new model (i.e., GP ′
n+1 = ⟨S′, T ′⟩), as a result of run-time
modiﬁcation to the policies in P.
A. Types of Modiﬁcations
By run-time policy modiﬁcations, we mean any type of
modiﬁcation resulting in changes to the characteristics of the
policies driving autonomic management. Since the structure
speciﬁc to the model of the environment is derived from the
characteristics of the enabled policies (see Section II-A), any
run-time changes to these characteristics could have possible
ramiﬁcations on the learning process. This would likely de-
pend on what kind of modiﬁcation is done on the policies.
1) Adding/Removing a Policy: A policy modiﬁcation could
involve adding a policy onto the policies in P; i.e.,
Given:
I) PS = ⟨P, WC⟩
II) p∗ = ⟨C, A⟩ where p∗ /∈ P :
• C
=
{c1, c2, . . . , cx} : cx
=
⟨ID,
metricName, operator, Γ⟩
• A
=
{a1, a2, . . . , ay} : ay
=
⟨ID,
function, parameters, τ⟩
Then:
1) PS′ = ⟨P ′, WC⟩ : P ′ ← P ∪ p∗
A policy modiﬁcation could also involve removing an
existing policy from P; i.e.,
Given:
I) PS = ⟨P, WC⟩
II) p∗ = ⟨C, A⟩ where p∗ ∈ P :
• C
=
{c1, c2, . . . , cx} : cx
=
⟨ID,
metricName, operator, Γ⟩
• A
=
{a1, a2, . . . , ay} : ay
=
⟨ID,
function, parameters, τ⟩
4

481
International Journal on Advances in Intelligent Systems, vol 2 no 4, year 2009, http://www.iariajournals.org/intelligent_systems/
mi
ck
Policy Condition
Rmi
Rj
mi
f(Rj
mi)
m1
3
CPU:utilization < 15.0
m1.value < 15.0
R1
m1
100
15.0 ≤ m1.value ≤ 85.0
R2
m1
50
1
CPU:utilization > 85.0
m1.value > 85.0
R3
m1
0
m2
m2.value ≤ 0.0
R1
m2
100
2
CPU:utilizationTREND > 0.0
m2.value > 0.0
R2
m2
0
m3
m3.value ≤ 40.0
R1
m3
100
4
MEMORY:utilization > 40.0
m3.value > 40.0
R2
m3
0
m4
m4.value ≤ 0.0
R1
m4
100
5
MEMORY:utilizationTREND > 0.0
m4.value > 0.0
R2
m4
0
m5
8
APACHE:responseTime < 250.0
m5.value < 250.0
R1
m5
100
250.0 ≤ m5.value ≤ 2000.0
R2
m5
50
6
APACHE:responseTime > 2000.0
m5.value > 2000.0
R3
m5
0
m6
m6.value ≤ 0.0
R1
m6
100
7
APACHE:responseTimeTREND > 0.0
m6.value > 0.0
R2
m6
0
TABLE II
METRICS STRUCTURE DERIVED FROM THE EXPECTATION POLICIES OF FIGURE 1.
mi
Metric Value
Rmi
Rj
mi
f(Rj
mi)
m1
CPU:utilization = 90.0
m1.value > 85.0
R3
m1
0
m2
CPU:utilizationTREND = 1.0
m2.value > 0.0
R2
m2
0
m3
MEMORY:utilization = 32.0
m3.value ≤ 40.0
R1
m3
100
m4
MEMORY:utilizationTREND = -2.0
m4.value ≤ 0.0
R1
m4
100
m5
APACHE:responseTime = 1500.0
250.0 ≤ m5.value ≤ 2000.0
R2
m5
50
m6
APACHE:responseTimeTREND = -1.0
m6.value ≤ 0.0
R1
m6
100
TABLE III
s∗ - A STATE DERIVED FROM THE MONITOR EVENTS IN FIGURE 2 AND THE METRICS STRUCTURE OF TABLE II.
ai
Q(s∗, ai)
Policy Actions
a0
γ-action
a2
AdjustMaxClients(-25) test{newMaxClients > 49}
a4
AdjustMaxKeepAliveRequests(-30) test{newMaxKeepAliveRequests > 1}
a6
AdjustMaxBandwidth(-128) test{newMaxBandwidth > 255}
TABLE IV
A(s∗) - A SET OF UNIQUE ACTIONS FROM THE ACTIONS OF THE POLICIES THAT ARE VIOLATED WHEN THE SYSTEM IS IN STATE s∗.
Fig. 3.
Adapting to run-time policy modiﬁcations.
Then:
1) PS′ = ⟨P ′, WC⟩ : P ′ ← P|p∗
Since the impact of such modiﬁcations on the state-
transition model would depend on what conditions and/or
actions are affected as a result, adding/removing an entire
policy can be modelled in the context of adding/removing
individual conditions and actions within policies.
On the one hand, adding policy p∗ = ⟨C, A⟩ into the
policies in P can be modelled by the following sequence of
policy modiﬁcations: (i) Adding the policy without conditions
or actions; i.e., p∗ = ⟨∅, ∅⟩ (see below). (ii) Adding one con-
dition at a time from the conditions in C (see Section III-A2).
(iii) Adding one action at a time from the actions in A (see
Section III-A5).
Policy Modiﬁcation - △x[P]: Adding a policy without con-
ditions or actions.
Given:
I) PS = ⟨P, WC⟩
II) M P
R = {r1, r2, . . . , rl} : ri = ⟨αi, σi⟩
III) p∗ = ⟨∅, ∅⟩ such that p∗ /∈ P
5

482
International Journal on Advances in Intelligent Systems, vol 2 no 4, year 2009, http://www.iariajournals.org/intelligent_systems/
Then:
1) PS′ = ⟨P ′, WC⟩ : P ′ ← P ∪ p∗
2) M ′ ← M
3) M P ′
R ← M P
R
On the other hand, removing policy p∗ = ⟨C, A⟩ from the
policies in P can be modelled by the following sequence of
policy modiﬁcations: (i) Removing one action at a time from
the actions in A (see Section III-A6). (ii) Removing one con-
dition at a time from the conditions in C (see Section III-A3).
(iii) Removing the policy without conditions or actions; i.e.,
p∗ = ⟨∅, ∅⟩ (see below).
Policy Modiﬁcation - △y[P]: Removing a policy without
conditions or actions.
Given:
I) PS = ⟨P, WC⟩
II) M P
R = {r1, r2, . . . , rl} : ri = ⟨αi, σi⟩
III) p∗ = ⟨∅, ∅⟩ such that p∗ ∈ P
Then:
1) PS′ = ⟨P ′, WC⟩ : P ′ ← P|p∗
2) M ′ ← M
3) M P ′
R ← M P
R
2) Adding a Policy Condition: Another type of policy
modiﬁcation could involve adding a condition into one of the
policies in P.
Given:
• PS = ⟨P, WC⟩
• p∗ = ⟨C, A⟩ such that p∗ ∈ P
• c∗ = ⟨ID, metricName, operator, Γ⟩
Then:
1) PS′ = ⟨P ′, WC⟩ : p′
∗ = ⟨C′, A⟩ where C′ ←
C ∪ c∗
There are several ways in which adding a policy condition
could impact the system’s metrics structure (see Deﬁnition 4):
In the ﬁrst case, adding a policy condition may result in an
increase in the number of system metrics. This may be a result
of adding a policy condition whose metricName is not in
any of the conditions of the policies in P.
Policy Modiﬁcation - △1[P]: Adding a policy condition re-
sulting in a new metric.
Given:
I) PS = ⟨P, WC⟩
II) M P
R = {r1, r2, . . . , rl} : ri = ⟨αi, σi⟩
III) p∗ = ⟨C, A⟩ such that p∗ ∈ P where ∀ pi ∈ P,
c∗ /∈ pi
IV) c∗
=
⟨ID, metricName, operator,
Γ⟩ such that m = c∗.metricName and where
m /∈ M
V) rm /∈ M P
R : rm = ⟨αm, σm⟩ :
• αm = ⟨ID, metricName, ω⟩
• σm = {Γ}
Then:
1) PS′ = ⟨P ′, WC⟩ : p′
∗ = ⟨C′, A⟩ where C′ ←
C ∪ c∗
2) M ′ ← M ∪ m
3) M P ′
R ← M P
R ∪ rm
In the second case, adding a policy condition may result in
changes to the regions of an existing metric; i.e., rm ∈ M P
R .
This may be a result of adding a policy condition whose
metricName is in at least one of the conditions of the
policies in P and where there is no occurrence of the condition
within the policies in P.
Policy Modiﬁcation - △2[P]: Adding a policy condition re-
sulting in changes to the regions of existing metrics.
Given:
I) PS = ⟨P, WC⟩
II) M P
R = {r1, r2, . . . , rl} : ri = ⟨αi, σi⟩
III) p∗ = ⟨C, A⟩ such that p∗ ∈ P where ∀ pi ∈ P,
c∗ /∈ pi
IV) c∗
=
⟨ID, metricName, operator,
Γ⟩ such that m = c∗.metricName and where
m ∈ M
V) rm ∈ M P
R : rm = ⟨αm, σm⟩ :
• αm = ⟨ID, metricName, ω⟩
• σm = {Γ1, Γ2, . . . , Γj, Γj+1, . . . , Γk}
Then:
1) PS′ = ⟨P ′, WC⟩ : p′
∗ = ⟨C′, A⟩ where C′ ←
C ∪ c∗
2) M ′ ← M
3) M P ′
R ← M P
R : ∃ rm ∈ M P
R : r′
m = ⟨α′
m, σ′
m⟩ :
a) α′
m = ⟨ID, metricName, ω⟩
b) σ′
m
=
σm
∪
c∗.Γ
:
σ′
m
=
{Γ1, Γ2, . . . , Γj, Γ, Γj+1, . . . , Γk}
where
Γj < Γ < Γj+1 for some j.
In the third case, adding a policy condition may result in
no changes to the number of metrics and their regions. This
may be a result of adding a policy condition that is identical
(i.e., share the same metricName, operator, and Γ) to at
least one of the conditions of the policies in P.
Policy Modiﬁcation - △3[P]: Adding a policy condition re-
sulting in no changes to the metrics’ regions.
Given:
I) PS = ⟨P, WC⟩
II) M P
R = {r1, r2, . . . , rl} : ri = ⟨αi, σi⟩
III) p∗ = ⟨C, A⟩ such that p∗ ∈ P and where
∃ pi ∈ P such that c∗ ∈ pi
IV) c∗
=
⟨ID, metricName, operator,
Γ⟩ such that m = c∗.metricName and where
m ∈ M
V) rm ∈ M P
R : rm = ⟨αm, σm⟩ :
• αm = ⟨ID, metricName, ω⟩
• σm = {Γ1, Γ2, . . . , Γj−1, Γj, Γj+1 . . . Γk} :
c∗.Γ = Γj
Then:
1) PS′ = ⟨P ′, WC⟩ : p′
∗ = ⟨C′, A⟩ where C′ ←
C ∪ c∗
2) M ′ ← M
3) M P ′
R ← M P
R
3) Removing a Policy Condition: Another type of policy
modiﬁcation could involve removing a condition from one of
the policies in P.
Given:
6

483
International Journal on Advances in Intelligent Systems, vol 2 no 4, year 2009, http://www.iariajournals.org/intelligent_systems/
• PS = ⟨P, WC⟩
• p∗ = ⟨C, A⟩ such that p∗ ∈ P and c∗ ∈ C
• c∗ = ⟨ID, metricName, operator, Γ⟩
Then:
1) PS′ = ⟨P ′, WC⟩ : p′
∗ = ⟨C′, A⟩ where C′ ←
C|c∗
There are several ways in which removing a policy condi-
tion could impact the system’s metrics structure:
In the ﬁrst case, removing a policy condition may result in
a decrease in the number of system metrics. This may be a
result of removing a policy condition whose metricName is
not in any of the remaining conditions of the policies in P;
i.e., there is only a single occurrence of the condition with the
given metricName within the policies in P.
Policy Modiﬁcation - △4[P]: Removing a policy condition
resulting in a decrease in the number of metrics.
Given:
I) PS = ⟨P, WC⟩
II) M P
R = {r1, r2, . . . , rl} : ri = ⟨αi, σi⟩
III) p∗ = ⟨C, A⟩ such that p∗ ∈ P and c∗ ∈ C
where ∀ pi ∈ P|p∗, c∗ /∈ pi
IV) c∗
=
⟨ID, metricName, operator,
Γ⟩ such that m = c∗.metricName and where
m ∈ M, such that, ∀ mi ∈ M|m, mi ̸= m
V) rm ∈ M P
R : rm = ⟨αm, σm⟩ :
• αm = ⟨ID, metricName, ω⟩
• σm = {Γ}
Then:
1) PS′ = ⟨P ′, WC⟩ : p′
∗ = ⟨C′, A⟩ where C′ ←
C|c∗
2) M ′ ← M|m
3) M P ′
R ← M P
R |rm
In the second case, removing a policy condition may result
in changes to the regions of existing metrics. This may be a
result of removing a policy condition whose metricName is
in at least one of the conditions of the policies in P and where
there is only a single occurrence of the condition within the
policies in P.
Policy Modiﬁcation - △5[P]: Removing a policy condition
resulting in changes to the regions of existing metrics.
Given:
I) PS = ⟨P, WC⟩
II) M P
R = {r1, r2, . . . , rl} : ri = ⟨αi, σi⟩
III) p∗ = ⟨C, A⟩ such that p∗ ∈ P and c∗ ∈ C
where ∀ pi ∈ P|p∗, c∗ /∈ pi
IV) c∗
=
⟨ID, metricName, operator,
Γ⟩ such that m = c∗.metricName where
m ∈ M, and where ∃ mi ∈ M|m such that
m = mi
V) rm ∈ M P
R : rm = ⟨αm, σm⟩ :
• αm = ⟨ID, metricName, ω⟩
• σm = {Γ1, Γ2, . . . , Γj−1, Γj, Γj+1, . . . , Γk}
: c∗.Γ = Γj
Then:
1) PS′ = ⟨P ′, WC⟩ : p′
∗ = ⟨C′, A⟩ where C′ ←
C|c∗
2) M ′ ← M
3) M P ′
R ← M P
R : ∃ rm ∈ M P
R : r′
m = ⟨α′
m, σ′
m⟩
:
a) α′
m = ⟨ID, metricName, ω⟩
b) σ′
m
=
σm|c∗.Γ
:
σ′
m
=
{Γ1, Γ2, . . . , Γj−1, Γj+1, . . . , Γk}
In the third case, removing a policy condition may result in
no changes to the number of metrics and their regions. This
may be a result of removing a policy condition that is identical
(i.e., share the same metricName, operator, and Γ) to at
least one of the conditions of the policies in P; i.e., there are
multiple occurrences of the condition within the policies in P.
Policy Modiﬁcation - △6[P]: Removing a policy condition
resulting in no changes to the number of metrics and their
regions.
Given:
I) PS = ⟨P, WC⟩
II) M P
R = {r1, r2, . . . , rl} : ri = ⟨αi, σi⟩
III) p∗ = ⟨C, A⟩ such that p∗ ∈ P and c∗ ∈ C
where ∃ pi ∈ P|p∗ such that c∗ ∈ pi
IV) c∗
=
⟨ID, metricName, operator,
Γ⟩ such that m = c∗.metricName and where
m ∈ M
V) rm ∈ M P
R : rm = ⟨αm, σm⟩ :
• αm = ⟨ID, metricName, ω⟩
• σm = {Γ1, Γ2, . . . , Γj−1, Γj, Γj+1, . . . , Γk}
: c∗.Γ = Γj
Then:
1) PS′ = ⟨P ′, WC⟩ : p′
∗ = ⟨C′, A⟩ where C′ ←
C|c∗
2) M ′ ← M
3) M P ′
R ← M P
R
4) Modifying a Policy Condition: Another type of policy
modiﬁcation could involve modifying the threshold of a policy
condition within the conditions of the policies in P.
Policy Modiﬁcation - △7[P]: Modifying the threshold of a
policy condition.
Given:
I) PS = ⟨P, WC⟩
II) M P
R = {r1, r2, . . . , rl} : ri = ⟨αi, σi⟩
III) c∗
=
⟨ID, metricName, operator,
Γ⟩ such that m = c∗.metricName and where
m ∈ M
IV) rm ∈ M P
R : rm = ⟨αm, σm⟩ :
• αm = ⟨ID, metricName, ω⟩
• σm = {Γ1, Γ2, . . . , Γj−1, Γj, Γj+1, . . . , Γk}
: c∗.Γ = Γj
Then:
1) PS′
=
⟨P ′, WC⟩
:
c′
∗
=
⟨ID,
metricName, operator, Γ′⟩ ∀ p∗ ∈ P
: c∗ ∈ p∗
2) M ′ ← M
3) M P ′
R ← M P
R : ∃ rm ∈ M P
R : r′
m = ⟨α′
m, σ′
m⟩
:
a) α′
m = ⟨ID, metricName, ω⟩
7

484
International Journal on Advances in Intelligent Systems, vol 2 no 4, year 2009, http://www.iariajournals.org/intelligent_systems/
b) σ′
m
=
{Γ1, Γ2, . . . , Γl, Γ′, Γl+1, . . . , Γk}
where Γl < Γ′ < Γl+1 for some l
5) Adding a Policy Action: Another type of policy modi-
ﬁcation could involve adding a policy action onto one of the
policies in P.
Policy Modiﬁcation - △8[P]: Adding a policy action onto
a policy in P.
Given:
I) PS = ⟨P, WC⟩
II) M P
R = {r1, r2, . . . , rl} : ri = ⟨αi, σi⟩
III) p∗ = ⟨C, A⟩ such that p∗ ∈ P
IV) a∗
=
⟨ID, function, parameters,
τ⟩
V) rm ∈ M P
R : rm = ⟨αm, σm⟩ :
• αm = ⟨ID, metricName, ω⟩
• σm = {Γ1, Γ2, . . . , Γk}
Then:
1) PS′ = ⟨P ′, WC⟩ : p′
∗ = ⟨C, A′⟩ where A′ ←
A ∪ a∗
2) M ′ ← M
3) M P ′
R ← M P
R
6) Removing a Policy Action:
Another type of policy
modiﬁcation could involve removing a policy action from one
of the policies in P.
Policy Modiﬁcation - △9[P]: Removing an action from a
policy in P.
Given:
I) PS = ⟨P, WC⟩
II) M P
R = {r1, r2, . . . , rl} : ri = ⟨αi, σi⟩
III) p∗ = ⟨C, A⟩ such that p∗ ∈ P and a∗ ∈ A
IV) a∗
=
⟨ID, function, parameters,
τ⟩
V) rm ∈ M P
R : rm = ⟨αm, σm⟩ :
• αm = ⟨ID, metricName, ω⟩
• σm = {Γ1, Γ2, . . . , Γk}
Then:
1) PS′ = ⟨P ′, WC⟩ : p′
∗ = ⟨C, A′⟩ where A′ ←
A|a∗
2) M ′ ← M
3) M P ′
R ← M P
R
7) Modifying a Policy Action:
Another type of pol-
icy modiﬁcation could involve changing the attributes (i.e.,
parameters, or τ) of a policy action within the actions of
the policies in P.
Policy Modiﬁcation - △10[P]: Modifying the attributes of a
policy action.
Given:
I) PS = ⟨P, WC⟩
II) M P
R = {r1, r2, . . . , rl} : ri = ⟨αi, σi⟩
III) a∗
=
⟨ID, function, parameters,
τ⟩
IV) rm ∈ M P
R : rm = ⟨αm, σm⟩ :
• αm = ⟨ID, metricName, ω⟩
• σm = {Γ1, Γ2, . . . , Γk}
Then:
1) PS′ = ⟨P ′, WC⟩ : a′
∗ = ⟨ID, function,
parameters′, τ ′⟩ ∀ p∗ ∈ P : a∗ ∈ p∗
2) M ′ ← M
3) M P ′
R ← M P
R
B. Adaptation Transformations
This section presents our approach for adapting the model
learned from the use of an active set of policies to the
policy modiﬁcations described in Section III-A. We have
identiﬁed several cases relating to the types of changes to an
active set of policies, P, in the context of how they might
impact the model (i.e., the state-transition graph) leaned from
using P. For illustration purposes, this section assumes that
initially, set P consists of the expectation policies depicted in
Figure 1. Suppose also that the current model of the system
(i.e., GP
n = ⟨S, T⟩), as depicted in Figure 4, includes the
seven states shown in Table V. Note that, states s2 and s6
are considered “non-violation” whereas the other states are
considered “violation” states (see µ in Deﬁnition 6). The focus
of our discussion centers on how the various changes to the
policies in P might affect the original model.
Fig. 4.
A sample state transition model.
1) Adapting to △[P] Affecting States Actions: This de-
scribes our approach for dealing with policy modiﬁcations that
only impact the actions within states and not states transitions.
From the deﬁnitions of Section III-A, this may be in response
to policy modiﬁcation △8[P] or △10[P].
Model Transformation - Ψ1[GP
n ]: Adapting to policy mod-
iﬁcation △8[P] or △10[P].
Given:
I) GP
n = ⟨S, T⟩ :
• ∀ si ∈ S, si
=
⟨µ, M(si), P(si), A(si)⟩
: si.mk
=
⟨ID, ω, value, Rl
mk⟩ where
si.mk ∈ M(si)
• ∀
t(si, a, sj)
∈
T,
t(si, a, sj)
=
⟨λ, Qt(si, a)⟩
II) Policy modiﬁcation △8[P] or △10[P]
Then: GP ′
n+1 = ⟨S′, T ′⟩ where
1) S′ ← S :
8

485
International Journal on Advances in Intelligent Systems, vol 2 no 4, year 2009, http://www.iariajournals.org/intelligent_systems/
f(Rk
mj )
A(si)
t(si, al, s∗)
si
f(Rk
m1)
f(Rk
m2)
f(Rk
m3)
f(Rk
m4)
f(Rk
m5)
f(Rk
m6)
al
State action
λ
s∗
s1
0
0
0
0
0
0
a0
γ-action
a1
AdjustMaxClients(+25)
4
s4
a2
AdjustMaxClients(-25)
a4
AdjustMaxKeepAliveRequests(-30)
1
s5
a6
AdjustMaxBandwidth(-128)
7;2
s3;s6
s2
0
100
0
100
0
100
a0
γ-action
5;3
s1;s2
s3
50
100
0
0
100
100
a0
γ-action
6
s2
a2
AdjustMaxClients(-25)
8
s2
s4
0
100
0
100
0
0
a0
γ-action
6
s4
a1
AdjustMaxClients(+25)
a4
AdjustMaxKeepAliveRequests(-30)
9
s1
a6
AdjustMaxBandwidth(-128)
1
s7
s5
0
0
100
100
0
0
a0
γ-action
a1
AdjustMaxClients(+25)
3
s5
a2
AdjustMaxClients(-25)
a4
AdjustMaxKeepAliveRequests(-30)
3
s2
a6
AdjustMaxBandwidth(-128)
s6
50
100
0
100
100
100
a0
γ-action
5
s1
s7
100
100
100
100
100
100
a0
γ-action
2
s1
a2
AdjustMaxClients(-25)
1
s7
a3
AdjustMaxKeepAliveRequests(+30)
4
s3
TABLE V
SAMPLE STATES BASED ON THE METRICS STRUCTURE OF TABLE II AND THE STATE-TRANSITION MODEL OF FIGURE 4.
a) ∀ si
∈ S : p∗
∈ P ′(si); A′(si) ←
A(si) ∪ a∗ (△8[P])
b) ∀ si ∈ S; a∗ ← a′
∗
⇐⇒ ∃ a ∈ A(si) :
a∗ = a (△10[P])
2) T ′ ← T
Suppose,
for
example,
that
policy
modiﬁca-
tion
△10[P]
involves
modifying
policy
action
“AdjustMaxBandwidth(-128)”
(which
corresponds
to action a6 in Table V) to “AdjustMaxBandwidth
(-64)” in the policies of Figure 1. The following describes
the steps involved in transforming the state-transition model
of Figure 5(a), whose states are shown in Table V and
correspond to the steps of model-transformation Ψ1[GP
n ]:
1) For each state that has been encountered to date
(see Table V), the agent must update the actions
within the states to ensure that the appropriate ac-
tion is modiﬁed (see Step 1(b)). Since action a6
was initially in actions sets A(s1), A(s4), and A(s5),
each occurrence of the action would be modiﬁed
to “AdjustMaxBandwidth(-64)” as illustrated in
Figure 5(c).
2) This change, however, would not affect the transitions
associated with the modiﬁed action. Consider, for ex-
ample, transitions t(s1, a6{7}, s3) and t(s1, a6{2}, s6),
both involving action a6 (see Figure 5(a)). The fact that
the action is still part of the actions within the states
means that the agent may still take such transitions.
Hence, no changes would be made to the transitions
unless they are encountered in the future, in which case
action-value estimates and frequencies will be updates
accordingly.
2) Adapting to △[P] Affecting States Transitions: This
describes our approach for dealing with policy modiﬁcations
that only affect the state transitions as a result of modiﬁcations
to the composition of policy actions within states. From
the deﬁnitions of Section III-A, this may be in response to
policy modiﬁcation △3[P], △6[P], or △9[P]. Our approach
essentially involves ensuring that the state transitions set T(si)
is consistent with the state actions set A(si).
Model Transformation - Ψ2[GP
n ]: Adapting to policy mod-
iﬁcation △3[P], △6[P], or △9[P].
Given:
I) GP
n = ⟨S, T⟩ :
• ∀ si ∈ S, si
=
⟨µ, M(si), P(si), A(si)⟩
: si.mk
=
⟨ID, ω, value, Rl
mk⟩ where
si.mk ∈ M(si)
• ∀
t(si, a, sj)
∈
T,
t(si, a, sj)
=
⟨λ, Qt(si, a)⟩
II) Policy modiﬁcation △3[P], △6[P], or △9[P]
Then: GP ′
n+1 = ⟨S′, T ′⟩ where
1) S′ ← S : ∀ si ∈ S ; a ∈ A′(si) ⇐⇒ a ∈
{P ′(si) ∪ a0}
2) ∀ t(si, a∗, sj) ∈ T(si) : a∗ /∈ A′(si); T ′ ←
T|t(si, a∗, sj)
3) ∀ ai ∈ A(si : ∃ t(si, a, s∗) : si, s∗ ∈ S′,
compute Q(si, a) (see Equation 2).
Q(s, a) =
X
ti(s,a,s′
j)∈T (s)
Pr[ti(s, a, s′
j)] × Qti(s, a)
(2)
Suppose,
for
example,
that
policy
modiﬁ-
cation
△9[P]
involves
deleting
policy
action
“AdjustMaxBandwidth(-128)”
(which
corresponds
to action a6 in Table V) from policy p2 in Figure 1. The
following
describes
the
steps
involved
in
transforming
the state-transition model of Figure 6(a), whose states
are shown in Table V and correspond to the steps of
model-transformation Ψ2[GP
n ]:
9

486
International Journal on Advances in Intelligent Systems, vol 2 no 4, year 2009, http://www.iariajournals.org/intelligent_systems/
(a) Original model: GP
n = ⟨S, T⟩
(b) New model: GP ′
n+1 = ⟨S′, T ′⟩
f(Rk
mj)
A(si)
t(si, al, s∗)
si
m1
m2
m3
m4
m5
m6
al
State action
λ
s∗
s1
0
0
0
0
0
0
a0
γ-action
a1
AdjustMaxClients(+25)
4
s4
a2
AdjustMaxClients(-25)
a4
AdjustMaxKeepAliveRequests(-30)
1
s5
a6
AdjustMaxBandwidth(-64)
7;2
s3;s6
s2
0
100
0
100
0
100
a0
γ-action
5;3
s1;s2
s3
50
100
0
0
100
100
a0
γ-action
6
s2
a2
AdjustMaxClients(-25)
8
s2
s4
0
100
0
100
0
0
a0
γ-action
6
s4
a1
AdjustMaxClients(+25)
a4
AdjustMaxKeepAliveRequests(-30)
9
s1
a6
AdjustMaxBandwidth(-64)
1
s7
s5
0
0
100
100
0
0
a0
γ-action
a1
AdjustMaxClients(+25)
3
s5
a2
AdjustMaxClients(-25)
a4
AdjustMaxKeepAliveRequests(-30)
3
s2
a6
AdjustMaxBandwidth(-64)
s6
50
100
0
100
100
100
a0
γ-action
5
s1
s7
100
100
100
100
100
100
a0
γ-action
2
s1
a2
AdjustMaxClients(-25)
1
s7
a3
AdjustMaxKeepAliveRequests(+30)
4
s3
(c) System states of the new model.
Fig. 5.
State-transition model after △10[P]; i.e., modifying action a6 =“AdjustMaxBandwidth(-128)” to “AdjustMaxBandwidth(-64)” in the
policies of Figure 1.
1) Since policy p2 is the only violated policy in state s4,
△9[P] would result in the removal of action a6 from
the actions set A(s4) as illustrated in Figure 6(c). While
policy p2 is also violated in states s1 and s5, △9[P]
would not affect such states. This is because action a6
is also part of other violated policies in those states,
such as policy p1 (i.e., a6 ∈ p1) where p1 ∈ P ′(s1) and
p1 ∈ P ′(s5).
2) The removal of action a6 from state s4 in the above
step would mean that transition t(s4, a6{1}, s7) (see
Figure 6(a)) is no longer valid. As such, the transition
will be removed as illustrated in Figure 6(b).
3) The ﬁnal step of the adaptation is for the agent to
perform backup updates so that any impact on the model
as a result of removing transitions is propagated to the
action-value estimates of the states actions.
3) Adapting to △[P] Affecting Metrics Regions: This de-
scribes our approach for dealing with policy modiﬁcations that
affects the regions within state metrics. From the deﬁnitions of
Section III-A, this may be in response to policy modiﬁcation
△2[P], △5[P], or △7[P].
Model Transformation - Ψ3[GP
n ]: Adapting to policy mod-
iﬁcation △2[P], △5[P], or △7[P].
Given:
I) GP
n = ⟨S, T⟩ :
• ∀ si ∈ S, si
=
⟨µ, M(si), P(si), A(si)⟩
: si.mk
=
⟨ID, ω, value, Rl
mk⟩ where
si.mk ∈ M(si)
• ∀
t(si, a, sj)
∈
T,
t(si, a, sj)
=
⟨λ, Qt(si, a)⟩
II) Policy modiﬁcation △4[P] or △5[P] or △7[P]
Then: GP ′
n+1 = ⟨S′, T ′⟩ where
1) S′ ← S : ∀ si ∈ S;
a) si.m.Rl
m
←
(Γl, Γl+1)
:
σ′
m.Γl
<
si.m.value < σ′
m.Γl+1
b) a ∈ A′(si) ⇐⇒ a ∈ {P ′(si) ∪ a0}
10

487
International Journal on Advances in Intelligent Systems, vol 2 no 4, year 2009, http://www.iariajournals.org/intelligent_systems/
(a) Original model: GP
n = ⟨S, T⟩
(b) New model: GP ′
n+1 = ⟨S′, T ′⟩
f(Rk
mj)
A(si)
t(si, al, s∗)
si
m1
m2
m3
m4
m5
m6
al
State action
λ
s∗
s1
0
0
0
0
0
0
a0
γ-action
a1
AdjustMaxClients(+25)
4
s4
a2
AdjustMaxClients(-25)
a4
AdjustMaxKeepAliveRequests(-30)
1
s5
a6
AdjustMaxBandwidth(-128)
7;2
s3;s6
s2
0
100
0
100
0
100
a0
γ-action
5;3
s1;s2
s3
50
100
0
0
100
100
a0
γ-action
6
s2
a2
AdjustMaxClients(-25)
8
s2
s4
0
100
0
100
0
0
a0
γ-action
6
s4
a1
AdjustMaxClients(+25)
a4
AdjustMaxKeepAliveRequests(-30)
9
s1
s5
0
0
100
100
0
0
a0
γ-action
a1
AdjustMaxClients(+25)
3
s5
a2
AdjustMaxClients(-25)
a4
AdjustMaxKeepAliveRequests(-30)
3
s2
a6
AdjustMaxBandwidth(-128)
s6
50
100
0
100
100
100
a0
γ-action
5
s1
s7
100
100
100
100
100
100
a0
γ-action
2
s1
a2
AdjustMaxClients(-25)
1
s7
a3
AdjustMaxKeepAliveRequests(+30)
4
s3
(c) System states of the new model.
Fig. 6.
State-transition model after △9[P]; i.e., removing action a6 =“AdjustMaxBandwidth(-128)” from policy p2 in Figure 1.
mi
ck
Policy Condition
Rmi
Rj
mi
f(Rj
mi)
m6
m6.value ≤ −1.0
R1
m6
100
7
APACHE:responseTimeTREND > -1.0
m6.value > −1.0
R2
m6
0
TABLE VI
METRIC m6 STRUCTURE AFTER △7[P]; I.E., MODIFYING THE THRESHOLD OF CONDITION c7 =“APACHE:responseTimeTREND > 0.0” TO
“APACHE:responseTimeTREND > -1.0” IN THE POLICIES OF FIGURE 1.
2) ∀ t(si, a∗, sj) ∈ T(si) : a∗ /∈ A′(si); T ′ ←
T|t(si, a∗, sj)
3) ∀ si, sj ∈ S′ : si = sj where i ̸= j;
a) T ′(si) ← T ′(si) ∪ T ′(sj)
∀ t(si, a, s) ∈ T ′(si) and ∀ t′(sj, a′, s′) ∈
T ′(sj) : a = a′ and s = s′;
i) Qt(si, a)
←
t.λ × Qt(si, a) + t′.λ × Qt′(sj, a′)
t.λ + t′.λ
ii) t.λ ← t.λ + t′.λ
iii) T ′ ← T|t′(sj, a′, s′)
b) S′ ← S′|sj
4) ∀ ai ∈ A(si : ∃ t(si, a, s∗) : si, s∗ ∈ S′,
compute Q(si, a) (see Equation 2).
Suppose, for example, that policy modiﬁcation △7[P]
involves
modifying
the
threshold
of
policy
condition
c7 =“APACHE: responseTimeTREND > 0.0” (which
corresponds to metric m6
in Table II) to “APACHE:
responseTimeTREND > -1.0” in the policies of Fig-
ure 1. Thus, instead of σm6
= {0.0}, as was the case
prior to △7[P] (see Table II), the new regions will be
deﬁned by σm6 = {−1.0}, as is illustrated in Table VI.
Suppose also that the following measurements correspond to
the average value of metric m6; “s2.m6.value = −0.5”,
“s3.m6.value = −2.0”, “s6.m6.value = −1.5”, and
11

488
International Journal on Advances in Intelligent Systems, vol 2 no 4, year 2009, http://www.iariajournals.org/intelligent_systems/
(a) Original model: GP
n = ⟨S, T⟩
(b) New model: GP ′
n+1 = ⟨S′, T ′⟩
f(Rk
mj)
A(si)
t(si, al, s∗)
si
m1
m2
m3
m4
m5
m6
al
State action
λ
s∗
s1
0
0
0
0
0
0
a0
γ-action
a1
AdjustMaxClients(+25)
4
s4
a2
AdjustMaxClients(-25)
a4
AdjustMaxKeepAliveRequests(-30)
1
s5
a6
AdjustMaxBandwidth(-128)
7;2
s3;s6
s24
0
100
0
100
0
0
a0
γ-action
5;9
s1;s24
a1
AdjustMaxClients(+25)
a4
AdjustMaxKeepAliveRequests(-30)
9
s1
a6
AdjustMaxBandwidth(-128)
1
s7
s3
50
100
0
0
100
100
a0
γ-action
6
s2
a2
AdjustMaxClients(-25)
8
s2
s5
0
0
100
100
0
0
a0
γ-action
a1
AdjustMaxClients(+25)
3
s5
a2
AdjustMaxClients(-25)
a4
AdjustMaxKeepAliveRequests(-30)
3
s2
a6
AdjustMaxBandwidth(-128)
s6
50
100
0
100
100
100
a0
γ-action
5
s1
s7
100
100
100
100
100
100
a0
γ-action
2
s1
a2
AdjustMaxClients(-25)
1
s7
a3
AdjustMaxKeepAliveRequests(+30)
4
s3
(c) System states of the new model.
Fig. 7.
State-transition model after △7[P]; i.e., modifying the threshold of condition c7
=“APACHE:responseTimeTREND > 0.0” to
“APACHE:responseTimeTREND > -1.0” in the policies of Figure 1.
“s7.m6.value = −2.5”. The following describes the steps
involved in transforming the state-transition model of Fig-
ure 7(a), whose states are shown in Table V and correspond
to the steps of model-transformation Ψ3[GP
n ]:
1) Since “m6.value < −1.0” in states s3, s6, and s7,
f(Rl
m6) would not be affected by the policy modiﬁca-
tion in those states since the measurements would re-
main in region R1
m6. However, since “s2.m6.value =
−0.5”, f(Rk
m6) would recompute the region value by
changing it from 100 (see Table V) to 0 since the mea-
surement would now fall in region R2
m6 (see Table VI)
instead of the previous region R1
m6 (see Table II). The
agent must also update the states actions sets of each
state to ensure that actions set A′(si) contains only
the actions of the policies that are violated when the
system is in state si (see Step 1(b)). Note that △7[P]
would result in the violation of policy p2 when the
agent is in state s2. This was not the case prior to
the policy modiﬁcation since only one of the policy
conditions was violated. As such, actions a1, a4, and
a6 would be added onto the actions set A(s2); i.e.,
A′(s2) = {a0, a1, a4, a6}.
2) Since the previous stage did not result in the deletion of
states actions, no transition will be affected.
3) Changing the region from R1
m6 to R2
m6 in state s2
would mean states s2 and s4 are identical; i.e., they
have the same region value assignment (i.e., f(Rl
mi))
for each identical metric. Thus, the two states would
be merged together onto state s24, as illustrated in
Figure 7. This would also involve merging the states
transitions such that transition t(s4, a6{1}, s7) becomes
transition t(s24, a6{1}, s7), while t(s4, a4{9}, s1) be-
comes transition t(s24, a4{9}, s1) (see Step 3(a)). In the
case of two identical transitions such as t(s4, a0{6}, s4)
and t(s2, a0{3}, s2), a new transition t(s24, a0{9}, s24)
would be formed as illustrated in Figure 7(b) such that
the frequency of the new transition is the sum of the
frequencies of the two merged transitions. Once the
transitions have been updated, the duplicate state is
removed (see Step 3(b)).
12

489
International Journal on Advances in Intelligent Systems, vol 2 no 4, year 2009, http://www.iariajournals.org/intelligent_systems/
4) The ﬁnal step of the adaptation is for the agent to
perform backup updates so that any impact on the model
as a result of merging states and transitions is propagated
to the action-value estimates of the states actions.
4) Adapting to △[P] Resulting in a Decrease in State
Metrics: This describes our approach for dealing with policy
modiﬁcations that reduces the size of the metrics set M, pos-
sibly resulting in two or more identical states. As mentioned
previously, two states are said to be identical if they share
the same metrics region values as determined by the mapping
f(Rj
mi) (see Equation 1) based on each metric’s measurement
(i.e., mi.value). From the deﬁnitions of Section III-A, this
may be in response to policy modiﬁcation △4[P].
Model Transformation - Ψ4[GP
n ]: Adapting to policy mod-
iﬁcation △4[P].
Given:
I) GP
n = ⟨S, T⟩ :
• ∀ si ∈ S, si
=
⟨µ, M(si), P(si), A(si)⟩
: si.mk
=
⟨ID, ω, value, Rl
mk⟩ where
si.mk ∈ M(si)
• ∀
t(si, a, sj)
∈
T,
t(si, a, sj)
=
⟨λ, Qt(si, a)⟩
II) Policy modiﬁcation △4[P]
Then: GP ′
n+1 = ⟨S′, T ′⟩ where
1) S′ ← S : ∀ si ∈ S;
a) M ′(sj) ← M(sj)|m : m is the deleted
metric
b) a ∈ A′(si) ⇐⇒ a ∈ {P ′(si) ∪ a0}
2) ∀ t(si, a∗, sj) ∈ T(si) : a∗ /∈ A′(si); T ′ ←
T|t(si, a∗, sj)
3) ∀ si, sj ∈ S′ : si = sj where i ̸= j;
a) T ′(si) ← T ′(si) ∪ T ′(sj)
∀ t(si, a, s) ∈ T ′(si) and ∀ t′(sj, a′, s′) ∈
T ′(sj) : a = a′ and s = s′;
i) Qt(si, a)
←
t.λ × Qt(si, a) + t′.λ × Qt′(sj, a′)
t.λ + t′.λ
ii) t.λ ← t.λ + t′.λ
iii) T ′ ← T|t′(sj, a′, s′)
b) S′ ← S′|sj
4) ∀ ai ∈ A(si : ∃ t(si, a, s∗) : si, s∗ ∈ S′,
compute Q(si, a) (see Equation 2).
Suppose,
for
example,
that
policy
modiﬁca-
tion
△4[P]
involves
removing
policy
condition
c5
=“MEMORY:utilizationTREND > 0.0”
(which
corresponds to metric m4 in Table II) from policy p4 in
Figure 1. The following describes the steps involved in
transforming the state-transition model of Figure 8(a), whose
states are shown in Table V and correspond to the steps of
model-transformation Ψ4[GP
n ]:
1) For each state that has been encountered to date (see
Table V), the agent must ﬁrst remove all the information
associated with metric m4 (see step 1(a)). This would
result in a system’s model with seven states, each with
ﬁve metrics; i.e., M(si) = {m1, m2, m3, m5, m6}. The
agent must also update the states actions sets of each
state to ensure that actions set A′(si) contains only
the actions of the policies that are violated when the
system is in state si (see Step 1(b)). Note that △4[P]
would result in the violation of policy p4 when the
agent is in states s2, s4, or s6. This was not the case
prior to the policy modiﬁcation since only one of the
policy conditions was violated; i.e., f(Rl
m1) = 0 while
f(Rl
m4) = 100. Thus, removing m4 from state the
above states would mean that p4 is now violated. As
such, action a2 would be added onto actions sets A(s2),
A(s4), and A(s6), making states s2 and s6 violation
states.
2) The state-transition model must also be updated to
ensure that the actions within states transitions are
consistent with the updated states actions sets. Since the
previous stage did not result in the deletion of states
actions, no transition will be affected.
3) The agent must then determine whether any states
should be merged together if, in fact, the modiﬁcations
above may have resulted in two or more identical
states. Note that the deletion of metric m4 would
mean states s3 and s6 are identical; i.e., they have
the same region value assignment (i.e., f(Rl
mi)) for
each identical metric. Thus, the two states would be
merged together onto state s36, as illustrated in Fig-
ure 8. This would also involve merging the states
transitions such that transition t(s6, a0{5}, s1) becomes
transition t(s36, a0{5}, s1) (see Step 3(a)). In the case
of two identical transitions such as t(s1, a6{2}, s6)
and t(s1, a6{7}, s3), a new transition t(s1, a6{9}, s36)
would be formed, as illustrated in Figure 8(b), such
that the frequency of the new transition is the sum
of the frequencies of the two merged transitions. Once
the transitions have been updated, the duplicate state is
removed (see Step 3(b)).
4) The ﬁnal step of the adaptation is for the agent to
perform backup updates so that any impact on the model
as a result of removing transitions and/or merging states
is propagated to the action-value estimates of the states
actions.
5) Adapting to △[P] Resulting in an Increase in State
Metrics: This describes our approach for dealing with policy
modiﬁcations that increase the size of the metrics set M. From
the deﬁnitions of Section III-A, this may be in response to
policy modiﬁcation △1[P].
Model Transformation - Ψ5[GP
n ]: Adapting to policy mod-
iﬁcation △1[P].
Given:
I) GP
n = ⟨S, T⟩ :
• ∀ si ∈ S, si
=
⟨µ, M(si), P(si), A(si)⟩
: si.mk
=
⟨ID, ω, value, Rl
mk⟩ where
si.mk ∈ M(si)
• ∀
t(si, a, sj)
∈
T,
t(si, a, sj)
=
⟨λ, Qt(si, a)⟩
II) Policy modiﬁcation △1[P]
Then: GP ′
n+1 = ⟨S′, T ′⟩ where
1) S′ ← {∅}
13

490
International Journal on Advances in Intelligent Systems, vol 2 no 4, year 2009, http://www.iariajournals.org/intelligent_systems/
(a) Original model: GP
n = ⟨S, T⟩
(b) New model: GP ′
n+1 = ⟨S′, T ′⟩
f(Rk
mj)
A(si)
t(si, al, s∗)
si
m1
m2
m3
m5
m6
al
State action
λ
s∗
s1
0
0
0
0
0
a0
γ-action
a1
AdjustMaxClients(+25)
4
s4
a2
AdjustMaxClients(-25)
a4
AdjustMaxKeepAliveRequests(-30)
1
s5
a6
AdjustMaxBandwidth(-128)
7;2
s3;s6
s2
0
100
0
0
100
a0
γ-action
5;3
s1;s2
a2
AdjustMaxClients(-25)
s36
50
100
0
100
100
a0
γ-action
5;6
s1;s2
a2
AdjustMaxClients(-25)
8
s2
s4
0
100
0
0
0
a0
γ-action
6
s4
a1
AdjustMaxClients(+25)
a2
AdjustMaxClients(-25)
a4
AdjustMaxKeepAliveRequests(-30)
9
s1
a6
AdjustMaxBandwidth(-128)
1
s7
s5
0
0
100
0
0
a0
γ-action
a1
AdjustMaxClients(+25)
3
s5
a2
AdjustMaxClients(-25)
a4
AdjustMaxKeepAliveRequests(-30)
3
s2
a6
AdjustMaxBandwidth(-128)
s7
100
100
100
100
100
a0
γ-action
2
s1
a2
AdjustMaxClients(-25)
1
s7
a3
AdjustMaxKeepAliveRequests(+30)
4
s3
(c) System states of the new model.
Fig. 8.
State-transition model after △4[P]; i.e., deleting condition c5 =“MEMORY:utilizationTREND > 0.0” from policy p4 in Figure 1.
mi
ck
Policy Condition
Rmi
Rj
mi
f(Rj
mi)
m7
10
APACHE:refusedRequests < 10.0
m7.value ≤ 10.0
R1
m7
100
m7.value > 10.0
R2
m7
0
TABLE VII
METRICS m7 STRUCTURE AFTER △1[P]; I.E., ADDING CONDITION c10 =“APACHE:refusedRequests < 10.0” ONTO POLICY p5 IN FIGURE 1.
2) T ′ ← {∅}
Suppose,
for
example,
that
policy
modiﬁ-
cation
△1[P]
involves
adding
the
condition
c10
=“APACHE:refusedRequests < 10.0”
onto
policy p5 in Figure 1. This would result in a new state
metric (i.e., m7 =“APACHE:refusedRequests”) with
two regions associated with it, whose structure is shown in
Table VII. For a system whose model is derived from visiting
the states in Table V, such a change to the policies set P
would mean a binary split of each of the seven states to
account for the two regions of the new metric. However, since
no measurements associated with metrics m7 would have
been collected as was pointed out in Section III-B5, there
would be no way of knowing how to map the old transitions
onto the new states set. As such, △1[P] is the only policy
modiﬁcation requiring learning a new model from scratch.
C. A Sequence of Transformations
Consider a more complex example where △[P] involves
removing policy p4 in Figure 1; i.e.,
Given:
I) PS = ⟨P, WC⟩
II) p4 = ⟨C, A⟩ such that p4 ∈ P :
14

491
International Journal on Advances in Intelligent Systems, vol 2 no 4, year 2009, http://www.iariajournals.org/intelligent_systems/
• C = {c4, c5} :
a) c4
=
“MEMORY:utilization
> 40.0”
:
m3
=
“MEMORY:utilization” and where
m3 ∈ M : ∀ mi ∈ M|m3, mi ̸= m3
and where ∀ pi ∈ P|p4, c4 /∈ pi
b) c5 = “MEMORY:utilizationTREND
> 0.0”
:
m4
=
“MEMORY:utilizationTREND”
and where m4 ∈ M : ∀ mi ∈ M|m4,
mi ̸= m4 and where ∀ pi ∈ P|p4,
c5 /∈ pi
• A
=
{a2}
:
a2
=
AdjustMaxClients(-25)
Then:
1) PS′ = ⟨P ′, WC⟩ : P ′ ← P|p4
Note that since the policy consists of two conditions and one
action, such a policy modiﬁcation can be modelled as a series
of transformations involving the following:
1) Modifying p4 to create p1
4 by removing condition c5
from policy p4 = ⟨{c4, c5}, {a2}⟩ such that △4[P] =
P1.
2) Modifying p1
4 to create p2
4 by removing action a2 from
policy p1
4 = ⟨{c4}, {a2}⟩ such that △9[P1] = P2.
3) Modifying p2
4 to create p3
4 by removing condition c4
from policy p2
4 = ⟨{c5}, {∅}⟩ such that △4[P2] = P3.
4) Removing policy p3
4 = ⟨{∅}, {∅}⟩ from P3 such that
△y[P3] = P ′.
In the following, we expand on each of these transformations,
elaborating on the steps involved in transforming the state-
transition model whose states are shown in Table V.
The
ﬁrst
transformation
involves
adapting
the
state-
transition model as a result of removing policy condition
c5 =“MEMORY:utilizationTREND > 0.0” from pol-
icy p4, which relates to policy modiﬁcation △4[P]; i.e.,
Policy Modiﬁcation - △4[P]: Removing a policy condition
resulting in a decrease in the number of metrics.
Given:
I) PS = ⟨P, WC⟩
II) M P
R = {r1, r2, . . . , rl} : ri = ⟨αi, σi⟩
III) p4 = ⟨{c4, c5}, {a2}⟩ such that p4 ∈ P where
∀ pi ∈ P|p4, c5 /∈ pi
IV) c5
= “MEMORY:utilizationTREND >
0.0” :
m4 = “MEMORY:utilizationTREND” and
where m4 ∈ M : ∀ mi ∈ M|m4, mi ̸= m4
V) rm4 ∈ M P
R : rm4 = ⟨αm4, σm4⟩ :
• αm4 = ⟨4, m4, ω⟩
• σm4 = {0.0}
Then:
1) PS1 = ⟨P1, WC⟩ : p1
4 = ⟨{c4}, {a2}⟩
2) M1 ← M|m4
3) M P1
R ← M P
R |rm4
Such
a
modiﬁcation
can
be
modelled
using
model-
transformation Ψ4[GP
n ], as was illustrated in Section III-B4
(see Figure 8).
Suppose
that
we
apply
the
second
transformation
(i.e.,
in
response
to
removing
policy
action
a2
=
“AdjustMaxClients(-25)”
from
policy
p1
4)
onto
the state-transition model of Figure 8(b) whose states are
shown in Figure 8(c). Note that such a change relates to
policy modiﬁcation △9[P1]; i.e.,
Policy Modiﬁcation - △9[P1]: Removing an action from a
policy in P1.
Given:
I) PS1 = ⟨P1, WC⟩
II) M P1
R = {r1, r2, . . . , rl} : ri = ⟨αi, σi⟩
III) p1
4 = ⟨{c4}, {a2}⟩ such that p1
4 ∈ P1
IV) a2 = AdjustMaxClients(-25)
V) rm ∈ M P
R : rm = ⟨αm, σm⟩ :
• αm = ⟨ID, metricName, ω⟩
• σm = {Γ1, Γ2, . . . , Γk}
Then:
1) PS2 = ⟨P2, WC⟩ : p2
4 = ⟨{c4}, {∅}⟩
2) M2 ← M1
3) M P2
R ← M P1
R
Such
a
modiﬁcation
can
be
modelled
using
model-
transformation Ψ2[GP
n ]. The following describes the steps
involved in transforming the model and corresponds to the
steps of model-transformation Ψ2[GP1
1 ]:
1) For each state that has been encountered to date (see
ﬁgure 8(c)), the agent must update the actions within
the states to ensure that actions set A′(si) contains only
the actions of the policies that are violated when the
system is in state si. Note that since policy p1
4 is the
only violated policy in states s2 and s36, △9[P 1] would
result in the removal of action a2 from the actions sets
A(s2) and A(s36). While policy p1
4 is also violated in
states s1 and s4, △9[P 1] would not affect such states.
This is because action a2 is also part of other violated
policies in those states, such as policy p1 (i.e., a2 ∈ p1)
and policy p2 (i.e., a2 ∈ p2) where p1 ∈ P ′(s1) and
p2 ∈ P ′(s4). Consequently, both states s2 and s36 will
change from “violation” to “non-violation” states.
2) The state-transition model must also be updated to
ensure that transitions within states are consistent with
the updated states actions sets. Note that the removal of
action a2 from state s36 in the above stage would mean
that transition t(s36, a2{8}, s2) (see Figure 9(a)) is no
longer valid. As such, the transition will be removed as
illustrated in Figure 9(b).
3) The ﬁnal step of the adaptation is for the agent to
perform backup updates so that any impact on the model
as a result of removing transitions is propagated to the
action-value estimates of the states actions.
Suppose
that,
we
apply
the
third
transformation
(i.e.,
in
response
to
removing
policy
condition
c4
=“MEMORY:utilization > 40.0”
from
policy
p2
4) onto the state-transition model of Figure 9(b), whose
states are shown in Figure 9(c). Note that such a change
relates to policy modiﬁcation △4[P2]; i.e.,
Policy Modiﬁcation - △4[P]: Removing a policy condition
resulting in a decrease in the number of metrics.
15

492
International Journal on Advances in Intelligent Systems, vol 2 no 4, year 2009, http://www.iariajournals.org/intelligent_systems/
(a) Original model: GP
n = ⟨S, T⟩
(b) New model: GP ′
n+1 = ⟨S′, T ′⟩
f(Rk
mj)
A(si)
t(si, al, s∗)
si
m1
m2
m3
m5
m6
al
State action
λ
s∗
s1
0
0
0
0
0
a0
γ-action
a1
AdjustMaxClients(+25)
4
s4
a2
AdjustMaxClients(-25)
a4
AdjustMaxKeepAliveRequests(-30)
1
s5
a6
AdjustMaxBandwidth(-128)
7;2
s3;s6
s2
0
100
0
0
100
a0
γ-action
5;3
s1;s2
s36
50
100
0
100
100
a0
γ-action
5;6
s1;s2
s4
0
100
0
0
0
a0
γ-action
6
s4
a1
AdjustMaxClients(+25)
a2
AdjustMaxClients(-25)
a4
AdjustMaxKeepAliveRequests(-30)
9
s1
a6
AdjustMaxBandwidth(-128)
1
s7
s5
0
0
100
0
0
a0
γ-action
a1
AdjustMaxClients(+25)
3
s5
a2
AdjustMaxClients(-25)
a4
AdjustMaxKeepAliveRequests(-30)
3
s2
a6
AdjustMaxBandwidth(-128)
s7
100
100
100
100
100
a0
γ-action
2
s1
a2
AdjustMaxClients(-25)
1
s7
a3
AdjustMaxKeepAliveRequests(+30)
4
s3
(c) System states of the new model.
Fig. 9.
Adapting state-transition model of Figure 8 to △9[P1]; i.e., removing action a2 =“AdjustMaxClients(-25)” from policy p1
4.
Given:
I) PS2 = ⟨P2, WC⟩
II) M P2
R = {r1, r2, . . . , rl} : ri = ⟨αi, σi⟩
III) p2
4 = ⟨{c4}, {∅}⟩ such that p2
4 ∈ P 2 where
∀ pi ∈ P2|p2
4, c4 /∈ pi
IV) c4 = “MEMORY:utilization > 40.0” :
m3 = “MEMORY:utilization” and where
m3 ∈ M : ∀ mi ∈ M2|m3, mi ̸= m3
V) rm3 ∈ M P2
R
: rm3 = ⟨αm3, σm3⟩ :
• αm3 = ⟨3, m3, ω⟩
• σm3 = {40.0}
Then:
1) PS3 = ⟨P3, WC⟩ : p3
4 = ⟨{∅}, {∅}⟩
2) M3 ← M2|m3
3) M P3
R ← M P2
R |rm3
Such
a
modiﬁcation
can
be
modelled
using
model-
transformation Ψ4[GP
n ]. The following describes the steps
involved in transforming the model and corresponds to the
steps of model-transformation Ψ4[GP
2 ]:
1) For each state that has been encountered to date (see Ta-
ble 9(c)), the agent must ﬁrst remove all the information
associated with metric m3 (see step 1(a)). This would
result in a system’s model with six states each with four
metrics; i.e., M(si) = {m1, m2, m5, m6}. The agent
must also update the states actions sets of each state to
ensure that actions set A′(si) contains only the actions
of the policies that are violated when the system is in
state si (see Step 1(b)). Since △4[P2] is the third step in
the sequence of changes to the policy p4 (which, at this
point, would have no actions associated with it), such
a policy modiﬁcation would result in no changes to the
composition of the states actions.
2) The state-transition model must also be updated to
ensure that the actions within states transitions are
consistent with the updated states actions sets. Since the
previous stage did not result in the deletion of states
actions, no transition will be affected.
3) The agent must then determine whether any states
should be merged together if, in fact, the modiﬁcations
above may have resulted in two or more identical
16

493
International Journal on Advances in Intelligent Systems, vol 2 no 4, year 2009, http://www.iariajournals.org/intelligent_systems/
(a) Original model: GP
n = ⟨S, T⟩
(b) New model: GP ′
n+1 = ⟨S′, T ′⟩
f(Rk
mj)
A(si)
t(si, al, s∗)
si
m1
m2
m5
m6
al
State action
λ
s∗
s15
0
0
0
0
a0
γ-action
a1
AdjustMaxClients(+25)
3;4
s15;s4
a2
AdjustMaxClients(-25)
a4
AdjustMaxKeepAliveRequests(-30)
1;3
s15; s2
a6
AdjustMaxBandwidth(-128)
9
s36
s2
0
100
0
100
a0
γ-action
5;3
s15;s2
s36
0
100
100
100
a0
γ-action
5;6
s15;s2
s4
0
100
0
0
a0
γ-action
6
s4
a1
AdjustMaxClients(+25)
a4
AdjustMaxKeepAliveRequests(-30)
9
s15
a6
AdjustMaxBandwidth(-128)
1
s7
s7
100
100
100
100
a0
γ-action
2
s15
a2
AdjustMaxClients(-25)
1
s7
a3
AdjustMaxKeepAliveRequests(+30)
4
s36
(c) System states of the new model.
Fig. 10.
Adapting state-transition model of Figure 9 to △4[P2]; i.e., removing condition c4 =“MEMORY:utilization > 40.0” from policy p2
4.
states. Note that the deletion of metric m3 would
mean states s1 and s5 are identical; i.e., they have
the same region value assignment (i.e., f(Rl
mi)) for
each identical metric. Thus, the two states would be
merged together into state s15 as illustrated in Figure 10.
This would also involve merging the states transitions
(see Step 3(a)) such that, transitions t(s1, a4{1}, s5),
t(s5, a1{3}, s5), and t(s5, a4{3}, s2) (see Figure 10(a))
become transitions t(s15, a4{1}, s15), t(s15, a1{3}, s15),
and t(s15, a4{3}, s2), respectively (see Figure 10(b)).
4) The ﬁnal step of the adaptation is for the agent to
perform backup updates so that any impact on the model
as a result of removing transitions and/or merging states
is propagated to the action-value estimates of the states
actions.
Finally, we apply the fourth transformation (i.e., in response
to removing policy p3
4) onto the state-transition model of
Figure 10(b), whose states are shown in Figure 10(c). Note
that such a change relates to policy modiﬁcation △y[P3]; i.e.,
Policy Modiﬁcation - △y[P3]: Removing a policy without
conditions or actions.
Given:
I) PS3 = ⟨P3, WC⟩
II) M P3
R = {r1, r2, . . . , rl} : ri = ⟨αi, σi⟩
III) p3
4 = ⟨∅, ∅⟩ such that p3
4 ∈ P3
Then:
1) PS′ = ⟨P ′, WC⟩ : P ′ ← P3|p3
4
2) M ′ ← M3
3) M P ′
R ← M P3
R
Since △y[P3] has no impact on the state-transition model,
removing policy p3
4 from Figure 1 would result in no changes
to the state-transition model of Figure 10. As such, the removal
of policy p4 can be modelled as a sequence of transformations
on the original state-transition model involving: △4[P] = P1,
△9[P1] = P2, △4[P2] = P3, and △y[P3] = P ′.
IV. EXPERIENCE
In this section, we present experiments illustrating the
effectiveness of the adaptation mechanisms on the behavior
of the server in response to run-time policy modiﬁcations. In
particular, we compare results from two experimental settings
based on how the agent responded to run-time policy modiﬁ-
cations. The ﬁrst experiment (EXP-1) investigated the behavior
of the server where the learning agent discards everything it
may have learned prior to △[P] and begins learning from
scratch; i.e., GP ′
n+1 = {∅, ∅}. The second experiment (EXP-2)
looked at the behavior of the server where the learning agent
adapts the model of the environment dynamics to take into
account the changes to an active set of policies, essentially
17

494
International Journal on Advances in Intelligent Systems, vol 2 no 4, year 2009, http://www.iariajournals.org/intelligent_systems/
(a) Response Time
(b) CPU Utilization
(c) Throughput
Fig. 11.
Adapting to run-time policy modiﬁcation: △[P] involves replacing “mixed” expectation policies with “simple” expectation policies
reusing some of the learned information; i.e., Ψ : GP
n → GP ′
n+1
(see Figure 3).
A. Testbed Environment
Our testbed consisted of a collection of networked work-
stations: an administrative workstation to run the experiments;
a Linux workstation with a 2.0 GHz processor and 2.0 Gb
of memory, which hosted the Apache Web Server along with
the PHP module and the MySQL database server; and three
workstations used for generating the server’s workload, which
consisted of clients requests associated with gold, silver, and
bronze service classes.
To emulate the stochastic behavior of users, an Apache
benchmark tool called JMeter [12] was used. The tool pro-
vides support for concurrent sampling of requests through a
multi-threaded framework. It provides the ability to specify
the client’s think time as well as the number of concurrent
- independent - keep-alive connections. In the experiments
reported here, these values were set to ensure that the server
was under overload conditions (i.e., saturated) for the entire
duration of the experiments. In our current implementation,
the tool has been conﬁgured to traverse a Web graph of an
actual Web site: in our case, phpBB [13]. In the experiments
reported in this section, only read-only database requests were
18

495
International Journal on Advances in Intelligent Systems, vol 2 no 4, year 2009, http://www.iariajournals.org/intelligent_systems/
considered.
In order to support service differentiation, a Linux Trafﬁc
Controller (TC) was used to conﬁgure the bandwidth associ-
ated with the gold, silver, and bronze service classes. The ser-
vice classes bandwidth was assigned proportionately according
to the ratio 85:10:5. Bandwidth sharing was also permitted.
The tuning parameter MaxBandwidth is what determines
how much bandwidth is assigned to each service class. It
corresponds to the physical capacity (in kbps) of the network
connection to the workstation hosting the servers. Thus, given
that the ﬁrst policy of Figure 1 has been violated and that
it is no longer possible, for example, to adjust the parameters
MaxClients and MaxKeepAliveRequests, then the last
policy action (i.e., AdjustMaxBandwidth(-128)) would
be executed. This action essentially reduces the total band-
width by 128 kbps. The percentage of the new bandwidth is
what is eventually assigned to the different service classes.
B. Results
The results presented in this section aim at demonstrating
the effectiveness of the adaptation mechanisms on the policy
modiﬁcations involving changing policy sets. Two sets of
policies were considered and included “simple” and “mixed”
expectation policies. Simple expectation policies included
policies describing possible sets of actions to be taken when-
ever a single objective is violated. Mixed expectation policies
include policies consisting of both “simple” and “complex”
policies, with complex expectation policies describing the
actions of the system whenever several objectives are violated.
△[P], in this case, involved replacing “mixed” expectation
policies with “simple” expectation policies; i.e., disabling all
the complex policies within the mixed expectation policies
set. The performance comparisons in terms of the averages
and magnitude of violations reported in this section focused
speciﬁcally on the behavior of the server after △[P] occurred.
Figure 11 shows the behavior of the server when dealing
with the above policy modiﬁcation, which occurred at about
1,900 seconds after the start of the experiment. In terms of
response time measurements (see Figure 11(a)), the quality
of service speciﬁc to the violations as well as the stability
of the measurements greatly improved after △[P] occurred.
This is clearly visible in the graph where there are very few
instances where the measurements exceeded the threshold.
The same is also true when CPU utilization and throughput
measurements are considered, as Figures 11(b) and 11(c),
respectively, illustrate. Thus, the overall quality of service
greatly improved as a result of the adaptation mechanisms.
The top graph of Figure 12 compares the average through-
put measurements of identical service classes. EXP-1 cor-
responded to the case where the learning agent discarded
everything it had learned prior to △[P] and began learning
from scratch, while EXP-2 corresponded to the experiment
where the agent adapted the model in response to the policy
modiﬁcation. Thus, results reported for EXP-2 are essentially
those of Figure 11, but whose averages only include the
measurements after △[P] occurred. From these results, the
server performed slightly better in EXP-2 when compared
to the results in EXP-1, particularly for the gold service
class where more requests were served. For the other classes
(i.e., silver and bronze), however, the performance gains were
statistically insigniﬁcant since the measurements fell within
the standard error, as is illustrated by the error bars associated
with the mean throughput.
Fig. 12.
Mean performance comparisons.
The same was also true when clients response time mea-
surements across the service classes were compared as shown
in the bottom graph of Figure 12. Thus, we can conclude
that, model adaptation had a positive impact on the overall
performance of the server when averages associated with the
server’s throughput and clients response times were consid-
ered. That is, the server performed slightly better in EXP-
2 when compared to the results in EXP-1, particularly for
the gold service class where more requests were served.
For the other classes (i.e., silver and bronze), however, the
performance differences were statistically insigniﬁcant since
the measurements fell within the standard error.
While the results above show slight improvement in the
overall quality as a result of the adaptation mechanisms, mean
performance is essentially an approximation of the quality of
service performance of the system. The more accurate measure
is the severity and the frequency with which performance
objectives are violated, as summarized in Figure 13. Note that,
EXP-2 recorded at least an 80% reduction in the magnitude
of response time violations (see Figure 13(a)) and a 96%
reduction in the magnitude of CPU utilization violations (see
Figure 13(b)). Thus, reducing the magnitude of violations
such as instances in which clients requests wait on a server’s
queue for longer than 2500 ms, for example, is likely to
19

496
International Journal on Advances in Intelligent Systems, vol 2 no 4, year 2009, http://www.iariajournals.org/intelligent_systems/
(a) Response Time
(b) CPU Utilization
Fig. 13.
Magnitude of violations performance comparisons.
improve the quality of service from a user’s prospective -
a measure mean performance tend to obscure. Furthermore,
performance objective under these kinds of environments
often need to account for not only customers’ needs, but
also system’s constraints. For example, in order to control
power consumption, an autonomic system may need to limit
CPU utilization. Thus, the ability to achieve system-wide
performance objectives, which may seem conﬂicting at times,
under dynamically changing conﬁguration conditions is what
the experiments presented in this paper have demonstrated.
Furthermore, the results summarized in this section high-
light one of the key beneﬁts of utilizing reinforcement learning
methodologies in policy-driven autonomic management, par-
ticularly when it comes to policy speciﬁcation. That is, very
“simple” expectation policies are sufﬁcient to deliver signiﬁ-
cant performance gains. With learning enabled, more complex
policies, particularly those spanning multiple objectives, could
be learned instead of requiring systems administrators to
manually encode into a single policy how systems should
achieve multiple objectives. Thus, a general knowledge of
the performance impact due to a change in the value of a
parameter is sufﬁcient to deﬁne simple but effective policies.
This can signiﬁcantly reduce the burden faced by human
administrators, by having them specify simple policies, leaving
it to autonomic systems to ﬁgure out how to achieve more
complex objectives.
V. RELATED WORK
Policy-based management approaches have attracted signif-
icant interest within autonomic computing. The work in [14],
for example, proposes a ﬂexible policy-expression language
called AGILE [15], which facilitates run-time policy con-
ﬁguration and adaptation of autonomic systems. AGILE in
itself is both a policy expression language and a framework
that facilitates dynamic composition of autonomic techniques
including signal processing, trend analysis, and utility func-
tions. IBM [6] has also been at the forefront in policy-driven
autonomic management research. Their work in [16] proposes
an Agent Building and Learning Environment (ABLE) capable
of conﬁguring new behaviors and capabilities to intelligent
autonomic systems. In this approach, policies are speciﬁed
using rule beans which make use of simple scripting text or
Extensible Markup Language (XML) [17] to deﬁne simple as-
signments including if-then, if-then-else rules, when-do pattern
match rules, etc.
Policy-based management has recently evolved to recognize
the need to evaluate the correctness and effectiveness of poli-
cies speciﬁed by system administrators. For example, there has
been some interest in policy conﬂicts detection and resolution
(see, for example, [18]). Others have proposed frameworks that
make use of event calculus (EC) [19] for policy veriﬁcations
which adds the notion of time for ﬁrst-order predicate logic. It
provides a formalism for representation and reasoning about
actions and their effects. This approach, however, requires that
rules are speciﬁed to identify all possible inconsistencies from
a set of policies, which is intractable for complex systems. This
is particularly the case because, in most situations, some of the
characteristics for identifying conﬂicts can only be detected at
run-time.
The need to dynamically adapt the use of policies in
autonomic management has also gained some traction. The
work in [20], for example, proposes an adaptive policy-
based framework that support dynamic policy conﬁguration
in response to changes within the managed environment.
In their approach, policy adaptation describes the ability to
modify network behavior by dynamically changing the policy
parameters as well as selecting, enabling, or disabling policies
at run-time. Reconﬁguration events are used to trigger high-
level control policies, which then determines which lower-
level policies must be adapted to reconﬁgure the managed
system. They use the Ponder deployment framework [21] to
distribute policies to the different management components.
They describe several ways in which the system’s behavior
could be adapted by changing the way policies are used [20]:
• Dynamic Modiﬁcation of Policy Parameters: In this ap-
proach, the system can adapt policies by making changes
to the policy parameters. This may involve computing
new attribute values based, for example, on Service Level
Agreements (SLAs), resource availability, etc. Related
work on this type of adaptation in the use of policies
can be found in [20], [22].
• Enabling/Disabling Policies From a Set of Active Poli-
cies: In this approach, the system can use run-time
context to dynamically use the information provided by
the policies, such as enabling/disabling a policy under
certain circumstances or selecting actions based on con-
text. Related work on adaptation through policy selection
can be found in [20], [23].
• Adaptation By Learning: In this approach, the system can
determine policy use through some learning mechanisms
based on past experience in the use of policies. This type
of adaptation is still in its very early stages, particularly
in the ﬁled of autonomic management, with the majority
of the research work focusing on learning high-level
policies. The most recent work can be found in [24],
[25], [26], [27].
Our interest is on how learning approaches could facili-
tate dynamic use of policies within autonomic computing.
20

497
International Journal on Advances in Intelligent Systems, vol 2 no 4, year 2009, http://www.iariajournals.org/intelligent_systems/
In our most recent work [7], we have explored the use of
Reinforcement Learning methodologies in providing guidance
to the autonomic system in terms of how to effectively use
existing policies. One drawback to this and other approaches
was that when policies change, the existing model is discarded
and the system must learn a new model of the environment
from scratch. Our experience, however, suggest that policy
modiﬁcations are frequently “incremental” - e.g., a change to
a threshold, the disabling of a single policy, etc. The ability
to reuse a learned model, or part there of, has the potential
beneﬁt of signiﬁcantly accelerating the learning process, as
this paper has demonstrated.
VI. CONCLUSION
The approach taken in this paper is only the ﬁrst step in the
broader goal of developing adaptive policy driven autonomic
solutions involving multiple agents working together towards a
set of common, and at times seemingly competing, objectives.
This is often the case since performance objectives may need
to account for both customers needs and systems constraints.
The complexity of today’s IT infrastructure, however, means
that we can no longer depend on centralized approaches to per-
formance management. It is becoming increasingly common
to ﬁnd multiple agents coexisting within a single computing
environment. The work in [28], for example, proposes an ap-
proach for coordinating two independent autonomic managers:
one designed to address power consumption objectives while
another deals with performance objectives such as response
time based on some learning mechanisms. As such, policy
modiﬁcations directives could come not only from the users
of the systems, but also from other autonomic managers. The
fact that our approach to modelling the learning process is
dependent only on the structure of the policies means that
such changes to policies could be traced back to the model
derived from the use of those policies. In this paper, we have
elaborated on how a model learned from the use of an active
set of policies could be adapted (i.e., reused) to cope with
run-time policy modiﬁcations.
While the initial results are encouraging, we intend to
validate these approaches with a comprehensive prototype
evaluating the impact of the different kinds of policy modiﬁ-
cations on the effectiveness of the adaptation transformations.
We are also working on formulating formal proofs that show
that, indeed if P is changed to P ′ and GP
n is transformed to
GP ′
n+1, then GP ′
n+1 is a model of P ′.
REFERENCES
[1] N. Damianou, N. Dulay, E. C. Lupu, and M. S. Sloman, “Ponder:
A Language for Specifying Security and Management Policies for
Distributed Systems: The Language Speciﬁcation,” Imperial College,
London, England, Version 2.2, 2000.
[2] (2009, December) The Internet Engineering Task Force (IETF).
[Online]. Available: http://www.ietf.org/
[3] (2009, December) The Distributed Management Task Force (DMTF).
[Online]. Available: http://www.dmtf.org/
[4] (2009, December) The Object Management Group (OMG). [Online].
Available: http://www.omg.org/
[5] (2009, December) Policy Research Group. Imperial College. [Online].
Available: http://www-dse.doc.ic.ac.uk/Research/policies/ponder.shtml
[6] (2009, December) Autonomic Computing Research. IBM. [Online].
Available: http://www.research.ibm.com/autonomic/
[7] R. M. Bahati and M. A. Bauer, “Modelling Reinforcement Learning in
Policy-driven Autonomic Management,” in IARIA International Journal
On Advances in Intelligent Systems, vol. 1, no. 1, December 2008.
[8] R. M. Bahati and M. A. Bauer, “Adapting to Run-time Changes in
Policies Driving Autonomic Management,” in International Conference
on Autonomic and Autonomous Systems (ICAS’08), Guadeloupe, March
2008, pp. 88–93.
[9] R. M. Bahati, M. A. Bauer, C. Ahn, O. K. Baek, and E. M. Vieira,
“Using Policies to Drive Autonomic Management,” in WoWMoM-2006
Workshop on Autonomic Communications and Computing (ACC’06),
Buffalo, NY, USA, June 2006, pp. 475–479.
[10] R. S. Sutton and A. G. Barto, Reinforcement Learning: an Introduction.
Cambridge, MA, USA: MIT Press, 1998.
[11] C. Watkins, “Learning from Delayed Rewards,” Ph.D. dissertation,
Cambridge University, Cambridge, UK, 1989.
[12] (2009, December) Apache JMeter. The Apache Software Foundation.
[Online]. Available: http://jakarta.apache.org/jmeter/
[13] (2009,
December)
PHP
Bulletin
Board.
[Online].
Available:
http://www.phpbb.com/
[14] R. J. Anthony, “Policy-centric Integration and Dynamic Composition
of Autonomic Computing Techniques,” in International Conference on
Autonomic Computing (ICAC’07), Jacksonville, FL, USA, June 2007.
[15] (2009,
December)
AGILE.
[Online].
Available:
http://www.policyautonomics.net/
[16] J. P. Bigus, D. A. Schlosnagle, J. R. Pilgrim, W. N. M. III, and Y. Diao,
“ABLE: A toolkit for Building Multiagent Autonomic Systems,” in IBM
Systems Journal, vol. 41, no. 3, September 2002.
[17] (2009, December) Extensible Markup Language (XML). [Online].
Available: http://www.w3.org/XML/
[18] N. Dunlop, J. Indulska, and K. Raymond, “Dynamic Conﬂict Detection
in Policy-Based Management Systems,” in International Enterprise
Distributed Object Computing Conference (EDOC’02), Washington,
DC, USA, 2002, pp. 15–26.
[19] R. Kowalski and M. Sergot, “A Logic-based Calculus of Events,” in New
Generation Computing, vol. 4, no. 1, Tokyo, Japan, 1986, pp. 67–95.
[20] L. Lymberopoulos, E. C. Lupu, and M. S. Sloman, “An Adaptive Policy-
Based Framework for Network Services Management,” in Journal of
Networks and Systems Management, vol. 11, no. 3, 2003, pp. 277–303.
[21] N. Dulay, E. C. Lupu, M. S. Sloman, and N. Damianou, “A Policy De-
ployment Model for the Ponder Language,” in IEEE/IFIP International
Symposium on Integrated Network, Seattle, CA, USA, May 2001, pp.
529–544.
[22] K. Yoshihara, M. Isomura, and H. Horiuchi, “Distributed Policy-based
Management Enabling Policy Adaptation on Monitoring using Active
Network Technology,” in IFIP/IEEE International Workshop on Dis-
tributed Systems: Operations and Management (DSOM’01), Nancy,
France, October 2001.
[23] R. M. Bahati, M. A. Bauer, and E. M. Vieira, “Adaptation Strategies
in Policy-Driven Autonomic Management,” in International Conference
on Autonomic and Autonomous Systems (ICAS’07), July 2007.
[24] G. Tesauro, N. K. Jong, R. Das, and M. N. Bennani, “A Hybrid Re-
inforcement Learning Approach to Autonomic Resource Allocation,” in
International Conference on Autonomic Computing (ICAC’06), Dublin,
Ireland, June 2006, pp. 65–73.
[25] G. Tesauro, “Online Resource Allocation Using Decompositional Rein-
forcement Learning,” in National Conference on Artiﬁcial Intelligence
(AAAI’05), Pittsburgh, PA, USA, 2005, pp. 886–891.
[26] D. Vengerov and N. Iakovlev, “A Reinforcement Learning Framework
for Dynamic Resource Allocation: First Results,” in International Con-
ference on Autonomic Computing (ICAC’05), Seattle, WA, USA, January
2005, pp. 339–340.
[27] P. Vienne and J.-L. Sourrouille, “A Middleware for Autonomic QoS
Management based on Learning,” in International Conference on
Sofware Engineering and Middleware, Lisbon, Portugal, September
2005, pp. 1–8.
[28] J. O. Kephart, H. Chan, R. Das, D. W. Levine, G. Tesauro, F. Raw-
son, and C. Lefurgy, “Coordinating Multiple Autonomic Managers to
Achieve Speciﬁed Power-performance Tradeoffs,” in International Con-
ference on Autonomic Computing (ICAC’07), Jacksonville, FL, USA,
June 2007.
21

