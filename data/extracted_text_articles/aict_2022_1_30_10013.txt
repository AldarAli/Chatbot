A Machine Learning Approach for Resource Allocation  
in Wireless Industrial Environments 
Idayat O. Sanusi and  Karim M. Nasr   
Faculty of Engineering and Science, University of Greenwich,  
Kent, ME4 4TB, United Kingdom 
{i.o.sanusi, k.m.nasr}@gre.ac.uk  
 
Abstract— In this paper, we present a machine learning 
technique for channel selection in a Device to Device (D2D)-
enabled cellular network targeting a wireless industrial 
environment. The presented Base Station Assisted (BSA) 
reinforcement learning technique uses a distributed local Q-
table for the agents (users), to prevent global information 
gathering within the cellular network. A stateless Q-learning 
approach is adopted to reduce the complexity of learning and 
the dimension of the Q-table. After the training of the D2D 
agents, the Q-tables of the D2D users are uploaded to the base 
station for resource allocation to be implemented centrally. 
Simulations results show that the presented technique provides 
a Radio Resource Management (RRM) solution with a good 
Quality of Service (QoS) performance compared to other 
conventional approaches. 
Keywords— 5G and beyond networks; Radio Resource 
Management; 
Distributed 
Algorithms; 
Device-to-Device 
Communication; Reinforcement Learning. 
I. 
 INTRODUCTION  
   Device-to-Device communication (D2D) is considered as 
a promising solution for ultra-reliable low-latency use cases 
because of associated advantages in terms of reduced 
latency, 
improved 
reliability 
and 
throughput. 
The 
integration of D2D into future industrial wireless networks 
and smart manufacturing facilitates the creation of massive 
machine-type 
connections 
[1]. 
 
Machine-Type 
Communication (MTC) is expected to support large 
numbers of smart devices, predominantly with small data 
volume requirements, which aggregates into a massive 
amount of data from parallel transmissions of a large number 
of devices. Achieving ultra-high reliability and ultra-low 
latency pose challenges in terms of bandwidth requirements. 
Yilmaz et al. [2] studied the transmission bandwidth needed 
to enable Ultra-Reliable Low-Latency Communication 
(URLLC) for factory automation and found that the system 
bandwidth depends on the number of connected pieces of 
user equipment and the behaviour of their traffic. The 
scarcity of radio resources and the limitations on the 
available system bandwidth makes spectrum sharing a 
necessity for D2D implementation of MTC for factory 
automation [3]. RRM schemes need to be efficiently 
designed for interference management and coordination 
while guaranteeing tight URLLC demands.     
   Channel reuse among active devices in the same cells will 
generate interference which degrades system performance. 
Interference management is crucial to ensure efficient 
utilisation of available spectrum resources which is also 
particularly challenging for D2D deployments in underlay 
cellular networks.  
   The 
two 
major 
approaches 
to 
Radio 
Resource 
Management (RRM) are based on centralised and 
distributed methods. The centralised scheme requires global 
information gathering by base stations which often results in 
a high signalling overhead and increased complexity which 
tend to increase with the number of users, thus making it 
impractical. The distributed approach does not need a central 
entity; resource allocation is implemented by users therefore 
reducing the amount of information gathering and 
processing by base stations. However, a distributed RRM 
algorithm may also increase signalling overheads due to the 
high amount of information interchange among devices [4]. 
    Reinforcement Learning (RL) has recently gained a lot of 
attention because of its suitability for the decision-making 
process where there is unknown or partial channel 
information. RL has been widely studied for intelligent 
power and spectrum allocations for D2D communication in 
cellular networks. Asheralieva and Miyanga [5] formulated 
the resource allocation problem as a non-cooperative game 
model among D2D users and a mixed strategy Nash 
Equilibrium was obtained.   However, the Quality of 
Experience (QoS) metrics of cellular users sharing channel 
resources with D2D links were not taken into account in the 
reward model. A multi-agent actor-critic structure is 
proposed in [6] which involves interactions between users 
and centralised sharing of all historical information. This 
leads to an increase in signalling overheads and information 
exchange. In [5]-[8], the reward function captured the QoS 
metric of the cellular users in a centralised Q-learning 
approach, which leads to increased signalling overheads as 
well.  
   In this paper, we present a semi-distributed reinforcement 
learning scheme for D2D resource allocation in a cellular 
network in an industrial setting. After the decentralised 
training of the agents, their Q-tables uploaded are forwarded 
to the base station for a centralised resource allocation.  The 
reward function is modeled in such a way that there is no 
information exchange related to other agents’ action or 
reward. To address the problem of the ‘curse of 
dimensionality’ associated with Q-learning, a stateless Q-
learning is adopted to reduce the dimension of the Q-table, 
nonetheless capturing the QoS demands of the D2D users.  
The paper is organised as follows: The problem 
formulation and system model are presented in Section II. In 
Section III, a stateless reinforcement learning algorithm for 
base station-assisted resource allocation is presented. 
Section IV presents some results and discussions of the 
simulation scenarios considered. Conclusions and directions 
for future work are summarised in Section V. 
18
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-956-0
AICT 2022 : The Eighteenth Advanced International Conference on Telecommunications

II. 
SYSTEM MODEL  
   We consider D2D and cellular users coexisting within a 
cellular network for uplink spectrum-sharing. There are 𝑁 
Cellular User Equipment (CUEs) represented by a set 𝐶 and 
𝑀 D2D User Equipment (DUEs) denoted by a set 𝐷 
randomly deployed within the coverage of the base station 
(BS) in a single cell system. The cellular users have strict 
performance requirements in the form of minimum Signal to 
Interference plus Noise Ratio (SINR) values to guarantee 
their throughput. The D2D links also have minimum SINR 
thresholds to guarantee their throughput demands, in 
addition, to the reliability and delay constraints.    We 
assume that each CUE has been pre-allocated a resource 
block. The transmit power of the CUEs and DUEs are 
denoted by 𝑃𝑐𝑖 and 𝑃𝑑𝑗, respectively. We denote 
𝑔𝑐,𝐵,  𝑔𝑑𝑇,𝐵,  𝑔𝑑𝑇,𝑑𝑅 and 𝑔𝑐,𝑑𝑅 as the channel gains from the 
CUE 𝑐𝑖 to the BS, the interference link from the DUE 
transmitter 𝑑𝑇 to the BS, the D2D link from the DUE 
transmitter  𝑑𝑇 to the receiver 𝑑𝑅 and the interference link 
form the CUE transmitter to the DUE receiver 𝑑𝑅, 
respectively.  
   The instantaneous received signal-to-interference-plus-
noise-ratio (SINR) at the BS from 𝑖th CUE and 𝑗th DUE 
over 𝑖th sub-channel at time slot 𝑡 is given as:  
            Γ𝑐𝑖(𝑡) =
𝑃𝑐𝑖𝑔𝑐,𝐵(𝑡)
σ2 + ∑
λ𝑗
𝑑𝑗∈𝐷 𝑖(𝑡)𝑃𝑑𝑗𝑔𝑑𝑇,𝐵(𝑡)
                        (1)
 
                                                                                                      
             Γ𝑑𝑗(𝑡) =
𝑃𝑑𝑗𝑔𝑑𝑇,𝑑𝑅(𝑡)
σ2 + ∑
λ𝑗
𝑐𝑖∈𝐶 𝑖(𝑡)𝑃𝑐𝑖𝑔𝑐,𝑑𝑅(𝑡)
                        (2) 
   λ𝑗
𝑖 ∈ {0,1} denotes the binary resource reuse indicator, 
λ𝑗
𝑖 = 1 implying 𝑗th DUE selects 𝑖th CUE sub-channel at 
time slot 𝑡 and λ𝑗
𝑖(𝑡) = 0 otherwise.  
 The data rates of the 𝑖th CUE and 𝑗th DUE is at time slot 𝑡 
are given by: 
              T𝑐𝑖(𝑡) = 𝑊𝑖 log2(1 + Γ𝑐𝑖(𝑡))                          (3) 
               T𝑑𝑗(𝑡) = 𝑊𝑖 log2 (1 + Γ𝑑𝑗(𝑡))                         (4) 
where 𝑊𝑖 is the bandwidth of each resource block. The 
variance of the additive white Gaussian noise (AWGN) is 
denoted by σ2. The resource allocation problem for D2D 
communication in cellular network is NP hard and cannot be 
solved directly and often requires global information 
gathering which increases complexity. The channel gains for 
links 𝑞 to 𝑟 can be expressed as follows: 
                      𝑔𝑞,𝑟=𝐺𝑟γ𝑞,𝑟𝜒𝑞,𝑟𝐿𝑞,𝑟
−𝛼𝑟                                 (5) 
where 𝐺𝑟 is the pathloss constant, 𝛾𝑞,𝑟 is the small-scale 
fading gain due to multipath propagation and assumed to 
have an exponential distribution with unit mean. The large-
scale fading comprises pathloss with exponent 𝛼𝑟 and 
shadowing which has a slow fading gain 𝜒𝑞,𝑟 with a log-
normal distribution. 𝐿𝑞,𝑟 is the distance from terminal 𝑞 to 
terminal 𝑟 [9]. The channel gain from D2D link 𝑑𝑗 of 
transmitter 𝑑𝑇 to the receiver 𝑑𝑅 is 𝑔𝑑𝑇,𝑑𝑅, the channel gain 
of the interference link from 𝑑𝑇 to the base station is 𝑔𝑑𝑇,𝐵 
and from CUE 𝑐𝑖 to DUE 𝑑𝑗 receiver is ℎ𝑐,𝑑𝑅 and  ℎ𝑐,𝐵,  is 
the channel gain from CUE 𝑐𝑖 to the base station BS. The 
channel gain 𝑔𝑑𝑇,𝑑𝑅 and 𝑔𝑐,𝑑𝑅 can be estimated at the DUE 
receiver, 𝑑𝑅 and made available at its transmitter, 𝑑𝑇 
instantaneously [10]. Similarly, 𝑔𝑐,𝐵 and  𝑔𝑑𝑇,𝐵  can be 
obtained at BS through local information since uplink 
transmission is considered. The reliability of the DUE 𝑑𝑗 ∈
𝐷,  𝜉𝑑𝑗(𝑡), is defined as the probability of packet delay 
exceeding a predefined delay bound, 𝑙𝑑𝑗,max, for channel 𝑖 at 
slot 𝑡 is less than a threshold [11]. Only the transmission 
delay is considered in this work. The objective of the system 
is to maximise the total throughput, 𝑇𝑅, of paired CUE and 
DUEs while satisfying the QoS demands. 
 
Max
λ𝑗
𝑖 𝑇𝑅 = 𝑊𝑖( λ𝑗
𝑖(∑
𝑐𝑖 ∈𝐶 log2(1 + Γ𝑐𝑖)
+ ∑
log2(1 +
𝑑𝑗 ∈𝐷
Γ𝑑𝑗)))    
                                                               (6)  
 
subject to: 
        λ𝑗
𝑖Γ𝑐𝑖 − Γ𝑐𝑖,min ≥ 0           ∀𝑐𝑖 ∈ 𝐶                       (6a)                       
           P r (𝑙𝑑𝑗 > 𝑙𝑑𝑗,max) < 1 − 𝜉𝑑𝑗
∗      ⩝ 𝑑𝑗 ∈ 𝐷       (6b)                    
          ∑
λ𝑗
𝑖
𝑐𝑖∈𝐶
≤ 1             ⩝ 𝑑𝑗  ∈ 𝐷                           (6c)                     
            ∑
λ𝑗
𝑖
𝑑𝑗∈𝐷
≤ 1              ⩝ 𝑐𝑖  ∈ 𝐶                         (6d)                       
    
The minimum CUE SINR, Γ𝑐𝑖,min, to guarantee the 
throughput requirement of the CUEs is defined in constraint 
(6a). Constraint (6b) takes into account reliability and delay, 
where 𝑙𝑑𝑗 is the packet delay constraint for packet 
transmission of DUE 𝑑𝑗. The expression captures the fact 
that the end-to-end delay should be less than 𝑙𝑑𝑗,max with a 
probability of at least 1 − 𝜉𝑑𝑗
∗ . Constraints (6c) and (6d) are 
channel association criteria. The reliability of the DUE links 
in (6c) is evaluated using an empirical estimation of number 
of packets transmitted similar to [11], from 𝑑𝑇 to 𝑑𝑅 whose 
delay is within the budget 𝑙𝑑𝑗,max over the total number of 
packets sent to 𝑑𝑅 at time slot 𝑡 i.e., 
 
 𝜉𝑑𝑗(𝑡) = 1 − Pr (𝑙𝑑𝑗 > 𝑙𝑑𝑗,max) ≈ 1 −
𝐿𝑑𝑗(𝑡)
𝐵𝑑𝑗(𝑡) ≅
𝐿𝑑𝑗
′ (𝑡)
𝐵𝑑𝑗(𝑡)   (7)                      
                                                                                          
where 𝐿𝑑𝑗(𝑡) is the number of packets for which 𝑙𝑑𝑗 >
𝑙𝑑𝑗,max and 𝐿𝑑𝑗
′ (𝑡) is the number of packets transmitted with 
𝑙𝑑𝑗 ≤ 𝑙𝑑𝑗,max (or number of packet delivered within the 
delay bound). 𝐵𝑑𝑗(𝑡) is total packet transmitted by DUE 𝑑𝑗 
at time slot 𝑡. Reliability can also be measured in terms of 
the outage probability, which is the probability that the 
measured SINR is lower than a minimum is less than a 
predefined threshold. The closed expression of the outage 
probability of 𝑗th DUE conditioned on the selected 𝑖th 
channel at time slot 𝑡  is given below [12]. 
       𝑝𝑅(𝑡) = Pr (Γ𝑑𝑗 ≤ Γ𝑑𝑗,min)  
= 1 −
P𝑑𝑗𝑔𝑑𝑇,𝑑𝑅exp (−
Γ𝑑𝑗,minσ2
P𝑑𝑗𝑔𝑑𝑇,𝑑𝑅
)
P𝑑𝑗𝑔𝑑𝑇,𝑑𝑅 +Γ𝑑𝑗,minP𝑐𝑖𝑔𝑐,,𝑑𝑅
≤ 𝑝𝑅0                           (8) 
 
where 𝑝𝑅(𝑡) is the measured outage probability of DUE 𝑑𝑗 
at time slot 𝑡 and 𝑝𝑅0 is the maximum tolerable outage 
19
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-956-0
AICT 2022 : The Eighteenth Advanced International Conference on Telecommunications

probability of 𝑑𝑗. The reliability of the DUE in terms of 
outage probability is expressed as: 
 
                          𝜉𝑑𝑗(𝑡) = 1 − 𝑝𝑅(𝑡)                               (9) 
 
Transmission delay is given as the ratio of packet size 
transmitted within delay bound to transmission rate [13].  
From (7), (8) and (9) the transmission of 𝑗th DUE on the 𝑖th 
RB is formulated as: 
                      𝑙𝑑𝑗(𝑡) =
𝐿𝑑𝑗
′ (𝑡)
𝑊𝑖log2(1 + Γ𝑑𝑗)                    (10) 
 The resource allocation problem for D2D communication 
in a cellular network is complex and a direct solution is not 
feasible. We present next a base station-assisted resource 
allocation scheme which adopts a semi-distributive RRM 
approach. 
 
III. 
STATELESS REINFORCEMENT LEARNING FOR         
BASE STATION-ASSISTED  RESOURCE ALLOCATION  
   The goal of the agents is to maximise the throughput in a 
D2D-enabled cellular network. At each time slot 𝑡, a DUE, 
observes a state 𝑠𝑡 and takes an action 𝑎𝑡 from the action 
space, (i.e., select an RB 𝑘𝑖), according to the policy π. Q-
learning enables an agent to determine the optimal strategy 
that maximises its long term expected cumulative reward 
[14]. The Q-value is updated as follows: 
 
𝑄𝑡+1  =
        {
𝑄𝑡(𝑠𝑡, 𝑎𝑡) + 𝜎 [𝑟𝑡 + 𝜂 max
𝑎′ 𝑄𝑡(𝑠𝑡+1, 𝑎𝑡+1) − 𝑄𝑡(𝑠𝑡, 𝑎𝑡)]  
if  𝑠 = 𝑠𝑡,   𝑎 = 𝑎𝑡
𝑄𝑡(𝑠𝑡, 𝑎𝑡) , otherwise                                                    (11)
     
 
where 𝜎 ∈ [0,1] is the learning rate. With 𝜎 = 0,  the Q-
values are never updated, hence no learning has taken place; 
setting 𝜎 to a high value such as means that learning can 
occur quickly and 0 ≤ 𝜂 ≤ 1 is the discount factor used to 
balance immediate and future reward [14].     
  The state-action dimension is reduced by adopting a 
stateless learning approach. For the considered scenario, any 
action 𝑎𝑖 ∈ 𝐴 taken by an agent will result in the end of an 
episode i.e., states 0 and 1 are terminal states, where 
𝑆𝑑𝑗
𝑖 (𝑡)  = 1 is the goal state of the DUEs. Therefore, the 
learning environment can be modelled entirely using a 
stateless Q-learning i.e., action-reward only since the state 
transition is not required. An agent can choose its action 
based solely on its Q-value and the updated Q-value of the 
chosen action is based on the current Q-value and the 
immediate reward from selecting that action. The update 
function in (11) is re-formulated as follows: 
 
𝑄𝑡+1(𝑎𝑡) = {𝑄𝑡(𝑎𝑡) + 𝜎[𝑟(𝑎𝑡) − 𝑄𝑡(𝑎𝑡)], if  𝑎 = 𝑎𝑡
𝑄𝑡(𝑎𝑡),          otherwise
  (12)            
                                                                                       
where 𝑟(𝑎𝑡) is the immediate reward of selecting 𝑎. In 
contrast to the standard Q-value update function in (11), it 
can be seen in (12) that not only the state-action formation 
(𝑠, 𝑎) is not necessary, but also the information of the next 
state 𝑠𝑡+1 is not required because the actions lead to a 
terminal state. Therefore, the Q-table is defined in terms of 
the actions only and updated using the immediate reward. 
This results in 1 × |𝑁| dimension Q-table for 𝑗th DUE.This 
method reduces the learning complexity and the Q-table 
dimension.  
   The traditional cellular users in the network need to be 
protected from the interference caused by the DUEs for their 
minimum SINR to be satisfied. This may be achieved by 
integrating the SINR of the CUE, Γ𝑐𝑖 in the state space or 
reward function modelling. This way, the DUEs can obtain 
the information from BS at time slot 𝑡 as in [15]-[17]; hence, 
the DUEs get a reward if the CUE SINR Γ𝑐𝑖 ≥ Γ𝑐𝑖,min , on 
the and a penalty otherwise. Rather than the BS exchange 
the measured CUE SINR, Γ𝑐𝑖, with the DUEs for every 
action 𝑎𝑡 taken at each time slot, we adopt a scheme in which 
the BS keeps a look-up table of the 𝑖th CUE based on the 
actions on the DUEs.  
 
BSA  Reinforcement Learning Algorithm 
    
1: Initialise the action-value function for the DUEs     
           [𝑄𝑑𝑗(𝑎) = 0|𝑄𝑑𝑗(𝑎) ≡ 𝑄𝑑𝑗
𝑖 (𝑎𝑡) , 𝑖 = 1,2, … , 𝑁] ⩝ 𝑑𝑗 ∈ 𝐷 
  2: Initialise the action-value function for the BS for the actions of     
      the 𝑗th DUE on the 𝑖th RB   
[𝑄𝑐𝑖(𝑎) = 0|𝑄𝑐𝑖(𝑎) ≡ 𝑄𝑐𝑖
𝑗 (𝑎𝑡), 𝑗 = 1,2, … , 𝑀 ]    ⩝ 𝑐𝑖 ∈ 𝐶                 
  3:    for 𝑑𝑗 ∈ 𝐷  1 ≤ 𝑗 ≤ 𝑀  do 
  4:        while not converge do  
  5: 
 generate a random number 𝑥 ∈ {0,1} 
  6:            if 𝑥 < 𝜀 then 
  7:                  Select action 𝑎𝑖
𝑡 randomly 
  8:            else 
  9:                   Select action 𝑎𝑖
𝑡=argmax
𝑎∈𝐴
𝑄𝑑𝑗(𝑎𝑡) 
10:            end 
11:            Evaluate 𝜉𝑑𝑗, Γ𝑑𝑗 and 𝑙𝑑𝑗 of 𝑑𝑗 ∈ 𝐷 for the action 𝑎𝑡 
12:            Measure the SINR, 𝜉𝑐𝑖, of CUE 𝑐𝑖 ∈ 𝐶 for the action 
                𝑎𝑡 taken by 𝑑𝑗 ∈ 𝐷 
13: 
 Observe immediate reward of 𝑑𝑗 ∈ 𝐷 and 𝑐𝑖 ∈ 𝐶,   
                  
14:            Update action-value for action of  𝑑𝑗 ∈ 𝐷 on the 𝑖th 
     RB 𝑄𝑑𝑗
𝑖 (𝑎) = 𝑄𝑑𝑗
𝑖 (𝑎) + 𝜎 [𝑟𝑑𝑗(𝑎𝑡) + 𝑄𝑑𝑗
𝑖 (𝑎)] 
15:            Update action-value for 𝑐𝑖 ∈ 𝐶 for action 𝑎𝑡 of 𝑗th  
                 DUE  𝑄𝑐𝑖
𝑗 (𝑎) = 𝑄𝑐𝑖
𝑗 (𝑎) + 𝜎[𝑟𝑐𝑖(𝑎𝑡) + 𝑄𝑐𝑖
𝑗 (𝑎)]  
16:          end while 
17:    end for 
18: Load 𝑄𝑑𝑗(𝑎) to the BS          ⩝ 𝑑𝑗 ∈ 𝐷 
19: for 𝑑𝑗 ∈ 𝐷  1 ≤ 𝑗 ≤ 𝑀  do 
20:      Obtain 𝑄(𝑎) = {𝑄𝑑𝑗
𝑖 (𝑎), 𝑄𝑐𝑖
𝑗 (𝑎)}  𝑖 = 1,2, … , 𝑁 
21:      𝑄̅(𝑎) ⊆ 𝑄(𝑎)| {𝑄𝑑𝑗
𝑖 (𝑎), 𝑄𝑐𝑖
𝑗 (𝑎)} ∈ ℝ+, where ℝ+   
            positive real number 
22:       𝑄TOT = 𝑄𝑑𝑗
𝑖 (𝑎) + 𝑄𝑐𝑖
𝑗 (𝑎)         ⩝ 𝑞 ∈ 𝑄̅(𝑎) 
23: end for 
24: Set up a list for unmatched DUE 𝐷𝑢 =  {𝑑𝑗 : ⩝ 𝑑𝑗 ∈ 𝐷𝑢} 
25: while 𝐷𝑢 ≠ ∅ do 
26:       Rank 𝐷𝑢 in increasing order of |0 𝑄̅(𝑎)| 
27:       Start DUE 𝑑𝑗 ∈ 𝐷𝑢: 𝑄̅(𝑎) ≠ ∅ with the least | 𝑄̅(𝑎)| 
28:        𝑐𝑖
∗ = max
𝑟𝑖 ∈𝑅 𝑄TOT 
29:        𝐷𝑢 = 𝐷𝑢 − 𝑑𝑗 
30:        𝑄̅(𝑎) = 𝑄̅(𝑎)\𝑐𝑖
∗  
     ⩝ 𝑑𝑗′ ∈ 𝐷𝑢| 𝑗′ ≠ 𝑗 
31: end while 
 
20
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-956-0
AICT 2022 : The Eighteenth Advanced International Conference on Telecommunications

   There are a number of methods to select an action based 
on the current evaluation of the Q-value at every time slot 𝑡 
using a policy denoted by 𝑝𝑑𝑗
𝑡 . These methods are used to 
balance exploration and exploitation [18]. Epsilon greedy 
(𝜀-greedy) is one of the methods of choosing an optimal Q-
value.  
   The reward function is modelled such that it relies only on 
local observations and can be implemented in a distributive 
manner. The rewards of the 𝑗th DUE and 𝑖th CUE for taking 
an action 𝑎𝑖
𝑡 is expressed in terms of the achievable 
throughput using the Shannon capacity formula. Therefore, 
the reward is directly related to the objective function of the 
optimisation problem. The following is a summary of the 
Base Station Assisted (BSA) Reinforcement Learning 
Algorithm: 
  The 𝑗th DUE only gets a reward when all state variables 
are 1 (i.e., the minimum QoS demands are met) while 𝑖th 
CUE gets a reward if its minimum SINR is satisfied at each 
time slot for the action taken by 𝑗th DUE. From the reward 
function defined above, learning can be implemented 
independently in a decentralised manner such that each 
agent maintains a local Q-table. There is no information 
exchange relating to other agents’ actions or rewards and no 
cooperation is needed between the agents, which results in 
reduced signalling overheads and reduced complexity 
compared with a centralised Q-learning approach.  
IV. 
PERFORMANCE EVALUATION 
   The performance of the presented BSA scheme is verified 
by considering a single-cell network in an industrial 
scenario. The simulation setup and channel models are 
summarised in Tables I and II. The network dynamics are 
captured by generating the channel fading effects randomly. 
The throughput is the main metric used to evaluate the 
performances of the algorithms. The performances of BSA 
are compared with centralised optimisation and the game 
theoretic Deferred Acceptance (DA) schemes [9]. 
 
TABLE I. MAIN SIMULATION PARAMETERS [9]  
 
Parameter 
Value 
Carrier frequency, 𝑓𝑐 
2GHz 
System bandwidth 
10MHz 
Number of resource blocks (RB), 𝐾 
50 
RB bandwidth 
180 kHz 
Maximum CUE transmit power, P𝑐𝑖,max 
23dBm 
Maximum DUE transmit power, P𝑑𝑗,max 
13dBm 
D2D distance, 𝐿𝑑𝑇,𝑑𝑅 
10m ≤ 𝐿𝑑𝑇,𝑑𝑅
≤ 20m 
CUE SINR Threshold, Γ𝑐𝑖,min 
7 dB 
DUE SINR Threshold, Γ𝑑𝑗,min 
3 dB 
Noise power density 
−174 dBm/Hz 
Number of CUEs, 𝑁 
50 
Number of DUEs, 𝑀 
50 
Reliability for DUE, 𝑝𝑅0 
10−5 
Exploration rate, 𝜀 
0.7 
Learning rate, 𝜎 
0.9 
DUE Maximum Delay, 𝑙𝑑𝑗,max 
50ms 
DUE Message Size, 𝐵𝑑𝑗 
15kB 
 
TABLE II. CHANNEL MODEL FOR LINKS [9] 
 
Parameter 
In-factory         
DUE link 
UE-UE link 
BS-UE link 
Pathloss 
model 
𝟑𝟔. 𝟖 𝐥𝐨𝐠𝟏𝟎(𝐝[𝐦])
+ 𝟑𝟓. 𝟖 
 𝟒𝟎 𝐥𝐨𝐠𝟏𝟎(𝐝[𝐦])
+ 𝟐𝟖 
𝟑𝟕. 𝟔 𝐥𝐨𝐠𝟏𝟎(𝐝[𝐦])
+ 𝟏𝟓. 𝟑 
Shadowing 
4dB 
6dB 
8dB 
Fast fading 
Rayleigh 
Fading 
Rayleigh 
Fading 
Rayleigh 
Fading 
 
  
 The throughput performance of matched DUEs as a 
function of the number of DUEs in the system 𝑀, is shown 
in Fig. 1. It can be concluded that the sum throughput of the 
DUEs increases with the number of cellular users 𝑀 for all 
the considered algorithms. As expected, the number of 
admitted DUEs increases with the introduction of new 
DUEs to the system but remains unchanged if a valid 
cellular resource-sharing partner cannot be found because 
the minimum QoS requirements are not satisfied.  
   The centralised optimisation and BSA approaches are 
comparable, while the DA method shows the least 
performance. The BS-A algorithm outperforms the DA 
algorithms by up to 9.69% increase in the DUE throughput 
performance. However, it is semi-distributive as the final 
resource allocation is implemented by the BS whereas the 
DA approach is decentralised (the channel selection is user-
centric with no BS intervention to achieve autonomy). 
Players can make their resource allocations choices to 
maximise their individual and ultimately achieve system 
stability.   
   The performance of the sum throughput of the matched 
UEs (that is valid pairings between CUEs and DUEs) with 
respect to the number of cellular users 𝑀 is presented in Fig. 
2. The sum throughput increases with 𝑀. The BS-A 
approach shows better performance at 𝑀 ≤ 35 with up to 
12.05% increase in sum throughput compared to the 
centralised approach while the centralised approach 
performed better at 𝑀 > 35 with up to 9.39% increase in 
throughput. The DA algorithm again shows the least 
performance with up to 11.29% decrease compared with the 
BS-A technique.  The effect of the outage probability of the 
𝑝𝑅0, and delay threshold of the DUEs 𝑙𝑑𝑗,max  on the sum 
rate of the matched UEs for all algorithms is shown in Fig. 
3 and Fig. 4. The sum throughput of the matched UEs 
increases with 𝑝𝑅0 and 𝑙𝑑𝑗,max. This is because higher 𝑝𝑅0 
causes the interference from the CUEs to be more tolerable 
by the DUEs, therefore making potential CUE-DUE pairing 
possible. Similarly, higher 𝑙𝑑𝑗,max increases the sum 
throughput at fixed outage probability and payload since the 
delay requirement is less stringent. More DUEs are able to 
satisfy the delay constraint and the number of admitted 
DUEs are increased. 
 
V.  CONCLUSION AND FUTURE WORK 
   We presented a semi-distributed Base Station Assisted 
(BSA) scheme for Radio Resource Management (RRM) of 
a network with D2D and cellular users, targeting wireless 
industrial scenarios. The reinforcement learning based 
21
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-956-0
AICT 2022 : The Eighteenth Advanced International Conference on Telecommunications

approach relies on distributed training of the D2D agents. 
Subsequently, the look-up tables for the D2D agents are 
loaded to the base station for centralised channel allocation. 
Simulation results show that the throughput of the presented 
approach 
is 
comparable 
to 
traditional 
centralised 
optimisation and demonstrates an improved performance 
relative to the deferred acceptance (DA) scheme. The future 
work will focus on evaluating the trade-off between 
performance, complexity and signaling overheads for the 
BSA scheme relative to other techniques. 
 
 
Fig. 1.  Sum-rate of matched DUEs with varying number of DUEs,  𝑀 in 
the System, for 𝑁 = 50 
 
 
 
Fig. 2.  Sum Throughput of matched UEs as a function of the number of 
DUEs  𝑀, in the system, for 𝑁 = 50   
 
 
 
Fig. 3.  Effect of the DUE outage ratio 𝑝𝑅0, on the sum throughput for 
𝑁 = 𝑀 = 50, 𝑙𝑑𝑗,max = 50ms 
 
 
Fig. 4.   Effect of the delay bound, 𝑙𝑑𝑗,max on the sum throughput of 
matched CUE-DUE pair for 𝑁 = 𝑀 = 50, 𝑝𝑅0 = 10−5 
 
 
REFERENCES 
[1] J. Wan et al., “Toward dynamic resources management for 
IoT-based manufacturing,” IEEE Communications Magazine, 
vol. 56, no. 2, pp. 52-59, Feb. 2018. 
[2] O.N. Yilmaz et al., “Analysis of ultra-reliable and low-latency 
5G communication for a factory automation use case,” 
in Proc. of 2015 IEEE International Conference on 
Communication Workshop (ICCW), pp. 1190-1195, Sep. 
2015. 
[3] N. Brahmi, O. N. Yilmaz, K. W. Helmersson, S. A. Ashraf 
and J. Torsner, “Deployment strategies for ultra-reliable and 
low-latency communication in factory automation,” in Proc. 
of 2015 IEEE Globecom Workshops (GC Wkshps), pp. 1-6, 
Feb. 2016. 
22
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-956-0
AICT 2022 : The Eighteenth Advanced International Conference on Telecommunications

[4] Z. Li and C. Guo, “Multi-agent deep reinforcement learning 
based 
spectrum 
allocation 
for 
D2D 
underlay 
communications,” 
IEEE 
Transactions 
on 
Vehicular 
Technology, vol. 69, no. 2, pp. 1828-1840, Dec. 2019. 
[5] A. Asheralieva and Y. Miyanaga, “An autonomous learning-
based algorithm for joint channel and power level selection by 
D2D pairs in cellular networks,” IEEE transactions on 
communications, vol. 64, no. 9, pp. 3996-4012, Jul. 2016. 
[6] Z. Li and C. Guo, “Multi-agent deep reinforcement learning 
based 
spectrum 
allocation 
for 
D2D 
underlay 
communications,” 
IEEE 
Transactions 
on 
Vehicular 
Technology, pp. 1828-1840, Feb, 2020. 
[7] S. Nie, Z. Fan, M. Zhao, X. Gu and L. Zhang, “Q-learning 
based power control algorithm for D2D communication,” in 
Proc. of   IEEE 27th Annual International Symposium on 
Personal, Indoor, and Mobile Radio Communications 
(PIMRC), pp. 1-6, Sep. 2016. 
[8] K.  Zia et al., “A distributed multi-agent RL-based 
autonomous spectrum allocation scheme in D2D enabled 
multi-tier HetNets”. IEEE Access, no.7, pp. 6733-6745, Jan. 
2019.  
[9] I.O. Sanusi, K.M. Nasr and K. Moessner, “Radio resource 
management approaches for reliable Device-to-Device (D2D) 
communication in wireless industrial applications,” IEEE 
Transactions of Cognitive Communication and Networking, 
vol. 7, no. 3, pp.905-916, Oct. 2021. 
[10] L. Liang, H. Ye and G.Y. Li, “Spectrum sharing in vehicular 
networks based on multi-agent reinforcement learning,” IEEE 
Journal on Selected Areas in Communications, vol. 37, no. 10, 
pp. 2282-2292, Aug. 2019. 
[11] A.T. Kasgari and W. Saad, “Model-free ultra-reliable low 
delay communication (URLLC): A deep reinforcement 
learning framework,” in Proc. IEEE International Conference 
on Communications (ICC), pp. 1-6, May 2019. 
[12] H. Wang and X. Chu, “Distance-constrained resource-sharing 
criteria for device-to-device communications underlaying 
cellular networks,” Electronics letters, vol. 48, no. 9, pp. 528-
530, Apr. 2012. 
[13] H. Yang, X. Xie and M. Kadoch, “Intelligent resource 
management based on reinforcement learning for ultra-
reliable and low-delay IoV communication networks,” IEEE 
Transactions on Vehicular Technology, vol. 68, no. 5, pp. 
4157-4169, Jan. 2019. 
[14] F.E. Souhir, A. Belghith and F. Zarai, “A reinforcement 
learning-based radio resource management algorithm for 
D2D-based V2V communication,” in Proc. 15th International 
Wireless Communications & Mobile Computing Conference 
(IWCMC), pp. 1367-1372, Jun. 2019. 
[15] Z. Li and C. Guo, “Multi-agent deep reinforcement learning 
based 
spectrum 
allocation 
for 
D2D 
underlay 
communications,” 
IEEE 
Transactions 
on 
Vehicular 
Technology, pp. 1828-1840, Feb. 2020. 
[16] S. Nie, Z. Fan, M. Zhao, X. Gu and L. Zhang, “Q-learning 
based power control algorithm for D2D communication,” in 
Proc. of  IEEE 27th Annual International Symposium on 
Personal, Indoor, and Mobile Radio Communications 
(PIMRC), Sep. 2016. 
[17] Y. Wei, Y. Qu, M. Zhao, L. Zhang and F.R. Yu, “Resource 
allocation and power control policy for Device-to-Device 
communication 
using 
multi-Agent 
reinforcement 
learning,” Computers, Materials & Continua, vol. 63, no. 3, 
pp.1515-1532, May 2020. 
[18] J. Kim, J. Park, J. Noh and S. Cho, “Autonomous power 
allocation based on distributed deep learning for device-to-
device communication underlaying cellular network,” IEEE 
Access, Vol. 8, 107853-107864, Jun. 2020. 
 
23
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-956-0
AICT 2022 : The Eighteenth Advanced International Conference on Telecommunications

