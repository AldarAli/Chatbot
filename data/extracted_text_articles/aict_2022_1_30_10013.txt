A Machine Learning Approach for Resource Allocation  
in Wireless Industrial Environments 
Idayat O. Sanusi and  Karim M. Nasr   
Faculty of Engineering and Science, University of Greenwich,  
Kent, ME4 4TB, United Kingdom 
{i.o.sanusi, k.m.nasr}@gre.ac.uk  
 
Abstractâ€” In this paper, we present a machine learning 
technique for channel selection in a Device to Device (D2D)-
enabled cellular network targeting a wireless industrial 
environment. The presented Base Station Assisted (BSA) 
reinforcement learning technique uses a distributed local Q-
table for the agents (users), to prevent global information 
gathering within the cellular network. A stateless Q-learning 
approach is adopted to reduce the complexity of learning and 
the dimension of the Q-table. After the training of the D2D 
agents, the Q-tables of the D2D users are uploaded to the base 
station for resource allocation to be implemented centrally. 
Simulations results show that the presented technique provides 
a Radio Resource Management (RRM) solution with a good 
Quality of Service (QoS) performance compared to other 
conventional approaches. 
Keywordsâ€” 5G and beyond networks; Radio Resource 
Management; 
Distributed 
Algorithms; 
Device-to-Device 
Communication; Reinforcement Learning. 
I. 
 INTRODUCTION  
   Device-to-Device communication (D2D) is considered as 
a promising solution for ultra-reliable low-latency use cases 
because of associated advantages in terms of reduced 
latency, 
improved 
reliability 
and 
throughput. 
The 
integration of D2D into future industrial wireless networks 
and smart manufacturing facilitates the creation of massive 
machine-type 
connections 
[1]. 
 
Machine-Type 
Communication (MTC) is expected to support large 
numbers of smart devices, predominantly with small data 
volume requirements, which aggregates into a massive 
amount of data from parallel transmissions of a large number 
of devices. Achieving ultra-high reliability and ultra-low 
latency pose challenges in terms of bandwidth requirements. 
Yilmaz et al. [2] studied the transmission bandwidth needed 
to enable Ultra-Reliable Low-Latency Communication 
(URLLC) for factory automation and found that the system 
bandwidth depends on the number of connected pieces of 
user equipment and the behaviour of their traffic. The 
scarcity of radio resources and the limitations on the 
available system bandwidth makes spectrum sharing a 
necessity for D2D implementation of MTC for factory 
automation [3]. RRM schemes need to be efficiently 
designed for interference management and coordination 
while guaranteeing tight URLLC demands.     
   Channel reuse among active devices in the same cells will 
generate interference which degrades system performance. 
Interference management is crucial to ensure efficient 
utilisation of available spectrum resources which is also 
particularly challenging for D2D deployments in underlay 
cellular networks.  
   The 
two 
major 
approaches 
to 
Radio 
Resource 
Management (RRM) are based on centralised and 
distributed methods. The centralised scheme requires global 
information gathering by base stations which often results in 
a high signalling overhead and increased complexity which 
tend to increase with the number of users, thus making it 
impractical. The distributed approach does not need a central 
entity; resource allocation is implemented by users therefore 
reducing the amount of information gathering and 
processing by base stations. However, a distributed RRM 
algorithm may also increase signalling overheads due to the 
high amount of information interchange among devices [4]. 
    Reinforcement Learning (RL) has recently gained a lot of 
attention because of its suitability for the decision-making 
process where there is unknown or partial channel 
information. RL has been widely studied for intelligent 
power and spectrum allocations for D2D communication in 
cellular networks. Asheralieva and Miyanga [5] formulated 
the resource allocation problem as a non-cooperative game 
model among D2D users and a mixed strategy Nash 
Equilibrium was obtained.   However, the Quality of 
Experience (QoS) metrics of cellular users sharing channel 
resources with D2D links were not taken into account in the 
reward model. A multi-agent actor-critic structure is 
proposed in [6] which involves interactions between users 
and centralised sharing of all historical information. This 
leads to an increase in signalling overheads and information 
exchange. In [5]-[8], the reward function captured the QoS 
metric of the cellular users in a centralised Q-learning 
approach, which leads to increased signalling overheads as 
well.  
   In this paper, we present a semi-distributed reinforcement 
learning scheme for D2D resource allocation in a cellular 
network in an industrial setting. After the decentralised 
training of the agents, their Q-tables uploaded are forwarded 
to the base station for a centralised resource allocation.  The 
reward function is modeled in such a way that there is no 
information exchange related to other agentsâ€™ action or 
reward. To address the problem of the â€˜curse of 
dimensionalityâ€™ associated with Q-learning, a stateless Q-
learning is adopted to reduce the dimension of the Q-table, 
nonetheless capturing the QoS demands of the D2D users.  
The paper is organised as follows: The problem 
formulation and system model are presented in Section II. In 
Section III, a stateless reinforcement learning algorithm for 
base station-assisted resource allocation is presented. 
Section IV presents some results and discussions of the 
simulation scenarios considered. Conclusions and directions 
for future work are summarised in Section V. 
18
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-956-0
AICT 2022 : The Eighteenth Advanced International Conference on Telecommunications

II. 
SYSTEM MODEL  
   We consider D2D and cellular users coexisting within a 
cellular network for uplink spectrum-sharing. There are ğ‘ 
Cellular User Equipment (CUEs) represented by a set ğ¶ and 
ğ‘€ D2D User Equipment (DUEs) denoted by a set ğ· 
randomly deployed within the coverage of the base station 
(BS) in a single cell system. The cellular users have strict 
performance requirements in the form of minimum Signal to 
Interference plus Noise Ratio (SINR) values to guarantee 
their throughput. The D2D links also have minimum SINR 
thresholds to guarantee their throughput demands, in 
addition, to the reliability and delay constraints.    We 
assume that each CUE has been pre-allocated a resource 
block. The transmit power of the CUEs and DUEs are 
denoted by ğ‘ƒğ‘ğ‘– and ğ‘ƒğ‘‘ğ‘—, respectively. We denote 
ğ‘”ğ‘,ğµ,  ğ‘”ğ‘‘ğ‘‡,ğµ,  ğ‘”ğ‘‘ğ‘‡,ğ‘‘ğ‘… and ğ‘”ğ‘,ğ‘‘ğ‘… as the channel gains from the 
CUE ğ‘ğ‘– to the BS, the interference link from the DUE 
transmitter ğ‘‘ğ‘‡ to the BS, the D2D link from the DUE 
transmitter  ğ‘‘ğ‘‡ to the receiver ğ‘‘ğ‘… and the interference link 
form the CUE transmitter to the DUE receiver ğ‘‘ğ‘…, 
respectively.  
   The instantaneous received signal-to-interference-plus-
noise-ratio (SINR) at the BS from ğ‘–th CUE and ğ‘—th DUE 
over ğ‘–th sub-channel at time slot ğ‘¡ is given as:  
            Î“ğ‘ğ‘–(ğ‘¡) =
ğ‘ƒğ‘ğ‘–ğ‘”ğ‘,ğµ(ğ‘¡)
Ïƒ2 + âˆ‘
Î»ğ‘—
ğ‘‘ğ‘—âˆˆğ· ğ‘–(ğ‘¡)ğ‘ƒğ‘‘ğ‘—ğ‘”ğ‘‘ğ‘‡,ğµ(ğ‘¡)
                        (1)
 
                                                                                                      
             Î“ğ‘‘ğ‘—(ğ‘¡) =
ğ‘ƒğ‘‘ğ‘—ğ‘”ğ‘‘ğ‘‡,ğ‘‘ğ‘…(ğ‘¡)
Ïƒ2 + âˆ‘
Î»ğ‘—
ğ‘ğ‘–âˆˆğ¶ ğ‘–(ğ‘¡)ğ‘ƒğ‘ğ‘–ğ‘”ğ‘,ğ‘‘ğ‘…(ğ‘¡)
                        (2) 
   Î»ğ‘—
ğ‘– âˆˆ {0,1} denotes the binary resource reuse indicator, 
Î»ğ‘—
ğ‘– = 1 implying ğ‘—th DUE selects ğ‘–th CUE sub-channel at 
time slot ğ‘¡ and Î»ğ‘—
ğ‘–(ğ‘¡) = 0 otherwise.  
 The data rates of the ğ‘–th CUE and ğ‘—th DUE is at time slot ğ‘¡ 
are given by: 
              Tğ‘ğ‘–(ğ‘¡) = ğ‘Šğ‘– log2(1 + Î“ğ‘ğ‘–(ğ‘¡))                          (3) 
               Tğ‘‘ğ‘—(ğ‘¡) = ğ‘Šğ‘– log2 (1 + Î“ğ‘‘ğ‘—(ğ‘¡))                         (4) 
where ğ‘Šğ‘– is the bandwidth of each resource block. The 
variance of the additive white Gaussian noise (AWGN) is 
denoted by Ïƒ2. The resource allocation problem for D2D 
communication in cellular network is NP hard and cannot be 
solved directly and often requires global information 
gathering which increases complexity. The channel gains for 
links ğ‘ to ğ‘Ÿ can be expressed as follows: 
                      ğ‘”ğ‘,ğ‘Ÿ=ğºğ‘ŸÎ³ğ‘,ğ‘Ÿğœ’ğ‘,ğ‘Ÿğ¿ğ‘,ğ‘Ÿ
âˆ’ğ›¼ğ‘Ÿ                                 (5) 
where ğºğ‘Ÿ is the pathloss constant, ğ›¾ğ‘,ğ‘Ÿ is the small-scale 
fading gain due to multipath propagation and assumed to 
have an exponential distribution with unit mean. The large-
scale fading comprises pathloss with exponent ğ›¼ğ‘Ÿ and 
shadowing which has a slow fading gain ğœ’ğ‘,ğ‘Ÿ with a log-
normal distribution. ğ¿ğ‘,ğ‘Ÿ is the distance from terminal ğ‘ to 
terminal ğ‘Ÿ [9]. The channel gain from D2D link ğ‘‘ğ‘— of 
transmitter ğ‘‘ğ‘‡ to the receiver ğ‘‘ğ‘… is ğ‘”ğ‘‘ğ‘‡,ğ‘‘ğ‘…, the channel gain 
of the interference link from ğ‘‘ğ‘‡ to the base station is ğ‘”ğ‘‘ğ‘‡,ğµ 
and from CUE ğ‘ğ‘– to DUE ğ‘‘ğ‘— receiver is â„ğ‘,ğ‘‘ğ‘… and  â„ğ‘,ğµ,  is 
the channel gain from CUE ğ‘ğ‘– to the base station BS. The 
channel gain ğ‘”ğ‘‘ğ‘‡,ğ‘‘ğ‘… and ğ‘”ğ‘,ğ‘‘ğ‘… can be estimated at the DUE 
receiver, ğ‘‘ğ‘… and made available at its transmitter, ğ‘‘ğ‘‡ 
instantaneously [10]. Similarly, ğ‘”ğ‘,ğµ and  ğ‘”ğ‘‘ğ‘‡,ğµ  can be 
obtained at BS through local information since uplink 
transmission is considered. The reliability of the DUE ğ‘‘ğ‘— âˆˆ
ğ·,  ğœ‰ğ‘‘ğ‘—(ğ‘¡), is defined as the probability of packet delay 
exceeding a predefined delay bound, ğ‘™ğ‘‘ğ‘—,max, for channel ğ‘– at 
slot ğ‘¡ is less than a threshold [11]. Only the transmission 
delay is considered in this work. The objective of the system 
is to maximise the total throughput, ğ‘‡ğ‘…, of paired CUE and 
DUEs while satisfying the QoS demands. 
 
Max
Î»ğ‘—
ğ‘– ğ‘‡ğ‘… = ğ‘Šğ‘–( Î»ğ‘—
ğ‘–(âˆ‘
ğ‘ğ‘– âˆˆğ¶ log2(1 + Î“ğ‘ğ‘–)
+ âˆ‘
log2(1 +
ğ‘‘ğ‘— âˆˆğ·
Î“ğ‘‘ğ‘—)))    
                                                               (6)  
 
subject to: 
        Î»ğ‘—
ğ‘–Î“ğ‘ğ‘– âˆ’ Î“ğ‘ğ‘–,min â‰¥ 0           âˆ€ğ‘ğ‘– âˆˆ ğ¶                       (6a)                       
           P r (ğ‘™ğ‘‘ğ‘— > ğ‘™ğ‘‘ğ‘—,max) < 1 âˆ’ ğœ‰ğ‘‘ğ‘—
âˆ—      â© ğ‘‘ğ‘— âˆˆ ğ·       (6b)                    
          âˆ‘
Î»ğ‘—
ğ‘–
ğ‘ğ‘–âˆˆğ¶
â‰¤ 1             â© ğ‘‘ğ‘—  âˆˆ ğ·                           (6c)                     
            âˆ‘
Î»ğ‘—
ğ‘–
ğ‘‘ğ‘—âˆˆğ·
â‰¤ 1              â© ğ‘ğ‘–  âˆˆ ğ¶                         (6d)                       
    
The minimum CUE SINR, Î“ğ‘ğ‘–,min, to guarantee the 
throughput requirement of the CUEs is defined in constraint 
(6a). Constraint (6b) takes into account reliability and delay, 
where ğ‘™ğ‘‘ğ‘— is the packet delay constraint for packet 
transmission of DUE ğ‘‘ğ‘—. The expression captures the fact 
that the end-to-end delay should be less than ğ‘™ğ‘‘ğ‘—,max with a 
probability of at least 1 âˆ’ ğœ‰ğ‘‘ğ‘—
âˆ— . Constraints (6c) and (6d) are 
channel association criteria. The reliability of the DUE links 
in (6c) is evaluated using an empirical estimation of number 
of packets transmitted similar to [11], from ğ‘‘ğ‘‡ to ğ‘‘ğ‘… whose 
delay is within the budget ğ‘™ğ‘‘ğ‘—,max over the total number of 
packets sent to ğ‘‘ğ‘… at time slot ğ‘¡ i.e., 
 
 ğœ‰ğ‘‘ğ‘—(ğ‘¡) = 1 âˆ’ Pr (ğ‘™ğ‘‘ğ‘— > ğ‘™ğ‘‘ğ‘—,max) â‰ˆ 1 âˆ’
ğ¿ğ‘‘ğ‘—(ğ‘¡)
ğµğ‘‘ğ‘—(ğ‘¡) â‰…
ğ¿ğ‘‘ğ‘—
â€² (ğ‘¡)
ğµğ‘‘ğ‘—(ğ‘¡)   (7)                      
                                                                                          
where ğ¿ğ‘‘ğ‘—(ğ‘¡) is the number of packets for which ğ‘™ğ‘‘ğ‘— >
ğ‘™ğ‘‘ğ‘—,max and ğ¿ğ‘‘ğ‘—
â€² (ğ‘¡) is the number of packets transmitted with 
ğ‘™ğ‘‘ğ‘— â‰¤ ğ‘™ğ‘‘ğ‘—,max (or number of packet delivered within the 
delay bound). ğµğ‘‘ğ‘—(ğ‘¡) is total packet transmitted by DUE ğ‘‘ğ‘— 
at time slot ğ‘¡. Reliability can also be measured in terms of 
the outage probability, which is the probability that the 
measured SINR is lower than a minimum is less than a 
predefined threshold. The closed expression of the outage 
probability of ğ‘—th DUE conditioned on the selected ğ‘–th 
channel at time slot ğ‘¡  is given below [12]. 
       ğ‘ğ‘…(ğ‘¡) = Pr (Î“ğ‘‘ğ‘— â‰¤ Î“ğ‘‘ğ‘—,min)  
= 1 âˆ’
Pğ‘‘ğ‘—ğ‘”ğ‘‘ğ‘‡,ğ‘‘ğ‘…exp (âˆ’
Î“ğ‘‘ğ‘—,minÏƒ2
Pğ‘‘ğ‘—ğ‘”ğ‘‘ğ‘‡,ğ‘‘ğ‘…
)
Pğ‘‘ğ‘—ğ‘”ğ‘‘ğ‘‡,ğ‘‘ğ‘… +Î“ğ‘‘ğ‘—,minPğ‘ğ‘–ğ‘”ğ‘,,ğ‘‘ğ‘…
â‰¤ ğ‘ğ‘…0                           (8) 
 
where ğ‘ğ‘…(ğ‘¡) is the measured outage probability of DUE ğ‘‘ğ‘— 
at time slot ğ‘¡ and ğ‘ğ‘…0 is the maximum tolerable outage 
19
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-956-0
AICT 2022 : The Eighteenth Advanced International Conference on Telecommunications

probability of ğ‘‘ğ‘—. The reliability of the DUE in terms of 
outage probability is expressed as: 
 
                          ğœ‰ğ‘‘ğ‘—(ğ‘¡) = 1 âˆ’ ğ‘ğ‘…(ğ‘¡)                               (9) 
 
Transmission delay is given as the ratio of packet size 
transmitted within delay bound to transmission rate [13].  
From (7), (8) and (9) the transmission of ğ‘—th DUE on the ğ‘–th 
RB is formulated as: 
                      ğ‘™ğ‘‘ğ‘—(ğ‘¡) =
ğ¿ğ‘‘ğ‘—
â€² (ğ‘¡)
ğ‘Šğ‘–log2(1 + Î“ğ‘‘ğ‘—)                    (10) 
 The resource allocation problem for D2D communication 
in a cellular network is complex and a direct solution is not 
feasible. We present next a base station-assisted resource 
allocation scheme which adopts a semi-distributive RRM 
approach. 
 
III. 
STATELESS REINFORCEMENT LEARNING FOR         
BASE STATION-ASSISTED  RESOURCE ALLOCATION  
   The goal of the agents is to maximise the throughput in a 
D2D-enabled cellular network. At each time slot ğ‘¡, a DUE, 
observes a state ğ‘ ğ‘¡ and takes an action ğ‘ğ‘¡ from the action 
space, (i.e., select an RB ğ‘˜ğ‘–), according to the policy Ï€. Q-
learning enables an agent to determine the optimal strategy 
that maximises its long term expected cumulative reward 
[14]. The Q-value is updated as follows: 
 
ğ‘„ğ‘¡+1  =
        {
ğ‘„ğ‘¡(ğ‘ ğ‘¡, ğ‘ğ‘¡) + ğœ [ğ‘Ÿğ‘¡ + ğœ‚ max
ğ‘â€² ğ‘„ğ‘¡(ğ‘ ğ‘¡+1, ğ‘ğ‘¡+1) âˆ’ ğ‘„ğ‘¡(ğ‘ ğ‘¡, ğ‘ğ‘¡)]  
if  ğ‘  = ğ‘ ğ‘¡,   ğ‘ = ğ‘ğ‘¡
ğ‘„ğ‘¡(ğ‘ ğ‘¡, ğ‘ğ‘¡) , otherwise                                                    (11)
     
 
where ğœ âˆˆ [0,1] is the learning rate. With ğœ = 0,  the Q-
values are never updated, hence no learning has taken place; 
setting ğœ to a high value such as means that learning can 
occur quickly and 0 â‰¤ ğœ‚ â‰¤ 1 is the discount factor used to 
balance immediate and future reward [14].     
  The state-action dimension is reduced by adopting a 
stateless learning approach. For the considered scenario, any 
action ğ‘ğ‘– âˆˆ ğ´ taken by an agent will result in the end of an 
episode i.e., states 0 and 1 are terminal states, where 
ğ‘†ğ‘‘ğ‘—
ğ‘– (ğ‘¡)  = 1 is the goal state of the DUEs. Therefore, the 
learning environment can be modelled entirely using a 
stateless Q-learning i.e., action-reward only since the state 
transition is not required. An agent can choose its action 
based solely on its Q-value and the updated Q-value of the 
chosen action is based on the current Q-value and the 
immediate reward from selecting that action. The update 
function in (11) is re-formulated as follows: 
 
ğ‘„ğ‘¡+1(ğ‘ğ‘¡) = {ğ‘„ğ‘¡(ğ‘ğ‘¡) + ğœ[ğ‘Ÿ(ğ‘ğ‘¡) âˆ’ ğ‘„ğ‘¡(ğ‘ğ‘¡)], if  ğ‘ = ğ‘ğ‘¡
ğ‘„ğ‘¡(ğ‘ğ‘¡),          otherwise
  (12)            
                                                                                       
where ğ‘Ÿ(ğ‘ğ‘¡) is the immediate reward of selecting ğ‘. In 
contrast to the standard Q-value update function in (11), it 
can be seen in (12) that not only the state-action formation 
(ğ‘ , ğ‘) is not necessary, but also the information of the next 
state ğ‘ ğ‘¡+1 is not required because the actions lead to a 
terminal state. Therefore, the Q-table is defined in terms of 
the actions only and updated using the immediate reward. 
This results in 1 Ã— |ğ‘| dimension Q-table for ğ‘—th DUE.This 
method reduces the learning complexity and the Q-table 
dimension.  
   The traditional cellular users in the network need to be 
protected from the interference caused by the DUEs for their 
minimum SINR to be satisfied. This may be achieved by 
integrating the SINR of the CUE, Î“ğ‘ğ‘– in the state space or 
reward function modelling. This way, the DUEs can obtain 
the information from BS at time slot ğ‘¡ as in [15]-[17]; hence, 
the DUEs get a reward if the CUE SINR Î“ğ‘ğ‘– â‰¥ Î“ğ‘ğ‘–,min , on 
the and a penalty otherwise. Rather than the BS exchange 
the measured CUE SINR, Î“ğ‘ğ‘–, with the DUEs for every 
action ğ‘ğ‘¡ taken at each time slot, we adopt a scheme in which 
the BS keeps a look-up table of the ğ‘–th CUE based on the 
actions on the DUEs.  
 
BSA  Reinforcement Learning Algorithm 
    
1: Initialise the action-value function for the DUEs     
           [ğ‘„ğ‘‘ğ‘—(ğ‘) = 0|ğ‘„ğ‘‘ğ‘—(ğ‘) â‰¡ ğ‘„ğ‘‘ğ‘—
ğ‘– (ğ‘ğ‘¡) , ğ‘– = 1,2, â€¦ , ğ‘] â© ğ‘‘ğ‘— âˆˆ ğ· 
  2: Initialise the action-value function for the BS for the actions of     
      the ğ‘—th DUE on the ğ‘–th RB   
[ğ‘„ğ‘ğ‘–(ğ‘) = 0|ğ‘„ğ‘ğ‘–(ğ‘) â‰¡ ğ‘„ğ‘ğ‘–
ğ‘— (ğ‘ğ‘¡), ğ‘— = 1,2, â€¦ , ğ‘€ ]    â© ğ‘ğ‘– âˆˆ ğ¶                 
  3:    for ğ‘‘ğ‘— âˆˆ ğ·  1 â‰¤ ğ‘— â‰¤ ğ‘€  do 
  4:        while not converge do  
  5: 
 generate a random number ğ‘¥ âˆˆ {0,1} 
  6:            if ğ‘¥ < ğœ€ then 
  7:                  Select action ğ‘ğ‘–
ğ‘¡ randomly 
  8:            else 
  9:                   Select action ğ‘ğ‘–
ğ‘¡=argmax
ğ‘âˆˆğ´
ğ‘„ğ‘‘ğ‘—(ğ‘ğ‘¡) 
10:            end 
11:            Evaluate ğœ‰ğ‘‘ğ‘—, Î“ğ‘‘ğ‘— and ğ‘™ğ‘‘ğ‘— of ğ‘‘ğ‘— âˆˆ ğ· for the action ğ‘ğ‘¡ 
12:            Measure the SINR, ğœ‰ğ‘ğ‘–, of CUE ğ‘ğ‘– âˆˆ ğ¶ for the action 
                ğ‘ğ‘¡ taken by ğ‘‘ğ‘— âˆˆ ğ· 
13: 
 Observe immediate reward of ğ‘‘ğ‘— âˆˆ ğ· and ğ‘ğ‘– âˆˆ ğ¶,   
                  
14:            Update action-value for action of  ğ‘‘ğ‘— âˆˆ ğ· on the ğ‘–th 
     RB ğ‘„ğ‘‘ğ‘—
ğ‘– (ğ‘) = ğ‘„ğ‘‘ğ‘—
ğ‘– (ğ‘) + ğœ [ğ‘Ÿğ‘‘ğ‘—(ğ‘ğ‘¡) + ğ‘„ğ‘‘ğ‘—
ğ‘– (ğ‘)] 
15:            Update action-value for ğ‘ğ‘– âˆˆ ğ¶ for action ğ‘ğ‘¡ of ğ‘—th  
                 DUE  ğ‘„ğ‘ğ‘–
ğ‘— (ğ‘) = ğ‘„ğ‘ğ‘–
ğ‘— (ğ‘) + ğœ[ğ‘Ÿğ‘ğ‘–(ğ‘ğ‘¡) + ğ‘„ğ‘ğ‘–
ğ‘— (ğ‘)]  
16:          end while 
17:    end for 
18: Load ğ‘„ğ‘‘ğ‘—(ğ‘) to the BS          â© ğ‘‘ğ‘— âˆˆ ğ· 
19: for ğ‘‘ğ‘— âˆˆ ğ·  1 â‰¤ ğ‘— â‰¤ ğ‘€  do 
20:      Obtain ğ‘„(ğ‘) = {ğ‘„ğ‘‘ğ‘—
ğ‘– (ğ‘), ğ‘„ğ‘ğ‘–
ğ‘— (ğ‘)}  ğ‘– = 1,2, â€¦ , ğ‘ 
21:      ğ‘„Ì…(ğ‘) âŠ† ğ‘„(ğ‘)| {ğ‘„ğ‘‘ğ‘—
ğ‘– (ğ‘), ğ‘„ğ‘ğ‘–
ğ‘— (ğ‘)} âˆˆ â„+, where â„+   
            positive real number 
22:       ğ‘„TOT = ğ‘„ğ‘‘ğ‘—
ğ‘– (ğ‘) + ğ‘„ğ‘ğ‘–
ğ‘— (ğ‘)         â© ğ‘ âˆˆ ğ‘„Ì…(ğ‘) 
23: end for 
24: Set up a list for unmatched DUE ğ·ğ‘¢ =  {ğ‘‘ğ‘— : â© ğ‘‘ğ‘— âˆˆ ğ·ğ‘¢} 
25: while ğ·ğ‘¢ â‰  âˆ… do 
26:       Rank ğ·ğ‘¢ in increasing order of |0 ğ‘„Ì…(ğ‘)| 
27:       Start DUE ğ‘‘ğ‘— âˆˆ ğ·ğ‘¢: ğ‘„Ì…(ğ‘) â‰  âˆ… with the least | ğ‘„Ì…(ğ‘)| 
28:        ğ‘ğ‘–
âˆ— = max
ğ‘Ÿğ‘– âˆˆğ‘… ğ‘„TOT 
29:        ğ·ğ‘¢ = ğ·ğ‘¢ âˆ’ ğ‘‘ğ‘— 
30:        ğ‘„Ì…(ğ‘) = ğ‘„Ì…(ğ‘)\ğ‘ğ‘–
âˆ—  
     â© ğ‘‘ğ‘—â€² âˆˆ ğ·ğ‘¢| ğ‘—â€² â‰  ğ‘— 
31: end while 
 
20
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-956-0
AICT 2022 : The Eighteenth Advanced International Conference on Telecommunications

   There are a number of methods to select an action based 
on the current evaluation of the Q-value at every time slot ğ‘¡ 
using a policy denoted by ğ‘ğ‘‘ğ‘—
ğ‘¡ . These methods are used to 
balance exploration and exploitation [18]. Epsilon greedy 
(ğœ€-greedy) is one of the methods of choosing an optimal Q-
value.  
   The reward function is modelled such that it relies only on 
local observations and can be implemented in a distributive 
manner. The rewards of the ğ‘—th DUE and ğ‘–th CUE for taking 
an action ğ‘ğ‘–
ğ‘¡ is expressed in terms of the achievable 
throughput using the Shannon capacity formula. Therefore, 
the reward is directly related to the objective function of the 
optimisation problem. The following is a summary of the 
Base Station Assisted (BSA) Reinforcement Learning 
Algorithm: 
  The ğ‘—th DUE only gets a reward when all state variables 
are 1 (i.e., the minimum QoS demands are met) while ğ‘–th 
CUE gets a reward if its minimum SINR is satisfied at each 
time slot for the action taken by ğ‘—th DUE. From the reward 
function defined above, learning can be implemented 
independently in a decentralised manner such that each 
agent maintains a local Q-table. There is no information 
exchange relating to other agentsâ€™ actions or rewards and no 
cooperation is needed between the agents, which results in 
reduced signalling overheads and reduced complexity 
compared with a centralised Q-learning approach.  
IV. 
PERFORMANCE EVALUATION 
   The performance of the presented BSA scheme is verified 
by considering a single-cell network in an industrial 
scenario. The simulation setup and channel models are 
summarised in Tables I and II. The network dynamics are 
captured by generating the channel fading effects randomly. 
The throughput is the main metric used to evaluate the 
performances of the algorithms. The performances of BSA 
are compared with centralised optimisation and the game 
theoretic Deferred Acceptance (DA) schemes [9]. 
 
TABLE I. MAIN SIMULATION PARAMETERS [9]  
 
Parameter 
Value 
Carrier frequency, ğ‘“ğ‘ 
2GHz 
System bandwidth 
10MHz 
Number of resource blocks (RB), ğ¾ 
50 
RB bandwidth 
180 kHz 
Maximum CUE transmit power, Pğ‘ğ‘–,max 
23dBm 
Maximum DUE transmit power, Pğ‘‘ğ‘—,max 
13dBm 
D2D distance, ğ¿ğ‘‘ğ‘‡,ğ‘‘ğ‘… 
10m â‰¤ ğ¿ğ‘‘ğ‘‡,ğ‘‘ğ‘…
â‰¤ 20m 
CUE SINR Threshold, Î“ğ‘ğ‘–,min 
7 dB 
DUE SINR Threshold, Î“ğ‘‘ğ‘—,min 
3 dB 
Noise power density 
âˆ’174 dBm/Hz 
Number of CUEs, ğ‘ 
50 
Number of DUEs, ğ‘€ 
50 
Reliability for DUE, ğ‘ğ‘…0 
10âˆ’5 
Exploration rate, ğœ€ 
0.7 
Learning rate, ğœ 
0.9 
DUE Maximum Delay, ğ‘™ğ‘‘ğ‘—,max 
50ms 
DUE Message Size, ğµğ‘‘ğ‘— 
15kB 
 
TABLE II. CHANNEL MODEL FOR LINKS [9] 
 
Parameter 
In-factory         
DUE link 
UE-UE link 
BS-UE link 
Pathloss 
model 
ğŸ‘ğŸ”. ğŸ– ğ¥ğ¨ğ ğŸğŸ(ğ[ğ¦])
+ ğŸ‘ğŸ“. ğŸ– 
 ğŸ’ğŸ ğ¥ğ¨ğ ğŸğŸ(ğ[ğ¦])
+ ğŸğŸ– 
ğŸ‘ğŸ•. ğŸ” ğ¥ğ¨ğ ğŸğŸ(ğ[ğ¦])
+ ğŸğŸ“. ğŸ‘ 
Shadowing 
4dB 
6dB 
8dB 
Fast fading 
Rayleigh 
Fading 
Rayleigh 
Fading 
Rayleigh 
Fading 
 
  
 The throughput performance of matched DUEs as a 
function of the number of DUEs in the system ğ‘€, is shown 
in Fig. 1. It can be concluded that the sum throughput of the 
DUEs increases with the number of cellular users ğ‘€ for all 
the considered algorithms. As expected, the number of 
admitted DUEs increases with the introduction of new 
DUEs to the system but remains unchanged if a valid 
cellular resource-sharing partner cannot be found because 
the minimum QoS requirements are not satisfied.  
   The centralised optimisation and BSA approaches are 
comparable, while the DA method shows the least 
performance. The BS-A algorithm outperforms the DA 
algorithms by up to 9.69% increase in the DUE throughput 
performance. However, it is semi-distributive as the final 
resource allocation is implemented by the BS whereas the 
DA approach is decentralised (the channel selection is user-
centric with no BS intervention to achieve autonomy). 
Players can make their resource allocations choices to 
maximise their individual and ultimately achieve system 
stability.   
   The performance of the sum throughput of the matched 
UEs (that is valid pairings between CUEs and DUEs) with 
respect to the number of cellular users ğ‘€ is presented in Fig. 
2. The sum throughput increases with ğ‘€. The BS-A 
approach shows better performance at ğ‘€ â‰¤ 35 with up to 
12.05% increase in sum throughput compared to the 
centralised approach while the centralised approach 
performed better at ğ‘€ > 35 with up to 9.39% increase in 
throughput. The DA algorithm again shows the least 
performance with up to 11.29% decrease compared with the 
BS-A technique.  The effect of the outage probability of the 
ğ‘ğ‘…0, and delay threshold of the DUEs ğ‘™ğ‘‘ğ‘—,max  on the sum 
rate of the matched UEs for all algorithms is shown in Fig. 
3 and Fig. 4. The sum throughput of the matched UEs 
increases with ğ‘ğ‘…0 and ğ‘™ğ‘‘ğ‘—,max. This is because higher ğ‘ğ‘…0 
causes the interference from the CUEs to be more tolerable 
by the DUEs, therefore making potential CUE-DUE pairing 
possible. Similarly, higher ğ‘™ğ‘‘ğ‘—,max increases the sum 
throughput at fixed outage probability and payload since the 
delay requirement is less stringent. More DUEs are able to 
satisfy the delay constraint and the number of admitted 
DUEs are increased. 
 
V.  CONCLUSION AND FUTURE WORK 
   We presented a semi-distributed Base Station Assisted 
(BSA) scheme for Radio Resource Management (RRM) of 
a network with D2D and cellular users, targeting wireless 
industrial scenarios. The reinforcement learning based 
21
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-956-0
AICT 2022 : The Eighteenth Advanced International Conference on Telecommunications

approach relies on distributed training of the D2D agents. 
Subsequently, the look-up tables for the D2D agents are 
loaded to the base station for centralised channel allocation. 
Simulation results show that the throughput of the presented 
approach 
is 
comparable 
to 
traditional 
centralised 
optimisation and demonstrates an improved performance 
relative to the deferred acceptance (DA) scheme. The future 
work will focus on evaluating the trade-off between 
performance, complexity and signaling overheads for the 
BSA scheme relative to other techniques. 
 
 
Fig. 1.  Sum-rate of matched DUEs with varying number of DUEs,  ğ‘€ in 
the System, for ğ‘ = 50 
 
 
 
Fig. 2.  Sum Throughput of matched UEs as a function of the number of 
DUEs  ğ‘€, in the system, for ğ‘ = 50   
 
 
 
Fig. 3.  Effect of the DUE outage ratio ğ‘ğ‘…0, on the sum throughput for 
ğ‘ = ğ‘€ = 50, ğ‘™ğ‘‘ğ‘—,max = 50ms 
 
 
Fig. 4.   Effect of the delay bound, ğ‘™ğ‘‘ğ‘—,max on the sum throughput of 
matched CUE-DUE pair for ğ‘ = ğ‘€ = 50, ğ‘ğ‘…0 = 10âˆ’5 
 
 
REFERENCES 
[1] J. Wan et al., â€œToward dynamic resources management for 
IoT-based manufacturing,â€ IEEE Communications Magazine, 
vol. 56, no. 2, pp. 52-59, Feb. 2018. 
[2] O.N. Yilmaz et al., â€œAnalysis of ultra-reliable and low-latency 
5G communication for a factory automation use case,â€ 
in Proc. of 2015 IEEE International Conference on 
Communication Workshop (ICCW), pp. 1190-1195, Sep. 
2015. 
[3] N. Brahmi, O. N. Yilmaz, K. W. Helmersson, S. A. Ashraf 
and J. Torsner, â€œDeployment strategies for ultra-reliable and 
low-latency communication in factory automation,â€ in Proc. 
of 2015 IEEE Globecom Workshops (GC Wkshps), pp. 1-6, 
Feb. 2016. 
22
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-956-0
AICT 2022 : The Eighteenth Advanced International Conference on Telecommunications

[4] Z. Li and C. Guo, â€œMulti-agent deep reinforcement learning 
based 
spectrum 
allocation 
for 
D2D 
underlay 
communications,â€ 
IEEE 
Transactions 
on 
Vehicular 
Technology, vol. 69, no. 2, pp. 1828-1840, Dec. 2019. 
[5] A. Asheralieva and Y. Miyanaga, â€œAn autonomous learning-
based algorithm for joint channel and power level selection by 
D2D pairs in cellular networks,â€ IEEE transactions on 
communications, vol. 64, no. 9, pp. 3996-4012, Jul. 2016. 
[6] Z. Li and C. Guo, â€œMulti-agent deep reinforcement learning 
based 
spectrum 
allocation 
for 
D2D 
underlay 
communications,â€ 
IEEE 
Transactions 
on 
Vehicular 
Technology, pp. 1828-1840, Feb, 2020. 
[7] S. Nie, Z. Fan, M. Zhao, X. Gu and L. Zhang, â€œQ-learning 
based power control algorithm for D2D communication,â€ in 
Proc. of   IEEE 27th Annual International Symposium on 
Personal, Indoor, and Mobile Radio Communications 
(PIMRC), pp. 1-6, Sep. 2016. 
[8] K.  Zia et al., â€œA distributed multi-agent RL-based 
autonomous spectrum allocation scheme in D2D enabled 
multi-tier HetNetsâ€. IEEE Access, no.7, pp. 6733-6745, Jan. 
2019.  
[9] I.O. Sanusi, K.M. Nasr and K. Moessner, â€œRadio resource 
management approaches for reliable Device-to-Device (D2D) 
communication in wireless industrial applications,â€ IEEE 
Transactions of Cognitive Communication and Networking, 
vol. 7, no. 3, pp.905-916, Oct. 2021. 
[10] L. Liang, H. Ye and G.Y. Li, â€œSpectrum sharing in vehicular 
networks based on multi-agent reinforcement learning,â€ IEEE 
Journal on Selected Areas in Communications, vol. 37, no. 10, 
pp. 2282-2292, Aug. 2019. 
[11] A.T. Kasgari and W. Saad, â€œModel-free ultra-reliable low 
delay communication (URLLC): A deep reinforcement 
learning framework,â€ in Proc. IEEE International Conference 
on Communications (ICC), pp. 1-6, May 2019. 
[12] H. Wang and X. Chu, â€œDistance-constrained resource-sharing 
criteria for device-to-device communications underlaying 
cellular networks,â€ Electronics letters, vol. 48, no. 9, pp. 528-
530, Apr. 2012. 
[13] H. Yang, X. Xie and M. Kadoch, â€œIntelligent resource 
management based on reinforcement learning for ultra-
reliable and low-delay IoV communication networks,â€ IEEE 
Transactions on Vehicular Technology, vol. 68, no. 5, pp. 
4157-4169, Jan. 2019. 
[14] F.E. Souhir, A. Belghith and F. Zarai, â€œA reinforcement 
learning-based radio resource management algorithm for 
D2D-based V2V communication,â€ in Proc. 15th International 
Wireless Communications & Mobile Computing Conference 
(IWCMC), pp. 1367-1372, Jun. 2019. 
[15] Z. Li and C. Guo, â€œMulti-agent deep reinforcement learning 
based 
spectrum 
allocation 
for 
D2D 
underlay 
communications,â€ 
IEEE 
Transactions 
on 
Vehicular 
Technology, pp. 1828-1840, Feb. 2020. 
[16] S. Nie, Z. Fan, M. Zhao, X. Gu and L. Zhang, â€œQ-learning 
based power control algorithm for D2D communication,â€ in 
Proc. of  IEEE 27th Annual International Symposium on 
Personal, Indoor, and Mobile Radio Communications 
(PIMRC), Sep. 2016. 
[17] Y. Wei, Y. Qu, M. Zhao, L. Zhang and F.R. Yu, â€œResource 
allocation and power control policy for Device-to-Device 
communication 
using 
multi-Agent 
reinforcement 
learning,â€ Computers, Materials & Continua, vol. 63, no. 3, 
pp.1515-1532, May 2020. 
[18] J. Kim, J. Park, J. Noh and S. Cho, â€œAutonomous power 
allocation based on distributed deep learning for device-to-
device communication underlaying cellular network,â€ IEEE 
Access, Vol. 8, 107853-107864, Jun. 2020. 
 
23
Copyright (c) IARIA, 2022.     ISBN:  978-1-61208-956-0
AICT 2022 : The Eighteenth Advanced International Conference on Telecommunications

