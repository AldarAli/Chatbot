Performance of Spectral Amplitude Warp based WDFTC in a Noisy Phoneme
and Word Recognition Tasks
R. Muralishankar
PES Centre for Intelligent Systems
Dept. of Telecommunication Engineering,
PES Institute of Technology, Bangalore, India.
muralishankar@pes.edu
H. N. Shankar
PES Centre for Intelligent Systems
Dept. of Telecommunication Engineering,
PES Institute of Technology, Bangalore, India.
hnshankar@pes.edu
Abstract
In this paper, we investigate the noise robustness of three
features, namely, the warped discrete Fourier transform cep-
strum (WDFTC, [1]), perceptual minimum variance distor-
tionless response (PMVDR) and Mel-frequency cepstral co-
efﬁcients (MFCC). We generate WDFTC and PMVDR fea-
tures by all-pass based warping; we use spectral warp-
ing for MFCC. PMVDR and WDFTC use warped-LP and
warped discrete Fourier transforms, respectively. We employ
WDFTC, PMVDR and MFCC features in continuous noisy
monophone and word recognition tasks using the TIMIT cor-
pus. We also test these features on gender-speciﬁc mono-
phone and word recognition tasks. Further, we employ spec-
tral amplitude warping (SAW) in WDFTC feature extraction
(WDFTC SAW) and demonstrate enhanced robustness of this
feature. We observe that SAW does not improve robustness
for the MFCC and PMVDR features. Finally, we report the
recognition performance and discuss many interesting prop-
erties of these features. Our study shows that the PMVDR
and WDFTC SAW achieve recognition performance superior
to the MFCC and WDFTC in noisy conditions.
Index Terms:Robustness, Speech recognition, Warped Dis-
crete Fourier Transform, Cepstrum, WDFTC, PMVDR,
Spectral Amplitude Warping, WDFTC SAW.
1. Introduction
Contemporary automatic speech recognition (ASR) sys-
tems perform satisfactorily when the test and training con-
ditions are close.
However, ASR performance degrades
rapidly in various conditions such as background acoustical
noise, stressed speech (e.g., lombard), channel conditions,
and speaker variability [2, 3]. With additive acoustical noise
the problem is as pragmatic as it is challenging. Interest-
ingly, humans do a far better job than ASR systems in noise,
thus pointing to scope for further improvement [2]. More-
over, rapid proliferation of mobile phones concomitant with
the large number of interactive voice applications being de-
veloped around them continues to present increasingly varied
and complex acoustical backgrounds to the ASR systems [4].
In general, there are three approaches towards address-
ing the problem of noise robustness in ASR. The ﬁrst incor-
porates a speech enhancement unit as a part of the feature
extraction process, thereby presenting clean features to the
ASR unit. Missing features approach [5], vector Taylor se-
ries approach [6] and spectral subtraction techniques [7] are
instances in point. The second approach involves compensat-
ing the trained ASR models using techniques such as Paral-
lel Model Combination (PMC) and dynamic Hidden Markov
Model (HMM) variance compensation [8]. The third aims
to develop new feature analysis methods which are relatively
robust to distortion such as relative spectra (RASTA) [9] or
cepstral mean subtraction (CMS). It is within the domain
of robust features that in a companion paper we developed
and introduced in the warped discrete cosine transform cep-
strum (WDCTC) [10]. There, we benchmarked the new fea-
ture against the popular Mel-frequency cepstral coefﬁcients
(MFCC) in terms of its statistical properties and performance
in simple recognition tasks [11]. A new feature representa-
tion called the Perceptual-MVDR (PMVDR) [12] has been
proposed by Yapanel et al. They compute cepstral coefﬁ-
cients from the speech signal. Warping is incorporated di-
rectly into the DFT power spectrum. A variant of this feature,
proposed in [13], uses warped-LP coefﬁcients in generating
warped-MVDR spectrum. The building block for all these
features is the warping adopted to transforms or to the LP
model.
In this paper, we propose a variant of MFCC, the warped
discrete Fourier transform cepstrum (WDFTC); spectral
warping is achieved using the warped discrete Fourier trans-
form (WDFT). We then employ spectral amplitude warp-
ing (SAW) in addition to the frequency warping to gener-
ate WDFTC, i.e., WDFTC SAW. We perform a compara-
tive analysis of the features derived from the warping with
the MFCC. The four features that we examine in our com-
97
International Journal On Advances in Systems and Measurements, vol 2 no 1, year 2009, http://www.iariajournals.org/systems_and_measurements/

0
1
2
3
4
5
6
7
8
0
5
10
15
Frequency (kHz)
Magnitude
Frequency response of Warped DFT Filters with β = 0.56 
0
1
2
3
4
5
6
7
8
0
5
10
15
Frequency (kHz)
Magnitude
Frequency response of Warped DFT Filters with β = −0.56 
(a)
(b)
Figure 1. 16-Band Warped Filter bank shown for fs/2. (a) β = 0.56 (b) β = −0.56
parative analysis are WDFTC SAW, WDFTC, PMVDR and
MFCC. We focus on studying their noise robustness prop-
erties. Particularly, we benchmark WDFTC SAW, WDFTC
and PMVDR against the MFCC in monophone and word
recognition using the TIMIT corpus without using language
models in monophone recognition and postprocessing of fea-
tures like CMS. Thus the recognition performance is due to
the features alone. We test the features in six different noise
conditions – babble, car, fan, factory, tank and F-16 cock-
pit noise – at signal-to-noise ratio (SNR) from 0 dB through
20 dB. We report simulations reﬂecting phoneme recognition
rates and recognition accuracies for monophone recognition.
In the second part of this work, we use Sphinx-III rec-
ognizer for word recognition task using TIMIT corpus. We
report word error rate (WER) of the four features under clean
and babble noise conditions. Finally, we highlight several in-
teresting observations on noise robustness properties of warp-
based features.
2. Warped-DFT Cepstrum
In this section, we brieﬂy review WDFTC with a dyad of
purposes in mind. First, to serve a didactic cause and second,
to facilitate unfolding of the notations used in the sequel.
Let
the
N-point
DFT
of
the
input
vec-
tor
[x(0), x(1), ..., x(N
−
1)]T
be
given
by
{X(0), X(1), ..., X(N − 1)}, where the frequency samples
of the z−transform of the sequence evaluated at uniformly-
spaced points z = e
j2πk
N , 0 ⩽ k ⩽ N − 1, on the unit circle
are
X(k) = X(z)|
z=e
j2πk
N
=
N−1
X
n=0
x(n) e
j2πkn
N
(1)
for k = 0, 1, ..., N − 1. For spectral analysis applications,
DFT provides a ﬁxed frequency resolution given by 2π/N
over [0, 2π]. WDFT proposed in [14] is the most general
form of DFT that can be employed to evaluate the frequency
samples arbitrarily at distinct points in the z-plane. If zk,
0 ⩽ k ⩽ N − 1, denote distinct frequency points in the z-
plane, the N-point WDFT of the length-N sequence is given
by
XWDF T (k) = X(zk) =
N−1
X
n=0
x(n)zk−n, 0 ⩽ k ⩽ N − 1.
(2)
Now, incorporating a nonlinear frequency resolution closely
following the psychoacoustic Bark scale, yields enhanced
representation for speech. Thus we warp DFT by an all-pass
transformation z−1 = A(z):
A(z) = −β + z−1
1 − βz−1 ,
(3)
where β controls warping. It may be useful to note that Smith
and Abel [15] have shown that for β = 0.56 warping closely
resembles psychoacoustic Bark scale for sampling at 16kHz.
We also use β = 0.56 in computing the perceptually moti-
vated speech spectrum. WDFT ﬁlters for length-16 is in Fig-
98
International Journal On Advances in Systems and Measurements, vol 2 no 1, year 2009, http://www.iariajournals.org/systems_and_measurements/

Time
Frequency
(b)
Spectrogram: Clean Speech
0.5
1
1.5
2
2.5
0
2000
4000
Time
Frequency
(a)
Spectrogram: Noisy Speech
0.5
1
1.5
2
2.5
0
2000
4000
Frequency
(c)
PMVDR Log Spectrum: Clean Speech
50
100
150
200
250
100
200
300
400
500
Frequency
(d)
PMVDR Log Spectrum: Noisy Speech
50
100
150
200
250
100
200
300
400
500
Frequency
(e)
WDFTC Log Spectrum: Clean Speech
50
100
150
200
250
100
200
300
400
500
Frequency
(f)
WDFTC Log Spectrum: Noisy Speech
50
100
150
200
250
100
200
300
400
500
time (frame number)
Frequency
(g)
MFCC Log Spectrum: Clean Speech
50
100
150
200
250
100
200
300
400
500
time (frame number)
Frequency
(h)
MFCC Log Spectrum: Noisy Speech
50
100
150
200
250
100
200
300
400
500
Figure 2. Illustrating the impact of 5 dB babble noise on PMVDR, WDFTC and MFCC Log spectra: (a) Spectrogram of clean
speech, (b) Spectrogram of noise corrupted speech (a), (c) PMVDR log spectrum of clean speech (a), (d) PMVDR log spectrum
of noise corrupted speech (a), (e) WDFTC log spectrum of clean speech (a), and (f) WDFTC log spectrum of noise corrupted
speech (a, (g) MFCC log spectrum of clean speech (a), and (h) MFCC log spectrum of noise corrupted speech (a).
ure 1. Details of the implementation of WDFT are in [14].
The WDFTC algorithm is outlined in Algorithm 1.
3
MVDR Spectral Envelope Estimation
MVDR spectral estimation has been explored for speech
parameterization [16, 17, 18]. Here, we present only the com-
putational algorithm and general properties of MVDR and
perceptual MVDR (PMVDR). In the MVDR spectrum esti-
mation method, the power spectral density at ωl is determined
by ﬁltering the signal by a distortionless FIR ﬁlter, h(n), de-
signed to minimize the output power while constraining the
ﬁlter gain to unity at the frequency of interest, ωl. This pro-
vides a lower bias with a smaller ﬁlter length. The parametric
Algorithm 1 Algorithm to compute the WDFTC
1: Obtain an N-point WDFT, XWDF T (k), 0 ≤ k ≤ N − 1
for a ﬁnite duration, real sequence x(n), 0 ≤ n ≤ N −1.
2: Compute ζ(k) of the WDFT coefﬁcients:
ζ(k) = |XWDF T (k)|,
(4)
where |.| evaluates the absolute value.
3: Compute the WDFTC bx(n) as
bx(n) = (IDCT (ln(ζ(k))).
(5)
99
International Journal On Advances in Systems and Measurements, vol 2 no 1, year 2009, http://www.iariajournals.org/systems_and_measurements/

0
5
10
15
20
Clean
0
20
40
60
80
SNR (dB)
PRR, PRA (%)
Babble Noise
 
 
0
5
10
15
20
Clean
0
20
40
60
80
SNR (dB)
PRR, PRA (%)
F−16 Cockpit Noise
 
 
0
5
10
15
20
Clean
0
20
40
60
80
SNR (dB)
PRR, PRA (%)
Tank Noise
 
 
0
5
10
15
20
Clean
0
20
40
60
80
SNR (dB)
PRR, PRA (%)
Fan Noise
 
 
0
5
10
15
20
Clean
0
20
40
60
80
SNR (dB)
PRR, PRA(%)
Car Noise
 
 
0
5
10
15
20
Clean
0
20
40
60
80
SNR (dB)
PRR, PRA (%)
Factory Noise
 
 
MFCC (PRR)
MFCC (PRA)
PMVDR (PRR)
PMVDR (PRA)
WDFTC (PRR)
WDFTC (PRA)
(f)
(a)
(d)
(e)
(b)
(c)
Figure 3. Illustrating the impact of different noises at various SNRs on MFCC, WDFTC and PMVDR phoneme recognition rate
(PRR) and phoneme recognition accuracy (PRA): (a) babble, (b) tank, (c) car, (d) F-16 cockpit, (e) fan, and (f) factory noises.
Acoustic Models trained on the entire TIMIT corpus using diagonal (DC) covariance.
form of the Mth order MVDR spectrum is given by
PMV (ω) =
1
PM
k=−M µ(k)e−jωk =
1
|B(ejω)|2 .
(6)
The MVDR coefﬁcients, µ(k), are obtained from a non-
iterative computation using the LP coefﬁcients ak and pre-
diction error variance, Pe.
µ(k) =



1
Pe
PM−k
i=0
Laia∗
i+k,
k = 0, ..., M
µ∗(−k),
k = −M, ..., −1
(7)
where L = (M + 1 − k − 2i). We compute the MVDR
envelope using LP coefﬁcients of order M and the prediction
error power, ϵM, as
SMV DR(ejω) =
ϵM
PM
k=−M µ(k)e−jωk .
(8)
4. PMVDR Feature Extraction
MVDR spectrum exhibits useful properties such as low
variance, low distortion and good spectral envelope match-
ing across a wide range of pitch frequencies. Therefore it is
widely considered as a robust speech parameterization tech-
nique in speech recognition. MVDR has been used in spectral
estimation [17] and in envelope estimation [18]. A natural ex-
tension to the MVDR scheme is the incorporation of the per-
ceptually motivated mel frequency into the otherwise linear
frequency scale. In [18], perceptual information was incor-
100
International Journal On Advances in Systems and Measurements, vol 2 no 1, year 2009, http://www.iariajournals.org/systems_and_measurements/

0
5
10
15
20
Clean
0
20
40
60
80
SNR (dB)
PRR, PRA (%)
Babble Noise
0
5
10
15
20
Clean
0
20
40
60
80
SNR (dB)
PRR, PRA (%)
F−16 Cockpit Noise
0
5
10
15
20
Clean
0
20
40
60
80
SNR (dB)
PRR, PRA (%)
Tank Noise
0
5
10
15
20
Clean
0
20
40
60
80
SNR (dB)
PRR, PRA (%)
Fan Noise
0
5
10
15
20
Clean
0
20
40
60
80
SNR (dB)
PRR, PRA (%)
Car Noise
 
 
0
5
10
15
20
Clean
0
20
40
60
80
SNR (dB)
PRR, PRA (%)
Factory Noise
MFCC (PRR)
MFCC (PRA)
PMVDR (PRR)
PMVDR (PRA)
WDFTC (PRR)
WDFTC (PRA)
(a)
(b)
(c)
(d)
(e)
(f)
Figure 4. Illustrating the impact of different noises at various SNRs on MFCC, WDFTC and PMVDR phoneme recognition rate
(PRR) and phoneme recognition accuracy (PRA): (a) babble, (b) tank, (c) car, (d) F-16 cockpit, (e) fan, and (f) factory noises.
Acoustic Models trained on speech samples from a male speaker in the TIMIT corpus using diagonal (DC) covariance.
porated directly into spectral estimation by using mel-scaled
ﬁlter banks. It was easily seen that the ﬁlter bank structure
is only a rough approximation to the perceptual scale since
it samples the perceptual spectrum at the center frequencies
of the ﬁlter bank. Furthermore, the ﬁlter bank is less effec-
tive in completely removing the harmonic excitation infor-
mation from the spectrum. Alternatively, the use of warping
techniques is also popular in contemporary literature, e.g.,
incorporating warping directly into the DFT power spectrum
[12], or the use of warped-LP coefﬁcients in generating the
warped-MVDR spectrum [13]. Presently, we generate the
PMVDR features using the warped-LP coefﬁcients.
5. Spectral Amplitude Warping (SAW)
Features discussed so far employ frequency warping with
improved resolution in the lower frequency band of the power
spectrum. SNR in this band is often higher than at high fre-
quencies. The result of nonuniform resolution provided by
the frequency warping in turn helps in improved phoneme
recognition performance speciﬁcally under noisy conditions.
It is well known that the acoustic models generated by clean
speech performs poorly under mismatched conditions and in
a simulation the same models trained on mismatched condi-
tions perform superior to the former case. Generating such
an acoustic model is possible only for a few simulated mis-
matched conditions. It is useful here to incorporate the gen-
eral effect of noise (such as reduced peak to valley differ-
101
International Journal On Advances in Systems and Measurements, vol 2 no 1, year 2009, http://www.iariajournals.org/systems_and_measurements/

0
5
10
15
20
Clean
0
20
40
60
80
SNR (dB)
PRR, PRA (%)
Babble Noise
0
5
10
15
20
Clean
0
20
40
60
80
SNR (dB)
PRR, PRA (%)
F−16 Noise
0
5
10
15
20
Clean
0
20
40
60
80
SNR (dB)
PRR, PRA (%)
Tank Noise
0
5
10
15
20
Clean
0
20
40
60
80
SNR (dB)
PRR and PRA (%)
Fan Noise
0
5
10
15
20
Clean
0
20
40
60
80
SNR (dB)
PRR, PRA (%)
Car Noise
 
 
0
5
10
15
20
Clean
0
20
40
60
80
SNR (dB)
PRR, PRA (%)
Factory Noise
MFCC (PRR)
MFCC (PRA)
PMVDR (PRR)
PMVDR (PRA)
WDFTC (PRR)
WDFTC (PRA)
(b)
(a)
(c)
(d)
(e)
(f)
Figure 5. Illustrating the impact of different noises at various SNRs on MFCC, WDFTC and PMVDR phoneme recognition rate
(PRR) and phoneme recognition accuracy (PRA): (a) babble, (b) tank, (c) car, (d) F-16 cockpit, (e) fan, and (f) factory noises.
Acoustic Models trained on speech samples from female speaker in the TIMIT corpus using diagonal (DC) covariance.
ences and change in the formant positions) on speech spec-
trum used in front-end feature extraction.
In this paper, we model reduction in peak-to-valley dif-
ference using Spectral amplitude warping (SAW). SAW has
been employed to shape coding noise in speech and audio
coders [19] in pre- and post-processing blocks to provide
a non-linear transformations to the signal short-time spec-
trum before and after encoding. Adopting SAW improves the
noise shaping capability of an existing coder without modify-
ing the coder itself. It is reported [19] that the output quality
of G.722 wideband speech coder operating at 48 kbps is close
to the same coder operating at 64 kbps.
Consider
Xw(k) = fnl(X(k)),
(9)
where X(k) and Xw(k) are the DFT spectra of signals x(n)
and xw(n) respectively, and fnl(·) is nonlinear. In [19],
Xw(k) = X(k)|X(k)|α(k)
|X(k)|
,
(10)
where α(k). There, α(k) = 0.5 ∀k. This reduces the dy-
namic range of |Xw(k)|. The attenuation is relatively higher
for larger |X(k)|. In a nut shell, the effects of this transfor-
mation are (a) attenuation of the formants; and (b) attenua-
tion of the harmonics. The disparity between peak and val-
leys is reduced both at the formant and harmonic levels. We
use this transformation to generate front-end features from
the transformed spectra so as to distort the speech spectra
equivalent to the effect of noise on it. Consequently acous-
tic models generated from these spectra perform better than
those from clean speech spectra. In our monophone and con-
102
International Journal On Advances in Systems and Measurements, vol 2 no 1, year 2009, http://www.iariajournals.org/systems_and_measurements/

0
5
10
15
20
Clean
0
20
40
60
80
SNR (dB)
PRR, PRA (%)
Babble Noise
0
5
10
15
20
Clean
0
20
40
60
80
SNR (dB)
PRR, PRA (%)
F16−Cockpit Noise
0
5
10
15
20
Clean
0
20
40
60
80
SNR (dB)
PRR, PRA (%)
Tank Noise
0
5
10
15
20
Clean
0
20
40
60
80
SNR (dB)
PRR, PRA (%)
Fan Noise
0
5
10
15
20
Clean
0
20
40
60
80
SNR (dB)
PRR, PRA (%)
Car Noise
0
5
10
15
20
Clean
0
20
40
60
80
SNR (dB)
PRR, PRA (%)
Factory Noise
 
 
WDFTC_SAW (PRR)
 
WDFTC_SAW (PRA)
 
WDFTC (PRR)
WDFTC (PRA)
(b)
(a)
(d)
(e)
(f)
(c)
Figure 6. Illustrating the impact of different noises at various SNRs on WDFTC and WDFTC SAW phoneme recognition rate
(PRR) and phoneme recognition accuracy (PRA): (a) babble, (b) tank, (c) car, (d) F-16 cockpit, (e) fan, and (f) factory noises.
Acoustic Models trained on the entire TIMIT corpus using diagonal (DC) covariance.
tinuous speech recognition we adopt this transformation with
α(k) = 0.5.
6. The Monophone Recognition Setup
We use GMM-HMM where the front-end features are the
WDFTC or PMVDR or MFCC. We train and test the HMM
recognizer using the HTK Toolkit [20], and the entire TIMIT
corpus of 6300 sentences recorded from 630 speakers. We
train phonetic HMMs using speech from the TIMIT train
set with 462 speakers and test on the TIMIT test set with
168 speakers. It is common that the 61 TIMIT phonemes
are mapped to a reduced set of 39 phonemes after training
and testing, and the results are reported on this reduced set
[21]. We train the HMMs using diagonal covariances (DCs).
We use a 3-state HMM model for each of the 39 phonemes
and for each state, a mixture splitting procedure under the
DC setup [20]. The mixture splitting procedure starts with
one mixture per state and it goes up to 8 mixtures in four
steps with a re-estimation algorithm in each step. Finally, we
present monophone performance under a DC setup for clean
and noisy cases.
All the speech ﬁles are sampled at 16 kHz and pre-
emphasized with 1 − 0.97z−1.
They are then Hamming
windowed. Speech signal is analyzed every 10 ms with a
frame width of 25 ms. We generate 13-dimensional WDFTC,
PMVDR and MFCC features from each speech frame includ-
ing the zeroth coefﬁcient. The WDFTC and MFCC are gen-
erated using the Algorithm 1 and a mel-scale triangular ﬁlter
bank with 24 ﬁlter bank channels respectively. We append a
103
International Journal On Advances in Systems and Measurements, vol 2 no 1, year 2009, http://www.iariajournals.org/systems_and_measurements/

0
5
10
15
20
Clean
0
20
40
60
80
SNR (dB)
PRR, PRA (%)
Babble Noise
0
5
10
15
20
Clean
0
20
40
60
80
SNR (dB)
PRR, PRA (%)
F16−Cockpit Noise
0
5
10
15
20
Clean
0
20
40
60
80
SNR (dB)
PRR, PRA (%)
Tank Noise
0
5
10
15
20
Clean
0
20
40
60
80
SNR (dB)
PRR, PRA (%)
Fan Noise
0
5
10
15
20
Clean
0
20
40
60
80
SNR (dB)
PRR, PRA (%)
Car Noise
 
 
0
5
10
15
20
Clean
0
20
40
60
80
SNR (dB)
PRR, PRA (%)
Factory Noise
WDFTC_SAW (PRR)
WDFTC_SAW (PRA)
WDFTC (PRR)
WDFTC (PRA)
(a)
(c)
(b)
(f)
(e)
(d)
Figure 7. Illustrating the impact of different noises at various SNRs on WDFTC and WDFTC SAW phoneme recognition rate
(PRR) and phoneme recognition accuracy (PRA): (a) babble, (b) tank, (c) car, (d) F-16 cockpit, (e) fan, and (f) factory noises.
Acoustic Models trained on speech samples from a male speaker in the TIMIT corpus using diagonal (DC) covariance.
26-dimensional delta and delta-delta cepstral features to 13-
dimensional WDFTC and MFCC. We then employ the pro-
cedure in [22] to obtain delta and delta-delta for MFCC. We
may employ dynamic spectral parameters [23] to compute
delta and delta-delta for PMVDR and WDFTC.
7. The Word Recognition Problem
We employ the Sphinx-III speech recognizer [24]and
the TIMIT speech database to evaluate the WDFTC SAW
front-end parameters uunlike traditional feature like MFCC,
WDFTC and PMVDR. The utterances of the words in the
transcriptions of the database are generated by the lexicon
provided with the database. We use a phoneme set of 43
symbols including silence.
7.1
Generating Acoustic Models
For each of the above described features we train one
acoustic model in two stages – encoding and decoding.
7.1.1
Encoding
Using the transcriptions in the database, the acoustic mod-
els are force-aligned against the transcription of the training
data; thus pronunciations of the words with multiple pho-
netic representations are extracted. New acoustic models are
synthesized by tieing the existing models following the same
procedure. As before, the feature vectors are 13-D. We pre-
emphasize the signal in the time domain with a factor of 0.97.
The resulting speech waveforms are segmented into 25 ms
frames with a step of 10 ms. The ﬁrst and second deriva-
104
International Journal On Advances in Systems and Measurements, vol 2 no 1, year 2009, http://www.iariajournals.org/systems_and_measurements/

0
5
10
15
20
Clean
0
20
40
60
80
SNR (dB)
PRR, PRA (%)
Babble Noise
0
5
10
15
20
Clean
0
20
40
60
80
SNR (dB)
PRR, PRA (%)
F16−Cockpit Noise
0
5
10
15
20
Clean
0
20
40
60
80
SNR (dB)
PRR, PRA (%)
Tank Noise
0
5
10
15
20
Clean
0
20
40
60
80
SNR (dB)
PRR, PRA (%)
Fan Noise
0
5
10
15
20
Clean
0
20
40
60
80
SNR (dB)
PRR, PRA (%)
Car Noise
0
5
10
15
20
Clean
0
20
40
60
80
SNR (dB)
PRR, PRA (%)
Factory Noise
 
 
WDFTC_SAW (PRR)
WDFTC_SAW (PRA)
WDFTC (PRR)
WDFTC (PRA)
(a)
(b)
(c)
(d)
(e)
(f)
Figure 8. Illustrating the impact of different noises at various SNRs on WDFTC and WDFTC SAW phoneme recognition rate
(PRR) and phoneme recognition accuracy (PRA): (a) babble, (b) tank, (c) car, (d) F-16 cockpit, (e) fan, and (f) factory noises.
Acoustic Models trained on speech samples from female speaker in the TIMIT corpus using diagonal (DC) covariance.
tive coefﬁcients are appended to the static features in 13-D,
ﬁnally resulting in feature vectors in 39-D.
Using the 39-D feature vectors so extracted we build con-
text independent (CI) phone models for each of the 42 mono-
phones on a 3-state Bakis-topology HMMs [25] with a non-
emitting terminating state. CI phone models are trained by
the Baum-Welch algorithm. Next, context-dependent (CD)
untied triphone models are trained for every triphone that oc-
curs at least 8 times in the training data. The CI model pa-
rameters initialize the parameters of the CD models. The CD
models are now trained as above through the Baum-Welch al-
gorithm. Decision trees determining similar HMM states of
all untied models are built in order to be merge the common
states or senones. In all, 1000 senones are trained. Decision
trees were pruned to restrict the number of leaves to within
a prespeciﬁed number of tied states. Every state of all the
HMMs was modelled by a mixture of 16 Gaussians.
7.1.2
Decoding
We employ the Sphinx-III decoder. Throughout the recog-
nition process the most probable sequence of words is con-
sidered as the recognized one. This result depends on two
factors, namely, the acoustic score that the HMM models pro-
vide and the probability of the existence of the sequence of
words called language weight. We use a 3-gram language
model [26]. The training corpus of the language model in-
cluded all the transcriptions of the database.
105
International Journal On Advances in Systems and Measurements, vol 2 no 1, year 2009, http://www.iariajournals.org/systems_and_measurements/

0
5
10
15
20
Clean
0
10
20
30
40
50
60
70
80
90
100
SNR (dB)
Word Error Rate %
 
 
PMVDR
MFCC
WDFTC
WDFTC SAW
Figure 9. Word Error Rate (WER) of MFCC, PMVDR,
WDFTC and WDFTC SAW features under noise-free and
Babble noise conditions
Table 1. Monophone Performance of MFCC, PMVDR,
WDFTC and WDFTC SAW on gender independent and
speciﬁc speech samples from the clean TIMIT Corpus
Gender
All
Male
Female
Feature
PRR
PRA
PRR
PRA
PRR
PRA
MFCC
73.9
64.8
75.8
65.3
76.2
67.8
PMVDR
72.6
63.3
75.0
65.3
74.6
65.6
WDFTC
72.2
62.6
74.9
65.9
74.3
65.3
WDFTC SAW
70.0
59.5
72.5
62.8
72.5
62.9
8. Results and Discussions
Figure 2 shows the spectrograms of a TIMIT sentence
(She had your dark suit in greasy wash water all year) and
spectrograms of cepstral features from the PMVDR, WDFTC
and MFCC in Fig. 2(a),(c),(e) and (g) respectively. Cor-
respondingly, their noisy versions with 5 dB ‘babble’ noise
are in Fig. 2(b),(d),(f) and (h) respectively. Robustness of
WDFTC and PMVDR vis-a-vis MFCC is evident. Further,
PRR and PRA for the PMVDR, WDFTC and MFCC features
on the clean TIMIT corpus are outlined in Table 1 for gender
independent and speciﬁc samples from the entire dataset. It
can be seen from the tables that the MFCC performs slightly
better than both the PMVDR and WDFTC with a 1-2% mar-
gin on the PRR and PRA for the entire dataset.
It may be noted from Figs. 3, 4 and 5 that the car, fan
and tank noises are narrowband and stationary relative to the
factory, F-16 cockpit and babble noises. From Fig. 3, it is ob-
served that while the PMVDR and WDFTC exhibit a consis-
SER
SUB
DEL
INS
0
5
10
15
20
25
30
35
40
45
50
Error (%)
 
 
PMVDR
MFCC
WDFTC
WDFTC SAW
Figure 10. Sentence Error Rate (SER) and Errors in Word
Recognition Performance
tent degradation with falling SNR, their PRAs and PRRs per-
formances are better than the MFCC. Further, it can be seen
that the performance of PMVDR is marginally better than
that of WDFTC and appreciably better than that of MFCC.
The sensitivity of the acoustic models trained on MFCC in
noisy backgrounds is clearly obviated by this observation. In
comparison the PMVDR and WDFTC acoustic models seem
to degrade gracefully with falling SNRs. The DC models
of WDFTC and PMVDR are closer to each other in perfor-
mance. MFCC exhibit poor performance under DC condition
for most of the noise cases and SNRs. In the case of WDFTC
and PMVDR, the DC models seem to give a better PRA for
narrowband and broadband noise types, respectively. This
is especially true for the low values of SNR. We proceed to
test MFCC, PMVDR and WDFTC on gender-speciﬁc sam-
ples from the TIMIT database. We generate and test gender-
speciﬁc cases by employing DC models; the phoneme recog-
nition performance is shown in Figs. 4 and 5. We observe
that PMVDR and WDFTC outperform MFCC in all noisy
conditions except with F-16 cockpit noise.
We adopt WDFTC SAW as a front-end feature for mono-
phone and word recognition. Figure 6 presents the compar-
ative performance of the WDFTC and WDFTC SAW fea-
tures. We observe that WDFTC SAW outperforms WDFTC,
particularly, under low SNR conditions except the case for
car noise. Substantial improvement in PRR and PRA per-
formance can also be seen for fan noise. Results from the
gender-speciﬁc tests is shown in Figs. 7 and 8. Even in these
tests SAW has improved the overall performance of WDFTC
and the performance difference with the PMVDR is mini-
mal. However, this improved performance is at the cost of
marginal fall in noise-free condition.
106
International Journal On Advances in Systems and Measurements, vol 2 no 1, year 2009, http://www.iariajournals.org/systems_and_measurements/

In word recognition, we rank list word recognition per-
formance of these features with clean and babble noise for
SNR from 0 to 20 dB, vide Fig.9. It may be observed that
the MFCC has lower WER compared to other features under
clean conditions. However, PMVDR achieves lower WER
under babble noise conditions for SNRs from 0 to 20 dB.
The performance with WDFTC SAW feature is better than
WDFTC and MFCC. The performance difference between
WDFTC SAW and PMVDR is minimal.
Figure 10 shows sentence error rate (SER) and word
recognition errors such as Substitutions (SUB), deletions
(DEL) and Insertions (INS) from these four features under
noise-free conditions. We can observe that MFCC has lowest
SER, SUB, DEL and INS errors. PMVDR has heighest SER,
substitution and insertion errors. Finally, adopting SAW to
generate MFCC and PMVDR does not yield good results in
both monophone and word recognition tasks.
In general, it can be easily concluded on the basis of
all the results presented above that WDFTC SAW, WDFTC
and PMVDR outperform MFCC in different noise types and
SNRs. This may be attributed to an all-pass based spectral
warping in PMVDR, WDFTC and WDFTC SAW feature
generation. It has been shown [27] in addition that warp-
ing reduces the dynamic range of features and we believe
that it plays an important role in noise robustness of the fea-
tures. This is not very surprising as it is well known that
the low-frequency spectrum of speech contains more energy
than the high-frequency spectrum, and therefore it is more
robust to additive noise. The high frequency spectrum in
contrast is more susceptible to distortion by additive noise.
Naturally, we achieve noise robustness by adopting a warp-
ing scheme which boosts at low frequency and bucks at high
frequency. In other words, robustness is achieved by retain-
ing reliable spectral bands in speech spectrum and eliminat-
ing bands where SNR is poor. The non-unitary nature of the
WDFT which ampliﬁes the low frequency spectrum and at-
tenuates the high frequency part of the spectra also helps in
highlighting the signal and downplaying the impact of the
additive noise. It may be useful to note that the better per-
formance of the PMVDR, WDFTC and WDFTC SAW are
attributed to their noise robustness and low feature variance.
9. Conclusion
In this paper, we presented exhaustive results illustrating
the noise robust properties of the WDFTC, WDFTC SAW
and PMVDR features which have been benchmarked with
MFCC. We introduced SAW to enhance robustness of
WDFTC and demonatrate enhanced performance in noisy
conditions. MFCC and PMVDR however failed to match the
recognition results of these features alone. It was also shown
that unlike MFCC, the WDFTC SAW, WDFTC and PMVDR
models degrade gracefully with falling SNR. It is important
to note that we have not employed postprocessing of features
like cepstral mean subtraction and variance normalization.
Our experiments essentially shed light on some very useful
properties of MFCC, WDFTC, WDFTC SAW and PMVDR
features that may be useful in improving the performance of
current ASR systems in noise. Therein lies the take-home
message.
References
[1] R. Muralishankar and D. O’Shaughnessy,
“A com-
parative analysis of noise robust speech features ex-
tracted from all-pass based warping with mfcc in a
noisy phoneme recognition,”
in Proc. ICDT 2008,
Bucharest, Romania, July 2008, pp. 180–185.
[2] Douglas O’Shaughnessy, “Interacting with computers
with voice: automatic speech recognition and synthe-
sis,” Proc. of the IEEE, vol. 91, no. 9, pp. 1272–1305,
Sept. 2003.
[3] John H.L. Hansen and M.A. Clements, “Source genera-
tor equalization and enhancement of spectral properties
for robust speech recognition in noise and stress,” IEEE
Trans. on Speech & Audio Proc., vol. 3, no. 5, pp. 415–
421, Sep. 1995.
[4] I Varga, S Aalburg, B Andrassy, S Astrov, J.G. Bauer,
C Beaugeant, C Geissler, and H Hoge, “ASR in mo-
bile phones - an industrial approach,” IEEE Trans. on
Speech & Audio Proc., vol. 10, no. 8, pp. 562–569, Nov.
2002.
[5] Bhiksha Raj and R. M. Stern,
“Missing feature ap-
proaches in speech recognition,”
IEEE Signal Proc.
Mag., vol. 22, no. 5, pp. 101–116, Sept. 2005.
[6] Pedro J. Moreno, Bhiksha Raj, and Richard M. Stern,
“A vector taylor series approach for environment-
independent speech recognition,”
in ICASSP, May
1996, pp. 733–736.
[7] Guillaume Lathoud, Mathew Magimai.-Doss, Bertrand
Mesot, and Herve Bourland,
“Unsupervised spectral
subtraction for noise-robust ASR,” in ASRU, Dec. 2005,
pp. 343–348.
[8] Li Deng, Jasha Droppo, and Alex Acero, “Dynamic
compensation of HMM variances using the feature en-
hancement uncertainity computed from a parametric
model of speech distortion,” IEEE Trans. on Speech
& Audio Proc., vol. 13, no. 3, pp. 412–421, May 2005.
[9] H. Hermansky and N. Morgan, “RASTA processing of
speech,” IEEE Trans. on Speech & Audio Proc., vol. 2,
pp. 578–589, Oct. 1994.
107
International Journal On Advances in Systems and Measurements, vol 2 no 1, year 2009, http://www.iariajournals.org/systems_and_measurements/

[10] R. Muralishankar, A. Sangwan, and D. O’Shaughnessy,
“Warped Discrete Cosine Transform Cepstrum: A new
feature for speech processing,” in Proc. EUSIPCO, Sep.
2005.
[11] A. Sangwan, R. Muralishankar, and D. O’Shaughnessy,
“Performance analysis of the Warped Discrete Cosine
Transform Cepstrum with MFCC using different classi-
ﬁers,” MLSP, pp. 99–104, Sept. 2005.
[12] U. H. Yapanel and John. H. L. Hansen, “A new per-
ceptually motivated MVDR-based acoustic front-end
(PMVDR) for robust automatic speech recognition,”
Speech Communication, vol. 50, no. 2, pp. 142–152,
Feb. 2008.
[13] M. Wolfel, John. McDonough, and A. Waibel, “Warp-
ing and scaling of the minimum variance distortionless
response,” in ASRU, 2003, pp. 387–392.
[14] S. Bagchi and S. K. Mitra, Nonuniform Discrete Fourier
Transform and its Signal Processing Applications, Nor-
well, MA: Kluwer, 1999.
[15] J. O. Smith III and J. S. Abel, “Bark and ERB bilinear
transforms,” IEEE Trans. on Speech & Audio Proc., vol.
7, pp. 697–708, June 1999.
[16] M. N. Murthi and B. D. Rao, “All-pole modeling of
speech based on the minimum variance distortionless
response spectrum,” IEEE Trans. on Speech & Audio
Proc., vol. 8, no. 3, pp. 221–239, May 2000.
[17] S. Dharanipragada and B. D. Rao,
“MVDR-based
feature extraction for robust speech recognition,”
in
ICASSP, May 2001, vol. 1, pp. 309–312.
[18] U. H. Yapanel and S. Dharanipragada,
“Perceptual
MVDR-based cepstral coefﬁcients(PMCCs)for noise
robust speech recognition,”
in ICASSP, May 2003,
vol. 1, pp. 644–647.
[19] R. Lefebvre and C. Laﬂamme,
“Spectral amplitude
warping SAW for noise spectrum shaping in audio cod-
ing,” in Proc. ICASSP, Apr. 1997, vol. 1, pp. 335–338.
[20] S. J. Young, HTK Version 3.3: Reference Manual and
User Manual,
Cambridge Univ. Engg. Dept.-Speech
Grp., 2005.
[21] K. F. Lee and H. W. Hon, “Speaker-independent phone
recognition using hidden markov models,” IEEE Trans.
on Acoust. Speech & Signal Proc., vol. 37, no. 11, pp.
1641–1648, Nov. 1989.
[22] S. Furui, “Speaker-independent isolated word recogni-
tion using dynamic features of speech spectrum,” IEEE
Trans. on Acoust., Speech, Sig. Proc., vol. 34, no. 2, pp.
52–59, Feb. 1986.
[23] S. M. Ahadi, H. Sheikhzadeh, R. L. Brennan, G. H.
Freeman, and E. Chou, “On the use of dynamic spec-
tral parameters in speech recognition,” in Proc. ISSPIT,
Dec. 2003, pp. 757–760.
[24] K. F. Lee, H. W. Hon, and R. Reddy, “An overview of
the SPHINX speech recognition system,” IEEE Trans.
on Acoust. Speech & Signal Proc., vol. 38, no. 1, pp.
35–45, Jan. 1990.
[25] R. Bakis,
“Continuous speech word recognition via
centi-second acoustic states,” in Proc. of ASA Meeting
(Washington, DC), Apr. 1976.
[26] “The
CMU-Cambridge
statistical
lan-
guage
modelling
toolkit,
v2,”
Available:
http://www.speech.cs.cmu.edu/SLM/toolkit documenta
tion.html.
[27] R. Muralishankar, A. Sangwan, and D. O’Shaughnessy,
“Theoretical complex cepstrum of DCT and warped
DCT ﬁlters,” Proc. IEEE Signal Proc. Ltrs., vol. 14,
no. 5, pp. 367–370, May 2007.
108
International Journal On Advances in Systems and Measurements, vol 2 no 1, year 2009, http://www.iariajournals.org/systems_and_measurements/

