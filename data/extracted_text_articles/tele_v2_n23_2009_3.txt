88
International Journal on Advances in Telecommunications, vol 2 no 2&3, year 2009, http://www.iariajournals.org/telecommunications/
A FAMILY OF RECURSIVE LEAST-SQUARES ADAPTIVE 
ALGORITHMS SUITABLE FOR FIXED-POINT IMPLEMENTATION 
 
Constantin Paleologu, Silviu Ciochină, and Andrei Alexandru Enescu  
 
Telecommunications Department, University Politehnica of Bucharest, Romania 
e-mail: {pale, silviu, aenescu}@comm.pub.ro 
 
 
ABSTRACT 
 
The main feature of the least-squares adaptive algorithms is 
their high convergence rate. Unfortunately, they encounter 
numerical problems in finite precision implementation and 
especially in fixed-point arithmetic. The objective of this 
paper is twofold. First, an analysis of the finite precision 
effects of the recursive least-squares (RLS) algorithm is 
performed, outlining some specific problems that could 
appear in fixed-point implementation; consequently, we 
present a modified version of the RLS algorithm suitable 
for fixed-point implementation, using an asymptotically 
unbiased estimator for the algorithm’s cost. Second, we 
extend the procedure for the case of QR-decomposition-
based least-squares lattice (QRD-LSL) adaptive algorithm, 
a “fast” member of RLS family, with good numerical 
properties. The reduced dynamics of the algorithm’s 
parameters leads to facility for fixed-point implementation. 
The simulations performed on a fixed-point digital signal 
processor (DSP) sustain the theoretical findings. Also, as a 
practical aspect of this work, we illustrate the performance 
of the proposed QRD-LSL algorithm for noise reduction. 
 
Index 
Terms— 
Adaptive 
filters, 
fixed-point 
implementation, noise reduction, QR-decomposition-based 
least-squares lattice (QRD-LSL) algorithm, recursive least-
square (RLS) algorithm. 
 
1. INTRODUCTION 
 
The Recursive Least Squares (RLS) algorithm is one of the 
most popular adaptive algorithms, mainly due to its fast 
convergence rate [1]. Nevertheless, there are some major 
drawbacks related to the high computational complexity 
and the large dynamic range of the algorithm’s variables. 
The first issue could be overcome by using a fast RLS 
algorithm, in the meaning that the computational cost 
increases 
linearly 
with 
the 
number 
of 
adjustable 
parameters. The last drawback is more severe and could 
cause unwanted effects in a fixed-point arithmetic context, 
such as overflow or stalling phenomena [2]. In this paper 
we focus on some numerical problems of the RLS 
algorithm and present a modified version of this algorithm, 
which is more suitable for fixed-point implementation. For 
practical reasons, the proposed procedure is applied to the 
QR-decomposition-based least-squares lattice (QRD-LSL) 
algorithm, which is a fast member of the RLS family with 
robust numerical behavior. 
The QRD-LSL algorithm [3] combines the good 
numerical properties of QR-decomposition and the 
desirable features of a recursive least-squares lattice. 
Whereas the recursive QR-decomposition-based recursive 
least-squares (QRD-RLS) algorithm [1] requires a high 
computational load on the order of L2 (where L is the filter 
order), in terms of both the number of processing cells and 
the 
computation 
per 
iteration, 
the 
QRD-LSL 
implementation is fast in the sense that these numbers are 
reduced to a linear dependence on L. This algorithm 
exploits the shifting property of serialized input data (the 
Toeplitz structure of the data matrix) to perform joint-
process estimation in a fast manner. By virtue of this facts, 
the QRD-LSL algorithm is endowed with a highly desirable 
set of operational and implementation characteristics such 
as good numerical properties (inherited from QR-
decomposition), good convergence properties (due to the 
RLS nature), and a high level of computational efficiency 
(resulted from the modular, lattice-like structure). The 
combination of these characteristics makes the QRD-LSL a 
powerful adaptive algorithm, suitable for a wide range of 
applications, e.g., echo cancellation, interference rejection, 
or noise reduction [4]–[10]. 
Another implication of the modular structure of the 
QRD-LSL algorithm is that it lends itself to the use of very 
large-scale integration (VLSI) technology for its hardware 
implementation. Of course, the use of this sophisticated 
technology can be justified only if the application of 
interest calls for the use of VLSI chips in large number. 
Otherwise, a digital signal processor (DSP) implementation 
represents a proper solution. In this case, an important 
practical aspect is related to the dynamic range of the 
algorithm’s parameters. It is known that in two-
complement 
fixed-point 
implementation 
context 
the 
This work was supported by the UEFISCSU Romania under Grants PN-
II-“Idei” no. 65/01.10.2007 and no. 331/01.10.2007. 

89
International Journal on Advances in Telecommunications, vol 2 no 2&3, year 2009, http://www.iariajournals.org/telecommunications/
absolute values of all involved parameters have to be 
smaller than 1. In the case of the classical QRD-LSL 
algorithm the cost functions asymptotically increase; 
theoretically, they are upper bounded by 1/ (1
− λ)
, where 
λ  is the exponential weighting factor ( 0
1
< λ
≤ ) [1]. 
When dealing with a value of λ very close to 1 (which is 
the case in most of the applications due to stability reasons 
[11]), very large values of the cost functions are expected. 
In order to prevent any unwanted overflow phenomenon it 
is necessary to scale the cost function. As a consequence, 
the major drawback is the precision loss because of these 
factors. 
In the first part of this work we analyze the behavior of 
the RLS algorithm in fixed-point arithmetic, revealing 
some specific problems that could appear in this context. In 
order to overcome these potential issues we present a 
modified version of the RLS algorithm that is suitable for 
fixed-point implementation. The main idea is to use an 
asymptotically unbiased estimator for the algorithm’s cost 
function, in order to reduce the dynamic range of this 
parameter. The idea can be applied to other RLS-based 
algorithms. In this paper we extend the procedure for the 
case of QRD-LSL algorithm. Consequently, a modified 
version of this algorithm is obtained. The reduced dynamic 
of the parameters leads to facilities for fixed-point 
implementation. 
The paper is organized as follows. Section 2 contains 
certain backgrounds of the classical RLS algorithm, 
outlining several specific problems that could appear in 
fixed-point implementation. In Section 3 we establish a 
connection between the dynamic range of variables and the 
initial convergence rate, and we present a modified version 
of 
the 
RLS 
algorithm 
suitable 
for 
fixed-point 
implementation. The modified version of the QRD-LSL 
algorithm is developed in Section 4. The simulation results 
are presented in Section 5. A summarized discussion of the 
main results is given in Section 6. Finally, Section 7 briefly 
concludes this work. 
 
2. RLS ALGORITHM BACKGROUND AND 
ANALYSIS 
 
The well-known RLS adaptive algorithm [1] uses a cost 
function defined as an estimate of the mean square-error, 
i.e., 
 
  
( )
( )
(
)
( )
2
2
1
1
n
n i
i
J n
e i
J n
e
λ
λ
−
=
=
=
−
+
∑
n
1
,     (1) 
 
where 0
< λ
≤  is the exponential weighting factor and 
e(i) is the difference between the desired response d(i) and 
the output y(i) produced by an adaptive transversal filter. 
That is, 
 
( )
( )
( )
( )
H ( ) ( )
e i
d i
y i
d i
n
i
=
−
=
− w
x
, 
    (2) 
 
where  x(i)  is  the tap-input vector at time i and w(n) is the 
tap-weight vector at time n. The superscript H denotes 
Hermitian 
transposition 
(transposition 
and 
complex 
conjugation). 
This estimate of the cost function induces similar 
estimates for the correlation matrix Φ(n) and the cross-
correlation vector θ(n), i.e., 
 
    
,  (3) 
( )
( )
( )
(
)
( )
( )
1
1
n
n i
H
H
i
n
i
i
n
n
λ
λ
−
=
=
=
−
+
∑
Φ
x
x
Φ
x
x
n
n
 
       
,   (4) 
( )
( )
( )
(
)
( )
( )
*
*
1
1
n
n i
i
n
i d
i
n
n d
λ
λ
−
=
=
=
−
+
∑
θ
x
θ
x
 
where superscript * denotes complex conjugation. The 
optimum value of the tap-weight vector w(n), for which the 
cost function J(n) from (1) attains its minimum value is 
defined by the normal equation written in matrix form: 
 
( )
( )
(
n
n
n
=
Φ
w
θ
) . 
 
    (5) 
 
The regular procedure is to apply the matrix inversion 
lemma in (3) in order to solve (5). Denoting 
 
( )
1 ( )
n
n
−
=
P
Φ
  
 
    (6) 
 
and defining the Kalman vector 
 
( )
(
) ( )
( ) (
) ( )
1
1
1
1
1
H
n
n
n
n
n
n
λ
λ
−
=
⋅
+
−
P
x
k
x
P
x
 
    (7) 
 
the inverse of the estimate of the correlation matrix is 
computed in a recursive manner as [1] 
 
( )
(
)
( )
( ) (
)
1
1
1
1
H
n
n
n
n
n
λ
λ
=
−
−
−
P
P
k
x
P
.      (8) 
 
Finally, the recursive equation for updating the tap-weight 
vector is 
 
  
            
( )
(
)
( )
* ( )
1
n
n
n α
=
−
+
w
w
k
n ,              (9) 
 
where α(n) is the a priori estimation error defined by 
 
                         
( )
( )
(
1) ( )
H
n
d n
n
n
α
=
−
−
w
x
 
   (10) 
 
The initial value of P(n) is chosen 
 

90
International Journal on Advances in Telecommunications, vol 2 no 2&3, year 2009, http://www.iariajournals.org/telecommunications/
         
 
         ( )
1
0
= δ −
P
I , 
 
   (11) 
 
where δ is the regularization parameter (a positive constant) 
and I is the identity matrix. This initial value assures the 
non-singularity of the correlation matrix Φ(n). In the case 
of a stationary environment or a slowly time-varying one, 
the parameter δ should be assigned a small value for high 
signal-to-noise ratio (SNR) and a large value for low SNR 
[12]. 
Next, in order to analyze the behavior of the RLS 
algorithm in finite precision implementation, let us examine 
some of its main parameters. Following (1) and (3), the 
expectations of the cost function J(n) and of the matrix 
Φ(n) are  
 
            
{ ( )
}
{ ( )
}
2
1
1
n
E J n
λ E e n
λ
−
≅
−
, 
   (12) 
 
{ ( )
}
1
1
n
E
n
λ
λ
−
≅
−
Φ
R , 
 
   (13) 
 
where R is the correlation matrix of input data. It can be 
noticed that J(n) is a biased estimate of E{|e(n)|2} and 
similarly Φ(n) is a biased estimate of R, i.e., 
 
           
            
{ ( )
}
( )
{
2
1
1
n
E J n
E e n
λ
→∞ ≅
−
} , 
   (14) 
 
{ ( )
}
1
1
n
E
n
λ
→∞ ≅
−
Φ
R .  
   (15) 
 
Some classes of applications, e.g., [6], [7] require a 
high memory algorithm, which means that the value of the 
exponential weighting factor λ is very close to 1. In this 
case very large values for the parameters from (14) and 
(15) could result, causing unwanted finite precision effects 
in a practical implementation. Apparently, the RLS 
algorithm avoids this problem by using the inverse of the 
matrix Φ(n). Therefore, the maximum values of the 
elements of the matrix P(n) result in the initialization phase 
of the algorithm, according to (11). Nevertheless, the 
“reverse” problem persists because the values of the 
elements of the matrix P(n) decrease towards very small 
values close to zero, when λ is close to 1. 
For example let us consider the following scenario. 
The fixed point two’s complement arithmetic with a word 
length of B + 1 bits is used and the input signal is a white 
Gaussian noise, so that R = σx
2I, where σx
2 is the input 
signal variance. We assume the input signal power upper 
bounded, so that 
 
  
 
        
, 
 
 
   (16) 
2
x
a
σ
≤
 
where a is a positive constant. In addition, for the RLS 
algorithm to work, a persistent excitation condition [13] 
must be imposed, i.e., 
 
  
 
        
, 
 
 
   (17) 
2
x
b
σ
≥
 
where b is a positive constant. Following (13) results 
 
( )
2
1
x
n n
σ
λ
→∞ =
−
Φ
I , 
 
   (18) 
 
( )
2
1
n
x
n
λ
σ
→∞
−
=
P
I . 
 
   (19) 
 
Consequently, the elements of the main diagonal of P(n), 
denoted here by P(i,i)(n), are asymptotically bounded by 
 
(
, ) ( )
1
1
i i
n
a
b
λ
λ
−
−
≤
≤
P
. 
 
   (20) 
 
On the other hand, according to (11), it results that 
 
      
 
 
(
, ) ( )
1
0
i i
= δ
P
. 
 
 
   (21) 
 
Therefore, a scaling procedure is required to avoid 
overflow phenomenon. The scaling factor 0 < s < 1 has to 
be chosen such that 
 
          
 
           
, 
 
 
   (22) 
1
sM <
 
where 
 
1 1
max
,
M
b
λ
δ
−
⎧
⎫
=
⎨
⎬
⎩
⎭
. 
 
  (23) 
 
Nevertheless, reducing the values of the elements of P(n) 
by scaling may lead to a stalling phenomenon. This 
phenomenon appears when P(n) becomes a zeros matrix, so 
that, according to (8), the RLS algorithm is “frozen”. To 
avoid this situation it is necessary that 
 
        
 
       1
2 B
s
a
λ
−
−
>
. 
 
  (24) 
 
Consequently, 
 
       
 
    
2
1
B
a
s
1
M
λ
⋅ −
<
<
−
 
   
  (25) 
 

91
International Journal on Advances in Telecommunications, vol 2 no 2&3, year 2009, http://www.iariajournals.org/telecommunications/
 
and that implies 
 
         
 
       2
1
B
aM
λ
>
−
.  
 
  (26) 
 
In (23) the case (1 – λ)/b > 1/δ is improbable for a value of 
λ very close to 1, so that usually M = 1/δ and the algorithm 
is not sensitive to decrease of excitation. 
 
3. MODIFIED RLS COST FUNCTION 
 
Taking into account the previous discussion it would be 
very helpful to use an unbiased estimator of the matrix 
Φ(n). For this reason, the cost function from (1) can be 
modified as follows [14]: 
 
( )
(
)
( )
(
) (
) ( )
2
2
1
1
1
n
n i
i
1
J n
e i
J n
λ
λ
λ
λ
−
=
=
−
=
−
+
−
∑
e n
. 
(27) 
In this case 
 
          
       
{ ( )
}
(
)
{ ( )
}
2
1
n
E J n
E e n
λ
≅
−
  
   (28) 
 
is an asymptotically unbiased estimator of the mean square-
error. 
Following this idea we have to perform the same 
modification in (3) and (4) obtaining 
 
( )
(
)
( )
( )
(
) (
) ( )
(
1
1
1
1
n
n i
H
i
H
n
i
i
n
n
λ
λ
λ
λ
−
=
=
−
=
=
−
+
−
∑
Φ
x
x
Φ
x
x
)
n
 
   (29) 
 
            
( )
(
)
( )
( )
(
) (
) ( )
( )
*
1
*
1
1
1
n
n i
i
n
i d
i
n
n
λ
λ
λ
λ
−
=
=
−
=
=
−
+
−
∑
θ
x
θ
x
d
n
         (30) 
 
According, 
 
          
          
{ ( )
}
(
)
1
n
E
n
λ
≅
−
Φ
R  
 
   (31) 
 
is an asymptotically unbiased estimator of the correlation 
matrix. 
As a consequence of these modifications, the Kalman 
vector from (7) has to be evaluated as [14] 
 
     ( )
(
) ( )
( ) (
) ( )
1
1
1
H
n
n
n
n
n
n
λ
λ
−
=
+
−
−
P
x
k
x
P
x
, 
   (32) 
 
where 
( )
P n
 is the inverse of the matrix 
( )
Φ n
. The others 
algorithm’s relations remain the same [i.e., equations (8), 
(9), and (10)]. 
Next, let us perform a brief convergence analysis of 
this modified RLS algorithm. First we assume that the 
desired response d(i) and the tap-input vector x(i) [see (2)] 
are related by the linear regression model 
 
( )
( )
0
0
H
d i
i
e
i
=
+
w x
( ) , 
 
   (33) 
 
where w0 is the regression parameter vector of the model 
and e0(i) is the measurement noise, assumed to be white 
with zero mean and variance σ0
2, and independent of x(i). 
As a result of the initialization procedure, (29) becomes 
 
( )
0 ( )
n
n
= λ δ
+
Φ
I
Φ
n , 
 
   (34) 
 
where 
( )
Φ0 n
 is a particular solution, i.e., 
 
          
         
( )
(
)
( )
( )
0
1
1
n
n i
H
i
n
λ
λ −
=
=
−
∑
Φ
x
i x
i
)i
⎫⎬
0
. 
   (35) 
 
Using (34), (35), (33), and (30) in (5) we get 
 
(
)
( )
( ) ( )
( )
(
)
( )
( )
(
)
( )
(
1
*
0
0
1
1
1
1
1
n
n i
H
n
i
n
n
n i
H
n i
i
i
i
i
n
n
i
i
i e
λ
λ
λ δ
λ
λ
λ
λ
−
=
−
−
=
=
−
+
=
=
−
+
−
∑
∑
∑
x
x
w
w
x
x
w
x
  
(36) 
Taking the expectation of both sides of (36) and taking into 
account that 
 
( )
* ( )
0
1
n
n i
i
E
i e
i
λ −
=
⎧
=
⎨⎩
⎭
∑
x
,  
   (37) 
 
           
              
( )
( )
1
1
n
n i
H
i
E
i
i
λ
λ
−
=
⎧
⎫ =
⎨
⎬
−
⎩
⎭
∑
R
x
x
, 
   (38) 
 
we obtain 
 
               
(
)
{ ( )
}
0
n
E
n
R + λ δ
I
w
= R
w . 
   (39) 
 
Therefore, 
 
{ ( )
}
0
n
E
n
→∞ =
w
w  
 
   (40) 
 
so that w(n) is an asymptotically unbiased estimator of w0. 
The initial convergence rate of the modified algorithm 
depends on how “fast” the product λnδ decreases to zero. 

92
International Journal on Advances in Telecommunications, vol 2 no 2&3, year 2009, http://www.iariajournals.org/telecommunications/
Performing the same analysis for the classical RLS 
algorithm we get 
 
        
       
. 
   (41) 
(
)
(
)
{ ( )
}
0
1
n
E
n
λ λ δ
+
−
=
R
I
w
Rw
 
Comparing (39) to (41) it is obvious that the classical RLS 
algorithm has a faster initial convergence rate than the 
modified algorithm for the same λ and δ. Anyway, if we 
use for the classical RLS algorithm a value of the 
regularization parameter equal to δ and for the modified 
algorithm the value 
 
          
 
       
(
)
'
1
δ
δ
λ
=
−
 
 
  (42) 
 
both RLS algorithms achieve the same initial convergence 
rate. 
Finally, let us analyze the dynamic range of the 
elements of the matrix 
( )
P n
. In a similar manner as in the 
case of the classical RLS algorithm we find 
 
 
1
1
'
max
,'
M
b
δ
⎧
⎫
=
⎨
⎬
⎩
⎭
, 
 
   (43) 
 
    
 
 
1
2
'
B a
s
M
−
<
<
,  
 
   (44) 
 
so that 
 
         
 
     
. 
 
 
   (45) 
2
'
B
aM
>
 
For 
δ '
= δ
, the probability of stalling phenomenon 
becomes significantly lower. Nevertheless, if 
'
M  is chosen 
as 
'
1/
'
M
δ
=
, the probability of overflow due to small 
signal becomes higher. 
It can be concluded that the fast initial convergence 
rate, being dependent on δ, is conditioned by the numerical 
resolution. Even in the modified version, the estimate of the 
mean square error is biased at the beginning of the process 
and similar property results for the matrix 
( )
Φ n
. In 
addition, the initial value P(0) from (11) is an arbitrary 
starting point, without any relation with the input signal, 
but important for the initial convergence rate. We propose a 
more natural way for initialization of the cost function, 
suggested by the well-known averaging algorithm 
 
       ( )
(
)
( )
(
)
( )
2
2
1
1
1
,
1
1
1
1
,
n
J n
e n
n
n
n
J n
N
N
J n
e n
n
N
N
−
⎧
−
+
≤
≤
⎪⎪
= ⎨
−
⎪
−
+
>
⎪⎩



N
     (46) 
 
In the first N steps, 
( )
J n

is the sample mean of |e(n)|2, i.e., 
 
       
 
  ( )
( )
2
1
1
n
i
J n
e i
= n =
∑

 
 
   (47) 
 
Alternatively, the above cost function can be written as 
 
     
     ( )
( ) (
)
( )
(
) ( )
2
λ
1
1
λ
J n
n J n
n
e n
=
−
+
−


    (48) 
 
where 
 
       
        ( )
1
1
for
1
1
λ
1
for
1
n
n
n
n
n
λ
λ
λ
−
⎧
<
≤
⎪⎪
−
= ⎨
⎪
>
⎪
−
⎩
 
   (49) 
 
For n = 1, λ(1) = 0 is not acceptable so that the iterations 
start from n = 2, considering the initial value 
 
       
 
      
( )
( )
2
1
0
x
−
=
P
I  
 
   (50) 
 
In order to agree assumption (17) the algorithm starts 
only when x(0) > b. The initial convergence rate depends 
on the starting value x2(0) but this dependence is 
considerably reduced because of the low memory behavior 
(i.e., small λ) in the initial part of the process. The main 
advantage of the algorithm consists of the fact that 
( )
Φ n
 is 
an unbiased estimator of R, almost every time. 
 
4. QRD-LSL ALGORITHMS WITH REDUCED 
DYNAMICS OF PARAMETERS 
 
The classical RLS is not very frequently used in practical 
application mainly due to its high computational 
complexity (on the order of L2). For this reason, the fast 
RLS algorithms are preferred in practice (e.g., [15]). 
Among these, the QRD-LSL algorithm represents one of 
the most attractive choices, mainly due to its robust 
numerical features [9].  
In order to extend the idea of the modified cost 
function from Section 3 to the case of the QRD-LSL 
algorithm, 
let 
us 
consider 
the 
time 
series 
( )
( )
( )
1 ,
2 ,
,
x
x
… x
n  (i.e., the input signal) that occupies the 
time interval 1
i
≤ ≤ n
, assuming that 
( )
x i = 0
 for 
i ≤ 0
. 
Most of the notations from [1] will be involved in the 
following development. The data matrix used in a least-
squares estimation problem can be expressed as 
 
   
*
1
,
1
1
,
1
*
*
1
(1)
0
0
( )
(
1)
(
2)
(
2)
( )
(
1)
(
)
m
f m
m
b m
H
m
x
n
n
n
n
x
n
n
x
n
m
+
−
−
−
−
⎡
⎤
⎢
⎥
⎢
⎥
=
−
−
−
⎢
⎥
⎢
⎥
−
−
⎣
⎦
A
d
A
d
x
,  (51) 
 

93
International Journal on Advances in Telecommunications, vol 2 no 2&3, year 2009, http://www.iariajournals.org/telecommunications/
where subscript m = 1, 2,…, L is the prediction order and 
1(
2)
b
Jm
− n
 
 
*
*
*
1
*
*
*
(1)
0
0
(2)
(1)
0
(
2)
(
2)
(
1)
(
)
m
x
x
x
n
x
n
x
n
x
n
m
−
⎡
⎤
⎥
⎥
⎥
⎥
⎦
⎢
⎢
−
= ⎢
⎢
⎥
⎢
−
−
−
⎣
A
"
"
#
#
%
#
"
,
)
)
⎤
⎥
, (52) 
 
[
]
1(
1)
(
1),
, (
1) T
m
n
x n
x n
m
−
−
=
−
−
+
x
…
 
   (53) 
 
*
*
,
1(
1)
(2),
,
(
1)
,
T
f m
n
x
x
n
−
⎡
⎤
−
=
−
⎣
⎦
d
…
 
   (54) 
 
*
,
1(
2)
0,
,
(
1)
T
b m
n
x
n
m
−
⎡
⎤
−
=
−
−
⎣
⎦
d
…
. 
   (55) 
 
where superscript T denotes transposition. Let us denote by 
 an n-by-n unitary matrix and by 
 an 
-by-
 
upper 
triangular 
matrix. 
The 
exponential weighting matrix from the classical QR-
decomposition is 
( )
Qm n
( )
m n
R
(
m +1
(
1
m +
 
    
    (56) 
{
}
1
2
(
1)
( )
diag
,
,
,1
1
c
n
n
c
T
n
n
λ
λ
λ
−
−
−
⎡
=
= ⎢
⎢
⎥
⎣
⎦
Λ
0
Λ
0
…
 
In order to approach the cost function from (27), a 
modified form of the previous matrix will be used in our 
development, i.e., 
 
{
}
1
2
(
1)
( )
(1
)diag
,
,
,1
1
n
n
T
n
n
λ
λ
λ
λ
λ
−
−
−
⎡
⎤
=
−
= ⎢
⎥
−
⎢
⎥
⎣
⎦
Λ
0
Λ
0
…
.
 
 
  (57) 
Using (57) and following the QR-decomposition we have: 
 
1/2
2
1
1/2
(
1)/2 *
1/2
1/2
1/2
,
2
2
,
2
1/2
1/2
,
2
,
2
1/2 *
1/2
1/2 *
1
1
0
(
2)
( )
( )
0
1
(1
)
(1)
0
(
1)
(
2)
(
2)
(
1)
(
2)
(1
)
( )
(1
)
(
1)
(1
)
(
)
T
m
m
T
n
T
f m
m
b m
f m
b m
H
m
n
n
n
x
n
n
n
n
n
x
n
n
x
n
λ
λ
λ
λ
λ
λ
λ
λ
λ
λ
−
+
−
−
−
−
−
−
⎡
⎤
⎢
⎥
−
⎢
⎥
⎢
⎥
⎢
⎥
⎣
⎦
⎡
⎤
−
⎢
⎢
−
−
−
= ⎢
⎢
−
⎢
⎢
−
−
−
−
−
⎣
⎦
0
0
Q
0 Λ
A
0
0
p
R
p
v
0
v
x
m
−
−
⎥
⎥
⎥
⎥
⎥
⎥
 
  (58) 
Let B(n) denote the matrix on the right-hand term of (58). 
We use a unitary matrix 
 to annihilate the vector 
, except for its first element, denoted by 
(
n − 2
P
. A new element is generated, namely, 
−
,
1(
1)
f m
n
π
−
−
, in the first column. Next, we use the unitary 
matrix 
2(
1)
m
n
−
−
T
 to update the vectors 
,
2(
1)
f m
n
−
−
p
, 
,
2(
2
b m
n
−
− )
p
, and the matrix 
2(
2
m
n
−
− )
R
. Angle-
normalized forward and delayed backward prediction 
errors, 
,
f m 1( )
n
ε
−
 and 
,
1(
1)
b m
n
ε
−
−
, are generated in 
complex conjugate forms and all the elements of the vector 
1(
1)
H
m
− n
−
x
 are annihilated, i.e., 
 
2
1/2
(
1)/2 *
,
2
2
,
2
1/2
1/2
,
1
1
1/2
,
1
1/2 *
1/2 *
,
1
,
1
(
2)
1
( )
(
1)
1
(1
)
(1)
0
( )
(
1)
(
1)
(
1)
(
2)
(
1)
(1
)
( )
(1
)
(
1)
T
T
m
n
T
f m
m
b m
T
b
f m
m
f m
T
f m
b m
n
n
n
x
n
n
n
n
J
n
n
n
λ
λ
λ
π
λ
λ
λ
ε
λ
ε
−
−
−
−
−
−
−
−
−
−
−
⎡
⎤ ⎡
⎤
⋅
⋅
⎢
⎥ ⎢
⎥
−
⎢
⎥
⎢
⎥ ⎣
⎦
⎣
⎦
n
⎡
⎤
−
⎢
⎥
−
−
⎢
⎢
⎢
−
−
= ⎢
⎢
−
⎢
⎢
−
−
−
⎣
⎦
P
0
0
B
0
T
0
0
p
R
p
0
v
0
0
0
⎥
⎥
⎥
⎥
⎥
⎥
⎥
 
 
  (59) 
Let C(n) denote the matrix on the right-hand term of (59). 
We can write: 
 
1
*
,
1
,
1
1
,
1
,
1
1/2
(
1)/2 *
,
2
2
,
2
,
1
1
1/2
,
1
1
0
0
( )
0
(
1)
(
1)
0
(
1)
(
1
(1
)
(1)
0
( )
(
1)
(
1)
( )
(
1)
(
1)
(1
)
T
T
m
T
T
b m
b m
n m
T
T
b m
b m
n
T
f m
m
b m
T
b
f m
m
f m
n
c
n
s
n
s
n
c
n
x
n
n
n
J
n
λ
λ
π
λ
λ
−
−
−
−
−
−
−
−
−
−
−
−
−
−
⎡
⎤
⎢
⎥
⎢
⎥
⎢
⎥
−
−
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
−
−
−
⎣
⎦
−
−
−
−
=
−
−
0
0
0
I
0
0
0
C
0
0
0
0
0
I
0
0
0
0
p
R
p
0
v
0
1/2 *
, ( )
0
T
f m n
ε
)
n
n
0
⎡
⎤
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎣
⎦
0
 
(60) 
where I denotes identity matrices. Furthermore, the 
following updates result: 
 
1/2
1
,
1
1
(
2)
(
1)
(
1)
b
m
b m
b
m
J
n
c
n
J
n
λ
−
−
−
−
−
=
−
,  
   (61) 
)
)
,
2(
2
b m
n
−
−
v
 

94
International Journal on Advances in Telecommunications, vol 2 no 2&3, year 2009, http://www.iariajournals.org/telecommunications/
1/2
*
,
1
,
1
1
(1
)
(
1)
(
1)
(
1)
b m
b m
b
m
n
s
n
J
n
λ
ε
−
−
−
−
−
−
=
−
,  
   (62) 
 
     
2
1
1
,
1
(
1)
(
2)
(1
)
(
1)
b
b
m
m
b m
J
n
J
n
n
λ
λ ε
−
−
−
−
=
−
+
−
−
, (63) 
 
      
,
,
1
,
1
1/2
*
*
,
1
,
1
( )
(
1)
( )
(
1)
(
1) 1
f m
b m
f m
b m
f m
n
c
n
n
s
n
n
ε
ε
λ
π
λ
−
−
−
−
=
−
⎛
⎞
−
−
−
⎜
⎟
⎝ −
⎠
,  (64) 
 
        
        (65) 
*
1/2
*
,
1
,
1
,
1
1/2
,
1
,
1
( )
(
1)
(
1)
(
1)
( )(1
)
f m
b m
f m
b m
f m
n
c
n
n
s
n
n
π
λ
π
ε
λ
−
−
−
−
−
=
−
−
+
−
−
 
In the same manner, another two transformations over 
the matrix B(n) are performed, i.e., 
 
(
)
(
)
2
1/2
1/2
,
1
1
,
2
2
,
2
1/2
,
1
1/2
1/2
*
,
1
,
1
(
1)
1
( )
(
1)
1
(
1)
(
1)
( )
(
1)
(
1)
(
1)
1
( )
1
(
T
T
m
f
T
b m
m
f m
m
b m
b m
T
f m
b m
n
n
n
J
n
n
n
n
n
n
n
λ
λ
λ
λ
ε
λ
ε
−
−
−
−
−
−
−
−
−
⎡
⎤ ⎡
⎤
⋅
⋅
⎢
⎥ ⎢
⎥
−
⎢
⎥
⎢
⎥ ⎣
⎦
⎣
⎦
⎡
⎤
−
−
⎢
⎥
⎢
⎥
−
−
⎢
⎥
= ⎢
−
⎢
⎢
−
−
⎣
K
0
0
B
0
T
0
0
p
R
p
0
Ο
v
0
*
1)
n
π
−
⎥
⎥
⎥
−
⎦
                 
(66) 
Let D(n) denote the matrix on the right-hand term of (66). 
We can write: 
 
   
(
)
*
,
1
,
1
2
,
1
,
1
,
1
1
,
2
2
,
2
1/2
,
1
1/2
*
,
( )
( )
( )
( )
( )
( )
( )
( )
(
1)
(
1)
(
1)
0
1
T
f m
f m
n
T
f m
f m
f
T
b m
m
f m
m
b m
b m
T
b m
c
n
s
n
n
s
n
c
n
J
n
n
n
n
n
n
n
π
λ
λ
ε
−
−
−
−
−
−
−
−
−
−
−
⎡
⎤
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎣
⎦
⎡
⎢
⎢
−
−
⎢
⎥
= ⎢
−
⎢
⎥
⎢
⎥
−
⎣
0
0
I
0
D
0
0
p
R
p
0
Ο
v
0
( )
⎤
⎥
⎥
⎥
⎦
    
 
   (67) 
 
Similarly, a set of recursive relations for the forward 
prediction part of the algorithm are obtained, i.e., 
 
1/2
1
,
1
1
(
1)
( )
( )
f
m
f m
f
m
J
n
c
n
J
n
λ
−
−
−
−
=
, 
 
   (68) 
 
1/2 *
,
1
,
1
1
(1
)
( )
( )
( )
f m
f m
f
m
n
s
n
J
n
λ
ε
−
−
−
−
=
, 
 
   (69) 
 
2
,
1
1
1
( )
(
1)
(1
)
( )
f
f
f m
m
m
J
n
J
n
n
λ
λ ε
−
−
−
=
−
+
−
,  (70) 
 
          
,
,
1
,
1
1/2
*
*
,
1
,
1
( )
( )
(
1)
( )
(
1) 1
b m
f m
b m
f m
b m
n
c
n
n
s
n
n
ε
ε
λ
π
λ
−
−
−
−
=
−
⎛
⎞
−
−
⎜
⎟
⎝ −
⎠
,  (71) 
 
          
.    (72) 
*
1/2
*
,
1
,
1
,
1
1/2
,
1
,
1
( )
( )
(
1)
( )
(
1)(1
)
b m
f m
b m
f m
b m
n
c
n
n
s
n
n
π
λ
π
ε
λ
−
−
−
−
−
=
−
+
−
−
 
Finally, for the joint-process estimation part of the 
algorithm we have 
 
1/2
*
*
1
,
,
( )
( )
( )
( )
(
1) 1
m
b m
m
b m
m
n
c
n
n
s
n p
n
λ
ε
ε
λ
+
⎛
⎞
=
−
−
⎜
⎟
⎝ −
⎠
, (73) 
 
*
1/2
*
1/2
,
,
( )
( )
(
1)
( )
( )(1
)
m
b m
m
b m
m
p
n
c
n
p
n
s
n
n
λ
ε
=
−
+
− λ
.(74) 
 
Summarizing, the proposed algorithm uses (61)–(65) 
for the backward prediction part, together with (68)–(72) 
for the forward prediction part, and (73), (74) for the joint-
process estimation. Let us call this modified algorithm by 
QRD-LSL-m1. According to the discussion from the end of 
Section 3, this type of algorithm achieve a slower initial 
convergence rate than the classical one (for the same 
initialization parameters) but we may use a variable 
exponential weighting factor according to (49) in order to 
speed up the initial convergence of the algorithm. 
Therefore, it results a second modified algorithm, which we 
called QRD-LSL-m2. 
The computational complexity of the proposed 
algorithms is similar with the complexity of the classical 
QRD-LSL (i.e., around 20L), while the computational 
amount of the RLS algorithm is around 3L2. 
 
5. SIMULATION RESULTS 
 
For the first set of experiments we consider an adaptive 
“system identification” configuration [1]. In this class of 
applications an adaptive filter is used to provide a linear 
model that represents the best fit (in some sense) to an 
unknown system. The adaptive filter and the unknown 
system are driven by the same input; the unknown system 
output supplies the desired response for the adaptive filter. 
These two signals are used to compute the estimation error, 
in order to adjust the filter coefficients. Our  input  signal is  

95
International Journal on Advances in Telecommunications, vol 2 no 2&3, year 2009, http://www.iariajournals.org/telecommunications/
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Fig. 1.  Square errors [dB] and the cost functions of the classical 
QRD-LSL algorithm (column 1) and the modified versions QRD-
LSL-m1 (column 2) and QRD-LSL-m2 (column 2), in a system 
identification setup. Row 1 – Square errors [dB]; Row 2 – J b cost 
functions; Row 3 – J f cost functions. 
 
a random sequence with an uniform distribution in the 
interval (–1;1). The order of the adaptive filter is M = 64. In 
Fig. 1 are presented the convergence curves and the 
evolution of the cost functions for the classical QRD-LSL 
algorithm and its modified versions, QRD-LSL-m1 and 
QRD-LSL-m2, using 
λ = 0.9999
. In the case of both 
modified algorithms the values of the cost functions can not 
exceed 1 [according to (63) and (70)]. Hence, due to the 
reduced dynamic range of these parameters, the “effort” for 
scaling procedures is significantly reduced, On the other 
hand, the cost functions of the classical algorithm will be 
upper bounded (theoretical) by 1/(1 – λ), which leads to 
large values when λ is close to 1. Also, it can be noticed 
that the QRD-LSL-m2 achieve the same initial convergence 
rate as the classical QRD-LSL algorithm.   
The previous simulation was performed using the full 
precision of Matlab programming environment. Next, the 
algorithms are implemented in fixed-point precision, using 
a fixed-point DSP with a word length of 16 bits (15 bits for 
the magnitude and one sign bit). The usage of a higher 
precision (e.g., 24 or 32 bits) could lead to better 
performances but also increases the implementation costs. 
As a practical aspect of this work we choose to illustrate 
the algorithms performance in a noise reduction scenario 
(Fig. 2) [1]. In this type of application, the adaptive filter is 
use to synthesize at its output a replica of the perturbation 
that corrupts the voice signal. 
 
0
5
x 10
4
-300
-200
-100
0
100
QRD-LSL
dB
0
5
x 10
4
-300
-200
-100
0
100
QRD-LSL-m1
0
5
x 10
4
-300
-200
-100
0
100
QRD-LSL-m2
0
5
x 10
4
0
2000
4000
0
5
x 10
4
0
0.2
0.4
0
5
x 10
4
0
0.2
0.4
0
5
x 10
4
0
2000
4000
0
5
x 10
4
0
0.2
0.4
Iterations
0
5
x 10
4
0
0.2
0.4
 
 
 
 
 
 
 
 
 
 
voice
Source of noise
noise1
noise2 
microphone 1 
microphone 2 
 
 
 
 
Adaptive 
filter 
 
 
 
–
 
≈ noise1 
+
 
 
≈ voice 
 
Fig. 2.  Adaptive noise reduction scheme. 
 
In the original QRD-LSL algorithm the asymptotic 
value for the cost functions are 1/(1 – λ). Since λ is very 
close to 1, the original algorithm will certainly produce 
overflow and thus needs to be scaled, i.e., the cost 
functions must be right-shifted by a number of bits such 
chosen as to avoid the overflow in the convergence state. A 
simple calculus shows that the optimum number of bits to 
shift-right the cost functions is Bs = ⎡– log2(1 – λ)⎤, where 
⎡•⎤ denotes superior integer round. Nevertheless, this 
further leads to the reduction of the effective number of 
bits, especially when λ is very close to 1 and eventually to 
a low signal to quantization noise ratio, altering the 
algorithm performances. For this reasons we choose to 
compare the QRD-LSL-m2 algorithm (since it has a faster 
initial converge rate as compared to QRD-LSL-m1) with 
the normalized least-mean-square (NLMS) algorithm [1], 
which is one of the most common solution for noise 
reduction [16], [17]. Since the computational amount of the 
NLMS is around 3L, this algorithm is “cheaper” (in terms 
of complexity) as compared with the proposed QRD-LSL 
algorithm. Nevertheless, the performances of the NLMS 
algorithm are strongly reduced when high order adaptive 
filters and non-stationary inputs (e.g., speech) are used. In 
these cases, the RLS-based algorithms rule. 
The results of the noise reduction experiment are 
presented in Figs. 3 and 4, using two type of noise, i.e., a 
white Gaussian noise with SNR = 10dB (Fig. 3) and a 
highway noise (Fig. 4). The last one is more severe because 
it is a non-stationary signal. In both cases the QRD-LSL-
m2 algorithm outperforms the NLMS algorithm. 
 

96
International Journal on Advances in Telecommunications, vol 2 no 2&3, year 2009, http://www.iariajournals.org/telecommunications/
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Fig. 3.  (a) original signal; (b) corrupted signal (with white 
Gaussian noise); (c) recovered signal using the NLMS algorithm; 
(d) recovered signal using the QRD-LSL-m2 algorithm. 
 
In the first case (Fig. 3) the subjective tests indicate a mean 
opinion score (MOS) of 3.9 for the NLMS algorithm and 
4.5 for the QRD-LSL-m2 algorithm. In the second case the 
difference becomes more apparent, i.e., 3.2 for the NLMS 
algorithm and 4.1 for the QRD-LSL-m2. Note that the 
MOS scale is from 1 to 5, where 1 means very poor and 5 
means excellent quality. This was evaluated in a subjective 
manner, as the average of the scores given by 20 listeners. 
 
6. DISCUSSION 
 
A first goal of this paper was to present and analyze a 
modified version of the RLS adaptive algorithm with 
improved features for fixed-point implementation. The 
basic idea was to use an asymptotically unbiased estimator 
for the cost function. In this manner we try to prevent the 
stalling phenomenon which may appear when a high 
memory RLS algorithm is implemented using fixed-point 
arithmetic. A brief convergence analysis of the RLS 
algorithms was performed, together with a discussion 
concerning the proper scale factor, which has to be chosen 
in order to avoid the overflow and stalling effects.  
 
 
0
0.5
1
1.5
2
2.5
3
3.5
4
-1
0
1
a
0
0.5
1
1.5
2
2.5
3
3.5
4
-1
0
1
b
0
0.5
1
1.5
2
2.5
3
3.5
4
-1
0
1
c
0
0.5
1
1.5
2
2.5
3
3.5
4
-1
0
1
d
secunde
seconds 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
0
0.5
1
1.5
2
2.5
3
3.5
4
-1
0
1
a
0
0.5
1
1.5
2
2.5
3
3.5
4
-1
0
1
b
0
0.5
1
1.5
2
2.5
3
3.5
4
-1
0
1
c
0
0.5
1
1.5
2
2.5
3
3.5
4
-1
0
1
d
secunde
seconds 
Fig. 4.  (a) original signal; (b) corrupted signal (with highway 
noise); (c) recovered signal using the NLMS algorithm; (d) 
recovered signal using the QRD-LSL-m2 algorithm.  
 
In the case of the modified RLS algorithm only the initial 
convergence rate is affected when it operates with the same 
value of the regularization parameter as the classical RLS 
algorithm. Choosing the value of this parameter according 
to (42), the modified algorithm achieves the same initial 
convergence rate as the classical one. Moreover, the 
variable exponential weighting factor from (49) speeds up 
the initial convergence rate of this algorithm, leading to a 
reasonable compromise between the convergence rate and 
dynamic range of the algorithm’s parameters. 
The procedure presented in the case of the RLS 
algorithm was developed and applied in the case of the 
QRD-LSL algorithm, which is a fast member of the RLS 
family. Two modified versions of the QRD-LSL algorithm 
were proposed. Based on the asymptotically unbiased 
estimator for the cost functions, we improve the behavior 
of these algorithms when dealing with fixed-point 
arithmetic. As expected, only the initial convergence rate of 
the QRD-LSL-m1 algorithm is affected when it operates 
with the same parameters as the classical QRD-LSL 
algorithm. Also, the variable exponential weighting factor 
used for QRD-LSL-m2 algorithm speeds up its initial 

97
International Journal on Advances in Telecommunications, vol 2 no 2&3, year 2009, http://www.iariajournals.org/telecommunications/
convergence rate. The simulations performed in both 
Matlab and fixed-point DSP support the theoretical 
findings. 
 
7. CONCLUSIONS 
 
A class of RLS algorithms suitable for fixed-point 
implementation was presented in this paper. The proposed 
approach was applied in the case of the QRD-LSL 
algorithm. The performance of the resulted algorithm was 
evaluated in a noise reduction scenario, obtaining 
promising results. 
 
8. REFERENCES 
 
[1] S. Haykin, Adaptive Filter Theory – Fourth Edition. 
Prentice-Hall, Inc., Upper Saddle River, New Jersey, 
2002. 
 
[2] T. Adali and S. H. Ardalan, “Convergence and error 
analysis of the fixed point RLS algorithm with 
correlated inputs,” Proc. of IEEE Int. Conf. on 
Acoustics, Speech, and Signal Processing, ICASSP 
1990, vol. 3, pp. 1479-1482. 
 
[3] I.K. Proudler, J.G. McWhirter, and T.J. Shepherd, 
“QRD-based lattice filter algorithms,” Proc. SPIE, vol. 
1152, pp. 56-67, 1989. 
 
[4] J.-T. Yuan, “A modified QRD for smoothing and a 
QRD-LSL smoothing algorithm,” IEEE Trans. Signal 
Processing, vol. 47, no. 5, pp. 1414-1420, May 1999. 
 
[5] J.-T. Yuan and J.-N. Lee, “Narrow-band interference 
rejection in DS/CDMA systems using adaptive (QRD-
LSL)-based nonlinear ACM interpolators,” IEEE 
Trans. Vehicular Technology, vol. 52, no. 2, pp. 374-
379, Mar. 2003. 
 
[6] C. Paleologu, S. Ciochină, and A.A. Enescu, “A 
network echo canceler based on a SRF QRD-LSL 
adaptive algorithm implemented on Motorola StarCore 
SC140 DSP,” Lecture Notes in Computer Science, 
Springer-Verlag, vol. 3124, pp. 560-567, June 2004. 
 
[7] G. Rombouts and M. Moonen, “Fast QRD-lattice-
based unconstrained optimal filtering for acoustic 
noise reduction,” IEEE Trans. Speech and Audio 
Processing, vol. 13, no. 6, pp. 1130-1143, Nov. 2005. 
 
 
 
 
 
 
[8] J.-T. Yuan, C.-A. Chiang, and C.-H. Wu, “A square-
root-free QRD-LSL interpolation algorithm,” Proc. of 
IEEE Int. Conf. on Acoustics, Speech, and Signal 
Processing, ICASSP 2008, pp. 3813-3816, Apr. 2008. 
 
[9] C. Paleologu, A. A. Enescu, S. Ciochină, and F. Albu, 
“QRD-LSL Adaptive Algorithms Suitable for Fixed-
Point 
Implementation,” 
Proc. 
IEEE 
Advanced 
International Conference on Telecommunications 
(AICT), pp. 163-167, Venice, Italy, May 2009. 
 
[10] C. Paleologu, F. Albu, A. A. Enescu, and S. Ciochină, 
“Modified SRF-QRD-LSL Adaptive Algorithm with 
Improved Numerical Robustness,” IARIA International 
Journal on Advances in Systems and Measurements, 
vol. 2, no. 1, pp. 56-65, 2009. 
 
[11] J. Benesty and Y. Huang, Eds, Adaptive Signal 
Processing--Applications to Real-World Problems. 
Springer-Verlag, Berlin, Germany, 2003. 
 
[12] G. V. Moustakides, “Study of the Transient Phase of 
the Forgetting Factor RLS,” IEEE Transactions on 
Signal Processing, vol. 45, no. 10, pp. 2468-2476, Oct. 
1997. 
 
[13] M. H. Verhaegen, “Round-off Error Propagation in 
Four Generally-Applicable, Recursive Least-Squares 
Estimation Schemes,” Automatica, vol. 25, no. 3, pp. 
437-444, 1989. 
 
[14] S. Ciochină, C. Paleologu, and A.A. Enescu, “On the 
Behaviour of RLS Adaptive Algorithm in Fixed-Point 
Implementation,” Proc. of IEEE Int. Symp. on Signals, 
Circuits and Systems, SCS 2003, vol. 1, pp. 57-60, July 
2003. 
 
[15] C. Paleologu, A. A. Enescu, and S. Ciochină, 
“Recursive Least-Squares Lattice Adaptive Algorithm 
Suitable for Fixed-Point Implementation,” Proc of 
IEEE 
International 
Conference 
on 
Electronics, 
Circuits and Systems, ICECS 2006, pp.1105-1108, 
Dec. 2006 
 
[16] E. Haensler and G. Schmidt, Eds., Topics in Acoustic 
Echo and Noise Control. Springer-Verlag, Berlin, 
Germany, 2006. 
 
[17] J. Benesty, J. Chen, Y. Huang, and I. Cohen, Noise 
Reduction in Speech Processing. Springer-Verlag, 
Berlin, Germany, 2009. 
 

