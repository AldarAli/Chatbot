NICER: Aesthetic Image Enhancement with Humans in the Loop
Michael Fischer
University of W¨urzburg
W¨urzburg, Germany
email: m.ﬁscher@informatik.uni-wuerzburg.de
Konstantin Kobs
University of W¨urzburg
W¨urzburg, Germany
email: kobs@informatik.uni-wuerzburg.de
Andreas Hotho
University of W¨urzburg
W¨urzburg, Germany
email: hotho@informatik.uni-wuerzburg.de
Abstract—Fully- or semi-automatic image enhancement soft-
ware helps users to increase the visual appeal of photos and
does not require in-depth knowledge of manual image editing.
However, fully-automatic approaches usually enhance the image
in a black-box manner that does not give the user any control over
the optimization process, possibly leading to edited images that
do not subjectively appeal to the user. Semi-automatic methods
mostly allow for controlling which pre-deﬁned editing step is
taken, which restricts the users in their creativity and ability
to make detailed adjustments, such as brightness or contrast.
We argue that incorporating user preferences by guiding an
automated enhancement method simpliﬁes image editing and
increases the enhancement’s focus on the user. This work thus
proposes the Neural Image Correction & Enhancement Routine
(NICER), a neural network based approach to no-reference
image enhancement in a fully-, semi-automatic or fully manual
process that is interactive and user-centered. NICER iteratively
adjusts image editing parameters in order to maximize an
aesthetic score based on image style and content. Users can
modify these parameters at any time and guide the optimization
process towards a desired direction. This interactive workﬂow
is a novelty in the ﬁeld of human-computer interaction for
image enhancement tasks. In a user study, we show that NICER
can improve image aesthetics without user interaction and that
allowing user interaction leads to diverse enhancement outcomes
that are strongly preferred over the unedited image. We make
our code publicly available to facilitate further research in this
direction.
Keywords—aesthetic image enhancement; user-centered.
I. INTRODUCTION
With the ever-increasing amount of images taken, it is log-
ical that the casual user neither has the knowledge, time, nor
patience to manually edit all images towards pleasing versions.
This, combined with the fact that photography can beneﬁt
greatly from image enhancement, explains the availability
of numerous simple-to-use image enhancement applications.
Fully-automatic enhancement software that can be found in
most smartphone photo applications is usually intransparent,
leaving users with a result that neither was created in an
explainable way nor necessarily correlates with their individual
perception of aesthetics. Semi-automatic approaches often let
the users select a single, pre-deﬁned image ﬁlter that usually
combines different properties, such as higher contrast and
higher saturation. This, evidently, takes control from the user.
We argue that it is beneﬁcial for both, fully- and semi-
automatic image enhancement methods, to be able to incorpo-
rate the user’s individual perception of aesthetics before, dur-
ing, and after the enhancement process. We hence propose the
Neural Image Correction & Enhancement Routine (NICER),
which allows exactly this. It consists of two neural network
based components: An Image Manipulator ﬁrst applies a set
of learned image operations (e.g., contrast, brightness) with
variable magnitude onto the unedited source image while a
subsequent Quality Assessor then assesses the resulting en-
hancement quality. NICER iteratively optimizes the parameters
of the enhancement operations to maximize the Quality Asses-
sor’s aesthetic score. Due to the iterative approach, users can
modify the Image Manipulator’s parameters before, during,
and after the optimization process, directing the enhancement
procedure towards subjectively more appealing local optima.
While other enhancement tools merely provide preview op-
tions for the current ﬁlter setting, NICER’s semi-automatic
mode allows for an interactive back-and-forth between the user
and the automatic optimization and hence facilitates human-
computer interaction in image enhancement applications.
Although the ﬂexible architecture of our approach makes it
possible to exchange each component with a speciﬁcally tai-
lored version (from, e.g., training on a user’s photo collection),
NICER can enhance images in a no-reference setting, without
any previous info about the user’s liking. In a user study,
we show that the visual appeal of NICER’s fully-automatic
enhancement results already is superior to the original im-
ages. We further show that interweaving user interactions and
the automatic enhancement process results in highly diverse
images that are subjectively perceived superior. Our main
contributions are
1) NICER, a novel way of incorporating human aesthetic
preferences into the image enhancement process,
2) a user study assessing NICER’s performance, and
3) a publicly available repository containing our source code
and trained models [1].
The rest of this paper is organized as follows: Section II
presents related work on human-centered image enhancement.
Section III introduces the methodology and components of
NICER. Section IV then assesses NICER’s enhancement qual-
ity in a user study. We conclude this contribution in Section V
and outline starting points for future research.
II. RELATED WORK
The research area of learned perceptual image enhancement
has received ample attention in recent works, particularly
so after the emergence of neural image assessors [3]–[6].
However, most approaches do not consider user preferences
and enhance the image in a black-box fashion, leaving users
357
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-761-0
ACHI 2020 : The Thirteenth International Conference on Advances in Computer-Human Interactions

user preferences
unedited image
final image
Image 
Manipulator
Quality 
Assessor
Loss
Parameters
Image 
Manipulator
edited image
initial parameters
potential corrections
direct influence
Fig. 1. NICER’s optimization workﬂow. If desired, users can interfere with the enhancement routine before, during or after the image optimization. However,
user interference is voluntary and not necessary for a successful enhancement. The dashed line between loss and parameters implies that the loss does not
directly affect the parameters but instead is backpropagated through the architecture. Sample image from [2].
with a potentially sub-optimal result that has not been tailored
to their personal liking.
The few methods that do personalize image enhancement
rely on a given photo collection that has already been re-
touched to the user’s liking. In [7] and [8], users are directly
asked to create this photo collection during the setup phase
of their approaches, which is both inconvenient and time-
consuming. Similarly, Hu et al. [9] use Generative Adver-
sarial Networks to learn a latent space from a pre-enhanced
photo collection and then sample from this space to edit
unseen images. In [10], styles are grouped into clusters of
certain enhancement presets. New users are then assessed and
matched against the enhancement cluster that best suits their
preferences. This approach is most useful for large, cloud-
based solutions, where many users are averaged and less
suitable for individual, personal image enhancement.
Generally, the mentioned approaches do not explicitly con-
sider the user’s individual preferences for the image optimiza-
tion routine, but rather implicitly use the overall information
encoded in the edited image collection. We argue that such
an already edited photo collection is a requirement that might
not always be fulﬁlled (especially for casual users) and further
claim that sampling from the style-space might not necessar-
ily yield an edit that is well-suited for a particular image
content. Contrary to the previously mentioned approaches,
our method does not rely on a pre-enhanced photo collection
that implicitly represents the user’s preferences in an abstract
style-space. Although one could train the Quality Assessor to
be sensitive to personal preferences by using custom photo
collections, this is not necessary for NICER to work correctly.
Instead, we give the user the freedom to individually guide
the optimization by directly interfering with the optimization
routine. The higher amount of (voluntary) interaction can be
seen as drawback and beneﬁt at once: While users might
put more effort into getting enhanced images than in fully-
automatic enhancement approaches, our method really allows
for the individual preferences to be set per image, instead
of relying on a globally estimated preferred enhancement
style. Moreover, using a general purpose Quality Assessor and
letting users guide the optimization eliminates the need for a
pre-enhanced photo collection, making NICER a ready-to-use,
no-reference approach without time-consuming setup.
III. NICER
In this section, we introduce NICER, our proposed approach
for user-centered image enhancement. The structure of our
approach is shown in Figure 1 and was motivated by the idea
of using a perceptually motivated loss function to increase
enhancement appeal [11] [12].  The pipeline consists of the
Image Manipulator, that applies a set of image ﬁlters to
an unedited source image, and the Quality Assessor, that
estimates the aesthetic quality of the Image Manipulator’s
358
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-761-0
ACHI 2020 : The Thirteenth International Conference on Advances in Computer-Human Interactions

outcome. Using differentiable components allows us to itera-
tively optimize the Image Manipulator’s ﬁlter parameters with
respect to the Quality Assessor’s score using gradient descent,
resulting in a no-reference, automatic image enhancement. The
optimization procedure modiﬁes the ﬁlter parameters in each
iteration towards the nearest local score optimum and allows
user interference in every enhancement step. If a user changes
parameters during optimization, NICER continues from the
new parameter settings towards a different local optimum and
thus enables the user to interactively and individually alter the
image editing style.
A. Image Manipulator
The Image Manipulator ψF is used to apply a set of n image
editing ﬁlters F = {f1, f2, ..., fn} to the image I. The only re-
quirement of the Image Manipulator is differentiability with re-
spect to its image ﬁlter parameters K = {k1, k2, ..., kn}, ki ∈
R, as these are optimized via gradient descent. While, in
general, any image ﬁlter can be used, NICER implements six
common photographic ﬁlters: Contrast (Con), Saturation (Sat),
Brightness (Bri), Shadows (Sha), Highlights (Hig), Exposure
(Exp), and two artistic ﬁlters: Local Laplacian Filtering (LLF)
[13] and Non-local Dehazing (NLD) [14].
We implement the Image Manipulator as a Context Aggre-
gation Network (CAN), a Fully Convolutional Neural Network
with dilated convolutions [15] that has been shown to be well-
suited for image enhancement tasks [16]. As the CAN is able
to approximate a large variety of image ﬁlters [16], it is a very
ﬂexible and general, yet differentiable enhancement model.
NICER adapts the CAN24 model by Chen et al. (cf.
Table I), as it provides a good trade-off between accuracy and
speed [16]. Between layers one to eight, a leaky rectiﬁed linear
unit (Leaky ReLU) [17] activation is applied, with a negative
slope of 0.2, while the last layer uses no activation function.
We exclude Batch Normalization, as it showed no signiﬁcant
improvements in approximation accuracy or performance.
Each image ﬁlter intensity k ∈ [−1, 1] is fed into the
network by concatenating it to each pixel of the input image.
During training, we apply one image operation per sample
and let the CAN learn the relationship between the input and
target output. At inference time, multiple image ﬁlters can be
set, as the network interpolates correctly and applies the ﬁlters
simultaneously [16].
In order to learn our proposed image operations, we use
the MIT-Adobe FiveK dataset [2] with a 50/50 train/test split,
resulting in 2500 images per set. We employ the GNU Image
Manipulation Program (GIMP) [18] to create two manipulated
versions (ﬁlter intensity ±100%) of each original image for
the six photographic ﬁlters (Sat, Con, Sha, Hig, Bri, Exp) as
ground truth. To create the ground truth for the ﬁlters LLF
and NLD, we use the implementations from [13] and [14],
respectively. Note that for NLD, we only use positive values
(i.e., +100%), as negative values would haze the image, which
is usually undesired in image enhancement. We then train the
Image Manipulator as in [16].
TABLE I
CAN24 ARCHITECTURE OVERVIEW
Layer
1
2
3
4
5
6
7
8
9
Convolution
3×3 3×3
3×3
3×3
3×3
3×3
3×3
3×3
1×1
Dilation
1
2
4
8
16
32
64
1
1
Padding
1,1
2,2
4,4
8,8
16,16
32,32
64,64
1,1
-
Receptive Field 3×3 7×7 15×15 31×31 63×63 127×127 255×255 257×257 257×257
Width
24
24
24
24
24
24
24
24
3
The trained Image Manipulator can apply any set of ﬁlter
intensities onto a source image. This enables users to initially
set or modify ﬁlter intensities and provides a way of manually
controlling the image editing process, if desired.
B. Quality Assessor
Once the Image Manipulator ψF has edited the image I with
the current ﬁlter intensity combination K, the Quality Assessor
is used as a metric M to rate the manipulation’s outcome
with a score S = M(ψF (K, I)), which is then optimized by
NICER. The Quality Assessor must meet several criteria:
1) Full differentiability with respect to its input.
2) The Quality Assessor’s score prediction must correlate
with the human notion of aesthetics. This is especially
necessary in the automatic enhancement mode, as NICER
will optimize for this score.
3) S must be deterministic, i.e., same for identical images.
We use a neural network based model called Neural Image
Assessment (NIMA) [4] as Quality Assessor, as it complies
with the above desiderata and achieves state-of-the-art perfor-
mance on aesthetic image assessment. NIMA ﬁne-tunes a pre-
trained Convolutional Neural Network for image classiﬁcation;
in our case VGG16 (conf. D, [19]), as it achieved best cross-
dataset performance in [4]. The network’s output consists
of ten nodes that correspond to ten quality score buckets
{1, 2, ..., 10}, where 10 is the highest aesthetic rating. NIMA
then feeds the obtained logits through a Softmax function to
create a rating distribution, which is the score S.
We train NIMA with the Aesthetic Visual Analysis (AVA)
dataset [20], whose content ranges from blurry, low-quality
snapshots over artistic imagery and advertisements to high-
quality photography. 80 % of the dataset is used for training
and the remaining 20 % are equally split into validation and
test set. We follow the training procedure in [4].
We ﬁnd that training solely on the original AVA dataset
yields a Quality Assessor that is insensitive to illumination
changes, as they are highly under-represented in the dataset.
Therefore, we re-train NIMA’s dense layer with 3000 images
that are manually edited towards bad lightning and whose
ground truth scores are decreased to indicate the reduction
of aesthetics that comes with poor illumination.
Additionally, we introduce a preprocessing step called
Adaptive Brightness Normalization (ABN) to make all initial
images have similar brightness. ABN computes the perceived
image brightness P =
√
0.241R2 + 0.691G2 + 0.068B2 us-
ing the mean red, green, and blue pixel intensities [21]. With
P ∈ [0, 255], ABN normalizes the brightness to the range
359
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-761-0
ACHI 2020 : The Thirteenth International Conference on Advances in Computer-Human Interactions

128±30. If the image is too bright (P > 158), ABN evens out
the original histogram by linearly transforming the pixel values
and clipping its left and right side by 5.0 %. This percentage
is reduced if the Structural Similarity [22] is less than 0.8. If
P < 98, ABN brightens up the image by converting it to the
HSV color space and increasing the V-value by 20. As this
often introduces unwanted noise, ABN reduces the shift factor
if the Peak Signal-to-Noise Ratio [23] between the corrected
image and the original version is above 30. ABN also checks
for intended uses of white and black backgrounds, e.g., in
product photography. For this, ABN randomly samples 5 % of
the image’s pixels and checks if more than 60 % of the sampled
pixels are black or white. If this holds true, the image is not
modiﬁed to avoid washed out background colors.
C. Optimization Loop
The overall enhancement process of an image now works as
follows: The image is normalized using ABN and fed through
the Image Manipulator, which applies a set of ﬁlters with
initial parameters (zero, if not set by user) to the image I.
The image aesthetic of the resulting image is then scored by
the Quality Assessor. We now calculate the gradients of the
score w.r.t. the ﬁlter parameters K to optimize the parameters
via gradient descent towards the nearest local optimum. This
naturally ensures an iterative process a user can interact with
before, during, and after the optimization converges.
To optimize the image, NICER uses a loss function that
maximizes image beauty while balancing the ratio between
aesthetic gain and induced image change [11]. We hence
formulate the optimization loss as
L(K, I) = EMD(pt, pd)
|
{z
}
LQA
+ γ L2(K)
| {z }
LIM
,
(1)
where LQA is the loss that maximizes the Quality Assessor’s
score and LIM is a weighted regularization term (γ = 0.1) to
penalize large parameter changes by the Image Manipulator,
which might not be intended by the user. More speciﬁcally,
we deﬁne LQA to be the Earth Mover’s Distance (EMD)
[24] between the predicted rating distribution and a desired
distribution that corresponds to a highly aesthetic image. In
our implementation, a one-hot encoded target vector for the
largest score bucket would force the Quality Assessor to
extrapolate towards a “perfect” image, which it has never seen
during training. Therefore, we use a realistic target distribution
{0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.09, 0.15, 0.55, 0.20} that could
also be found in the AVA dataset. For Quality Assessors that
output a single scalar, LQA = −S can be used to maximize
the score S. NICER uses Stochastic Gradient Descent (SGD)
with Nesterov Momentum of 0.9 and a learning rate of 0.05.
D. Human in the Loop
Our approach allows users to intervene with the opti-
mization process at multiple stages: A user can set initial
ﬁlter intensities before the optimization loop. Adding the L2
regularization term to the global enhancement loss ensures
that the optimized parameters do not diverge too far from
the initial ﬁlter intensities set by the user. Also, by setting
the regularization weight γ, the user can control NICER’s
“diversity”, i.e., the strength with which divergence from the
initial ﬁlter parameters is penalized. A high γ could, e.g., be
used for ﬁne-tuning an already (subjectively) beautiful image.
A user can modify the ﬁlter parameters during the optimiza-
tion loop, since the gradients are calculated w.r.t. the current
parameters. Setting new parameters may lead to a new local
score optimum that the image is optimized towards. NICER
also provides the option of ﬁxing desired parameter values,
which prohibits further intensity updates for a ﬁlter and thus
ensures that the outcome is to the user’s individual liking.
A user can ﬁnally modify the parameter settings after the
optimization has converged. This is especially helpful if the
optimization yields an image that scores high regarding the
Quality Assessor, but tweaks are necessary to increase the
image’s subjective visual appeal.
IV. EXPERIMENTS
In this section, we conduct a user study to qualitatively as-
sess the performance of NICER. First, we show that the fully-
automatic enhancement without any user interaction improves
the quality of the original images. Second, we demonstrate that
user interactions can produce different enhancement outcomes
that improve the image’s subjective appeal.
A. Without User Interaction
While NICER is speciﬁcally designed to incorporate users
into the optimization process, it is also able to automatically
enhance images without any interactions. Showing that fully-
automatic enhancement results are perceived as more aesthetic
than the original images gives us a “lower-bound” that can then
be further improved by allowing user interactions.
In the user study, 51 subjects sit at a workstation and are
instructed to rate NICER’s automatically optimized images
(using γ = 0.1), comparing them to the unedited images and
versions that are obtained by choosing random ﬁlter intensities.
To this end, we use 500 randomly sampled images from [2]
and let each participant rate 30 image tuples which results in
a total of 1530 image ratings. The subjects are asked to rank
the images on a low-to-high scale, with the original reference
image centered in the middle of the scale.
The results show that our method is preferred over the
random baseline in 93.0 % of all cases. The subjects prefer
our enhancement result over the unedited original image in
53.7 % of all cases. To quantify the relative ratings, we map
the low-to-high scale to the interval [0, 10], where the original
image has a score of 5 and the other ratings are scaled
such that the best or worst rating has a score of 0 or 10,
respectively. The normalized rating histograms are shown in
Figure 3. NICER’s images receive a mean rating of 5.3 and a
median of 5.23. A 1-sample Wilcoxon test [26] shows that the
median is signiﬁcantly different from the original’s normalized
rating 5 in a conﬁdence interval of 1 %. This suggests that
our fully-automatic results on average are perceived more
beautiful than the unedited images. The high variance of the
360
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-761-0
ACHI 2020 : The Thirteenth International Conference on Advances in Computer-Human Interactions

6)
user step
2-1
user step
2-2
user result
2
user result
2
7)
3)
user result
3
user result
4
1)
2)
7)
8)
5)
9)
4)
Fig. 2.
The original image (left column, middle row) with user-deﬁned enhancements (red arrows) and auto-enhancement (blue arrows, default 50 steps,
γ = 0.1). The transformations (given in %) are: 1) Con = Sat = 20, ﬁxed. 2) Bri = 8, Sha = -13, Hig = 18, Exp = 24. 3) Con = Sat = 20 ﬁxed, NLD = 75
ﬁxed, Exp = 20. 4) & 6): Auto 5) Auto, γ = 0.5. 7) Auto, stopped by user after 10 steps. 8) NICER, γ = 0.005. 9) User post-correction, Hig reduced from
79% to 40%. Image from [25].
0
5
10
Normalized Rating
0
500
1000
Number of Votes
Random
0
5
10
Normalized Rating
0
100
Number of Votes
NICER
Fig. 3.
Normalized rating histograms for the random enhancement and
NICER’s automatic enhancement results. The original image always has a
score of 5.
automatic enhancements’ rating results (σ2 = 2.87) supports
our hypothesis that the perception of image beauty varies
greatly across subjects. To showcase NICER’s full potential,
we investigate the effects of directly involving the subjects in
the enhancement process.
B. With User Interaction
We have shown that NICER can obtain promising results
without user interaction. This section shows how different
interactions before, during, and after the optimization loop
produce remarkably different enhancement outcomes. To this
end, subjects choose interaction routes from NICER according
to their personal liking, starting from a baseline image. In
NICER, users have different interaction possibilities: manually
change ﬁlter settings, ﬁx single ﬁlter intensities such that they
are not optimized any further, or automatically optimize the
image from the current parameter settings for one or multiple
steps. Additionally, they can set the regularization weight
γ. One set of possible interactions is shown in Figure 2,
with not only four different outcomes that involve at least
one automatic enhancement step (4, 5, 7, 8), but also the
automatically edited version without user interaction (6). In
this experiment, some subjects prefer routes that lead to more
saturated looks, while others like high contrasts or slightly
tinted images better. A substantial 97.9 % of the subjects
agree that the achieved optimization results are better than the
unedited starting image (left column, middle row). Routes that
involve at least one of NICER’s automatic enhancement steps
are preferred by 68.1 %. This shows that combining automatic
enhancement with user guidance is a valid approach that yields
subjectively more beautiful results. Fully automatic approaches
do not necessarily lead to results that are subjectively aesthetic,
which is why NICER enables users to intervene with the
361
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-761-0
ACHI 2020 : The Thirteenth International Conference on Advances in Computer-Human Interactions

optimization process at any time and encourages users to bring
their individual, preferred style into the enhanced image.
V. DISCUSSION AND CONCLUSION
In this paper, we have presented a new method called
NICER to interactively edit and enhance images that allows
for the incorporation of user preferences before, during, and
after the enhancement process. The trained models and the
source code for NICER are available online [1].
NICER, being a ﬁrst implementation of the presented gen-
eral framework, has certain caveats and weak points that we
intend to address in the future: NICER runs in reasonable
time on a modern machine with a graphics card (1.36 s for
the full enhancement of a 1080p image), but might beneﬁt
from further optimization when used in hardware-constrained
environments like smartphones. As a ﬁrst optimization, our
enhancement routine rescales images to a width and height of
224 pixels during the parameter optimization and only applies
the found parameters once to the full-sized image at the end of
the optimization cycle. A further speedup could be achieved by
using a more lightweight Quality Assessor. While the Image
Manipulator could theoretically be replaced by a differentiable
image ﬁlter library (e.g., Kornia [27]), we explicitly renounced
from doing so, as using a neural network makes it possible to
not only learn single image ﬁlters, but whole editing styles.
This is especially helpful for casual users, who often lack the
photographic vocabulary to describe their desired outcome and
hence rely more on intuitive terms, such as “moodiness”, or
the indicated settings for a “sunset” atmosphere.
We evaluated our results in a user study and found that
NICER’s fully-automatic enhancement results usually outper-
form the unedited images. In a second experiment, we have
shown that NICER’s enhancement results in combination with
user interaction were favored by virtually all participants. In
the future, we plan to conduct further in-depth user studies
to examine the effects of different Image Manipulators and
Quality Assessors on NICER’s enhancement quality.
Since our approach allows users to intervene with the
enhancement process before, during, and after optimization,
NICER offers a ﬁrst step towards user-centered image editing
without reference images. Overall, we found our method and
implementation to be a promising start in this direction.
REFERENCES
[1]
M. Fischer, K. Kobs, and A. Hotho. (2020). NICER:
Neural
Image
Correction
and
Enhancement
Routine.
https://github.com/mr-Mojo/NICER, (visited on 10/11/2020).
[2]
V. Bychkovsky, S. Paris, E. Chan, and F. Durand, “Learning
photographic global tonal adjustment with a database of input
/ output image pairs,” in IEEE CVPR, 2011, pp. 97–104.
[3]
Z. Yan, H. Zhang, B. Wang, S. Paris, and Y. Yu, “Automatic
photo adjustment using deep neural networks,” ACM TOG,
vol. 35, no. 2, pp. 1–15, 2016.
[4]
H. Talebi and P. Milanfar, “Nima: Neural image assessment,”
IEEE Transactions on Image Processing, vol. 27, no. 8,
pp. 3998–4011, 2018.
[5]
X. Fu, J. Yan, and C. Fan, “Image aesthetics assessment using
composite features from off-the-shelf deep models,” in IEEE
ICIP, 2018, pp. 3528–3532.
[6]
S. Kong, X. Shen, Z. Lin, R. Mech, and C. Fowlkes, “Photo
aesthetics ranking network with attributes and content adapta-
tion,” in ECCV, Springer, 2016, pp. 662–679.
[7]
S. B. Kang, A. Kapoor, and D. Lischinski, “Personalization of
image enhancement,” in IEEE CVPR, 2010.
[8]
Y. Murata and Y. Dobashi, “Automatic image enhancement
taking into account user preference,” in IEEE CW, 2019,
pp. 374–377.
[9]
Y. Hu, H. He, C. Xu, B. Wang, and S. Lin, “Exposure:
A white-box photo post-processing framework,” ACM TOG,
vol. 37, no. 2, pp. 1–17, 2018.
[10]
A. Kapoor, J. C. Caicedo, D. Lischinski, and S. B. Kang,
“Collaborative personalization of image enhancement,” In-
ternational journal of computer vision, vol. 108, no. 1-2,
pp. 148–164, 2014.
[11]
H. Talebi and P. Milanfar, “Learned perceptual image enhance-
ment,” in IEEE ICCP, 2018, pp. 1–13.
[12]
P. D’Oro and E. Nasca. (2019). An empirical evaluation of
convolutional neural networks for image enhancement, [On-
line]. Available: https : / / github. com / proceduralia / pytorch -
neural-enhance (visited on 10/11/2020).
[13]
M. Aubry, S. Paris, S. W. Hasinoff, J. Kautz, and F. Durand,
“Fast local laplacian ﬁlters: Theory and applications,” ACM
TOG, vol. 33, no. 5, pp. 1–14, 2014.
[14]
D. Berman, S. Avidan, and T. Treibitz, “Non-local image
dehazing,” in IEEE CVPR, 2016, pp. 1674–1682.
[15]
F. Yu and V. Koltun, “Multi-scale context aggregation by
dilated convolutions,” arXiv preprint arXiv:1511.07122, 2015.
[16]
Q. Chen, J. Xu, and V. Koltun, “Fast image processing with
fully-convolutional networks,” in IEEE ICCV, 2017, pp. 2497–
2506.
[17]
B. Xu, N. Wang, T. Chen, and M. Li, “Empirical evaluation of
rectiﬁed activations in convolutional network,” arXiv preprint
arXiv:1505.00853, 2015.
[18]
The GIMP Team. (2020). GIMP - GNU Image Manipulation
Program, [Online]. Available: http://www.gimp.org (visited on
10/11/2020).
[19]
K. Simonyan and A. Zisserman, “Very deep convolutional
networks for large-scale image recognition,” arXiv preprint
arXiv:1409.1556, 2014.
[20]
N. Murray, L. Marchesotti, and F. Perronnin, “Ava: A large-
scale database for aesthetic visual analysis,” in IEEE CVPR,
2012, pp. 2408–2415.
[21]
D. R. Finley. (2020). Hsp color model — alternative to hsv
(hsb) and hsl, [Online]. Available: http://alienryderﬂex.com/
hsp.html (visited on 10/11/2020).
[22]
Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli,
“Image quality assessment: From error visibility to structural
similarity,” IEEE Transactions on Image Processing, vol. 13,
no. 4, pp. 600–612, 2004.
[23]
Q. Huynh-Thu and M. Ghanbari, “Scope of validity of psnr in
image/video quality assessment,” Electronics letters, vol. 44,
no. 13, pp. 800–801, 2008.
[24]
E. Levina and P. Bickel, “The earth mover’s distance is the
mallows distance: Some insights from statistics,” in IEEE
ICCV, 2001, pp. 251–256.
[25]
Pexels GmbH. (2020). Pexels, [Online]. Available: https : / /
www.pexels.com (visited on 10/11/2020).
[26]
B. Rosner, R. J. Glynn, and M.-L. T. Lee, “The wilcoxon
signed rank test for paired comparisons of clustered data,”
Biometrics, vol. 62, no. 1, pp. 185–192, 2006.
[27]
E. Riba, D. Mishkin, D. Ponsa, E. Rublee, and G. Bradski,
“Kornia: An open source differentiable computer vision library
for PyTorch,” in The IEEE Winter Conference on Applications
of Computer Vision, 2020, pp. 3674–3683.
362
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-761-0
ACHI 2020 : The Thirteenth International Conference on Advances in Computer-Human Interactions

