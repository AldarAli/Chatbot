Ontology based Spreading Activation for NLP related Scenarios
Wolf Fischer
Programming Distributed Systems Lab
University of Augsburg
Augsburg, Germany
wolf.ﬁscher@informatik.uni-augsburg.de
Bernhard Bauer
Programming Distributed Systems Lab
University of Augsburg
Augsburg, Germany
bauer@informatik.uni-augsburg.de
Abstract—To handle the ﬂood of information in the modern
world new technologies are needed. One problem is the han-
dling and ﬁltering of information itself. Semantic technologies
have been named to possess the potential to at least facilitate
this problem. Another difﬁculty is the representation of infor-
mation to humans. Different algorithms and user interface con-
cepts have been created allowing the access on a very speciﬁc
type and structure of information. However the most common
and natural way for humans is to use natural language. Natural
Language Processing tries to analyze the syntax and semantics
of language, but often delivers unsatisfying results because
of the many phenomena (e.g., ambiguity) humans use while
communicating. We therefore currently develop an approach,
which allows us to analyze the semantic content of natural
language text based on an ontology. In this paper we present
a spreading activation based algorithm, which not only helps
identify the correct semantic concepts for a natural language
text, but also partially solves other phenomena of natural
language.
Keywords-semantic. spreading activation. natural language.
ontology.
I. INTRODUCTION
Ontologies have provided a comfortable way to store
and perform reasoning on semantic information. Different
standards have been proposed in the past of which OWL
([1], [2]) became the de facto standard. The availability of
standards lead to ontologies being used even in big compa-
nies (e.g., the automotive sector). However this introduced
a new problem as a new source of information is stored
independently from all the other existing information. This is
especially a problem for natural language documents, which
contain the same or similar information as domain speciﬁc
ontologies.
Currently there are no concepts or components available
to bridge this gap, i.e., the gap between semantic and syntac-
tic information (we refer to syntactic information as meaning
both lexical and syntactic information). Ontologies only
contain semantic information, but lack the syntactic part. On
the other side documents contain a lot of information, which
is stored in natural language form. Todays natural language
processing components are capable of analyzing the syntac-
tic information with a certain degree of precision. However
this still leaves the question how semantic information can
be gathered from the documents and how this information
can be mapped to an ontology.
At the moment we are developing a prototype, which
creates a model, which links a given text to an ontology
(i.e. it does not extract new information from text, but try
to link different types of information, i.e., natural language
documents and ontologies). However there are many chal-
lenges because of the different types of ambiguities humans
tend to use while writing. Many of those problems can only
be solved during runtime i.e., during the analysis process.
For example identifying the correct concept for a given form
requires context and background knowledge. In our case this
knowledge exists within an ontology. The ’easiest’ case is if
one word is mapped to several different concepts and one of
these concepts is the correct one (e.g., ’bank’ might mean
the ﬁnancial institute as well as a physical object, which is
used for sitting). However in other cases humans tend to
either use more abstract forms for what they actually mean
(e.g., they refer to just ’the car’ however they refer to their
very own type of car). Also they could use a word, which
has nothing to do with what they actually mean (e.g., in a
sentence like ’I drive a red one’, ’red’ can be an indication
towards a speciﬁc car, which is colored red). As can be seen
by those simple examples there are many different cases in
which it is not trivial to identify the correct meaning of a
word.
We are currently developing a concept, which tries to
solve this problem. A consistent meta model, which com-
bines semantic and syntactic information at an early stage
has already been presented ([3], [4]). We have developed
an algorithm based on spreading activation (i.e., a marker
distribution within a graph like data structure), which helps
us solving exactly those problems as mentioned before. The
algorithm itself is not a complete Word Sense Disambigua-
tion (WSD) algorithm, but represents a core part of it as
our WSD is based on calculating the semantic relatedness
between concepts. Some more details are given in Section
III.
This paper is structured as follows: Section II presents
related work. Next, Section III gives a short introduction
in our previous work, on which this algorithm is based
on. Section IV speciﬁes the requirements our concept has
56
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-175-5
SEMAPRO 2011 : The Fifth International Conference on Advances in Semantic Processing

to fulﬁll. In Section V the concept is presented, before
in Section VI several examples demonstrate the working
mechanism of the algorithm. The paper is concluded in
Section VII.
II. RELATED WORK
Spreading Activation is a famous approach in many
different areas, e.g., in cognitive linguistics as well as WSD.
The latter is closely related to our problem (as mentioned
in the introduction), therefore we will especially delimit our
concept from WSD approaches.
The most closely related concept to our approach seems to
be that of Kleb and Abecker ([5]), which disambiguate word
senses based on RDF graphs. They state homonymy and
synonymy as their main problems (whereas we differenciate
some more problems as stated in the introduction). Their
approach does however not directly regard the problem
of overgeneralization as well as words, which reference a
seemingly unrelated concept at ﬁrst.
Tsatsaronis et al. ([6], [7]) describe a spreading activation
based approach, which uses the information from a thesauri
to create a spreading activation network (SAN) for WSD.
Their concept is used to disambiguate complete sentences
at once. The background knowledge used is from WordNet
2. Their approach is not capable of ’guessing’ better suited
concepts than those, which have already been found. In ([8])
Tstsaronis et al. further evaluate the state of the art of using
spreading activation for WSD. They state that concepts,
which use semantic networks show the best results.
Other approaches to WSD are seen by Agirre et al. ([9]),
which use a PageRank based algorithm to disambiguate
word senses in the biomedical domain. Kang et al. [10]
created a semi-automatic, domain independent approach to
WSD (whereas we focus on speciﬁc domains). An ontology
is created semi-automatically and then used for disambiguat-
ing the words of a given sentence by ﬁnding a least weighted
path through the concepts within the ontology. In contrast
to our approach they seem to be limited regarding the
identiﬁcation of the correct sense for seemingly not related
words (e.g., ’red’ can still refer to ’car’) as they rely on
WordNet only.
Spreading Activation has been used in other domains as
well. Hussein et al. ([11]) used it for context adaptation.
Therefore they model application domains within an ontol-
ogy and after each user action an activation ﬂow through
the network ﬁlters those nodes, which are seemingly most
important to the current circumstances.
III. BASICS
Our approach is based on a consistent meta model
combining semantic with syntactic information ([3], [4]).
Our algorithm uses the semantic information available and
automatically identiﬁes the most probable concepts at hand.
Based on this, syntactic structures can be mapped to speciﬁc
semantic structures.
Our prototype gets as an input a natural language text,
which is ﬁrst being preprocessed (i.e., tokenized, POS tagged
and then a syntax tree is being created). Afterwards this
information is used to parse the syntax tree bottom-up and
create new semantic information based on previous informa-
tion. To check how existing information can be combined
the algorithm takes the ontology into consideration. The
focus of this paper is exactly on that step of the analysis.
The algorithm is called with two or more concepts and
returns a value indicating the semantic relatedness. Further
it might determine concepts, which might be better suited
based on the context of the original input concepts. These
new concepts will then be integrated into the solution set.
The best concepts with respect to a global solution are then
selected as part of an evaluation in the following steps.
The ﬁnal result is a semantic model of the initial input
text, i.e., it contains, which words of the text correspond to
which concept in the ontology. Further the relations of the
concepts as indicated by the text are stored in the semantic
interpretation result.
The algorithm in this paper is therefore a key component
within our overall analysis process and has a great inﬂuence
on the outcome of the result. Its working mechanism is
described in the following sections.
IV. REQUIREMENTS
As mentioned previously there are several cases, in which
it is difﬁcult to identify what concepts a human might have
related to. The following gives a short overview of the
requirements our approach has to fulﬁll.
1) Analyzing text requires disambiguating the senses of
each word. Therefore it is necessary to have some
kind of measurement indicating if different concepts
are semantically related to each other. We assume that
this information helps us in solving the WSD problem.
Therefore the algorithm should return a value between
0 and 1, which indicates if speciﬁc information is
available within the ontology and how closely it is
related. 1 should indicate that there deﬁnitely is such a
relation available. 0 means that no information could be
found. This is important for disambiguating synonyms
and homonyms in general.
2) As humans tend to overgeneralize their expressions
(e.g., instead of talking about ’E3’ in Figure 1 they
talk of their ’Car’) our concept should be capable of
identifying the most speciﬁc information possible (hy-
ponym), i.e., if a human talks about a ’Car’, but further
mentions speciﬁc attributes (e.g., the color ’Red’), it is
clear to his communication partner, which type of car is
meant (i.e., the ’E3’). This process should be mimiced
by the concept.
57
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-175-5
SEMAPRO 2011 : The Fifth International Conference on Advances in Semantic Processing

3) Humans sometimes only mention speciﬁc attributes of
what they actually refer to, i.e., in contrast to the
previous requirement they don’t mention the ’Car’
concept, but may only refer to the car by one of its
attribute. An example could be ’I drive a red one’.
Still the listener knows what the speaker most probably
meant (a car or here again the ’E3’). The concept should
try to identify and solve this problem.
The last two requirements can be summed up by saying
that although some concepts might not be linked to the
correct word or the semantic relation is missing between two
concepts it should still be possible to identify the actually
meant concepts of the user. Such a task is difﬁcult to achieve.
Usually algorithms ’only’ identify the most likely concepts
for a given text out of a set of directly available concepts.
Since many algorithms are based on WordNet only, domain
speciﬁc information might not be available, which could
indicate a relation between ’Car’, ’E3’ and ’Red’. Statistical
WSD concepts, which rely on n-grams might in some cases
be capable of handling this problem. However they require
that a fact has to be stated at least once in textual form to
correctly disambiguate a speciﬁc context.
V. CONCEPT
The algorithm is separated into three different phases:
Initialize tokens, create token ﬂow and analyse token ﬂow.
All phases will be explained in the following sections.
A. Deﬁnitions
For our concept we need an Ontology O := (C, R, G),
where C is a set of concepts, R deﬁnes a set of relations
between the concepts in C and G deﬁnes a set of general-
izations links between the concepts in C. The algorithm is
initialized using an input I := (cs, cy, ct, Sc), where cs is
the source concept, cy is the concept of a relation, which
has cs as its source (e.g., ’Drive’ would be the concept of
a relation between ’Driver’ and ’Vehicle’) and ct speciﬁes
the target of the relation of cy. Finally Sc := c1..cn is a
set of further concepts, which act as additional information
(context) to the spreading process. I can also consist of
(cs, cy) or (cs, ct) only. Sc is always optional.
A token container a is deﬁned by the tuple (c, T, act, d).
a is associated with a concept c ∈ O (this is also the ID of
the token container) and a set of tokens T := t1..tn. It
basically acts as a container for all the tokens, which have
reached the speciﬁc concept c. It further contains an attribute
act (we will refer to attributes like a.act in the following),
which indicates if the concept a.c has been a part of the
spreading activation input I (if we talk about a being part of
I or another set of concepts in further references, we actually
mean a.c, which should be contained in the corresponding
concept set). d represents the depth of the tokens concept
c within the ontologies generalization hierarchy. The depth
value is calculated as the position of c relative to the length
of the longest branch it is located in. In the following we
will refer to as as the container of cs, at as the container of
ct and ay as the container of cy.
A
token
t
is
deﬁned
by
the
tuple
(orig, start, pos, e, s, dir). t.orig holds a reference to
its original container (this must be a container of one of
the concepts in I). Next, it contains a reference t.start
to the container where it originally started from (this can,
but does not have to be the original container; it may also
be a container whose concept is related to the concept of
t.orig via generalization). t.pos is the container, which
represents the current position of the token. t.e indicates the
remaining energy of the token (if the energy drops below
a certain threshold this token can not spread any further).
t.s describes the steps the token has already traveled within
the ontology. t.dir deﬁnes the direction a token is traveling
in. Values can be up / down (within the generalization
hierarchy) or sidewards (i.e., on an association).
B. Initialize tokens
The algorithm is initialized based on each c ∈ I with
Algorithm 1 (e.g., INIT(cs, 1.5)). As can be seen the
initialization is based on the generalization hierarchy of
the corresponding concept. All concepts of I are basically
treated the same (i.e., their energy value is the same). The
only exception is cs, which receives a higher initial energy
value than the remaining elements. The cause for this is that
we especially want to know if there is a path from the source
to the target concept. Therefore tokens from cs receive a
higher energy, which allows them to travel further.
As can be seen in algorithm 1 the initialization is done
going in both generalization directions (INITGENUP
means that the initialization is done up the generalization
hierarchy, i.e., more general elements are initialized, whereas
INITGENDOWN initializes more speciﬁc elements).
This is done because humans tend to be ambiguous while
communicating and often use more generalized terms than
they actually mean (see requirement 2 in IV). Only the
context of a word helps in deciding, which concept they
actually refer to. Therefore, the call down the hierarchy helps
to initialize all elements, which eventually are meant by
a human. In contrast the call upwards initializes all those
elements, which may contain the corresponding semantic
information that the current concept c inherited from them.
This information is necessary in order to correctly analyze
the current input.
INITGENUP initializes a single concept and its gen-
eralization hierarchy upwards by creating a container for
every concept in the upwards generalization hierarchy and
further creating the initial tokens for each of these concepts
(INITGENDOWN works analogously). It is important
that every concept, which will be reached by a call of
INITGENUP in the generalization hierarchy is treated
as being a part of the original input. Therefore the a.act
58
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-175-5
SEMAPRO 2011 : The Fifth International Conference on Advances in Semantic Processing

attribute of their containers will be set to true. The cause
for this is that each of these concepts could be the carrier
of the information we will later on be searching for.
Algorithm 1 Initialization
procedure INIT(c, ENERGY )
INITGENUP(c, c, ENERGY , null)
INITGENDOWN(c, c, ENERGY , null)
end procedure
C. Create token ﬂow
The set of initial tokens has been created. Now the
token ﬂow itself has to be generated. The overall process
is shown in algorithm 2. As can be seen the process
itself is discretized in single phases. Each current token
generation Tcurrent leads to a new token generation Tnext,
which will only be processed after every token from the
current generation has been processed. This methodology is
important as the POSTPROCESS call initializes a back
propagation mechanism. A non discretized process would
yield indeterministic results.
CREATETOKENFLOW gets the set of current as
well as next tokens. For every single token in Tcurrent it
does the following: First it checks if the t.pos, t.dir and
t.e attributes allow a next step. If t.dir is unknown, it is
allowed to travel both on associations (sidewards) as well
as on generalizations (up / down). A token is however not
allowed to go up, if it was going down before. Also it may
not go up if it was going sidewards before. The cause for
these restrictions is that the tokens elseway could reach not
necessary or false concepts.
Next new tokens are being generated for the next step
of the current token (i.e., tokens for the relation itself as
well as the target of the relation) and added to the Tnext
set. The energy of the new tokens is based on the current
tokens t.e attribute and is being decreased by a ﬁxed value.
However if the container of the relation has been activated
(i.e., a.act == true), no energy will be subtracted from the
energy of the new token. This process allows us to enhance
the energy of paths, which are likely to be more relevant to
the spreading activation input.
Next the POSTPROCESS method is called. It starts
the back propagation mechanism on all containers whose
a.act attribute is set to true and have received new to-
kens in the last token ﬂow phase. Each token on such
a container then gains an increase of its energy value:
t.e = t.e + (EMAX − t.e) ∗ Ce, where EMAX denotes the
maximum energy a token can have and Ce is a constant
factor between 0 and 1. This mechanism is recursively
continued on the predecessor of this token. By activating
the propagation mechanism on such containers, which are
probably relevant to the input (again a.act == true), only
such token path are strengthened, which seem to indicate the
most likely results. The cause for this is that the concepts
we search for are most likely closely connected (i.e., there
are only few relations and therefore few steps to get from
one concept to another) and also super- or subtypes of the
original input concepts cs, cy and ct.
Algorithm 2 Process Tokens
procedure PROCESSTOKENS
while Tnext.size ̸= 0 do
Tcurrent ← Tcurrent ∪ Tnext
Tnext ← {}
PREPROCESS
CREATETOKENFLOW(Tcurrent, Tnext)
POSTPROCESS
Tcurrent ← {}
end while
end procedure
D. Analyze token ﬂow
The ﬁnal step consists of gathering the results from the
token ﬂow process. We ﬁrst start by identifying more speciﬁc
elements of the actual input (see Section IV). For this we
ﬁrst collect all containers for every c ∈ I, which are more
speciﬁc than c. Next we sort them based on the number
of relevant tokens, which arrived there (i.e., tokens from
concepts of I/c), their token weight (higher is better),
activation times (i.e., how often the container was activated
in the POSTPROCESS method, more is better) and the
depth of their concept (deeper is better). We then pick the
best element from this list. This then is the more speciﬁc
element of cm. However in case that we ﬁnd too many
elements, which might be relevant to our criteria we don’t
pick any elements as this would contradict the idea of
specifying the initial input.
All information necessary for the ﬁnal result has been
computed. However it might be the case that this result
might not be perfect, i.e., the initial input was ambiguous
(because of ambiguous statements of a human speaker, e.g.,
requirement three in Section IV). For such a situation we
have developed a heuristic, which identiﬁes this case and
tries to identify a better solution. First there are however
some restrictions to be made: Such an ’imperfect’ situation
can only be identiﬁed if cs, cy and ct are provided in I.
In other cases there would be too few information, which
would lead the heuristic to imprecise decisions. Further only
situations in which either cs or ct are wrong can be detected.
For the following we will use the example from Section IV
in, which case ct is wrong (as it references ’Red’ instead of
’Car’).
We ﬁrst collect all available associations, which are of
type cy and reference ct. Those are stored in a list Ap.
Ap is then sorted based on the weight of the associations
source and target container weights (i.e., the weight of
59
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-175-5
SEMAPRO 2011 : The Fifth International Conference on Advances in Semantic Processing

the containers based on the tokens, which arrived there).
Now the algorithm looks if one of the associations in
Ap has a source and a target, which matches cs or ct:
(r.s ⊆ cs ∨ r.s ⊇ cs) ∧ (r.t ⊆ ct ∨ r.t ⊇ ct), where r.s
is the source concept of a relation of Ap and r.t is the
target concept. If this is the case the algorithm seemingly
has been used on a correct input and the spreading activation
is ﬁnished. If however no association of Ap matches this
condition, the algorithm will be reinitialized. For this the
best association of Ap (i.e., the one with the highest source
and target container weights) is used because based on
the current token ﬂow this association has been marked as
the best possible match. Now the spreading activation is
reinitialized with a new I′:
1) The ’wrong’ concept (either cs or ct) will be replaced
with the new concept (r.s or r.t) of the best association
of Ap (in our example this means that ct ’Red’ will be
replaced with r.t ’Car’. A more elaborate example will
be given in Section VI).
2) The old element (cs or ct) will be added to the
list of context elements, as it might provide helpful
information for the next spreading activation iteration.
This is done because the user might have had a reason
to mention this speciﬁc concept initially therefore the
concept is not thrown away, but used as a context
concept).
Regarding
the
example
from
Section
IV,
I
was
(Person, Drive, Red, {})
and
I′
is
now
(Person, Drive, Car, {Red}). With I′
the process is
now being restarted and the same steps are applied as
described before. If in this second iteration a seemingly
correct result could be found the algorithm will return it.
If however the conditions for starting the heuristic would
match again, we stop the process. We then return the best
result from both iterations. This has proved to provide good
results.
Finally a value is computed, which indicates if the infor-
mation we searched for exists within the ontology. There are
two different cases to be distinguished:
1) The ﬁrst case occurs, if the heuristic did not step in,
i.e., the initial source and target elements are still the
same. Then a token t from at is searched, which has
t.start == a.s, i.e., it happens to have the source
container as its starting position. If such a token could
be found the computation of the ﬁnal value depends on
the average energy of the token regarding the length of
the token path (excluding the generalization).
2) The second case happens if the original source or target
containers have been exchanged for a new container. If
this is the case, the value depends on the semantic sim-
ilarity (based on a lowest common ancestor approach)
between the initial cs / ct concepts from I and the
current, ’new’ c′
s / c′
t from I′ concepts.
VI. CASE STUDY
Due to the structure of our concept there are no known
gold standards for our case, as existing ones like Senseval or
Semeval are difﬁcult to use for us. Senseval-2 for example
provides texts, which have been annotated with WordNet
2. However WordNet is a lexical database and therefore
mainly contains linguistic information, not domain relevant
semantic information. Other stochastically motivated test
data is not suited for our scenario at all. We can therefore not
provide any elaborate statistical evaluations yet. Therefore
we focus on some actual examples from our test scenario.
Our scenario currently consists of an ontology with about
100 concepts. We will show some different examples in
detail in the following section. Figure 1 shows a simpliﬁed
excerpt from this ontology. Its structure describes a simple
car domain, which contains drivers (driving cars), different
cars with different colors (E2, E3), another car E1, which
has problems with its engine. Further a CEO is supposed to
drive speciﬁc cars (the E2 and E3).
The ﬁrst request will show the resolution of overgener-
alization. ’Driver Drives E2’ is supposed to detect if the
concept ’Driver’ is related to ’E2’ using a relation of type
’Drives’. As can be seen in the picture there is a ’Drives’-
relation from ’Driver’ to ’Car’, which is the supertype of
E2. However there is also a more speciﬁc information,
which could state exactly the same and in this case is even
shorter: ’CEO Drives E2’. As the ’CEO’ is a subconcept
of ’Driver’, it will be activated in the inialization phase
and will itself spread tokens. As ’Drives’ is also activated
the token will pass with no loss of energy to ’E2’. The
same is the case for the token, which will arrive at ’E2’
from ’Driver’. However, this one needed more steps for its
’journey’. After the spreading activation has ﬁnished the
algorithm checks every initial starting element for more
concrete information. ’Driver’ is the only concept, which
contains a subconcept. As there are enough hints (due to
backpropagation as described in Section V) that ’CEO’
might be a better suited alternative to the initial request, the
algorithm proposes ’CEO’ as an alternative for ’Driver’ to
the user. As there is a direct relation available, the semantic
value of the request is calculated to be 1.
A more complex request is the triple ’Driver Drives Red’,
i.e., a concept ’Driver’ is connected to a concept ’red’ using
a relation of the type ’Drives’ (such a request could be the
case in a sentence like ’The driver drives a red one’). As
can be seen in Figure 1, ’Car’ is related to color and ’E3’ is
related to ’Red’. If the spreading activation starts the tokens
will spread through the network and due to backpropagation
the ’Car’ concept receives a signiﬁcantly higher energy than
the remaining elements, as it is part of an important path
between ’Driver’ and ’Red’ / ’Color’. As we are searching
for a triple the algorithm ’sees’ that there is no direct relation
available between ’Driver’ and ’Red’. However the ’Car’
60
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-175-5
SEMAPRO 2011 : The Fifth International Conference on Advances in Semantic Processing

element could be matching, as one relation has ’Driver’
as its start concept and ’Drive’ as its type. Therefore the
algorithm reinitializes itself and replaces the ’Red’ element
with the ’Car’ element (’Red’ becomes a context element).
In the second pass tokens from ’Driver’ as well as ’CEO’
will reach ’Car’ as well as ’E3’. Tokens from ’E3’ will
reach ’Red’. Backpropagation will then again lead to an
increase of energy in ’E3’ and ’CEO’. As the algorithm
could successfully solve the initial request it proposes ’E3’
instead of ’Red’ and ’CEO’ instead of ’Driver’. The semantic
similarity of the request however is weighted with 0.75
because we can not be absolutely sure that the user really
meant ’Car’ with ’Red’.
Figure 1.
Example of our ontology
VII. CONCLUSION
The biggest problem is the knowledge acquisition problem
as it is the case with every knowledge intensive system.
Especially the creation of an ontology, which provides a
good representation of the corresponding domain is a huge
problem. We try to tackle this one by creating a correspond-
ing set of tools and workﬂows, which allow an easy and
semi-automatic process for this task.
In this paper we have presented a spreading activation
based algorithm, which works directly on a domain ontology
without creating its own SAN. It helps us in solving the
WSD problem and in certain cases also proposes concepts,
which are more likely to be meant instead of the initial input
concepts.
The algorithm is still ongoing work and its prototypical
implementation is constantly being used within our frame-
work for creating semantic interpretations of natural lan-
guage text. As such it delivers good results in our scenarios.
Especially its feature of ’guessing’ better suited concepts
greatly helps in interpreting natural language text with all
its ambiguities.
REFERENCES
[1] P. Hitzler, B. Parsia, P. F. Patel-Schneider, and S. Rudolph,
“OWL
2
Web
Ontology
Language
Primer,”
Director,
no.
October,
pp.
1–123,
2009.
[Online].
Available:
http://www.w3.org/TR/2009/REC-owl2-primer-20091027/
[2] W. Ontology, “OWL 2 Web Ontology Language Document
Overview,” October, vol. 2, no. October, pp. 1–12, 2009.
[Online]. Available: http://www.w3.org/TR/owl2-overview/
[3] W.
Fischer
and
B.
Bauer,
“Combining
Ontologies
And
Natural
Language,”
Proceedings
of
the
Sixth
Australasian Ontology Workshop, 2010. [Online]. Available:
http://www.ncbi.nlm.nih.gov/pubmed/21409794
[4] W.
Fischer
and
B.
Bernhard,
“Cognitive-Linguistics-
based Request Answer System,” in Adaptive Multimedia
Retrieval. Understanding Media and Adapting to the User.
Madrid: Springer, 2011, pp. 135–146. [Online]. Available:
http://www.springerlink.com/content/l426675knx75765m/
[5] J.
Kleb
and
A.
Abecker,
“Entity
Reference
Res-
olution
via
Spreading
Activation
on
RDF-Graphs,”
The
Semantic
Web
Research
and
Applications,
vol.
6088,
pp.
152–166,
2010.
[Online].
Avail-
able: http://www.springerlink.com/index/10.1007/978-3-642-
13486-9
[6] G. Tsatsaronis, M. Vazirgiannis, and I. Androutsopoulos,
“Word Sense Disambiguation with Spreading Activation
Networks Generated from Thesauri,” in IJCAI 2007, M. M.
Veloso, Ed., 2007, pp. 1725–1730. [Online]. Available:
http://dblp.uni-trier.de/rec/bibtex/conf/ijcai/TsatsaronisVA07
[7] G. Tsatsaronis, I. Varlamis, and M. Vazirgiannis, “Word
Sense Disambiguation with Semantic Networks,” Work,
vol.
5246,
pp.
219–226,
2008.
[Online].
Available:
http://www.springerlink.com/content/87p101317131078t
[8] G.
Tsatsaronis,
I.
Varlamis,
and
K.
Nø
rv˚ag,
“An
experimental study on unsupervised graph-based word sense
disambiguation,” Computational Linguistics and Intelligent
Text Processing, pp. 184–198, 2010. [Online]. Available:
http://www.springerlink.com/index/N577Q110122R04J6.pdf
[9] E. Agirre, A. Soroa, and M. Stevenson, “Graph-based word
sense disambiguation of biomedical documents.” Bioinfor-
matics, vol. 26, no. 22, pp. 2889–2896, 2010. [Online].
Available: http://www.ncbi.nlm.nih.gov/pubmed/20934991
[10] S. Kang and J. Lee, “Ontology-based word sense disam-
biguation using semi-automatically constructed ontology,” MT
Summit VIII Machine Translation in the Information Age
Proceedings Santiago de Compostela Spain 1822 September
2001 pp181186 PDF 287KB, 2001.
[11] T. Hussein, D. Westheide, and J. Ziegler, “Context-adaptation
based on Ontologies and Spreading Activation,” Citeseer,
2005.
61
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-175-5
SEMAPRO 2011 : The Fifth International Conference on Advances in Semantic Processing

