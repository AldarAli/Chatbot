Collision Estimation Using Single Camera 
Discussion under the condition of constant velocity 
 
Ryunosuke Ikeno1, Kazuyuki Ito2 
Dept. of Electrical and Electronics Engineering 
Hosei University 
Tokyo, Japan 
e-mail: ryunosuke.0416@gmail.com, ito@hosei.ac.jp 
 
 
Abstract— Robots and autonomous vehicles that operate in 
complex dynamical environments have attracted considerable 
attention in recent years. Avoiding obstacles and finding a 
passable route to the destination is one of the important and 
basic functions of mobile robots. In this paper, we take our cue 
from the obstacle-avoidance mechanism in animals, and 
discuss how to enable robots to find a passable route using only 
two-dimensional images from a single usual camera without 
any distance information. We simulate the dynamic changes in 
visual information and obtain the necessary conditions for a 
mobile robot to determine impending collisions with an 
obstacle in its path. 
Keywords- Autonomous vehicles; Estimation of collision; 
visual information single camera. 
I. 
 INTRODUCTION 
In recent years, robots and autonomous vehicles that 
operate in complex dynamical environments such as daily 
life have attracted considerable attention. Autonomous 
navigation of automobiles or personal robots is among these 
fertile areas of research. Conventional robots were designed 
for well-known, simple environments, such as a factory. 
Thus, conventional methodology does not work effectively 
in designing robots that operate in unknown dynamic 
environments. 
Considerable research has been conducted to solve this 
problem and develop effective autonomous robots. DARPA 
ground challenge [1] and Google car [2] are one of them. In 
these conventional studies, the most common approach to 
such a problem is to create three-dimensional models of the 
environment in question. In this approach, robots have 
sensors, such as a laser range-finder, to measure distances to 
obstacles and create a precise three-dimensional model of the 
environment [3]-[7]. A passable route is obtained by 
calculations that use information from the three-dimensional 
model. While this approach is effective, the mechanism to 
obtain the passable route is very different from that of 
animals. Even lower-level animals and insects can act 
quickly to avoid obstacles, in spite of their small brains. 
They have no distance sensor and their brain is too small to 
find a passable route to their destination as quickly as they 
do. How these animals are nonetheless able to do so is still 
an open question. However, research in ecological 
psychology has revealed that animals can evaluate the time-
to-contact for an object in their path based solely on 
available visual information and without any distance 
information. Information about the time-to-contact is 
perceived directly and no complex computation is 
presumably required. 
In this paper, we focus on this mechanism in animals and 
discuss how to enable robots to find a passable route to their 
destination using two-dimensional images from a single 
camera without any distance information. We simulate the 
dynamic changes in visual information and establish the 
necessary conditions to accurately determine impending 
collisions with an obstacle. 
II. 
MODEL AND DEFINITION 
We define the global coordinate system and the view 
coordinate system as shown in Fig. 1 and Fig. 2, respectively. 
Table 1 elaborates on the meaning of the symbols used in 
these figures. The robot (camera) moves on the y-axis at a 
constant velocity and the direction of the camera is fixed, as 
shown in Fig 1(b). There is a static obstacle in the robot’s 
path, and the position of the obstacle in the global coordinate 
system is converted to that in the view coordinate system by 
(1) and ( 2). 
 
 
(a)  Side view                            (b) Top view 
Figure 1. Global coordinate system. 
 
 
Figure 2. View coordinate system. 
camera
obstacle
obstacle
camera
f
o
X
View coordinate 
system
o
W
o
114
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-352-0
INTELLI 2014 : The Third International Conference on Intelligent Systems and Applications

TABLE I. DEFINITION 
 
f 
Focal length 
W 
Width of the body 
(           ) 
Position of the obstacle in the view 
coordinate system 
(             
Position of the obstacle in the global 
coordinate system 
(     ) 
Position of the camera in the global 
coordinate system 
 
     
  
     
                                                   
     
  
     
                                                   
 
 
Figure 3. Temporal response of the edges of object. 
 
Fig. 3 illustrate the temporal response of the edges of the 
object in the view coordinate system. In this study, we focus 
on this temporal change, and discuss how to determine 
collision to the obstacle. 
III. 
SIMULATION 
A. Case of ignoring body size 
In this subsection, we assume that the body of the robot 
is a point and has no width. We conduct simulations for two 
cases, as shown in Fig. 4 and Fig. 5. Fig. 4 shows the case 
where the robot comes into contact with the obstacle and Fig. 
5 represents one where the robot successfully evades it. In 
both cases, the robot moves in a straight line at the same 
constant velocity. By comparing these results, we can learn 
how to estimate future crashes using the visual information 
at hand. 
 
Figure 4. Case A-1: Robot contacts the obstacle. 
 
Figure 5. Case A-2: Robot does not contact the obstacle. 
 
Fig. 6 and Fig. 7 show the results of our simulation. These 
graphs show the change in position of the obstacle relative to 
the robot’s visual coordinate system as it moves forward. 
From these results, it is obvious that if both sides of the 
obstacle appear to expand outwards from the robot’s visual 
perspective, the robot will hit the obstacle. On the other hand, 
if both sides of the obstacle appear to move in one direction 
from the perspective of the robot’s visual coordinate system, 
the robot can evade the obstacle. 
 
Figure 6. Result of Case A-1 
 
 
Figure 7. Result of Case A-2 
Y
X
o
t
o
t
X
camera
obstacle
view range = 100 [deg]
Velocity
V = 3 [km/h]
o
Velocity
v= 3 [km/h]
camera
obstacle
o
5
6
7
8
9
10
11
12
13
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
Time [s]
X
sigth
5
6
7
8
9
10
11
12
13
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
Time [s]
X
115
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-352-0
INTELLI 2014 : The Third International Conference on Intelligent Systems and Applications

B. Case of considering body size 
In this subsection, we assume that the body of the robot 
has a certain non-negligible width. We conduct simulations 
with the robot at different distances from the obstacle, as 
shown in Fig. 8 and Table 2. In all cases, the robot moves in 
a straight line at the same constant velocity.  
 
Figure 8. Case B: Robot has a certain width 
 
TABLE II. POSITION OF THE OBSTACLE 
 
Case B-1 
              
Case B-2 
              
Case B-3 
              
Case B-4 
              
Case B-5 
              
 
Figure 9. Result of Case B-1 
 
Figure 10. Result of Case B-2 
 
Figure 11. Result of Case B-3 
 
Figure 12. Result of Case B-4 
 
Figure 13. Result of Case B-5 
 
Figs. 9 to 13 show the simulation results. In all cases, 
both sides of the obstacle appear to move in the same 
direction from the robot’s visual perspective. Thus, 
according to the condition stated in Section III.A, the robot 
should pass the obstacle by. However, in this simulation, the 
width of the robot’s body is 2m, because of which it hits the 
obstacle in cases B-3, B-4 and B-5. From these results, we 
can conclude that the condition in Section III.A is not 
Velocity
v [km/h]
camera
obstacle
W = 2 [m]
o
5
6
7
8
9
10
11
12
13
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
Time [s]
X
5
6
7
8
9
10
11
12
13
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
Time [s]
X
5
6
7
8
9
10
11
12
13
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
Time [s]
X
5
6
7
8
9
10
11
12
13
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
Time [s]
X
5
6
7
8
9
10
11
12
13
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
Time [s]
X
116
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-352-0
INTELLI 2014 : The Third International Conference on Intelligent Systems and Applications

sufficient for the robot to avoid the obstacle when it is 
sufficiently large in size.  
To take the size of the robot’s body into consideration, 
we focus on the temporal changes in values of XL and XR, the 
horizontal coordinates that demarcate the width of the 
obstacle, with the robot’s motion. Fig. 14 shows the temporal 
change in the value of XL. We see that this change depends 
on the distance between the robot and the obstacle. Therefore, 
we can calculate if a collision will occur from the change in 
the horizontal coordinates of the obstacle relative to the 
robot’s motion, if the width of the robot’s body and its 
velocity are known. 
 
Figure 14. Temporal change of XL for different settings of xL 
 
C. Case of different velocities 
In this subsection, we consider the influence of the 
velocity of the robot on obstacle avoidance. The simulation 
setting is the same as in Section III.B, except for the velocity 
of the robot. 
 
 
Figure 15. Temporal changes of XL for different velocities 
  
Fig. 15 shows the results. We see that the temporal 
change in XL depends on the robot’s velocity in addition to 
its distance from the obstacle. Thus, to determine if a 
collision will occur, the velocity of the robot relative to the 
obstacle is also required.  
Animate living beings do not have velocity sensors. They 
estimate relative velocity using visual information. Thus, the 
determination that a collision will occur or that a passable 
route that avoids obstacles is available can only be made 
through visual information.  
Our future research in this area will focus on the 
calculation of relative velocity using available visual 
information. 
IV. 
CONCLUSION 
In this paper, we discuss a method for a mobile robot to 
detect impending collision with an obstacle in its path and to 
find a passable route using only two-dimensional images 
from a single usual camera without any distance information. 
We simulated the dynamic changes in visual information and 
determined the necessary conditions to determine future 
collisions with an obstacle. 
Our future research will be geared towards estimating the 
relative velocity of a mobile robot with respect to obstacles 
using visual information, and applying it to determine 
impending collisions. 
 
ACKNOWLEDGEMENT 
This research was partially supported by the Japan 
Society for the Promotion of Science through the Grant-in-
Aid for Scientific Research (C) 24500181.  
 
REFERENCES 
[1] U. Ozguner, C. Stiller, and K. Redmill, "Systems for safety and 
autonomous behavior in cars," The DARPA Grand Challenge 
experience. Proceedings of the IEEE, 2007, vol. 95, 2, pp. 397-412. 
[2] “How Google’s Self-Driving Car Works – IEEE Spectrum,” 
Spectrum.ieee.org, Feb. 26. 2013. 
[3] S.Thrun, Robotic Mapping: A Survey,” CMU Technical Report, 
CMU-CS-02-111, School of Computer Science, Carnegie Mellon 
University, Pittsburgh, PA, 2002. 
[4] M.  
Montemerlo, S. Thrun, D. Koller, and B. Wegbreit, 
“FastSLAM: A Factored Solution to the Simultaneous Localization 
and Mapping Problem, ”  Proc. AAAI National Conference on 
Artificial Intelligence (Edmonton 2002), 2002, pp. 593–598. 
[5] A. Diosi, and L. Kleeman, “Laser Scan Matching in Polar 
Coordinates with Application to SLAM,” IEEE/RSJ International 
Conference on Intelligent Robots and Systems (IROS 2005), 2005, pp. 
3317–3322. 
[6] M. Bosse, P. Newman, J. Leonard, and S. Teller, “Simultaneous 
Localization and Map Building in Large-Scale Cyclic Environments 
Using the Atlas Framework,” The International Journal of Robotics 
Research, Dec. 2004, vol. 23, 12, pp. 1113–1139. 
[7] A. Nguyen, N. Martinelli, Tomatis, and R. Siegwart, “A Comparison 
of Line Extraction Algorithms using 2D Laser Rangefinder for Indoor 
Mobile Robotics,” Int. Conf. Intelligent Robots and Systems, 2005, 
pp. 1929–1934. 
 
5
6
7
8
9
10
11
12
13
0
1
2
3
4
5
6
7
8
9
10
Time [s]
Temporal change of XL
6
8
10
12
14
16
18
20
0
1
2
3
4
5
6
7
8
9
10
Time [s]
Temporal change of XL
v = 2 [km/h]
v = 3 [km/h]
v = 4 [km/h]
117
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-352-0
INTELLI 2014 : The Third International Conference on Intelligent Systems and Applications

