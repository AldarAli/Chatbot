Soniﬁcation of Large Datasets in a 3D Immersive Environment:
A Neuroscience Case Study
Panagiota Papachristodoulou, Alberto Betella
SPECS, NRAS, Universitat Pompeu Fabra
Barcelona, Spain
Paul F.M.J. Verschure
SPECS, NRAS, Universitat Pompeu Fabra and ICREA, Instituci´o Catalana de Recerca i Estudis Avanc¸ats
Barcelona, Spain
Email: paul.verschure@upf.edu
Abstract—Auditory display techniques can play a key role in
the understanding of hidden patterns in large datasets. In this
study, we investigated the role of soniﬁcation applied to an
immersive 3D visualization of a complex network dataset. As
a test case, we used a 3D interactive visualization of the so
called, connectome of the human brain, in the immersive space
called “eXperience Induction Machine (XIM)”. We conducted
an empirical validation where subjects were asked to perform
a navigation task through the network and were subsequently
tested for their understanding of the dataset. Our results showed
that soniﬁcation provides a further layer of understanding of the
dynamics of the network by enhancing the subjects’ structural
understanding of the data space.
Keywords-soniﬁcation; XIM; networks; neuroscience; complex
data; auditory display.
I.
INTRODUCTION
Visual representations of data (also called visual displays)
have a long successful history in scientiﬁc research. They have
been employed widely and commonly in most of the traditional
HCIs [1] playing a key role in uncovering hidden structures
and meaning in massive collections of data. However, relying
solely on visual representations of information can present
some limitations when dealing with large amounts of mul-
tidimensional data [2]. In some cases, visual representation of
data can also lead to cognitive saturation [3].
Humans have the capability to detect subtle temporal
patterns in sounds [4]. Due to the auditory system’s integrative
properties, sound can be efﬁciently used to enhance visual
representations without creating information overload for the
user [5]. In addition, sound might be more appropriate to
convey dynamical information as opposed to vision [6].
For this reason, in the last few decades, scientists have been
introducing auditory cues as a means to convey information,
giving rise to a new ﬁeld called auditory display. As a subset
of this, the ﬁeld of soniﬁcation has emerged with the purpose
of providing a new tool for the comprehension of complex data
and generate new insights. A number of soniﬁcation models
and techniques have been developed [7], [8], [9] with the
purpose of converting data into meaningful information.
Many studies have investigated the role of sound for
the representation of complex datasets [10], [11], [12], [13].
However, most of these studies focused on the effectiveness of
visual and auditory displays separately, yet a few among them
have investigated the role of sound as enhancer of a graphical
visualization (i.e., multimodal display) [14], [15], [16], [17].
Here, we investigated how soniﬁcation may function when
integrated in a multimodal display of large neuroscience
datasets such as the connectome [18]. We previously de-
veloped the so called XIM Neuroscience application[19], a
3D interactive framework to visually represent and explore
the massive connectivity of the human connectome, which
is “a comprehensive structural description of the network of
elements and connections forming the human brain”[18]. For
the purpose of our experiment, we displayed the Neuroscience
application in XIM, an immersive room equipped with a
number of sensors and effectors that have been constructed
to conduct experiments in mixed reality [20].
Our hypothesis was that the introduction of sound in
a multimodal display could enhance the understanding of
complex data describing the brain.
We paired the 3D visualization of a connectome dataset
[21] in XIM with a soniﬁcation system intended to represent
the changes in the dynamics related to different regions of
the network, thus providing the user with an extra channel of
information. For the empirical evaluation, we assessed whether
sound could augment the understanding of the network’s
dynamics.
This paper is composed of three different sections. Section
II introduces the methods and materials along with the soniﬁ-
cation model that was adopted. Section III presents the results
of the experiments and section IV provides an overview of the
conclusions.
II.
METHODS
A. The “eXperience Induction Machine (XIM)”
The XIM [20] is a multi-user mixed-reality space covering
a surface area of 5.5 x 5.5m equipped with a number of sensors
and effectors. XIM effectors include computer graphics content
projected via 8 projectors on 4 separate walls, a luminous
35
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-325-4
ACHI 2014 : The Seventh International Conference on Advances in Computer-Human Interactions

interactive ﬂoor, movable lights and soniﬁcation system. For
the purposes of this project, four projectors were used as a
visual display of the connectome network on 4 separate walls
and two speakers in the left and right corners of the room to
project the auditory display.
B. The Neuroscience Application
The Neuroscience application is a system that was
previously
built
in
XIM
[19]
that
uses
multimodal
input and output and permits the embodied exploration
of connectome datasets. In this study, we adopted the
connectome dataset from Hagmann et al. [21] (data source
http://www.cmtk.org/datasets/homo sapiens 01.cff).
The
dataset is composed of 998 regions of interest (ROIs), 28000
unidirectional connections and 66 anatomical subregions in
the left and right hemisphere of the human brain.
Figure 1.
3D visualization of the connectome in XIM.
The connectome network is visually represented in XIM.
Edges are represented as tubes mapped to different shades
of green in accordance to their strength. Nodes are repre-
sented as spheres and are highlighted when the region they
belong to is selected by the user. The network’s anatomical
regions contain the nodes and are spatially organized in
accordance with a standard brain atlas (Talairach coordinates
of ROIs[22]) (Fig. 1). The system is implemented using Unity
3D (http://unity3d.com).
C. Soniﬁcation methods
The goal of our soniﬁcation was to acoustically represent
changes in the connectome network’s parameters while navi-
gating through its different structures.
First, we analyzed the distribution of the values of the
parameters in the network in terms of a) number of nodes,
b) number of connections, c) average strength of each region
and d) brain hemisphere.
Second, two distinct sound sources were chosen whose
parameters were mapped to the network’s parameters: a) a
short sound sample (grain sound of 16 ms), acoustically
perceived as a “click” and b) a soft multi-layered ambient
sound, which could be characterized as a drone sound. For each
one of the 66 brain regions, the sound parameters used for the
soniﬁcation were: a) the repetition rate (playback frequency)
of the grain sound, which was mapped to the number of nodes,
b) the pitch of the ambient sound, mapped to the number of
connections, c) the loudness of the grain sound, representing
the average strength of each region and d) panning, to identify
the brain hemisphere (Table I).
TABLE I.
MAPPING BETWEEN THE NETWORK’S CHARACTERISTICS
AND THE CORRESPONDING SOUND PARAMETERS.
Connectome Char-
acteristics
Sound source
Sound Parameter
Number of connec-
tions
Sound grain of 16
ms
Rate of repetition
Number of nodes
Ambient
sound
at
380 Hz enhanced by
a pure sine wave at
the same frequency
Pitch
Average Strength
Sound grain of 16
ms
Loudness
The
soniﬁcation
engine
was
built
using
Pure
Data
(http://puredata.info/) and the communication between Unity
and Pure Data was implemented using OpenSoundControl
(OSC) messages.
1) Nodes: A narrow-band short sound sample with the
duration of 16ms was chosen for the soniﬁcation of the nodes.
The idea was to take inspiration from granular synthesis (a
method that operates on the microsound time scale [23]) and
simulate laboratory sound recordings of neural activity. These
microsound characteristics allowed the auditory display of the
micro-structures of the brain. By doing so, we associated the
ROIs in the brain (i.e., the nodes of the network) to a sound
texture composed of a large number of sound grains. The
number of nodes determined the time interval between the
grains, which deﬁned the number of clicks that would be
played back by our soniﬁcation engine.
The number of nodes was mapped to an interval that
determined the metro (or rate) of the playback in an inversely
proportional relation (i.e., lower number of nodes resulted in
larger time intervals between the grains and vice versa). In
order to avoid a mechanical and isochronous repetition of
the grain sound, the output in milliseconds was also used as
input for a random function. Likewise, a low number of nodes
resulted in large range of intervals of the random function. The
output of this function was used as a delay time, which was
added to the ﬁnal playback repetition.
2) Connections: Previous literature validated the role of
pitch in the soniﬁcation of complex data for the purpose
of different tasks [11], [12]. In addition, pitch offers large
resolution since the human auditory system is capable of
detecting very subtle changes in a very large range of fre-
quencies [24]. For this reason, we mapped the pitch to the
number of connections of each connectome region. A sound
parameter with a large resolution, such as the pitch allowed
for the auditory presentation of a wide range of number of
connections.
We adopted a soft ambient sound sample to provide a more
pleasant auditory experience to the user than a pure sine wave
[25]. First, a spectral analysis was conducted with SonicVisu-
aliser (http://www.sonicvisualiser.org), which revealed that the
most prominent frequency of the sample was at 380Hz. This
value was adopted to deﬁne the upper limit of the frequency
range. Hence, the number of connections was mapped to a
frequency range spanning from 60Hz to 1200Hz. Frequencies
above 1200Hz would have resulted in an unpleasant and
disturbing experience for the user.
To accomplish the sound mapping in our soniﬁcation
engine, we used a pitch shift object in pure data. This object
36
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-325-4
ACHI 2014 : The Seventh International Conference on Advances in Computer-Human Interactions

shifts the frequencies of the sound resulting in lower and
higher ﬁnal pitch. Three groups were deﬁned for the whole
range of numbers of connections. Each group was mapped
separately into a subset of values using linear mappings,
which depended on the distribution of the connections values
in the network. These values were used to transpose the
sound through the pitch shift function resulting in the ﬁnal
fundamental frequency. Finally, the amplitude of the sound
was adjusted according to the equal - loudness contours [24].
This correction ensured that all the frequencies would present
the same level of loudness.
3) Strength: Loudness is a perceptual sound parameter that
has been used previously in soniﬁcation models [12]. For
this reason, we used loudness to represent the strength of
the connections in the network. Since the manipulation of the
amplitude and the frequency of the same sound source simulta-
neously could lead to unperceived and confusing changes [26],
we chose to modulate the loudness of grain sound used for the
nodes, as opposed to the ambient sound of the connections.
We calculated the average strength for each region, which
was mapped to the amplitude of the grain sound. According
to the literature, “the most effective use of loudness change
usually occurs when changes in loudness are constrained to
two or three discrete levels that are mapped to two or three
discrete states of the data being soniﬁed. In this way, discrete
changes in loudness can be used to identify categorical changes
in the state of a variable or to indicate when a variable has
reached some criterion value” [27]. For this reason, 3 groups
of average strength were deﬁned for low, medium and high
levels of loudness so that the user would easily perceive the
changes from one group to the other.
We adopted a linear mapping for the three groups. We de-
ﬁned a lower and upper limit for each group and implemented
a scaling of the values of average strength into root mean
square (rms) values of the sound wave. The rms values were
then multiplied by the signal in order to determine the value
of the amplitude of the grain sound. Additionally, a reverb was
added to the higher values to enhance the perception of higher
strength for the correspondent regions of the network. The
scaling factor that was used for the mapping in rms depended
on the distribution of the values in the network. Thus, for the
ﬁrst two groups the variation was smoother and for the third
group it was more rapid.
4) Brain Hemispheres: For the brain hemispheres we used
sound panning. This allowed for the discrimination between
left and right hemisphere since the direction of the sound
source makes the understanding of the location of the re-
gions in the network more intuitive [24]. Binary signals were
received from Unity, which were then mapped to the two
speakers through the pan object in pure data.
D. Empirical Evaluation
The aim of this study was to assess whether sound could
enhance the understanding of the network’s characteristics
displayed in XIM and their properties during a navigation task
through the network.
Our hypothesis was that soniﬁcation would enhance the
accuracy in the estimation of the characteristics of the connec-
tome, thus providing a higher understanding of its dynamical
changes and a more precise discrimination between the brain
regions. By implementing a multimodal experiment using
soniﬁcation, based on two contrasting sound textures (i.e.,
sound grains and a long drone), we aimed to empirically
validate how sound could be used as an integrative tool to
enhance the human ability to detect structural aspects of the
connectome.
25 healthy adults (15 females, mean age=29.53, SD ±5.6)
with normal or corrected vision and hearing were recruited.
The subjects had no prior knowledge of the connectome dataset
that they were exposed to during the experiment.
To avoid between-subjects differences in auditory recogni-
tion skills, the experiment followed a paired-samples design,
where each subject was exposed (in random order) to two
conditions: a) only visualization and b) both visualization and
soniﬁcation. The independent variable was the presence of
sound (i.e., soniﬁcation condition) versus the absence of sound
(visualization condition), while the dependent variable was the
quantitative estimation of the properties related to different
regions in the network during a navigation task.
E. Experimental setup
The subjects’ task was to estimate the number of nodes,
connections and the average strength of each brain region and
to understand the changes of these values during the navi-
gation. They were presented with different brain regions and
were asked to mark their answers in closed-ended questions.
The experiment consisted of a training session, followed by
two experimental sessions.
The XIM was setup and equipped to provide an immersive
experience to the subjects. The lights were dimmed, a table and
a comfortable chair were placed in the middle of the room at
a symmetrical distance from the speakers to allow a balanced
auditory experience for all the subjects. The navigation was
done with a standard keyborad and mouse.
1) Regions selection: We preselected a number of brain
regions to be presented during the training and the ﬁrst session.
14 regions of the network were chosen for the training session,
resulting in 7 trials for each condition. The subjects were
exposed to regions with similar properties (number of nodes,
connections and average strength) in both conditions.
For the ﬁrst experimental session, we selected 36 regions
in total for both conditions resulting in 18 trials (9 pairs of
regions) for each condition. For each one of the 3 parameters
(nodes, connections, average strength) at least one pair of re-
gions presented evident acoustical difference in the parameters
(because of the high difference in their values).
2) Training Session:
Visual information displays (e.g.
graphs) are often familiar to the users given their former
education (they are taught since young age) and informal
experience [28]. However, complex auditory displays can be
not as intuitive as visual representations. For this reason,
most of the soniﬁcation studies indicate the importance of the
training sessions [29], where the subjects can get acquainted
with the network’s characteristics and their corresponding
visual and auditory meaning.
The task of the subjects during the training session con-
sisted in the estimation of the values of the connectome’s char-
acteristics for the preselected regions. Speciﬁcally, the subjects
37
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-325-4
ACHI 2014 : The Seventh International Conference on Advances in Computer-Human Interactions

were asked to estimate the number of nodes, connections and
average strength of each region and in which hemisphere the
region was located. The regions were presented in random
order in both conditions. After the exposure to a region,
subjects were asked to mark their answers in a closed-ended
questionnaire presenting different value ranges in accordance
with the properties of each area and their distribution in the
network. Subsequently, subjects were given feedback on their
answers so that they could learn their possible mistakes.
3) Session 1: In the ﬁrst experimental session, the subjects
were exposed to pairs of different regions (as speciﬁed in Sec-
tion II-E1 ). First, they were informed about the characteristic
they had to evaluate (number of nodes, connections or average
strength). Then, they were asked to choose whether the second
region presented had a higher or lower value of the parameter
measured compared to the ﬁrst region. After the presentation
of each pair, they were asked to mark with plus or minus their
answer in a questionnaire. Subjects where allowed to ask for
a repetition of the trial in case they wanted to listen (and see)
one more time the pair of regions.
4) Session 2: In the second experimental session, subjects
were asked to navigate freely through the connectome. Their
task consisted in ﬁnding a region with certain values (e.g.,
a region with low number of connections between 200-400
connections, a region with medium-high number of nodes
between 20-30 nodes, etc.) as asked by the experimenter at
the beginning of each trial. There were no time constraints
and subjects were given a printed reference table showing
the minimum and maximum values for each one of the
parameters. After each answer, subjects were given feedback
and proceeded to the following question. The task was repeated
twice, once for each condition. Each subject completed 6 trials
for each condition, resulting in a total of 12 trials.
5) Score attribution: We deﬁned the score attribution cri-
teria for the two experimental sessions. For each subject, we
calculated a score for each one of the parameters, along with
an overall score per condition. Table II summarizes the tasks
and the score attribution criteria for each one of the sessions.
TABLE II.
TASK FOR EACH SESSION AND SCORE ATTRIBUTION
CRITERIA.
Session
Task
Score
Attribution
Criteria
Session 1
Estimation of higher
and lower values be-
tween pairs of re-
gions
Correct
answers
were attributed with
1, otherwise with 0
Session 2
Finding
regions
with
certain
characteristics
in
predetermined
ranges of values
Error deviation from
correct
answer
in
predetermined scale
designed ad hoc for
the evaluation
For the ﬁrst session, the score was based on the number
of correct and wrong answers. A score of 1 was assigned to
the questions answered correctly and 0 to wrong answers. A
total score from 0 to 9 was calculated for each of the two
conditions for each subject. In the second session, we measured
the absolute distance from the correct answer [30] and divided
the resulting value by the number of possible answers, thus
obtaining normalized scores between 0 and 1, representing the
maximum and the minimum score respectively. We computed
these values according to the following formula:
βi = 1 − |i − k|
N
(1)
where β is the normalized score, i the subject’s answer,
k the correct answer and N the total number of probable
answers.
III.
RESULTS
During Session 1, we measured the subjects’ accuracy in
estimating the higher or lower values for the three parameters
of the network (number of nodes, number of connections and
average strength) between pairs of regions. The data collected
in session 1 was submitted to a Wilcoxon test between the two
conditions (visualization and soniﬁcation). The correct answers
for both conditions were calculated. The soniﬁcation condition
obtained a signiﬁcantly higher score (z=-2.96, p < .05) as
opposed to the visualization condition (Fig. 2a). The means
of the correct answers of the subjects for each one of the
parameters of the network are presented in Table III.
Wilcoxon tests were conducted for each one of the param-
eters between the two conditions. Although not statistically
signiﬁcant, the means for the soniﬁcation condition were
higher than their counterparts in the visualization condition
(Table III).
Since the experiment followed a paired samples design, the
data were tested for order effect. No order effect was found.
TABLE III.
MEANS OF THE SCORES FOR THE CORRECT ANSWERS
BETWEEN THE TWO CONDITIONS.
Session 2 Estimation of higher or lower values
Visualization
Soniﬁcation
Mean
Standard
Deviation
(SD)
Mean
Standard
Deviation
(SD)
Nodes
2.32
0.80
2.41
0.60
Connections
2.12
0.97
2.48
0.82
Average Strength
1.76
0.66
2.16
0.99
Total
6.00
1.52
7.00*
1.57
We conducted a Spearman’s correlation test between the
musical background of the subjects and the their estimations
for higher and lower values between pairs of regions in the
soniﬁcation condition. The results revealed a signiﬁcant neg-
ative correlation between the correct answers of the subjects
and their musical background (ρ=-0.37, p < .05). Subjects
with higher formal musical training scored lower than sub-
jects who didn’t have a musical background. These results
explain the skewness towards the lower score of the score
distribution in the soniﬁcation condition (Fig. 2a). As a follow
up, we conducted a further Wilcoxon test by excluding from
the analysis the scores of the subjects with higher formal
music training, seven subjects were excluded. We obtained
signiﬁcantly higher scores for the soniﬁcation condition as
compared to the visualization condition (z=-2.44, p < .05)
and a normal distribution in the scores (Fig. 2b).
During Session 2, we measured the accuracy of the subjects
to ﬁnd a region that would present speciﬁc values of each one
of the three parameters (number of nodes, connections and
average strength). For this experimental session, a dependent
38
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-325-4
ACHI 2014 : The Seventh International Conference on Advances in Computer-Human Interactions

Sonifcation
Visualization
10
8
6
4
2
0
*
Sonifcation
Visualization
10
8
6
4
2
0
*
Sonifcation
Visualization
10
9
8
7
6
15
*
a) Session 1
b) Session 1 
c) Session 2 
Figure 2.
a) Boxplot showing the signiﬁcantly higher score (p < .05) obtained for the soniﬁcation condition in Session 1. b) Boxplot showing the reduction
of skewness in the data for Session 1, after the exclusion of subjects with high level of musical background. c) Boxplot showing the signiﬁcant score obtained
for the soniﬁcation condition in Session 2 (p < .001).
T-test was conducted for the total scores obtained from the
error deviation measurements. The normalized scores for each
subject were calculated for each one of the regions successfully
found by the subject through the navigation in the network.
The average score was calculated for each subject in both
conditions. A signiﬁcantly higher score was obtained for the
soniﬁcation condition (t(24)=-5.03, p < .001) when compared
to the visualization condition (Fig. 2c).
Wilcoxon tests for the scores of the separate parameters
were conducted. The scores obtained were higher for the
soniﬁcation condition for all the parameters.
TABLE IV.
MEANS OF THE SCORES OBTAINED FROM THE ERROR
DEVIATION FOR THE CORRECT ANSWERS IN BOTH CONDITIONS. THE
STATISTICALLY SIGNIFICANT RESULTS ARE INDICATED WITH AN
ASTERISK.
Session 3 Error deviation
Visualization
Soniﬁcation
Mean
Standard
Deviation
(SD)
Mean
Standard
Deviation
(SD)
Nodes Low
0.94
0.09
0.96
0.10
Nodes High
0.90
0.14
0.95
0.11
Connections Low
0.48
0.35
0.85*
0.21
Connections High
0.83
0.21
0.87
0.13
Strength Low
0.77
0.29
0.87
0.13
Strength High
0.83
0.16
0.84
0.14
Total
7.15
0.62
8.03*
0.54
In addition, we found a statistically signiﬁcant difference
in score between the two conditions for the regions with low
number of connections. The soniﬁcation condition obtained a
signiﬁcantly higher score (z=-3.42, p < .05) compared to the
visualization (Table IV).
IV.
CONCLUSION
In this study, we investigated the effect of soniﬁcation
on the visualization of a complex network dataset. As a test
case, we used a 3D interactive visualization of the human
brain connectome displayed in the mixed reality “eXperience
Induction Machine (XIM)”.
We added a dynamic soniﬁcation to the 3D visualization
of the connectome dataset and we conducted an experiment,
where subjects were asked to perform a navigation task
through the network and subsequently were tested for their
understanding of the connectome dataset.
The parameters of the connectome network (i.e., nodes,
connections, average strength and brain hemisphere) were
mapped to repetition rate, pitch, loudness and panning, respec-
tively and the estimation of the values in the two conditions
(absence and presence of sound) was measured.
The empirical validation consisted of a training session
followed by two experimental sessions. After the training,
in the ﬁrst session we measured the accuracy in estimating
the higher or lower values of the three parameters between
pairs of regions in the network. In the second session, we
measured the accuracy of the subjects in ﬁnding regions
within given range of values in the network parameters. The
soniﬁcation condition obtained signiﬁcantly higher scores in
both experimental sessions.
Furthermore, the analysis of the data collected in the sec-
ond experimental session showed that subjects obtained higher
scores for the regions with low number of connections in the
soniﬁcation condition during the free navigation through the
network. In complex networks, such as the human connectome,
connections (or edges) can’t be easily discriminated visually.
Here, we have shown that the pitch sound parameter can act
as an enhancer in the understanding of structural connections.
The analysis of the results collected during the second
session revealed that subjects obtained lower scores for the
strength in the soniﬁcation condition for speciﬁc regions.
This may be due to the interaction of the sound parameters
(repetition rate and loudness). When an area presented high
number of nodes associated to low strength, the interactions
of these two parameters could have confused the subjects and
these regions may have been associated to higher values of
average strength. Further research on the interaction of the
sound parameters will provide better understanding of the
auditory perception.
In addition, we found a negative correlation between the
scores obtained in the soniﬁcation condition and the musical
formal training of the subjects. In the literature, there is no
agreement about the relation of the musical background in
auditory tasks. One explanation of this apparently counterin-
tuitive result, as suggested by Walker and Nees [31], may lie
on the fact that the skills acquired while undertaking musical
39
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-325-4
ACHI 2014 : The Seventh International Conference on Advances in Computer-Human Interactions

studies, especially during the childhood, could be removed
after many years. Another explanation for this result could be
due to the fact that the measurement of musical background
through a single question does not account for an accurate
assessment; a more speciﬁc test to measure musical abilities
should be administered.
In conclusion, the presence of sound enhanced the perfor-
mance of the subjects (in terms of structural understanding of
a network), hence we retain our alternative hypothesis.
Further improvements will include the soniﬁcation of ad-
ditional parameters of the networks (hubs, clusters, etc.) along
with the use of audio descriptors and spectral modelling tech-
niques to validate which features, among the sound parameters,
are more effective in the understanding of a complex network
dataset.
V.
ACKNOWLEDGEMENT
The research leading to these results has received funding
from the European Community’s Seventh Framework Pro-
gramme (FP7-ICT-2009-5) under grant agreement n. 258749
[CEEDS]. The Generalitat de Catalunya (CUR; Departament
d’Innovaci´o, Universitats i Empresa) and the European So-
cial Fund are supporting this research. Thanks to Sebastian
Mealla, Enrique Mart´ınez Bueno, Jonatas Manzolli, Xerxes D.
Arsiwalla, Anna Mura.
REFERENCES
[1]
B. J. Fry, “Computational Information Design,” Sciences New York,
vol. Ph. D, no. May 1997, 2004, p. 170.
[2]
A. Eldridge, “Issues in auditory display.” Artiﬁcial Life, vol. 12, no. 2,
2006, pp. 259–274.
[3]
N. Bearman and P. F. Fisher, “Using sound to represent spatial data in
ArcGIS,” Computers & Geosciences, vol. 46, Sep. 2012, pp. 157–163.
[4]
T. Hermann and H. Ritter, “Listen to your Data : Model-Based Soni-
ﬁcation for Data Analysis,” Advances in Intelligent Computing and
Multimedia Systems, vol. 8, 1999, pp. 189–194.
[5]
G. Kramer, B. N. Walker, T. Bonebright, P. Cook, J. H. Flowers,
N. Miner, and J. Neuhoff, “Soniﬁcation report: Status of the ﬁeld and
research agenda,” Quantum, 1999, p. 38.
[6]
A. S. Bregman, Auditory Scene Analysis: The Perceptual Organization
of Sound.
MIT Press, 1990.
[7]
T. Hermann, “Taxonomy and deﬁnitions for soniﬁcation and auditory
display,” in Proceedings of the 14th International Conference on Audi-
tory Display, Paris, France, 2008, pp. 1–8.
[8]
A. de Campo, “Toward a data soniﬁcation design space map,” Pro-
ceedings of the 13th International Conference on Auditory Display,
Montreal, Canada, 2007, pp. 342–347.
[9]
T. Hermann and H. Ritter, “Sound and Meaning in Auditory Data
Display,” in Proceedings of the IEEE, vol. 92, no. 4, 2004, pp. 730–741.
[10]
S. Pauletto and A. Hunt, “Interactive soniﬁcation of complex data,”
International Journal of Human-Computer Studies, vol. 67, no. 11, 2009,
pp. 923–933.
[11]
B. N. Walker, “Magnitude Estimation of Conceptual Data Dimensions
for Use in Soniﬁcation,” Journal of Experimental Psychology, vol. 8,
no. 4, 2002, pp. 211–221.
[12]
J. H. Flowers, “Thirteen Years of Reﬂection on Auditory Graphing :
Promises , Pitfalls , and Potential New Directions,” in Proceedings of
ICAD 05-Eleventh Meeting of the International Conference on Auditory
Display, Limerick, Ireland, 2005, pp. 1–5.
[13]
T. Hermann, “Soniﬁcation for Exploratory Data Analysis,” Ph.D. dis-
sertation, Bielefeld University, Bielefeld,Germany, 2002.
[14]
S. Bly, “Presenting information in sound,” in ACM CHI ’82 Conference
on Human Factors in Computer Systems.
ACM, New York, 1982, pp.
371–375.
[15]
P. Janata, “Marketbuzz: Soniﬁcation of real-time ﬁnancial data,” Pro-
ceedings of ICAD 04-Tenth Meeting of the International Conference
on Auditory Display, Sydney, Australia, 2004, pp. 4–10.
[16]
S. C. Peres and D. M. Lane, “Auditory graphs: The effects of redundant
dimensions and divided attention,” in Proceedings of ICAD 05-Eleventh
Meeting of the International Conference on Auditory Display, Limerick,
Ireland, 2005, pp. 169–174.
[17]
K. V. Nesbitt and S. Barrass, “Evaluation of a Multimodal Soniﬁcation
and Visualisation of Depth of Market Stock Data,” in Proceedings of
the 2002 International Conference on Auditory Display, Kyoto, Japan,
2002, pp. 1–5.
[18]
O. Sporns, G. Tononi, and R. K¨otter, “The human connectome: A
structural description of the human brain.” PLoS computational biology,
vol. 1, no. 4, 2005, p. e42.
[19]
A. Betella, E. Mart´ınez, R. Zucca, X. D. Arsiwalla, P. Omedas,
S. Wierenga, A. Mura, J. Wagner, F. Lingenfelser, E. Andr´e, D. Mazzei,
A. Tognetti, A. Lanat`a, D. De Rossi, and P. F. M. J. Verschure,
“Advanced interfaces to stem the data deluge in mixed reality: placing
human (un)consciousness in the loop,” in ACM SIGGRAPH 2013
Posters, ser. SIGGRAPH ’13.
New York, NY, USA: ACM, 2013,
pp. 68:1—-68:1.
[20]
U. Bernardet, S. Bermudez i Badia, and P. F. Verschure, “The expe-
rience induction machine and its role in the research on presence,” in
Proceedings of the 10th Annual International Workshop on Presence,
2007, pp. 329–333.
[21]
P. Hagmann, L. Cammoun, X. Gigandet, R. Meuli, C. J. Honey, V. J.
Wedeen, and O. Sporns, “Mapping the structural core of human cerebral
cortex.” PLoS biology, vol. 6, no. 7, 2008, p. e159.
[22]
J. J. Talairach, P. Tournoux, and O. Missir, Referentially oriented
cerebral MRI anatomy : an atlas of stereotaxic anatomical correlations
for gray and white matter / Jean Talairach, Pierre Tournoux, with the
collaboration of Odile Missir ; [translated from the French original
manuscript by Baris Tu.
Stuttgart ; New York : G. Thieme Verlag
; New York : Thieme Medical Publishers, 1993.
[23]
C. Roads, Microsound.
The MIT Press, 2004.
[24]
S. Carlile, “Psychoacoustics,” in The Soniﬁcation Handbook, T. Her-
mann, A. Hunt, and J. G. Neuhoff, Eds.
Logos Publishing House,
2011, ch. 3, pp. 41–61.
[25]
L. M. Brown, S. Brewster, R. Ramloll, M. Burton, and B. Riedel,
“Design guidelines for audio representation of graphs and tables,”
in Proceedings of the 9th International Conference on Auditory Dis-
play (ICAD2003), E. Brazil and B. Shinn-Cunningham, Eds., Boston
University Publications Production Department.
Boston University
Publications Production Department, 2003, pp. 284–287.
[26]
J. G. Neuhoff, G. Kramer, and J. Wayand, “Pitch and loudness interact
in auditory displays: can the data get lost in the map?” Journal of
experimental psychology. Applied, vol. 8, no. 1, 2002, pp. 17–25.
[27]
J. G. Neuhoff, “Perception, Cognition and Action in Auditory Displays,”
in The Soniﬁcation Handbook, T. Hermann, A. Hunt, and J. G. Neuhoff,
Eds.
Logos Publishing House, 2011, ch. 4, p. 6385.
[28]
S. Ferguson and D. Cabrera, “Exploratory sound analysis: Sonifying
data about sound,” in Proceedings of the 14th International Conference
on Auditory Display, Paris, France, 2008, pp. 1–8.
[29]
T. L. Bonebright and J. H. Flowers, “Evaluation of Auditory Display,”
in The Soniﬁcation Handbook, T. Hermann, A. Hunt, and J. G. Neuhoff,
Eds.
Logos Publishing House, 2011, ch. 6, pp. 111–144.
[30]
B. Walker, J. Lindsay, J. Godfrey, and C. Street, “The Audio Abacus :
Representing Numerical Values with Nonspeech Sound for the Visually
Impaired,” ACM SIGACCESS Accessibility and Computing, vol. 77-
78, 2004, pp. 9–15.
[31]
B. Walker and M. A. Nees, “Theory of Sonication,” in The Soniﬁcation
Handbook, T. Hermann, A. Hunt, and J. G. Neuhoff, Eds.
Logos
Verlag, 2011, pp. 9–40.
40
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-325-4
ACHI 2014 : The Seventh International Conference on Advances in Computer-Human Interactions

