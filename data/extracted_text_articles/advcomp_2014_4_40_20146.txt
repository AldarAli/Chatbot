Classiﬁcation of Pattern using Support Vector Machines: An Application for Automatic
Speech Recognition
Gracieth Batista∗, Washington Silva† and Orlando Filho†
∗Student of Electrical Engineering
Federal Institute of Maranh˜ao, S˜ao Luis, Maranh˜ao, Brazil 65053-155
Email: gracieth.cavalcanti@hotmail.com
†Department of Electrical and Electronics
Federal Institute of Maranh˜ao
Email: washington.wlss@ifma.edu.br and orlando.rocha@ifma.edu.br
Abstract—This paper proposes the implementation of a Support
Vector Machine (SVM) for automatic recognition of numerical
speech commands. Besides the pre-processing of the speech signal
with mel-ceptral coefﬁcients. Also, this paper is used to Discrete
Cosine Transform (DCT) to generate a two-dimensional matrix
used as input to SVM algorithm for generating the pattern of
words to be recognized. The Support Vector Machines represent a
new approach to pattern classiﬁcation. SVM is used to recognize
speech patterns from the mean and variance of the speech signal
input through the two-dimensional array aforementioned, the
algorithm trains and tests those data showing the best response.
Finally, the experimental results are presented for the speech
recognition applied to Brazilian Portuguese language process.
Keywords–Support Vector Machines; Classiﬁcation; Pattern
Recognition; Statistical Learning Theory; Application in Speech
Recognition.
I.
INTRODUCTION
A. Digital Processing of the Speech Signal
Digital speech processing is a specialty in full expansion.
There are numerous applications of this research area, we
can refer to automatic speech recognition for purposes of
interpretation of commands by machines or robots, automatic
speech recognition for the purpose of biometric authentication,
recognition of pathology in the mechanism of speech pro-
duction for biometric and or medicinal purposes. The speech
processing systems are divided basically into three sub-areas:
speech coding, speech synthesis and automatic speech recog-
nition. Regardless of the speciﬁc purpose, the initial stages of
a system for processing digital speech is sampling followed by
segmentation of words or phonemes [1] for short-term analysis
by Fourier transform [2] or by spectral analysis [2]. The
speech signal processing ﬁrst involves obtaining a parametric
representation based on a certain model and then applying a
transformation to represent the signal in a more convenient
form for recognition. The last step in the process is the
extraction of important characteristics for a given application.
This step can be performed either by human listeners or
automatically by machines [2]. Among the techniques that
have been developed for segmentation of speech, those based
on Hidden Markov Models (HMM) [2] are quite traditional.
Hybrid methods based on Artiﬁcial Neural Networks (ANN)
[3] and criteria, such as average energy, selection of voiced
phonemes and non voiced, Mel Frequency Cepstral Coefﬁ-
cients (MFCCs) [2], spectral metrics [2], and others, are also
used. Speech coding systems include those cases in which the
purpose is to obtain a parametric representation of the speech
signal, based on the analysis of the frequency, average power
and other characteristics of the spectrum of the signals. The
techniques of encoding the speech signal are used both for
transmission and for compact storage of speech signals. One
of the main applications of speech coding is to transmit the
speech signal efﬁciently [4]. Systems for automatic speech
recognition or Speech Recognition Systems (SRS) are focused
on the recognition of the human voice by intelligent machines.
B. Methodology Proposed
This
article
uses
as
a
recognition
default
locutions
from
Brazillian
Portuguese
of
the
digits
′0′,′ 1′,′ 2′,′ 3′,′ 4′,′ 5′,′ 6′,′ 7′,′ 8′,′ 9′.
The
speech
signal
is sampled and encoded in mel-cepstral coefﬁcients and
coefﬁcients of Discrete Cosine Transform (DCT) [2] in
order to parameterize the signal with a reduced number of
parameters. Then, it generates two dimensional matrices
referring to the mean and variance of each digit. The elements
of these matrices representing two-dimensional temporal
patterns will be classiﬁed by Support Vector Machines
(SVMs) [3]. The innovation of this work is in the reduced
number of parameters lies in the SVM classiﬁer and in the
reduction of computational load caused by this reduction of
parameters.
C. SVM (Support Vector Machine)
Based on Statistical Learning Theory, SVM classiﬁer is
another category of feed-forward, whose outputs of neurons
from a layer feed neurons from the next layer where feedback
doesn’t occur [3]. This technique originally developed for
binary classiﬁcation, seeks to build hyperplanes as decision
surfaces, in such a way so that the separation between classes
is maximum, assuming that the patterns are linearly separable.
As for non-linearly separable patterns, the SVM seeks an
appropriate mapping function to make the mapped set lin-
early separable. Due to its efﬁciency in working with high-
dimensional data, it is cited in the literature as a highly
robust technique [5]. The results of applying this technique
are comparable and often superior to those obtained by other
learning algorithms, such as ANN.
91
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-354-4
ADVCOMP 2014 : The Eighth International Conference on Advanced Engineering Computing and Applications in Sciences

1) Theory of Statistical Learning: The Theory of Statistical
Learning aims to establish mathematical conditions that allow
the selection of a classiﬁer with good performance for the
data set available for training and testing. In other words,
this theory seeks to ﬁnd a good classiﬁer with good gen-
eralization regarding the entire data set. But, this classiﬁer
abstains from particular cases, which deﬁnes the capability
to correctly predict the class of new data from the same
domain in which the learning occured. Machines Learning
(ML) [3] employs an inference principle called induction, in
which general conclusions are obtained from a particular set
of examples. A model of supervised learning based on Theory
of Statistical Learning is given in Figure 1 [3].
Environment X:
distribuition of
Probability
F ( )
X x
Machine of Learning
ω
W
є
(
x ,d ; x ,d ;.., x ,d )
1
1
2
2
N
N
x ,x ,...,x
1
2
N
F(x, )
ω ≈d
x
X
Supervisor
Figure 1.
Flowchart of a model of supervised learning.
Environment: It is stationary. It provides an input vector x
with a function of distribution of cumulative probability ﬁxed,
but unknown Fx(x).
Supervisor: It shows a desired response d for each input vector
x is provided by the environment accordance to a conditional
cumulative distribution function Fx(x|d) which is also ﬁxed
but unknown. The desired response due to input vector x is
related by (1):
d = f(x, v)
(1)
where v is noise that allows the supervisor to be noisy. The
kind of learning discussed in this work is supervised, but not
noisy.
2) Functional of Risk: The desired performance of a clas-
siﬁer f is that it gets the smallest mistake during training,
with the error being measured by the number of incorrect
predictions of f. Therefore, its deﬁned as Empirical Risk
Remp(f) the extent of loss between the desired response and
the actual response. In (2), it is shown the deﬁnition of the
Empirical Risk.
Remp(w) = 1
N
N
X
i=1
c(f i(xi, yi))
(2)
where yi = F(xi, wi), wi is a vector of adjustable weights, c
is the cost function related to the prediction f(xi), with desired
output f(yi), where one type of cost function is the “loss 0/1”
deﬁned by (3). The process of search by an equation f(x)
that represents a smaller value of Remp(f) is called Empirical
Risk Minimization.
c(f i(xi, yi) =

1,
if
yif(xi) < 0
0,
otherwise
(3)
Assuming that the patterns used for training (xi, yi) are gener-
ated by an independent and identically distributed distribution
(iid) of probability P(x, y). The probability of incorrect
Classiﬁcation from classiﬁer f is called Functional Risk, which
quantiﬁes the capability of generalization, according to (4) [6].
R(f) =
Z
c(f(xi, yi))dP(xi, yi)
(4)
During the training process, Remp(f) can be easily obtained,
while R(f) cannot, since probability P is unknown. Given a
set of training data (xi, yi) with xi ∈ ℜN and yi ∈ {±1}, i =
1, 2, ..., n, i = 1, 2, ..., n, the input vector xi and yi is the
output related to class xi, then the goal is to estimate a
function f : ℜN → {±1} and if no restriction is imposed
on the class of functions in which one chooses to estimate f,
it may happen that the function obtains a good performance
in the training set, but not having the same performance
in unknown patterns. This phenomenon is called the error
“overﬁtting”. Thus, the minimization of the empirical risk does
not guarantee a good generalization capability, and being a
great classiﬁer is desired f ∗ such that R(f ∗) = minf∈F R(f),
where F is the set of possible functions f . The Theory
of Statistical Learning provides ways to limit the class of
functions (hyperplanes), in order to exclude bad models, that
is, those leading to the error of overﬁtting, implementing a
function with an adequate capacity to correctly classify the
set of training data. Restrictions on Risk Functional use the
concept of VC dimension [7].
3) SVM (Mathematical Modeling): Classiﬁers that separate
the data through a hyperplane are called Linear and SVM ﬁts
this deﬁnition, therefore, we must pay attention to all that
there is to train and classify, for as a SVM must also deal
with non-linearly separable sets, this will resort to techniques.
In the application of Techniques of Statistics Learning (TSL),
the classiﬁer must be chosen the classiﬁer with the lowest
possible empirical risk and which also satisﬁes the constraint
of belonging to a family F with a small VC dimension. Also,
to determine the separability of the optimal hyperplane, as it
was assumed that the training set is linearly separable. The
equation of a decision surface folows below:
ωT x + b = 0
(5)
where x is an input vector, ω is a vector of adjustable
weight (maximum separation possible between true and false
examples) and b is a bias. And from this consideration follows
a sequence of calculations in order to ﬁnd the hyperplane with
higher separability between classes. Under these conditions,
the surface found is called optimal. In Figure 2, the geom-
etry of an optimal hyperplane for two-dimensional space is
illustrated.
For the case of a non-linear set, SVM creates another
feature space from the original space, and the concepts and
92
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-354-4
ADVCOMP 2014 : The Eighth International Conference on Advanced Engineering Computing and Applications in Sciences

x2
x1
Support Vectors
Support Vectors
Optimal Hyperplane
ρo
Figure 2.
Optimal hyperplane for linearly separable patterns.
calculations of linear optimal hyperplane are applied in this
new space [3].
4) SVM for multiple classes: The SVM is a dichotomic
algorithm, that is, for pattern classﬁcation based on two classes
[3]. However, it is possible to obtain a classiﬁer for multiple
classes using the SVM algorithm. Scholkopf et al. proposed
a classiﬁer model of type “one vs. all” [8]. Clarkson and
Brown have proposed a classiﬁer model of the “one vs. one”
[9]. However, both models are indeed classiﬁers of only two
classes: Class +1 and Class -1 [3]. On system “one vs. all”, one
machine for each group is used, in which each group is trained
separately from the rest of the set. In the system “one vs. one”,
only three machines are used, in which a group is classiﬁed
against another; then, this one is rated against another group
and so on, until the whole set is trained.
5) Functions Kernel: The decision surface of the SVM,
which in the feature space is always linear, usually is nonlinear
in the input space. As seen earlier, the idea of SVM depends
on two mathematical operations:
1)
Nonlinear mapping of an input vector into a feature
space of high dimensionality, which is hidden from
the entry and exit;
2)
Build an optimal hyperplane to separate the features
discovered in the ﬁrst step. To design the optimal
hyperplane, a kernel function is needed, or a core of
the inner product. A Kernel function is a function that
receives two points xi and xj of the input space and
calculates the scalar product of the data in the feature
space, given by (6).
k(xi, xj) = ΦT (xi) · Φ(xj)
(6)
To ensure the convexity of the optimization problem and
introduce the Kernel mapping in which the calculation of
scalar products is possible, a kernel function that follows the
conditions set by Mercers Theorem [10][11]must be used. The
kernels that satisfy Mercers conditions are characterized for
giving origin to semi-deﬁnite positive matrices k, in which
each element kij is deﬁned by kij = k(xi, xj), ∀i, j =
1, 2, ..., n. Once the mapping is performed by a SVM kernel
function, and not directly by Φ(x), it is not always possible to
know exactly which mapping is actually performed, because
the kernel functions perform an implicit mapping. Table I
shows the main features commonly used as kernel functions.
The expansion of the inner product core K(xi, xj) in (6)
makes it possible to ﬁnd a decision surface that is non-linear
in the input space, but whose image in the feature space is
linear [3].
TABLE I.
APPLICATION OF SVM
Name of Kernel
Function
Polinomial

xT xi + 1
p
RBF Kernel
exp

−
1
2σ2 ∥x − xi∥2 
Perceptron
tanh

β0xT xi + β1

The Kernel functions used have the following restrictions:
•
In the Polynomial kernel, the parameter p is ﬁrst
speciﬁed by the user;
•
In the kernel RBF, the parameter σ2 is common to all
cores;
•
In the perceptron, Mercer’s theorem is satisﬁed only
for some values of β0, β1;
6) Automatic Systems for Speech Recognition with SVM:
Hidden Markov Models (HMMs) have become the most em-
ployed technique for Automatic Speech Recognition (ASR).
However, the HMM-based ASR systems may reach their limit
of performance. Hybrid systems based on a combination of
artiﬁcial intelligence techniques provide signiﬁcant improve-
ments of performance. However, the progress in this paradigm
has been hindered by their training computational requireents,
which were excessive when these systems were proposed.
Recently, several methods of Speech Recognition have been
proposed using mel-frequency cepstral coefﬁcients and Neural
Networks Classiﬁers [12][13][14], Sparse Systems for Speech
Recognition [15], Hybrid Robust Voice Activity Detection
System [16], Wolof Speech Recognition with Limited vocabu-
lary Based HMM and Toolkit [17], Real-Time Robust Speech
Recognition using Compact Support Vector Machines [6].
Thus, the SVM has many functions; it is a binary algo-
rithm, based in the Theory of Statistical Learning and in the
Functional of Risk. And, ﬁnally, it has many functions for
classiﬁcation, such as in the case of multiple classes.
II.
SYSTEM OF SPEECH RECOGNITION
A. Pre-processing of Speech Signal
Initially, after the segmentation of the speech is passed
through the process of windowing, the speech signal is sam-
pled and segmented into frames and is encoded in a set of
melcepstral parameters. The number of parameters obtained
is determined by the order of mel-cepstral coefﬁcients. The
obtained coefﬁcients are then encoded by Discrete Cosine
Transform (DCT) [2] in a two dimensional matrix that will
represent the speech signal that to be recognized. The process
of windowing in a given signal, aims to select a small portion
of this signal, which will be analysed and named frame. A
short-term Fourier analysis performed on these frames is called
signal analysis frame by frame. The length of the frame Tf is
deﬁned as the length of time upon which a parameter set is
93
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-354-4
ADVCOMP 2014 : The Eighth International Conference on Advanced Engineering Computing and Applications in Sciences

valid. The term frame is used to determine the length of time
between successive calculations of parameters. Normally, for
speech processing, the time frame is between 10ms and 30ms
[18].
B. Generation of two-dimensional DCT-temporal matrix
After being properly parameterized in mel-cepstral coefﬁ-
cients, the signal is encoded by DCT performed in a sequence
of T observation vectors of mel-cepstral coefﬁcients on the
time axis. The coding by DCT is given by the equation
following:
Ck(n, T) = 1
N
T
X
t=1
mfcck(t)cos(2t + 1)nπ
2T
(7)
where k, 1 ≤ k ≤ K, refers to the k−th line (number of Mel
frequency cepstral coefﬁcients) of t−th segment of the matrix
n, 1 ≤ n ≤ N component refers to the n−th column (order
of DCT), mfcck(t) represents the mel-cepstral coefﬁcients.
Thus, one obtains the two-dimensional matrix that encode the
long term variations of the spectral envelope of the speech
signal [19]. This procedure is performed for each spoken word.
Thus, there is a two-dimensional matrix Ck(n, T) ≡ Ckn for
each input signal. The matrix elements are obtained as the
following:
1)
For a given model of spoken word P (digit),
ten examples of this model are pronounced. Each
example is properly divided into T frames dis-
tributed
along
the
time
axis.
Thus,
we
have:
P 0
0 , P 0
1 , ..., P 0
9 , P 1
0 , P 1
1 ..., P 1
9 , P 2
0 , P 2
1 , ..., P 2
9 ,..., P j
m,
where j=0,1,2,...,9 is the number of patterns to be
recognized and m=1,2,3,...,10, is the number of sam-
ples to generate each pattern.
2)
Each frame of a given example of model P generates
a total of K mel-frequency cepstral coefﬁcients, and
then, signiﬁcant characteristics are obtained within
each frame over this time. The DCT of order N is
then calculated for each mel-cepstral coefﬁcient of
the same order within the frame, that is, c1 in the
frame t1, c1 in the frame t2, ...,, c1 in the frame
tT , c2 in the frame t1, c2 in the frame t2, ...,,
c2 in the frame tT , and so on, generating ele-
ments {c11, c12, c13, ..., c1N}, {c21, c22, c23, ..., c2N},
{cK1, cK2, cK3, ..., cKN} in the matrix given in (7).
Thus, a two-dimensional temporal array DCT is gen-
erated for each m example of model P, represented by
Cjm
kn . Finally, arrays of mean CM j
kn (8) e variance
CV j
kn (9) are generated. The parameters of CM j
kn
and CV j
kn are used as datas of input in SVM algo-
rithm.
CM j
kn = 1
M
M−1
X
m=0
Cjm
kn
(8)
CV j
kn =
1
M − 1
M−1
X
m=0
"
Cjm
kn −
 
1
M
M−1
X
m=0
Cjm
kn
!#2
(9)
C. Generation of machines
In the technical literature about SVMs, the standards are
called classes. The mean and variance matrices are transformed
in two column vectors, CMe (vector with means) and CV ar
(vector with variances).
CMej
i
=
⟨CM 0
11,CM 0
12,...,CM 0
1N,CM 0
21,CM 0
22,...,CM 0
2N,
CM J
KN⟩
(10)
CV arj
i
=
⟨CV 0
11,CV 0
12,...,CV 0
1N,CV 0
21,CV 0
22,...,CV 0
2N,
CV J
KN⟩
(11)
For example, in the case of a matrix CM j
22, that is, where
K=2 e N=2, the matrices CMe and CV ar take the following
form:
CMej
i
=
⟨CM 0
11,CM 0
12,CM 0
21,CM 0
22,CM 1
11,
CM 1
12,CM 1
21,CM 1
22,...,CM J
22⟩
(12)
CV arj
i
=
⟨CV 0
11,CV 0
12,CV 0
21,CV 0
22,CV 1
11,CV 1
12,
CV 1
21,CV 1
22,...,CV J
22⟩
(13)
Each class in this example is represented by 4 elements
in the vector of mean and 4 elements in vector of variance
according to (12) and (13), that is, the ﬁrst 4 elements of the
vector of mean and of the vector of variance refer into class
0, the following 4 elements of each vector to the class 1, and
so on. Figure 3 shows data of the peers of mean and variance
of the speech signals from the examples of (12) and (13).
−0.3
−0.2
−0.1
0
0.1
0.2
0.3
0.4
0
0.005
0.01
0.015
0.02
0.025
Mean (MFCC x DCT)
Variance (MFCC x DCT)
SPACE TRANSFORMED OF PARAMETERS
 
 
Class	 0
Class 1
Class 2
Class 3
Class 4
Class 5
Class 6
Class 7
Class 8
Class 9
Figure 3.
Classes and their different points.
The set of functions mapping of type input-output is given
by (14):
Ω = f

[CMej
i; CV arj
i ], w

(14)
where Ω is the real response produced by the learning machine
associated with the entry pairs of means and variances, and
w is a set of free parameters, called weights for weighting,
selected from the parameter space related to patterns. Figure
4 shows a general model of the supervised learning from the
examples, having three components:
94
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-354-4
ADVCOMP 2014 : The Eighth International Conference on Advanced Engineering Computing and Applications in Sciences

Environment
Supervisor
d
Ω
xi
ML
Figure 4.
Model of Learning.
The
Environment
is
the
ﬁxed
input
system;
this
yields xi (points that come from the pairs of coordinates
(CMe, CV ar)) from the response of the DCT matrix of
speech signals. The Supervisor returns a value of the desired
output di for each input vector xi in accordance with a
conditional distribution function F (di|xi)), also set. Machine
of Learning (ML), is an algorithm capable of implementing
a set of functions f

[CMej
i; CV arj
i ], w

, where ω ∈ W,
where W is a set of parameters belonging to the set of
desired responses. In this context, the learning problem can be
interpreted as a problem of approximation, which involves
ﬁnding a function f

[CMej
i; CV arj
i ], w

that generates the
best approximation to the Ω output of the supervisor. The
selection is based on a set of independent training examples I
and identically distributed (iid), generated according to:
F(x, d) = F(x)F(d|x) : (xi, di)
(15)
where (xi, di) are peers with desired input and output with
di ∈ Rn and i = 1, ..., I.
III.
EXPERIMENTAL RESULTS
A. Training
After performing the pre-processing of the speech signal
coding and generation of temporal matrices CM j
kn and CV j
kn,
the models were trained by SVM machines CM j
22 and CV j
22,
that is, K=2 and N=2, as shown in Figure 5, for CM j
33 and
CV j
33, that is, K=3 and N=3, as shown in Figure 6 , and CM j
44
e CV j
44,i.e., K=4 e N=4, as shown in Figure 7. The best results
for matrices with K=2, N=2 , K=3 and N=3 were generated by
polynomial function of order 3. However, the best results for
matrices with K=4 e N=4 were generated by Kernelcachelimit
function, because as each class is represented by 16 points and
there are 10 classes to be classiﬁed (separated), there are 160
points separated and the Polynomial function obeys an order P
as shown in Table I and 1 ≤ P ≤ 3, P ∈ Z resulting in a very
limited hyperplane relative to the curvature that the function
line can make with a limit P equal to 3. The Kernelcachelimit
function provides a value that speciﬁes the size of the cache
memory of the kernel matrix, while the algorithm maintains a
matrix with up to 5000 × 5000 of double precision ﬂoating-
point numbers in memory.
In Bresolin [20], the use of SVM with wavelet digital voice
recognition in Brazilian Portuguese, obtained an average of
97.76% using 26 MFCC’s in the pre-processing of voice and
SVM machine’s with the following characteristics: MLP as
Kernel functions, ten machines (one for each class) and “one
vs. all” as method of multiple classes. In comparison to this
work, the results of this remain more effective, because the
amount of MFCC’s is smaller and, also, the input of parameters
in the machines are lower. Consequently, the computational
load is lower.
−0.3
−0.2
−0.1
0
0.1
0.2
0.3
0.4
0
0.005
0.01
0.015
0.02
0.025
Mean (MFCC x DCT)
Variance (MFCC x DCT)
Machine 7 | Class 7 vs All
.
Other (training)
Other (classiﬁed)
Class 7 (training)
Class 7 (classiﬁed)
Support Vectors
Figure 5.
Machine generated for class 7 from matrices CM7
22 and CV 7
22.
−0.3
−0.2
−0.1
0
0.1
0.2
0.3
0.4
0
0.005
0.01
0.015
0.02
0.025
Mean (MFCC x DCT)
Variance (MFCC x DCT)
Machine 7 | Class 7 vs All
.
Other (training)
Other (classiﬁed)
Class 7 (training)
Class 7 (classiﬁed)
Support Vectors
Figure 6.
Machine generated for class 7 from matrices CM7
33 and CV 7
33.
−0.3
−0.2
−0.1
0
0.1
0.2
0.3
0.4
0
0.005
0.01
0.015
0.02
0.025
M
(MFCC x
)
ean
DCT
Vari
(MFCC x D
)
ance
CT
Ma
chine 7 | Class 7 vs All
.
Other
a ing
(tr in
)
Ot
(classi
)
ﬁ
her
ed
Class 7 (tr in
)
a ing
Class 7 (classi
ed)
ﬁ
Su port
p
Vectors
Figure 7.
Machine generated for class 7 from matrices CM7
44 and CV 7
44.
B. Test
With the result of the best function from training, the
tests were made from voice banks where the speakers are
independent and classiﬁed with the best function of training:
Polynomial of order 3, except the matrices with K=4 e N=4
were tested (classiﬁed) with the same function of the training:
Kernelcachelimit. The speakers 1 and 2 are male and the
speaker 3 is female. The Tables II, III and IV show the rates
of successes.
95
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-354-4
ADVCOMP 2014 : The Eighth International Conference on Advanced Engineering Computing and Applications in Sciences

TABLE II.
TEST PERFORMED FROM MATRICES CMj
22 AND CV j
22
Machines
Training
Test
Speaker 1
Speaker 2
Speaker 3
Class 0
10
10
10
10
Class 1
10
10
7
10
Class 2
8
5
7
5
Class 3
8
5
5
5
Class 4
10
5
5
5
Class 5
10
3
5
7
Class 6
8
5
7
5
Class 7
10
7
5
5
Class 8
10
10
10
10
Class 9
10
3
0
0
TOTAL
94
63
61
62
TABLE III.
TEST PERFORMED FROM MATRICES CMj
33 AND CV j
33
Machines
Training
Test
Speaker 1
Speaker 2
Speaker 3
Class 0
10
9
8
9
Class 1
10
10
9
10
Class 2
8
8
7
7
Class 3
10
9
7
10
Class 4
10
3
4
2
Class 5
10
3
4
3
Class 6
10
6
7
7
Class 7
8
9
8
9
Class 8
10
9
10
7
Class 9
10
6
7
7
TOTAL
96
72
71
71
TABLE IV.
TEST PERFORMED FROM MATRICES CMj
44 AND CV j
44
Machines
Training
Test
Speaker 1
Speaker 2
Speaker 3
Class 0
8
8
8
8
Class 1
10
10
10
10
Class 2
10
8
9
8
Class 3
10
9
7
8
Class 4
10
6
7
8
Class 5
10
8
9
7
Class 6
8
8
7
8
Class 7
10
10
10
10
Class 8
10
10
10
10
Class 9
10
6
6
5
TOTAL
96
82
83
82
IV.
CONCLUSION
Analysing the methodology and applications of SVM, one
realises that it is a technique with excellent response time of
computational execution. Despite being a dichotomic method
of classiﬁcation, this also has possible means to work with a
larger number of classes of different data types to be separated.
In the standards classiﬁcation proposed in this work, the SVM
presented problems to correctly classify points very close
among to each other, because of the form generalization of one
versus all. However, as it has a very wide scope in relation to
the classiﬁcation functions during the learning process of the
machines, the SVM ends up compensating for the problem of
generalization with the use of more points for classiﬁcation.
That is, the greater the number of points to represent the
class the higher the amount of hits. In general, the patterns
were classiﬁed very well, except with the digit ‘9’. The digits
‘1’ and ‘8’ obtained the highest classiﬁcations. The use of
mean and variance chosen as characteristics of the data to
be generated patterns was the most appropriate way to ﬁnd
a better separability between points and therefore a better
classiﬁcation.
ACKNOWLEDGMENT
The authors thank the Scientiﬁc Initiation Program of the
Federal Institute of Maranhao for ﬁnancial support through
grant aid, and availability of Digital Systems Laboratory, and
the Research Group for Electronic Instrumentation Technology
Applied to the IFMA.
REFERENCES
[1]
P. Fantinato, Segmentacao de Voz baseada na Analise Fractal e na
Transformada Wavelet.
Prentice Hall, Outubro 2008.
[2]
L. Rabiner and R. Schafer, Digital Processing of Speech Signals.
Prentice Hall, 1978.
[3]
S. Haykin, Redes Neurais:Principio e pratica.
Bookman, 2002.
[4]
A. Bresolin, Reconhecimento de voz atraves de unidades menores do
que a palavra, utilizando Wavelet Packet e SVM, em uma nova Estrutura
Hierarquica de Decisao.
Tese de Doutorado, Natal 2008.
[5]
C. Ding and I. Dubchak, Multi-class protein fold recognition using
support vector machines and neural networks.
Bioinformatics, 2001.
[6]
R. Urena, A. Moral, C. Moreno, M. Ramon, and F. Maria, “Real-
time robust automatic speech recognition using compact support vector
machines.”
IEEE Transactions on Audio, Speech, and Language
Processing, May 2012, pp. 1347–1362.
[7]
V. Vapnik and A. Chervonenkis, On the Uniform Convergence of
Relative Frequencies of Events to Their Probabilities.
Dokl, 1968.
[8]
B. Scholkopf, O. Simard, A. Smola, and V. Vapnik, Prior knowledge
in support vector kernels.
The MIT Press, 1998.
[9]
P. C. Clarkson and P. Moreno, “Acoustics, speech and signal process-
ing.”
IEEE International Conference, March 1999, pp. 585–588.
[10]
J. Mercer, Functions of positive and negative type, and their connections
with theory of integral equations.
Transactions of the London
Philosophical Society, 1909.
[11]
C. De-Gang, Y. Heng, and E. Tsang, Generalized Mercer theorem and
its application to feature space related to indeﬁnite kernels.
Interna-
tional Conference Machine Learning and Cybernetics, 2008.
[12]
D. Hanchate, M. Nalawade, M. Pawar, V. Pohale, and P. Maurya, “vocal
digit recognition using artiﬁcial neural network”.”
2nd International
Conference on Coumputer Engineering and Technology, April 2010,
pp. 88–91.
[13]
R. Aggarwal and M. Dave, “application of genetically optimized neural
networks for hindi speech recognition system”.”
World Congress
on Information and Communication Technologies (WICT), December
2011, pp. 512–517.
[14]
S. Azam, Z. Mansor, M. Mughal, and S. Moshin, “urdu spoken
digits recognition using classiﬁeld mfcc and backpropagation neural
network”.”
4th International Conference on Computer Graphics,
Imaging and Visualization (CGIV), August 2007, pp. 414–418.
[15]
M. Mohammed, E. Bijov, C. Xavier, A. Yasif, and V. Supriya, “robust
automatic speech recognition systems:hmm vesus sparse”.” Third Inter-
national Conference on Intelligent Systems modelling and Simulation,
February 2012, pp. 339–342.
[16]
C. Ganesh, H. Kumar, and P. Vanathi, “performance analysis of hybrid
robust automatic speech recognition system”.”
IEEE International
Conference on Signal Processing, Computing and Control (ISPCC),
March 2012, pp. 1–4.
[17]
J. Tamgo, E. Barnard, C. Lishou, and M. Richome, “wolof speech
recognition model of digits and limited-vocabulary based on hmm and
toolkit”.”
14th International Conference on Computer Modelling and
Simulation (UKSim), March 2012, pp. 389–395.
[18]
J. Picone, “Signal modeling techniques in speech recognition.”
IEEE
Transactions on Computer, April 1991, pp. 1215–1247.
[19]
P. Fissore and E. Rivera, “Using word temporal structure in hmm speech
recongnition.”
ICASSP 97, April 1997, pp. 975–978.
[20]
A. Brasolin, A. Neto, and P. Alsin, “”digit recognition using wavelet
and svm in brazilian portuguese”.”
ICASSP 2008, April 2008, pp.
1–4.
96
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-354-4
ADVCOMP 2014 : The Eighth International Conference on Advanced Engineering Computing and Applications in Sciences

