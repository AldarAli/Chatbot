 
A Study of Retrieval Algorithms of Sparse Messages in Networks of Neural 
Cliques
Ala Aboudib, Vincent Gripon, Xiaoran Jiang 
Electronics Department 
Télécom Bretagne (Institut Mines-Télécom) 
Brest cedex 3, France 
e-mail: name.surname@telecom-bretagne.eu 
 
The authors are also with the Laboratory for Science and Technologies of Information, Communication and Knowledge, CNRS Lab-
STICC, Brest, France. 
 
Abstract – Associative memories are data structures ad-
dressed using part of the content rather than an index. They 
offer good fault reliability and biological plausibility. Among 
different families of associative memories, sparse ones are 
known to offer the best efficiency (ratio of the amount of bits 
stored to that of bits used by the network itself). Their re-
trieval process performance has been shown to benefit from 
the use of iterations. In this paper, we introduce several rules 
to enhance the performance of the retrieval process in recent-
ly proposed sparse associative memories based on binary 
neural networks. We show that these rules provide better 
performance than existing techniques. We also analyze the 
required number of iterations and derive corresponding 
curves. 
Keywords – associative memory; sparse coding; parsimony; 
iterative retrieval; threshold control 
I. 
INTRODUCTION 
Associative memories are alternatives to classical index-
based memories where content is retrieved using a part of it 
rather than an explicit address. Consider for example accessing a 
website using a search engine instead of a uniform resource 
locator (URL). This mechanism is analogous to human memory 
[1] and has inspired many neural-networks-based solutions such 
as [2] [3].  
A new artificial neural network model was proposed recently 
by Gripon and Berrou [4]. It employs principles from infor-
mation theory and error correcting codes and aims at explaining 
the long-term associative memory functionality of the neocortex. 
This model was proved to outperform the celebrated Hopfield 
neural network [3] in terms of diversity (the number of messages 
the network can store) and efficiency (the ratio of the amount of 
useful bits stored to that of bits used to represent the network 
itself) [5]. It was later extended in [6] to a sparser version based 
on the Willshaw-Palm associative memory model [2] [7].  
The key difference between the models proposed in [6] and 
[2] is the use of specific structures in the network. This is done 
by grouping neurons into clusters within which connections are 
not authorized (multi-partite graph). These clusters are consid-
ered analogous to cortical columns [4] of mammalian brains 
within which nodes are likened to micro-columns. This is sup-
ported by Mountcastle [8] who suggests that a micro-column is 
the computational building block of the cerebral neo-cortex. In 
addition, here are some reasons to motivate the use of clusters: 
 It is believed that micro-columns in each cortical column re-
act to similar inputs. The concept of clustering is meant to 
imitate this stimulus-similarity-based grouping. A conse-
quence is the possibility to use this network for retrieving 
messages from inaccurate observations. This type of retrieval 
is addressed by Gripon and Jiang in [9]. 
 Clusters allow for simple and natural mapping between non-
sparse input messages and sparse patterns representing them 
in the associative memory. In the case where clusters are all 
of size 1 each, a model equivalent to the classical Willshaw-
Palm networks is obtained, where input messages have to be 
sparse. 
 It was observed that micro-columns usually have many short 
inhibitory connections with their neighbors [10] [8], which 
means that the activation of one micro-column causes all of 
its near neighbors to be deactivated. This is due to the locally 
limited energy supply of the brain. This mechanism is repre-
sented by the local winner-takes-all rule introduced in [4]. 
 Using clusters allows for introducing guided data recovery in 
which a prior knowledge of the location of clusters contain-
ing the desired data can significantly enhance performance. 
A detailed study of this type of data retrieval is available in 
[6]. 
In this paper, we consider the extended version of the model 
proposed in [6]. We introduce several retrieval rules including 
adaptations of those proposed by Willshaw [2], Palm [11], 
Schwenker [12] and Gripon and Berrou [4]. We also propose 
new ones and make a comparison of these regarding performance 
and number of iterations. 
The rest of this paper is organized as follows: in Section II, 
we describe the general architecture of the network model we 
use. Section III introduces a generic formulation of the retrieval 
algorithm on such structures. Then, the following five sections 
develop each step of this algorithm. For each step, different rules 
are reviewed if available. Some of these rules have been pro-
posed in previous papers, and others we introduce here for the 
first time. In Section IX, performance comparisons of several 
combinations of retrieval rules are presented. Section X is a 
conclusion. 
II. 
NETWORK TOPOLOGY AND STORING MESSAGES 
This section focuses on the neural-network-based auto-
associative memory introduced by Gripon and Berrou in [4]. It is 
dedicated to defining this network and describing how it can be 
extended to store variable-length messages. 
 Architecture 
A.
The network can be viewed as a graph consisting of   verti-
ces or units initially not connected (zero adjacency matrix) orga-
nized in   parts called clusters with each vertex belonging to one 
and only one cluster. Clusters are not necessarily equal in size 
but for simplicity, they will be all considered of size    through-
out this work. Each cluster is given a unique integer label be-
tween   and  , and within each cluster, every vertex is given a 
unique label between   and  . Following from this, each vertex in 
the network can be referred to by a pair      , where   is its clus-
140
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-340-7
COGNITIVE 2014 : The Sixth International Conference on Advanced Cognitive Technologies and Applications

 
ter label, and   is the vertex label within cluster  . For biological-
ly-inspired reasons [13] [14], and as explained in [6], a unit in 
this model is chosen to represent a cortical micro-column instead 
of a single biological neuron which is why we shall not use the 
term “neuron”. 
At a given moment, a binary state     is associated with each 
unit       in the network.  It is given the value   if       is active 
and   if it is idle. Initially, all units are supposed to be idle. The 
adjacency matrix (also called the weight matrix  ) for this graph 
is a binary symmetric square matrix whose elements take values 
in      . In this representation, a zero means an absence of a 
connection while a   indicates that an undirected (or a symmet-
ric) connection is present. Note that despite the fact that physio-
logical neural networks are known to be asymmetric, we argue 
that units in the proposed model represent populations of tens of 
neurons, and therefore can be mutually connected.  
Row and column indexes of the weight matrix are       pairs. 
So in order to indicated that two units       and       are con-
nected, we write              . All connection combinations are 
allowed except those among units belonging to the same cluster, 
resulting in a  -partite undirected graph. When the memory is 
empty,   is a zero matrix. 
 Message Storing Procedure 
B.
We now describe how to store sparse messages using this 
network. This methodology has been first introduced in [6]. 
Suppose that each message consists of   submessages or seg-
ments. Some of these segments are empty, i.e., they contain no 
value that need to be stored, while the rest has integer values in 
        . For the sake of simplicity, let us consider that all mes-
sages contain the same number of submessages  . Only those 
nonempty submessages are to be stored while empty ones are 
ignored. For example, in a network with     and     , a  
message                  with     has two empty seg-
ments (the 1st and the 4th) while the remaining ones have values 
that need to be stored. In order to store  , each nonempty seg-
ment position   within this message is interpreted as a cluster 
label, and the segment value   is interpreted as a unit label within 
the cluster  . Thus, each nonempty segment is associated with a 
unique unit      . So the message   maps to the 10th unit of the 
2nd cluster, the 7th unit of the 3rd cluster, the 12th unit of the 5th 
cluster and the 11th unit of the 6th (last) cluster. A single message 
is not allowed to use more than one unit within the same cluster 
because, by definition, connections are not allowed within a 
cluster. 
Then, given these   elected units in   distinct clusters, the 
weight matrix of the network is updated according to (1) so that a 
fully connected subgraph (clique) is formed of these selected 
units. 
 
                            n             
(1) 
where             refers to the undirected connection between 
      and       which are two units associated to message seg-
ments    and   , respectively.   and   are cluster indices while 
  and   are unit indices.  
The value of the parameter   can be unified for all stored 
messages, or it can be variable. A description of how to choose 
an optimal value of   is provided in [6] where   is considered 
identical for all messages.  
It is important to note that if one wishes to store another mes-
sage    that overlaps with  , i.e., the clique corresponding to    
shares one or more connections with that of  , the value of these 
connections, which is  , should not be modified. Such a property 
is called the nondestructivity of the storing process. As a direct 
consequence, the network’s connect on m p  s the un on o   ll 
cliques corresponding to stored messages.  
It is worth noting that when    , the structure of this net-
work becomes equivalent to the Willshaw-Palm model. 
III. 
THE RETRIEVAL PROCESS 
The goal of the retrieval process introduced in this paper is to 
recover an already stored message (by finding its corresponding 
clique) from an input message that has undergone partial erasure. 
A message is erased partially by eliminating some of its nonemp-
ty segments. For example, if                 is a stored mes-
sage, a possible input for the network is  ̅             . 
The core of the retrieval process can be viewed as an iterative 
twofold procedure composed of a dynamic rule and an activation 
rule. Figure 1 depicts the steps of the retrieval process:  
Insert an Input Message. 
Apply a dynamic rule. 
Phase 1: 
 Apply an activation rule. 
 Apply a dynamic rule. 
Phase 2: 
   While (stopping criterion is not attained) { 
         Apply an activation rule. 
Apply a dynamic rule. 
}. 
output   active units. 
 
Each step of the algorithm of Figure 1 is described in detail 
in the next sections. 
IV. 
INPUT MESSAGE INSERTION 
An input message should be fed to the network in order to 
trigger the retrieval process. For example, suppose that we have a 
stored message                . Suppose now that we wish to 
retrieve   from the partially erased input             . In 
order to do that, all units corresponding to nonempty segments 
should be activated. That is, a unit       associated with the 
segment of  ̅ at position   whose value is   is activated by setting 
     . So,   activates two units:       and       .  Having a 
number of active units, a dynamic rule should be applied. 
V. 
DYNAMIC RULES 
A dynamic rule is defined as the rule according to which unit 
scores are calculated. We will denote the score of a unit       by 
   . C lcul t ng un ts’ scores  s cruc  l to deciding which ones 
are to be activated. A score is a way of estimating the chance that 
a unit belongs to a bigger clique within the set of active units and 
thus the chance that it belongs to the message we are trying to 
recover. In principle, the higher the score the higher this chance 
is. Two dynamic rules have been already introduced, namely, the 
Sum-of-Sum [4] and the Sum-of-Max [15] rules. We also intro-
duce for the first time what we call the Normalization rule. 
 The Sum-of-Sum Rule (SoS) 
A.
The Sum-of-Sum is the original rule. It states that the score 
of a unit       denoted by     is simply the number of active units 
connected to       plus a predefined memory effect   which is 
only added if       is active. Scores should be calculated for all 
of the units in the network. This Sum-of-Sum rule can be formal-
ized by the following equation: 
 
Figure 1. The generic algorithm for the retrieval process. 
141
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-340-7
COGNITIVE 2014 : The Sixth International Conference on Advanced Cognitive Technologies and Applications

 
    n                 
 
          ∑
∑
              
 
   
 
   
   
(2) 
Although intuitive, this rule has a major problem which is 
that in some cases, the scores give a false estimate of the chance 
that a given unit belongs to a bigger clique within the set of 
active units. To clarify this point we consider the example of 
Figure 2 where black circles represent active units at an iteration 
   . The clique we wish to restore is      which is maximal 
in size. Now, we will see what happens when we calculate the 
scores of units   and   given a memory effect     where   is 
part of the searched message while   is not. According to the 
Sum-of-Sum rule, unit   has a score of   while unit   has a score 
of  . This result indicates that the latter unit is more likely to 
belong to a bigger clique than the former because it has a higher 
score. This observation is not true since most of the active units 
connected to   belong to the same cluster and by conception, a 
message can only contain at most one unit per cluster. In order to 
solve this problem, the Sum-of-Max and the Normalization 
dynamic rules can be applied. 
 The Normalization Rule (Norm) 
B.
In the Normalization rule th t we  ntro uce here, un ts’ 
scores are calculated using the following equation: 
 
         ∑
 
|  | ∑
              
 
   
 
   
   
(3) 
where |  | is the number of active units in cluster  . Equa-
tion  (3) states that the contribution of a unit       to the score of 
another unit connected to it is normalized by the number of 
active units in cluster  . That is, if the cluster   contains   active 
units, then the contribution of the unit       becomes    . So, 
by applying this rule to the network of Figure 2, unit   gets a 
score of   and unit   gets a score of  
 
  which is a result that 
privileges the activation of the latter unit and thus solves the 
Sum-of-Sum rule problem. 
 The Sum-of-Max Rule (SoM) 
C.
According to the Sum-of-Max rule, the score of a unit       is 
the number of clusters in which there is at least one active unit 
      connected to       plus the memory effect   if       is 
active: 
 
         ∑
        (              )
 
   
   
(4) 
So referring back to Figure 2, and according to (4), unit   has 
a score of   whereas unit   has a score of 3. This is a more satis-
fying result than the one obtained by the Sum-of-Sum rule since 
it indicates that the latter unit, although connected to more active 
units, is less likely to belong to a bigger clique within the set of 
active units than unit  .  
Moreover, it has been shown in [15] that for the particular 
case, when    , the Sum-of-Max rule guarantees that the re-
trieved massage is always either correct or ambiguous but not 
wrong. An ambiguous output message means that in some clus-
ters more than one unit might be activated among which one is 
the correct unit. 
VI. 
ACTIVATION RULES 
The activation rule is applied for electing the units to be acti-
vated based on their scores after the application of a dynamic 
rule. So basically, a unit       is activated if its score     satisfies 
two conditions:  
     is greater or equal than a global threshold that may be 
chosen differently for each activation rule.  
         where     is the activation threshold proper to unit 
     . [16] 
The difference between the two thresholds defined above is 
that     could be set differently for each unit, so it can be used to 
control   un t’s sens t v ty to  ct v t on. For   very l rge v lue o  
   ,       is inhibited. This is helpful for excluding a group of 
units from the search of a certain message in order to save time. 
The global threshold has a unique value independent of any 
individual unit. So it is used to elect units to be activated in a 
competitive activation process. For example, in a winner-take-all 
competitive process, this threshold could be dynamically set to 
the value of the highest score in the network in order to activate 
only units with the highest score. 
The activation rule should be able to find two unknowns: The 
subset of clusters to which the message we are trying to recover 
belongs, and the exact units within these clusters representing the 
submessages. Two activation rules are introduced in this paper: 
the Global Winners Take All rule (GWsTA) which is a generali-
zation of the Global Winner Take All (GWTA) rule, and an 
enhanced version of the Global Losers Kicked Out (GLsKO) rule 
initially presented in [17] . 
 The GWsTA Rule 
A.
The GWTA rule introduced in [6] activates only units with 
the network-wide maximal score. The problem with this rule is 
that it supposes that units belonging to the message we are look-
ing for have equal scores. It also supposes that this unified score 
is maximal which is not necessarily the case. It has been shown 
in [6] that spurious cliques, i.e., cliques that share one or more 
edges with the clique we are searching, might appear and render 
the scores of the shared units of the searched clique higher than 
others’.  
For example, in the network of Figure 2, if the searched 
clique is     , then     is an example of a spurious one. Now, 
by applying the Sum-of-Max rule on the black units which are 
supposed to be active, and considering    , we get the scores: 
    ,      ,      ,      ,      ,            . 
Thus, according to the GWTA rule, only units   and   will be 
kept active and the clique      is lost. This is caused by the 
spurious clique     which increases the scores of   and  . 
The generalization of the GWTA rule we propose is meant to 
account for this problem. 
The behavior of the Global Winners Take All rule is the 
same in both phases of the retrieval process. It elects a subset of 
units with maximal and near-maximal scores to be activated. In 
other words, it defines a global threshold   at each iteration, and 
only units that have at least this threshold are activated. 
In order to calculate this threshold at a given iteration, we 
first fix an integer parameter  . Then we make a list compromis-
ing the   highest scores in the network including scores that 
appear more than once. For example, if the units scores in a 
network 
with 
a 
total 
number 
of 
units 
     
are 
Figure 2. Dynamic rule application phase. Black-filled units are active. 
142
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-340-7
COGNITIVE 2014 : The Sixth International Conference on Advanced Cognitive Technologies and Applications

 
                                and    , then our list 
becomes                       . The minimum score in this 
list which is    becomes the threshold  . Then we apply the 
equation: 
 
    n                                              
 
    {          n                                    
            otherw se                                         
(5) 
It is worth noting that this activation rule is equivalent to the 
retrieval rule proposed by Willshaw [2] in that units are activated 
by comparing their scores to a fixed threshold  .  
One problem with this rule is that the choice of an optimal   
for a certain message size would not be adapted for other mes-
sage sizes. This limits the possibility of using this rule for retriev-
ing messages of variable sizes. Moreover, this rule always acti-
vates a subset of units with maximal and near-maximal scores. 
But in some cases, when the number of stored messages reaches 
a high level, the units with the near-maximal score do not neces-
sarily belong to the searched message.  
 The GLsKO Rule 
B.
As we have seen, The GWsTA rule needs a prior knowledge 
of the value of the message size   in order to retrieve a message. 
This means that if   is not available at retrieval, the rule may not 
be able to correctly retrieve information. The Global Losers 
Kicked Out (GLsKO) rule is designed to address this problem by 
being independent of any prior information about   which should 
also enable it to retrieve variable-sized messages more efficiently 
than the GWsTA rule. In order to achieve this, the GLsKO rule 
has a behavior in phase 1 of the retrieval process that differs 
from that of phase 2 as follows: 
 
Phase 1: 
Apply the GWTA rule. 
Phase 2: 
   Kick losers out. 
 
In phase 1, the GWTA rule is applied resulting in the activa-
tion of a subset of units to which the searched message is guaran-
teed to belong. After this, the activation thresholds of inactive 
units are set to infinity because we are no more interested in 
searching outside the already activated units. 
In phase 2, the rule changes behavior, so, at each iteration, it 
makes a list compromising the   lowest nonzero scores of the 
active 
units 
only. 
For 
example, 
if 
the 
set 
{                               represents the scores of 
active units in a network with a total number of units      and 
we fix    , then the list of lowest scores becomes 
                   . After that, a threshold   equal to the max-
imum score in the latter list is set, and only units with scores 
greater than   are kept active. This can be described by the fol-
lowing equation: 
 
    n                                       
 
    {          n                                    
   n                  otherw se              
(6) 
The reason why     is set to an infinitely large value is that 
after the first phase of the algorithm, a subset of units is activat-
ed. The clique corresponding to the message we are looking for 
is guaranteed to exist in this subset given that we are dealing with 
partially erased messages. So, setting     in this fashion ensures 
that units that have failed to be active upon the first phase would 
be out of the search scope throughout the retrieval process. 
We propose to enhance the performance of the GLsKO rule 
by controlling the number of units   to be deactivated. This is 
only interesting when    . For example, if we set     in the 
network example of the previous paragraph, we get the following 
list of scores        . If   is not specified, all losers are deac-
tivated. But by setting    , only one of these two units is 
randomly chosen to be deactivated. This may be useful if we 
wish to exclude losers one at a time and thus reduce incautious 
quick decisions.  
VII. 
STOPPING CRITERIA 
Since the retrieval process is iterative, a stopping criterion 
should be used in order to put this process to an end. In parts A 
and B of this section we review the two classic criteria that are 
already in use. In parts C and D, we propose two new ones that 
are supposed to enhance performance. 
 A Fixed Number of Iterations (Iter) 
A.
A stopping criterion can be defined as a predefined number 
of iterations of the retrieval process. So dynamic and activation 
rules are applied iteratively, and when a counter announces that 
the desired number of iterations is attained, the retrieval process 
terminates and the activated units are taken for the retrieved 
message. The problem with this approach is that the stopping 
criterion which is a simple iteration counter is independent of the 
nature of retrieved message. That is, the activated units after the 
last iteration are not guaranteed to form a clique corresponding to 
an already stored message. This stopping criterion is only inter-
esting with the GWsTA rule. 
 The Convergence Criterion (Conv) 
B.
This criterion states that if the set of active units at iteration 
    is the same as that of iteration  , the retrieval process is 
taken as converged so it terminates and the result is output. The 
convergence criterion is only compatible with the GWsTA rule. 
In the case of the GLsKO rule, one or more active units are 
deactivated in each iteration. So it is not possible to have the 
same set of active units throughout two subsequent iterations.  
 The Equal Scores Criterion (EqSc) 
C.
The idea we propose here is that when all scores of active 
units are equal, the retrieval process terminates and the result is 
output. 
 The Clique Criterion (Clq) 
D.
This new criterion depends on the relationship between the 
number of activated units and their scores. If the activate units 
form a clique the retrieval process terminates. Thus, the retrieved 
message is more likely to make sense though it is not necessarily 
the correct result. In order to check if the activated units form a 
clique, we define the set of active units as     |         | |  , 
      as the score of the active unit    and   as an integer, then 
we apply the procedure shown in Figure 3: 
 
        | |. 
    
           (  )           n       | |            
 hen 
   output the result. 
term n te the retr ev l process  
Figure 3. The Clique stopping criterion (Clq). 
143
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-340-7
COGNITIVE 2014 : The Sixth International Conference on Advanced Cognitive Technologies and Applications

 
To make sense of this stopping criterion, we take an intuitive 
situation when      In this case, the stopping criterion is that 
when all active units have an equal score which is equal to the 
number of these units, a clique is recognized, so the process 
terminates and the result is output. 
It is worth noting that when using the GWsTA rule, it is al-
ways preferable to combine any stopping criterion with the Iter 
criterion so that when any one of them is satisfied the process 
terminates which prevents infinite looping. 
VIII. 
RESULTS 
We have seen that there are many possible combinations of 
dynamic, activation rules and stopping criteria in order to con-
struct a retrieval algorithm. In this section we will demonstrate 
the performance of some of these combinations. All messages 
used for the following tests are randomly generated from a uni-
form distribution over all possible message values. Reported 
retrieval error rates for a given number of stored messages are 
averaged over 100 trials. However, no significant difference was 
found between average error rates and error rates resulting from 
single trials. All the tests were written in C++, compiled with 
g++ and executed on a Fedora Linux operating system. 
 Comparing Dynamic Rules 
A.
Figure 4 shows that both the SoM and the SoS dynamic rules 
give a similar performance. The Norm rule was found to give the 
same results also, but it is not shown in the figure for clarity. 
This is not the case with the original network introduced in [4] 
where the Sum-of-Max rule was proved to give better results 
[15]. This is an interesting phenomenon that is worth studying. It 
may indicate that the major source of retrieval errors in this 
sparse version of the network is not related to the different meth-
o s o  c lcul t ng un ts’ scores. This renders the differences in 
performance due to the use of different dynamic rules insignifi-
cant. 
 Comparing Retrieval Strategies 
B.
We notice in Figure 5 that the GWsTA (    ) rule gives a 
better performance than the GWTA (equivalent to GWsTA with 
   ) rule used with the Conv stopping criterion with 30 itera-
tions allowed at most. This is due to the fact that the former rule 
has a better immunity to the phenomenon of spurious cliques 
described in Section VI.A. We also notice that the GWsTA 
(      rule gives even a lower error rate when the memory 
effect   is set to a large value such as       This is because 
setting   to a very large value restrains the search to only a lim-
ited region of the network where the target message is thought to 
exist. This is due to the fact that a large value of   guarantees that 
active units always get higher scores than other ones. So, in 
subsequent iterations, the set of active units would most often be 
the same or a subset of the previous active set. In all cases, the 
GLsKO (   ,    ) rule using the EqSc or the Clq (not 
shown on the figure for clarity) stopping criterion has the lowest 
error rate which almost achieves the performance of the brute 
force Maximum Likelihood retrieval algorithm (ML) (which is a 
simple exhaustive search for a maximum clique) for   erased 
input submessages out   . This is because the GLsKO rule con-
figured with such parameter values searches for the output in a 
limited subset of units resulting from phase 1 and excludes only 
one unit at a time before testing for the stopping criterion. This is 
proved by the degraded performance shown in Figure 6 of this 
same rule but without specifying a value of   which results in the 
exclusion of more than one unit at a time rendering the retrieval 
process less prudent and more susceptible to bad exclusions.  
We also notice that when a Willshaw-Palm network with 
       units is used with the GWsTA (        ) rule, 
the same performance as in a clustered network  is obtained. 
 The Number of Iterations 
C.
Figure 6 shows that the average number of iterations required 
to retrieve a message is relatively constant for all rules up to 
       messages learnt. Beyond this, the number of iterations 
required for the GLsKO and the GWsTA rules with      be-
gins to increase rapidly. It is worth emphasizing that the maxi-
mum number of iterations we allowed for the GWsTA rule is 30 
so the constant level reached by the curve representing this rule 
with     in Figure 6 is just a result of that constraint. However, 
the number of iterations for the GWsTA rule with        
increases only slightly approaching an average of 3.3 up to 
250000 messages stored.  
The reason for this explosion of the number of iterations in 
the case of the GLsKO rule is that the number of units activated 
after the first phase increases with the number of stored messag-
es. So more iterations would then be needed in order to exclude 
losers and thus shrink the set of active units. In the case of the 
GWsTA rule with    , all units in the network are concerned 
with the search for a message in each iteration. So when the 
number of stored messages increases, the connection density in 
the network gets higher and it then would be more likely that 
new winners appear in each iteration violating the Conv criterion. 
Figure 4. Influence of dynamic rules on retrieval error rates in a network 
with χ      𝑙     𝑐     γ    σ𝑖𝑗    initially, with 3 segments of 
partial erasure in input messages. 
Figure 5. Influence of activation rules on retrieval error rates in a net-
work with χ     , 𝑙    , 𝑐    , γ    if not stated otherwise σ𝑖𝑗  
  initially, with 3 segments of partial erasure in input messages. 
144
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-340-7
COGNITIVE 2014 : The Sixth International Conference on Advanced Cognitive Technologies and Applications

 
Setting   to 1000 limits the possibility of the apparition of new 
winners in each iteration and consequently decreases the number 
of iterations needed before satisfying the Conv criterion. 
IX. 
CONCLUSION AND FUTURE WORK 
In this paper, we analyzed the performance of retrieval algo-
rithms on extensions of a recently proposed sparse associative 
memory model. We demonstrated and compared the performance 
of these algorithms when using partially erased messages as 
inputs. We also provided comparisons between the model pro-
posed in this paper and the Willshaw-Palm model where we 
proved that the clustering constraint applied to the model we use 
which decreases the number of available connections does not 
necessarily affect performance when comparing with the Will-
shaw-Palm model. 
We found that our modified version of the GLsKO activation 
rule combined with the equal scores or the clique stopping crite-
ria gives the best results in terms of retrieval error rate but with a 
rapidly increasing number of iterations. Actually, the second 
phase of the GLsKO rule along with the clique criterion can be 
viewed as an operation equivalent to searching the maximum 
clique among active units. This is a famous NP-complete prob-
lem. However, many suboptimal solutions were suggested for 
this problem (or equivalently, the minimum vertex cover prob-
lem) such as [18] [19] and many more. We believe that such 
suboptimal solutions are adaptable to our problem and can be 
integrated in our retrieval algorithm in the future in order to give 
a better performance with a more reasonable number of itera-
tions. 
Finally, the retrieval algorithms presented in this work are all 
synchronous in the sense that, at each iteration, dynamic and 
activation rules are always applied to all clusters. In future work, 
we will consider asynchronous methods which can take into 
account the fact that some clusters may reach their final state 
before others, so application of dynamic and activation rules 
could then be limited to only a subset of clusters. We also aim at 
extending the scope of the algorithms presented in this paper to 
deal with other types of input messages, such as distorted ones in 
which some submessages have slightly modified values from 
their origin. 
 
ACKNOWLEDGMENT 
The authors would like to thank Zhe Yao and Michael Rab-
bat at McGill University for interesting discussions that helped 
leading to the proposal of the new retrieval techniques introduced 
in this paper. We also acknowledge funding from the NEUCOD 
project at the electronics department of Télécom Bretagne and 
support from the European Research Council (ERC-AdG2011 
290901 NEUCOD). 
REFERENCES 
[1] J. R. Anderson and G. H. Bower, Human associative 
memory.: Psychology press, 2013. 
[2] D. J. Willshaw, O. P. Buneman, and H. C. Longuet-Higgins, 
"Non-holographic associative memory," Nature, vol. 222, 
pp. 960-962, 1969. 
[3] J.J. Hopfield, "Neural networks and physical systems with 
emergent collective computational abilities," in Proceedings 
of the National Academy of Science, USA, vol. 79, 1982, 
pp. 2554-2558. 
[4] V. Gripon and C. Berrou, "Sparse neural networks with 
large learning diversity," IEEE Transactions on Neural 
Networks, vol. 22, no. 7, pp. 1087-1096, 2011. 
[5] V. Gripon and M. Rabbat, "Maximum Likelihood 
Associative Memories," in Proceedings of the IEEE 
Information Theory Workshop (ITW), 2013, pp. 1-5. 
[6] B. K. Aliabadi, C. Berrou, V. Gripon, and X. Jiang, 
"Learning sparse messages in networks of neural cliques," 
IEEE Transactions on Neural Networks and Learning 
Systems, August 2012, in press. 
[7] G. Palm, "Neural associative memories and sparse coding," 
Neural Networks., vol. 37, pp. 165-171, 2013. 
[8] V. B. Mountcastle, "The columnar organization of the 
neocortex," Brain, vol. 120, no. 4, pp. 701-722, 1997. 
[9] V. Gripon and X. Jiang, "Mémoires associatives pour 
observations floues," in Proceedings of XXIV-th Gretsi 
seminar, September 2013, in press. 
[10] D. P. Buxhoeveden and M. F. Casanova, "The minicolumn 
hypothesis in neuroscience," Brain, vol. 125, no. 5, pp. 935-
951, 2002. 
[11] F. T. Sommer and G. Palm, "Improved bidirectional 
retrieval of sparse patterns stored by Hebbian learning," 
Neural Networks, vol. 12, no. 2, pp. 281-297, 1999. 
[12] F. Schwenker, F. T. Sommer, and G. Palm, "Iterative 
retrieval of sparsely coded associative memory patterns," 
Neural Networks, vol. 9, no. 3, pp. 445-455, 1996. 
[13] L. Cruz et al., "A statistically based density map method for 
identification and quantification of regional differences in 
microcolumnarity in the monkey brain," Journal of 
Neuroscience Methods, vol. 141, pp. 321-332, 2005. 
[14] E. G. Jones, "Microcolumns in the cerebral cortex," PNAS, 
vol. 97, no. 10, pp. 5019-5021, 2000. 
[15] V. Gripon and C. Berrou, "Nearly-optimal associative 
memories based on distributed constant weight codes," in 
Proceedings of Information Theory and Applications 
Workshop, USA, 2012, pp. 269-273. 
[16] W. S. McCulloch and W. Pitts, "A logical calculus of the 
ideas immanent in nervous activity," The Bulletin of 
Mathematical Biophysics, vol. 5, no. 4, pp. 115-133, 1943. 
[17] X. Jiang, V. Gripon, and C. Berrou, "Learning long 
sequences in binary neural networks," in The Fourth 
International 
Conference 
on 
Advanced 
Cognitive 
Technologies and 
Applications 
(COGNITIVE 
2012) 
Figure 6. Average number of iterations for different scenarios in a 
network 
with 
χ      𝑙    , 𝑐    , 
γ    if 
not 
stated 
wise σ𝑖𝑗    initially, with 3 segments of partial erasure in input mes-
sages. 
145
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-340-7
COGNITIVE 2014 : The Sixth International Conference on Advanced Cognitive Technologies and Applications

 
IARIA, 2012, pp. 165-170. 
[18] X. Xu and J. Ma, "An efficient simulated annealing 
algorithm for the minimum vertex cover problem," 
Neurocomputing, vol. 69, no. 7, pp. 913-916, 2006. 
[19] X. Geng, J. Xu, J. Xiao, and L. Pan, "A simple simulated 
annealing algorithm for the maximum clique problem," 
Information Sciences, vol. 177, no. 22, pp. 5064-5071, 
2007. 
[20] Z. Yao, V. Gripon, and M. G. Rabbat, "A massively parallel 
associative memory based on sparse neural networks," 
Submitted to IEEE Transactions on Neural Networks and 
Learning Systems, arXiv:1303.7032v1 [cs.AI]. 
 
146
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-340-7
COGNITIVE 2014 : The Sixth International Conference on Advanced Cognitive Technologies and Applications

