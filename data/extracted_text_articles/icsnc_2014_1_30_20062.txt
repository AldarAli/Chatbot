Perceptual Semantics for Video in Situation Awareness
Mar´ıa Alejandra Pimentel-Ni˜no, Mar´ıa ´Angeles V´azquez-Castro and I˜nigo Hern´aez-Corres
Dept. of Telecommunications and Systems Engineering
Universitat Aut`onoma de Barcelona
Bellaterra, Spain 08193
Email: {mariaalejandra.pimentel, angeles.vazquez}@uab.es, inigo.hernaez@e-campus.uab.cat
Abstract—We introduce novel perceptual semantics for video
adaptation in multimedia communications. The target is to
enhance situation awareness in non-computer aided processes
as in emergency operations. Our proposed perceptual semantics
relate to end user requested resolution in the temporal domain
for a better assessment of event’s evolutions seen from streaming
video. Adaptation is enabled at transmission via a perceptual
semantics feedback loop to adapt source coding on-the ﬂy in
terms of frame rate. The overall framework contemplates the
use of an underlying cross-layer optimization that copes with
network congestion and erasures in best effort scenarios. We
show through simulations that within the proposed framework,
the perceptual semantics are preserved. Moreover, we show
it complies with information-centric-networking philosophy and
architecture, such that it is in line with content-aware trends in
networking.
Keywords–Perceptual
semantics;
adaptive
video;
situation
awareness; QoE.
I.
INTRODUCTION
The motivation of the work presented here is the possibility
of enhancing situational awareness using video streaming
through constraint networks. We target non-computer-aided
scenarios, where there is no artiﬁcial intelligence behind inter-
preting sensory information from the received video.
We consider band-limited, wireless best effort networks,
as possible means of communications for point-to-point live
video streaming during scenarios such as in emergency oper-
ations. Such networks pose a number of constraints affecting
Quality of Service (QoS), namely, congestion and erasures.
The topology envisioned is that of live user-generated content
being upstreamed to proper decision-makers.
Transmission alternatives that cope with the aforemen-
tioned network constraints include UDP- based frameworks for
live or real-time applications, with quality based [1], Quality of
Experience (QoE) driven cross-layer optimization [2], or TCP-
friendly [3] adaptive algorithms. Dynamic Adaptive Streaming
for HTTP over TCP streaming is suitable for server-client
architectures and video on demand applications, but with de-
graded performance in lossy networks with large propagation
delays [4]. With regards to the lossy nature of the network,
forward erasure correction methods are able to provide enough
protection and maintain QoE [5][3][6].
While these transmission frameworks can be satisfactory
to guarantee QoS/QoE in video-for-entertainmet scenarios,
we propose an additional dimension to target speciﬁc user
demands in scenarios using video for other purposes, such as in
emergency operations. We propose to improve non-computer-
aided situation awareness beyond standard improvements by
means of perceptual semantics.
In multimedia, “classic” semantics deals with heteroge-
neous metadata that sensors observe and/or tag when capturing
video. As such, it has applications in information retrieval,
integration and aggregation of varied data types as in semantic-
aware delivery of multimedia [7]. Further, semantic tagging de-
scribing pure observations is used in computer-based systems
with artiﬁcial intelligence to perceive and abstract situations
[8]. Rather than doing perception through classic semantics, we
propose a novel human-analysis-driven perceptual semantics to
tag the videos, based on the temporal/spatial characteristics a
user is perceiving as means to improve situation awareness.
The term perceptual semantics has been used by Cavallaro
and Winkler [9] for automatic feature extraction of video based
on image segmentation and target internal changes within the
video source coding mechanisms. Our approach differs, as it
does not limit to particular feature extraction and it focuses on
offering a solution for video communications in a non-intrusive
manner towards video codecs. Further, we involve the user
in tagging ﬁrst-level perceptual features, for perceptual-based
networking, rather than use only observations as in [10]. To the
best of our knowledge, such diversion from classic semantics
has not been explored before.
Finally, we frame the novelty of perceptual semantics such
that it complements cross-layer optimization schemes that help
cope with network constraints. Within this framework we
enable a perceptual semantic adaptation loop, which will target
the speciﬁc user’s demand for improved perception, compre-
hension and further projection in situation awareness pro-
cesses. Additionally, this framework can be mapped to current
content-centric approaches of information-centric-networking.
The structure of the paper is as follows. We present the
perceptual semantics model in Section II, followed by the
integration of perceptual semantics within a video adaptation
framework in Section III. We show simulation results in
Section IV, and draw ﬁnal conclusions in Section V.
II.
PERCEPTUAL SEMANTICS MODEL
In this section, we derive the model for perceptual seman-
tics in the context of situation awareness and how we propose
to perform semantic tagging.
A. Spatial/temporal decoupling for situation awareness
Situation awareness enables good decision-making [11] and
hence, it is a major asset in, e.g., emergency operations. A
11
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-368-1
ICSNC 2014 : The Ninth International Conference on Systems and Networks Communications

Figure 1.
Perceptual semantics vs “classic semantics”
broadly accepted deﬁnition is “the perception of the elements
in the environment within a volume of time and space, the
comprehension of their meaning and the projection of their
status in the near future.” [12]. The three-level model is thus
inferred, namely: perception, comprehension, and projection.
We focus on the spatial and temporal advantages of video
as a source of information for situation awareness. First, the
possibility of capturing dynamic scenes improves the assess-
ment of temporary evolving events. Second, video can provide
visual spatial accurate accounts of an ongoing situation [13].
If the temporal and spatial perceptual characteristics of the
video satisfy the situation-dependent speciﬁc user resolution,
then the user satisfaction will be fulﬁlled, and further higher
level cognitive processes will be beneﬁted.
B. Semantic tagging
Based on the spatio/temporal identiﬁcation of perceptual
features, our proposal is to utilize the end-user’s (analyst) per-
ception, to do semantic tagging that enables an enhancement of
the received video stream signal tailored to the user’s demand.
Semantic tagging is hence performed to describe perceptual
features in the video and as such represents more complex ab-
stractions of a viewed scene. In comparison, classic semantics
tagging would focus on unprocessed sensorial observations [8].
The difference between both approaches in semantics is shown
in Figure 1.
In scenarios where perception is not achieved by artiﬁcial
intelligence, it is the human analysis that will interpret the
sensory information and follow the three steps in the situation
awareness model. Hence, the semantic tagging is performed by
the user, as he is ultimately the one perceiving and foreseeing
what might be of interest in the video.
We propose a tagging that would indicate the tempo-
ral/spatial predominance according to the level of perception of
the user. A tag indicating predominance of temporal features,
means the user is perceiving a situation that demands more
attention to the dynamics of the scene (e.g., rapid movements,
evolution of an environmental hazard). On the other hand,
a predominance of spatial features indicate moments of less
movement but densely overloaded frames that requires more
detail to identify features (e.g., identifying persons or details
in a emergency scene).
III.
INTEGRATION OF PERCEPTUAL SEMANTICS TO VIDEO
ADAPTATION
We propose to integrate the perceptual semantics with
video adaptation in order to provide to the user the required
perceptual level for situation awareness. Further, we propose
to map the tags to actions, such that the speciﬁc perceptual
features are enhanced.
Following, we describe how our perceptual semantics
model can be mapped to video coding characteristics and
propose an algorithm to meet the end-user’s demands. We com-
ment on protocol aspects in the implementation and propose
an integrated framework with an adaptive video solution.
A. Mapping
We focus on using our proposed perceptual semantics for
enhancement at source coding level. In single layer or scalable
layer video encoding of state-of-the-art codecs, three types of
resolution are deﬁned, namely temporal (frame rate), amplitude
(quantization step), and spatial (frame size).
We map enhancement of temporal features to higher frame
rates, and predominance of spatial features to higher spatial
and amplitude frame resolution. In this way, dynamics of the
scene can be more closely followed (temporal preference) and
details of a scene can be better identiﬁed (spatial preference).
The mapping is intuitive and relies on the intrinsic architecture
of video codecs currently in use in a non-intrusive manner,
to facilitate the video communications. Finally, we show an
example architecture within Information-Centric Networking
(ICN) networking of a typical emergency scenario.
B. Algorithm
We propose to map the perceptual semantics to a system
quantiﬁed with the variable α ∈ [0, 1]. α = 0 and α = 1
express full preference of the spatial and temporal percep-
tual features, respectively. Intermediate values of α represent
weighed combinations of spatial and temporal preferences.
We denote the feasible set of ﬁnite values of frame rate,
as FT (rAP P ), while FS(rAP P ) is the feasible set for the
spatial factors, both a function of video coding rate rAP P . Note
that higher frame rates and frame sizes are possible to attain
with higher rAP P [14], hence the feasible sets FS(rAP P )
and FT (rAP P ) corresponding to higher values of rAP P will
contain more number of possible values that can be chosen
from. For example, in the case of scalable video coding, if
temporal dyadic scalability is performed, the available values
of frame rate contained in FT (rAP P ) would be the base
layer frame rate and the frame rates from enhancement layers
that would add up to i.e. a full 30Hz frame rate if rAP P is
sufﬁcient: FT (rAP P ) = {3.75Hz, 7.5Hz,15Hz,30Hz}.
In order to choose the appropriate value of frame rate and
resolution according to our mapping of perceptual semantics,
we formulate the following optimization function:
(r∗
fr, s∗
fr)
=
max (α¯rfr + (1 − α)¯sfr)
(1)
s.t.
rfr ∈ FT (rAP P ) and sfr ∈ FS(rAP P )
where ¯rfr = rfr/rmax
fr , and ¯sfr = sfr/smax
fr
are the normalized
values of frame rate rfr and spatial/amplitude resolution
12
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-368-1
ICSNC 2014 : The Ninth International Conference on Systems and Networks Communications

Figure 2.
Block diagram proposed cross-layer framework and APP-to-APP perceptual semantics loop
sfr with respect to maximum available values set for the
application. Note that the optimization in (1) can be applied
to single layer video coding or scalable video coding.
C. Implementation and compliance with standards
Figure 2 shows the diagram of the proposed framework.
1) Cross-layer framework: We assume an underlying stan-
dard cross-layer framework, that uses transport layer feedback
and interacts at transport-APP layers and transport-network
layers of the IP protocol stack.This framework provides the
application layer rate rAP P that can be used by the codec
(such that the video coding rate equals the application layer
rate), for an on-the-ﬂy adaptive video subject to network
constraints. Moreover, it provides to the network layer the
necessary parameters to perform forward erasure protection.
The cross-layer optimization is handling feedback with the
standard RTP/RTCP protocol [15] (Real Time Protocol/Real
Time Control Protocol). Note that we have assumed Forward
erasure protection being performed at network layer, in partic-
ular using Random linear Network Coding (RNC).
The cross-layer optimization has been designed such that
it copes with the network impairments that directly affect
negatively the spatial/temporal aspects of video and is therefore
QoE-driven. This time/space cross-layer optimization asso-
ciates congestion with temporal impairments in video playback
such as freezes. In addition, it associates erasures with artifacts
degrading video quality. Further, we keep in mind that higher
video quality is achieved with higher video codec rate, rAP P .
The above assumption is relevant in the design, given that
the cross-layer optimization is able to mitigate the negative
effects of network degradations due to congestion and erasures.
Therefore, network degradations will minimally affect the job
of the perceptual semantics when enhancing the temporal and
spatial features necessary for situation awareness. We will
further discuss this with the numerical results in Section IV.
2) Perceptual semantics: Figure 2 further shows how the
cross-layer optimization is integrated with the perceptual se-
mantics loop. The video streaming application uses a state-of-
the-art codec such that the frame rate, frame size and codec
rate can be conﬁgured on-the-ﬂy, either as a single layer or a
scalable layer coding.
In order to facilitate the perceptual semantics role, we use a
return path to send the tags chosen by the user according to the
perceptual semantics. The semantics-aware adaptation block in
Figure 2 interprets the semantic tags coming from the end-user
by mapping it to the proper decisions and forwarding to the
video codec, as explained in Section III.
Following the trends in current network architectures, we
propose to use semantic web protocols to enable the APP-to-
APP cross talk of the semantic tagging [10]. At the transport
layer, the application-speciﬁc information can be encapsulated
into RTCP feedback packets compliant with the extended re-
ports deﬁned in RFC4585. This way, the perceptual semantics
feedback loop is coherent with the cross-layer optimization.
3) Coherence
with
ICN
networking:
Considering
fu-
ture architectures, our framework complies with a semantic
information-based network [16]. The aim of the Information-
Centric Networking (ICN) approach is to integrate content
delivery as a native network feature, where focus is not on the
network as an enabler of communication links but as a platform
for information dissemination. ICN could allow for future
enhancements to the perceptual semantics as proposed in this
paper. In particular, our approach is coherent to the receiver-
driven nature of ICN. Further, caching, one of the appealing
attributes of ICN in data delivery, could enable more actions at
intermediate nodes concerning the incoming video stream. The
philosophy of ICN by which content information is available
to network/forwarding layers will allow the semantic loop we
have created to trigger further actions at these intermediate
nodes, such as adaptive network coding to enhance last-mile
network reliability.
Figure 3 shows the topology of our framework mapped to
the publish/subscribe ICN architecture for live streaming by
Tsilopoulos et al. [17]. It is a typical example of an emergency
application over a satellite access network, whose gateway
can be mapped to the rendevouz (RN) and topology manager
(TM) nodes. The publisher (operator in the ground during an
emergency) announces that it has a publication available to
the RN node. The subscriber (end-user/decision maker) issues
a subscription, as he is interested in obtaining live feed of the
current on-going events of the emergency. The RN and TM
nodes ﬁnd the publisher and resolve the publisher/subscriber
path. The subscriber can issue petitions or unsubscribe, and in
our framework, issue perceptual semantics tagging, which the
publisher will receive through the RN nodes.
13
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-368-1
ICSNC 2014 : The Ninth International Conference on Systems and Networks Communications

Figure 3.
Publish/Subscribe architecture suitable for our proposed perceptual
semantics framework
TABLE I.
FEASIBLE SETS CONSIDERED FOR SIMULATION
rAP P (in kbps)
Feasible Set FT
Feasible set FS
rAP P ≤ 64
{3.75, 7.5,10,15}
{QCIF}
64 < rAP P ≤ 192
{3.75, 7.5,10,15}
{QCIF,CIF}
192 < rAP P ≤ 384
{3.75, 7.5,10,15}
{CIF,QCIF}
384 < rAP P ≤ 500
{3.75, 7.5,10,15,30}
{QCIF,CIF,640x360}
IV.
SIMULATION RESULTS
We simulate a realistic scenario typical of emergency op-
erations, where mobile satellite services are used to upstream
live video from ﬁeld, to proper decision makers remotely
located. To our knowledge, there is no similar framework
in the literature to match our proposed perceptual semantics
framework and hence comparison to solutions that do not have
such aim would be unfair. Therefore, our results are compared
to not having such kind of framework.
A. Setup
We use a simulation system that allows to test the proposed
framework shown in Figure 2. The video streaming application
is simulated by generating packets of size l encoded at a rate
rAP P and frame rate rframe.
1) Cross-layer optimization setup for congestion and era-
sures: This block receives as inputs the feedback from the
receiver on current network conditions, and outputs the rate
rAP P that the video streaming application is allowed to use,
and the code rate ρ to be used for erasure correction, such that
the transmission rate is R = rAP P/ρ.
The transmission rate is online optimized through a QoE
delay-driven optimization at the sender side that uses receiver
feedback, as the one proposed in [2]. The resulting discrete
rate control update is given by (2)
R(tk+1) = R(tk) + f(τ(tk − τD))
(2)
where f(·) is a function of the delay τ measured at time tk −
τD, a delayed value due to the propagation delay τD in the
feedback loop. Updates on network measurements are received
every Tsamp = tk+1 − tk seconds.
Further, our additional novelty to cope with erasures, is
the use of adaptive network coding with Systematic Random
linear Network Coding, (SRNC). We use SRNC due to similar
performance to optimal forward erasure correction codes like
Reed-Solomon [5], but higher ﬂexibility and compliance with
future network-coded networks. For a rate budget given by
R in (2), the code rate ρ = rAP P /R, chosen for SRNC is
maximized such that the performance meets a target residual
erasure rate given the current erasure rate ϵ of the network.
Hence the application layer rate rAP P is maximized.
2) Network simulation: We simulate a network as a FIFO
ﬁnite queue of available rate rav with erasure rate ϵ. Simulated
packets are transmitted at the obtained rate R.
SRNC uses the allocated code rate ρ to meet the complete
budget rate R, such that its performance meets the residual
erasure rate ϵres.
Congestion events are simulated as a drop (step-like) in
maximum available rate rmax
av
to rmin
av
that occurs halfway
through one streaming session, at T/2 such that η
=
rmax
av
−rmin
av
rmax
av
, with η ∈ (0, 1]. (Higher η means higher conges-
tion). Each simulation, corresponding to one streaming session,
lasts 300s, one corresponding value of η and ϵ.
The values used correspond to a realistic satellite network
commonly used in emergency operation, operating in the L-
band offering up to 500kbps uplink in best effort mode.
3) Perceptual semantics: We model the user’s semantic
tagging from temporal/spatial features with the parameter α. α
may vary over time throughout one single streaming session,
such that the sender is receiving feedback of this changes
and will adapt to them using, e.g., (1). We assume these tags
are changed by the user with a period of at least 10s. Three
cases are considered for variation of semantic tagging, namely,
TAGT : only temporal tagging for the entire session, TAGS:
only spatial tagging, TAGT S: alternating tags, each of 10s.
Table I summarizes the feasible sets for values of frame rate
dependent on rAP P , in order to solve the algorithm in (1). The
values chosen correspond to typical feasible combinations in
current state-of-the art codecs.
B. Metrics
The following metrics relate to the effects of the network
constraints in terms of Quality of Experience.
1) QoEA. : This metric is related to degradations due to
erasures in the network, that cause artifacts in the image:
QoEA = 1 − ¯p, where ¯p is the average packet loss rate at
the receiver. QoEA ∈ [0, 1].
2) QoEF . : This metric is related to degradation due to
congestion, that cause freezes in video playback. QoEF =
1 − ¯f where ¯f is the probability of freezes occurring in
the playback. A freeze is the event where the time elapsing
between two consecutive frames displayed exceeds a tolerated
threshold. QoEF ∈ [0, 1].
3) ˆα and ∆α.: These metrics are related to the performance
of the adaptation through perceptual semantics. We measure
the value achieved by the algorithm as ˆα, and the mean
absolute error with respect to the user’s demanded α, as
∆α = |ˆα − α|.
14
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-368-1
ICSNC 2014 : The Ninth International Conference on Systems and Networks Communications

0
0.05
0.1
0.15
0.2
0
0.5
1
0
0.2
0.4
0.6
0.8
1
 
ε
Achieved values of  α
η
 
TAGT
TAGS
TAGTS
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Figure 4.
Achieved values of α vs. η vs. ϵ, using cross-layer optimization
4) Ω.: Combined metric to measure tradeoffs of using per-
ceptual semantics with and without cross-layer optimization.
It is deﬁned as:
Ω = w1·QoEA + w2·QoEF + w3·(1 − ∆α)
with w1 +w2 +w3 = 1. Ω ∈ [0, 1]. The best performance, i.e.,
Ω = 1, occurs when no losses degrade the video (QoEA →
1), freezes in playback are minimal (QoEF → 1) and the
perceptual semantic adaptation matches the one requested by
the user (∆α).
C. Results
1) Perceptual semantics with and without cross-layer op-
timization: Figure 4 shows the performance with respect to
metric α of the perceptual semantics together with the cross-
layer optimization, as a function of congestion drops, η, and
erasures ϵ. Each surface corresponds to one of the three
cases of time varying semantic tagging. TAGT achieves high
values of α close to the tagged from the user, representative
of preference on temporal features, while TAGT S offers an
intermediate values, corresponding to the alternating tags. α
only reﬂects on the performance of the perceptual semantics
algorithm, and whether it is capable to achieve the expected
user demand. However, it does not reﬂect the effects on QoEF
and QoEA, directly affected by network degradations. Ω will
express the full performance as a whole.
In order to observe the combined effects of the adaptation
through perceptual semantics with an underlying cross-layer
optimization, we observe the individual metrics. The compar-
ison is made between using cross-layer optimization to cope
with the network constraints, or no use of it.
Figure 5a shows the value of ∆α as a function of η and
ϵ. In order to achieve high QoS and QoE with the cross-layer
optimization, the application layer rate rAP P is sacriﬁced, as
more rate is needed to protect from network erasures. Hence,
the feasible set of frame rates is reduced, and the obtained
α can not achieve the highest expected value. This can be
observed with higher values of ∆α as ϵ increases.
Nevertheless, the cross-layer optimization is guaranteeing
very low packet losses, as Figure 5b shows, which translates
into minimal artifacts in the video. Hence, while seemingly ∆α
0
0.05
0.1
0.15
0.2
0
0.2
0.4
0.6
0.8
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
 
η
ε
TAGT
 
Ω
with cross−layer optimization
without cross−layer optimization
Figure 6.
Ω metric for TAGT
0
0.05
0.1
0.15
0.2
0
0.2
0.4
0.6
0.8
0.4
0.5
0.6
0.7
0.8
0.9
1
 
ε
η
TAGTS
 
Ω
with cross−layer optimization
without cross−layer optimization
Figure 7.
Ω for time-varying semantic tagging TAGT S
is not as low as expected, the user is guaranteed a seamless
video playback.
Figure 6 shows the combined metric Ω, where the above
trade-off result into higher performance when using cross-layer
optimization in combination with the perceptual semantics
loop, especially for highly degraded networks.
2) Time varying perceptual semantics tagging: We analyze
the effects of time-varying perceptual tagging, representing
a realistic case where the user identiﬁes different situations
that demand attention towards temporal or spatial features.
These variations are represented as alternations of temporal
and spatial tagging. Figure 7 shows the performance in terms
of the combined metric Ω.
In addition to achieving the expected α demanded through
the semantic tagging, the performance is above 80% regardless
of the degradations of the network, thanks to the cross-layer
optimization. The performance is highly degraded due to con-
gestion, as well as erasures when no cross-layer optimization
is used, with performance dropping to 40%. In conclusion,
Figure 7 shows that the cross-layer optimization preserves the
perceptual semantics.
V.
CONCLUSION AND FUTURE WORK
We have presented in this paper a framework where we
introduced perceptual semantics for video adaptation. Percep-
15
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-368-1
ICSNC 2014 : The Ninth International Conference on Systems and Networks Communications

0
0.05
0.1
0.15
0.2
0
0.2
0.4
0.6
0.8
0
0.05
0.1
0.15
0.2
0.25
 
TAGT 
ε
η
 
∆a
without cross layer optimization
With cross−layer optimization
0
0.05
0.1
0.15
0.2
0.25
(a) ∆α for case 1
0
0.05
0.1
0.15
0.2
0
0.2
0.4
0.6
0.8
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
 
ε
TAGT
η
 
packet loss rate
0.1
0.2
0.3
0.4
0.5
0.6
without cross−layer optimization
with cross−layer optimization
(b) ¯p
Figure 5.
∆α and ¯p for TAGT
tual semantics are used to acknowledge the user’s demand in
the context of situation awareness, where special attention is
required when using video as means to perceive, comprehend
and project ongoing situations, in particular for emergency
scenarios. We have presented a novel model for perceptual
semantics, based upon these demands, and propose a frame-
work to be integrated into a video adaptive solution, for non-
computer aided situation awareness. We discussed how to
practically implement perceptual semantics into an adaptive
loop that works with an underlying cross-layer optimization
in charge of coping with network constraints typical of best
effort wireless scenarios. Further, we have shown an adaptive
algorithm that translates the perceptual semantics into temporal
and spatial resolutions at codec level. Finally, our frame-
work is contextualized for information-centric-networking. Our
simulation results show how the perceptual semantic tagging
achieves the expected user demands while the underlying
cross-layer optimization preserves such performance. Future
work includes extensions of perceptual semantics in the ICN
context. Moreover, we will study more pertinent QoE metrics
to match user’s satisfaction when using perceptual semantics.
Finally, the presented framework will be further developed for
practical usage to implement a potential prototype.
REFERENCES
[1]
O. Habachi, Y. Hu, M. van der Schaar, Y. Hayel, and F. Wu, “MOS-
based congestion control for conversational services in wireless environ-
ments,” Selected Areas in Communications, IEEE Journal on, vol. 30,
no. 7, 2012, pp. 1225–1236.
[2]
M. A. Pimentel-Ni˜no, P. Saxena, and M. A. V´azquez-Castro, “QoE
Driven Adaptive Video with Overlapping Network Coding for Best
Effort Erasure Satellite Links,” in 31st AIAA International Commu-
nications Satellite Systems Conference, Florence,Italy, 2013.
[3]
H. Seferoglu, A. Markopoulou, U. C. Kozat, M. R. Civanlar, and
J. Kempf, “Dynamic FEC algorithms for TFRC ﬂows,” Trans. Multi.,
vol. 12, no. 8, Dec. 2010, pp. 869–885.
[4]
B. Wang, J. Kurose, P. Shenoy, and D. Towsley, “Multimedia Streaming
via TCP: An Analytic Performance Study,” ACM Trans. Multimedia
Comput. Commun. Appl., vol. 4, no. 2, May 2008, pp. 1–22.
[5]
P. Saxena and M. A. V´azquez-Castro, “Network Coding Advantage
over MDS Codes for Multimedia Transmission via Erasure Satellite
Channels,” in Personal Satellite Services (PSATS), 2013 International
Conference on, October 2013, pp. 199–210.
[6]
R. Immich, E. Cerqueira, and M. Curado, “Adaptive video-aware fec-
based mechanism with unequal error protection scheme,” in Proceedings
of the 28th Annual ACM Symposium on Applied Computing, ser. SAC
’13.
New York, NY, USA: ACM, 2013, pp. 981–988.
[7]
J. Thomas-Kerr, C. Ritz, and I. Burnett, “Semantic-aware delivery of
multimedia,” in Communications and Information Technology, 2009.
ISCIT 2009. 9th International Symposium on, Sept 2009, pp. 1498–
1503.
[8]
C. Henson, A. Sheth, and K. Thirunarayan, “Semantic perception:
Converting sensory observations to abstractions,” Internet Computing,
IEEE, vol. 16, no. 2, 2012, pp. 26–34.
[9]
A. Cavallaro and S. Winkler, “Perceptual semantics,” in Multimedia
Technologies: Concepts, Methodologies, Tools, and Applications, S. M.
Rahman, Ed.
Information Science Reference, Jun. 2008, ch. 7.5, pp.
1441–1455.
[10]
S. B. Kodeswaran and A. Joshi, “Content and context aware networking
using semantic tagging,” in Data Engineering Workshops, 2006. Pro-
ceedings. 22nd International Conference on.
IEEE, 2006, pp. 77–77.
[11]
J. Strassner, J. Betser, R. Ewart, and F. Belz, “A Semantic Architecture
for Enhanced Cyber Situational Awareness,” in Secure and Resilient
Cyber Architectures Conference, 2010.
[12]
M. R. Endsley, “Theoretical underpinnings of situation awareness: A
critical review,” Situation awareness analysis and measurement, 2000,
pp. 3–32.
[13]
S. S. Krupenia, C. Aguero, and K. C. Nieuwenhuis, “The value
of different media types to support command and control situation
awareness,” in Proceedings of 9th International ISCRAM Conference,
2012, pp. 1–5.
[14]
Z. Ma, F. Fernandes, and Y. Wang, “Analytical rate model for com-
pressed video considering impacts of spatial, temporal and amplitude
resolutions,” in Multimedia and Expo Workshops (ICMEW), 2013 IEEE
International Conference on, July 2013, pp. 1–6.
[15]
H. Schulzrinne, S. Casner, R. Frederick, and V. Jacobson, “RTP: A
transport protocol for real-time applications, IETF RFC 3550,” United
States, 2003.
[16]
H. Wirtz and K. Wehrle, “Opening the loops - towards semantic,
information-centric networking in the internet of things,” in Sensor,
Mesh and Ad Hoc Communications and Networks (SECON), 2013 10th
Annual IEEE Communications Society Conference on, June 2013, pp.
18–24.
[17]
C. Tsilopoulos, G. Xylomenos, and G. Polyzos, “Are Information-
Centric Networks Video-Ready?” in Packet Video Workshop (PV), 2013
20th International, Dec 2013, pp. 1–8.
16
Copyright (c) IARIA, 2014.     ISBN:  978-1-61208-368-1
ICSNC 2014 : The Ninth International Conference on Systems and Networks Communications

