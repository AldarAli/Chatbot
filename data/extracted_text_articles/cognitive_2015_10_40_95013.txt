Multidimensional Pilot Crew State Inference for
Improved Pilot Crew-Automation Partnership
Stefan Suck and Florian Fortmann
OFFIS - Institute for Information Technology
Oldenburg, Germany
Email: stefan.suck, ﬂorian.fortmann@ofﬁs.de
Abstract—Automation is a substantial technology of modern air-
craft. Even though automation has signiﬁcantly improved aviation
safety, insufﬁcient partnership between the pilot crew and the
automation, and confusion over the status of the automation is
still a problem. The European project A-PiMod addresses these
problems by developing a virtual crew member, which takes the
position of classical aircraft automation. As part of the crew, the
virtual crew member must be able to anticipate the internal states
of the human crew members. This ability helps, e.g., to improve
the task share in the cockpit by means of dynamic adaptions
of task distributions. In this paper, we present the concept of
the A-PiMod pilot model, which will be used for inferring the
internal state of the human crew members. The internal state is
composed of different sub-states, which have been deﬁned during
the initial phase of the project. The addressed sub-states are
situation awareness, workload, and intentions. The target states
will be inferred based on real-time data about the mission, tasks,
and pilot behaviors, including what they say, where they look at,
and how they act.
Keywords–Human-Machine Cooperation; Cognitive Model; Air-
craft Crew.
I.
INTRODUCTION
Automation is a substantial technology of modern aircraft
[1]. Automation accomplishes (partially or fully) a task that
was previously carried out (partially or fully) by a human
operator [2]. Overall, automation has signiﬁcantly improved
aviation safety [3]. However, after many years of automation
it turned out that this technology is like a two-edged sword. It
has been shown that there are several pitfalls associated with
automation [4], such as insufﬁcient partnership between the
pilot crew and the automation, and confusion over the status
of the automation. These pitfalls refer, e.g., to the lack of com-
municating internal states, including the situational picture and
the intents of the pilot crew and the automation. Insufﬁcient
partnership between the pilot crew and the automation has led
to several accidents in the past. A well-studied example is the
crash of China Airlines Flight 140, which can be attributed
to conﬂicting intentions [5] between the pilot crew and the
automation.
The European project Applying Pilot Models for Safer
Aircraft (A-PiMod) [6] addresses two major issues of the
aviation domain: (1) poor pilot crew-automation partnership,
and (2) confusion over the status of automation. As introduced
above, both issues are highly connected to each other. In
order to tackle these issues, the project aims to develop a
virtual crew member, which takes the position of classical
aircraft automation. The virtual crew member should perfectly
integrate into the pilot crew resulting in a cooperative human-
machine cockpit crew. As part of the crew, the virtual crew
member must be able to anticipate the internal states of the
Figure 1: Partnership in the aircraft cockpit between human crew members
and a virtual crew member.
human crew members, as well as the human crew members
must be able to anticipate the internal states of the human and
virtual crew members. This ability helps, e.g., to improve the
task share in the cockpit by means of dynamic adaptions of
task distributions. The concept underlying the A-PiMod project
is sketched in Figure 1. A mission is achieved cooperatively by
sharing tasks according to the individual capabilities of each
crew member. The basis of good partnership is a sufﬁcient
understanding of each crew member about the internal states
of the other crew members.
In this paper, we present the concept of the A-PiMod pilot
model, which will be used for inferring the internal state of
the human crew members. The concept of our pilot model
combines cognitive and probabilistic modelling approaches.
The internal state is composed of different sub-states, which
have been deﬁned during the initial phase of the project. The
addressed sub-states are situation awareness, workload, and
intentions. The sub-states will be inferred based on real-time
data about the mission, tasks, and pilot behaviors, including
what they say, where they look at, and how they act.
In Section II, a short overview of cognitive and prob-
abilistic operator models and some of their applications is
given. Section III introduces the pilot model and describes the
different target states. The integration into the A-PiMod archi-
tecture and the interaction with other A-PiMod components
is explained in Section IV. Section V concludes and reveals
future steps.
II.
RELATED WORK
There is a great effort within the human modelling commu-
nity to develop operator models to support the development of
complex human-machine systems. The technology underlying
201
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-390-2
COGNITIVE 2015 : The Seventh International Conference on Advanced Cognitive Technologies and Applications

these models is as diverse as the purpose of using them. The
A-PiMod pilot model combines cognitive and probabilistic
modelling approaches. For this reason, we provide an overview
of these modelling approaches in this section.
A. Cognitive Models
Cognitive models are intended to describe mental processes
of human agents. An overview of extant cognitive compu-
tational models is provided in [7]–[11]. Cognitive models
describes cognitive processes like human perception, decision
making, memory and learning processes. When cognitive mod-
els are implemented in software, they can be used to simulate
human behavior and to predict human error. For example, these
cognitive architectures can be used to support the development
of user interfaces in early design phases. A cognitive architec-
ture can be understood as a generic interpreter that executes
formalized task models in a psychological plausible way.
Cognitive architectures were established in the early eight-
ies as research tools to unify psychological models of par-
ticular cognitive processes [12]. The most noted cognitive
architectures are Adaptive control of Thought Rational (ACT-
R) [13][14], State Operator Apply Result (SOAR) [15][16]
and Man-Machine Integration Design and Analysis System
(MIDAS) [17][18]. These early models only dealt with labora-
tory tasks in non-dynamic environments [19][20]. Furthermore,
they neglected processes such as multitasking, perception and
motor control that are essential for simulating human-machine
interaction in highly dynamic environments. Models such
as ACT-R and SOAR have been extended in this direction
[21][22] but still have their main focus on processes suitable
for static, non-interruptive environments. Other cognitive mod-
els like MIDAS [23], Architecture for Procedure Execution
(APEX) [24] and Cognitive Network of Tasks (COGNET)
[25] were explicitly motivated by the needs of human-machine
interaction and thus focused for example on multitasking right
from the beginning. The cognitive architecture Cognitive Ar-
chitecture for Safety Critical Task Simulation (CASCaS) was
developed by [26] and recognized by [27] as one of the best
in the world. CASCaS has been applied in several projects, in
order to analyse perception [28], attention allocation [10][29],
decision making [26], and error [26][30] of humans in the
automotive and aviation domains.
B. Probabilistic Models
While cognitive architectures are usually based on rules
(CASCaS) or semantic networks (MIDAS) other approaches
utilize probabilistic methods to model human operators. In
[31] the author employs Hidden Markov Models (HMM), to
describe the instrument scanning behaviour of aircraft pilots.
In [32] HMMs are used to infer on the behaviour of operators
of unmanned aerial vehicles and the currently performed task
by monitoring operators’ interactions with a User Interface.
In the automotive domain, there are approaches which
employ probabilistic driver models. In [33], a hierarchical
structure of Dynamic Bayesian Networks (DBN) is used to
generate driving behaviours and actions from driving goals. It
is also shown that it is possible to derive the behaviours and
driving manoeuvers of the driver from his actions. Another ex-
ample for the inference and classiﬁcation of driving behaviours
can be found in [34]. In the domain of Intention Recognition
Systems, DBNs are used to determine the intentions of drivers
[35] and to infer the intent of software users to provide speciﬁc
help [36].
C. Related Applications
The pilot model used in the Crew Assistant Military
Aircraft (CAMA) consists of a petri-net based part to model the
normative pilot behaviour and a adaptive part which is based
on fuzzy rules [37]. The adaptive part determines if deviations
from the normative model are errors or were intended by the
pilot due to, e.g., high workload.
Cognition Monitor (COGMON) [38] is a multidimensional
approach to provide information about the state of a air-
craft pilot. It relies on subjective, contextual, behavioural and
physiological measures. However, to collect the physiological
data intrusive techniques like electroencephalography are used,
which we aim to avoid in A-PiMod.
III.
CREW STATE INFERENCE
Knowledge about the cognitive pilot crew state provides
the possibility of adapting the systems state accordingly. The
cognitive pilot crew state cannot be estimated as a whole.
Instead, the cognitive pilot crew state has to be decomposed
into target states which will be estimated. In the past, there
has been research on a broad range of target states, such as
situation awareness [39][40] and workload [41][42]. Although
there may be more target states it turned out, during the
requirements engineering phase of A-PiMod that the Crew
State Inference (CSI) will be focused on the following target
states:
•
Intentions: Does the pilot intend to perform the tasks
he is assigned to or is he intending to do something
different?
•
Workload: In how far is the pilot cognitively and
physically used to capacity?
•
Situation awareness: Is the pilot aware of the things
around him that are of interest in context of the ﬂight
task?
For the acquisition of the necessary data for the deﬁned
target states, the Crew State Inference relies on non-intrusive
techniques. The CSI infers the state of every human crew
member separately. The intention is the ﬁrst target state to
be estimated. The results of the intention recognition are also
used as inputs for the assessment of the situation awareness.
A. Intention
The A-PiMod architecture provides adaptiveness in the
manner of automation and crew-automation interaction. To
realize this adaptiveness the automation needs information
about when to provide assistance or when it is necessary to
interfere. To determine this, it is desirable for the system to
know the pilot’s intention. If the intention is not consistent
with the situation known to the system, which could lead to a
critical situation, there would be a need for further interaction
with the crew.
In [43], intention is described as a composite concept
specifying what the agent has chosen and how the agent is
committed to that choice. The agent from this statement can
be a pilot and his choice refers to a goal. This reﬂects that
the pilot’s intentions are strongly connected to the goal he is
actually trying to achieve. As already mentioned, the agent
202
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-390-2
COGNITIVE 2015 : The Seventh International Conference on Advanced Cognitive Technologies and Applications

Figure 2: Basic Bayesian Network for a change altitude task
needs to be committed to this goal. That means that the
agent must be able to take part in a plan, which is needed
to achieve the goal. A plan is mainly a certain behaviour,
which is a sequence of observable actions, that leads to the
achievement of a speciﬁc goal. Plans can be more or less
complex. Complex plans usually can be separated into sub-
plans. Thus, the complex plans become goals of their sub-
plans. In the literature the intention recognition becomes plan
recognition in this case. The tasks of a pilot also serve the
achievement of a goal. Complex task can be separated into
sub-tasks and can become the goals of their sub-task, too. So,
a task of a pilot can be interpreted as equivalent to a plan or a
goal. To execute a task, a pilot has to show a speciﬁc behaviour
which consists of certain actions. This means that, for each
task, there exists some set of actions which is typical for this
task. Many of these actions can be observed, e.g., interactions
with the conventional cockpit interfaces or touch displays, or
the gaze on instruments. On the basis of these observations the
CSI Intention module infers on the tasks which are currently
performed by the pilot. For the task inference, a Bayesian
network is used. A basic example for the task to change
the altitude is shown in Figure 2. The depicted network is a
segment of the currently implemented network. The nodes in
the network can be, depending on the type of information they
represent, divided into the following groups: task nodes, con-
text observation nodes, and action nodes. The node CHG ALT
represents the task, the node clearance is a general observation
of the context and represents the availability of an clearance
from Air Trafﬁc Control (ATC) for an altitude change. Context
information can make the inference more robust if the action
patterns of tasks are very similar. The nodes SEL TGT ALT
and ACT DESC MODE are actions and represent interactions
with the Flight Control Unit (FCU). CHK FCU, CHK PFD
and CHK FMA are also actions and represent if the pilot has
looked, e.g., at the FCU. Every network node has a probability
table which quantiﬁes the inﬂuence of the parent nodes on
the current node. For nodes without parents (no incoming
edges) a-priori probabilities have to be deﬁned. If an action
is performed, its corresponding node gains the state true, this
results in an increasing probability of all tasks that can cause
this action. The Bayesian network is currently constructed
manually. The structure is based on a task analysis which was
made in advance. The necessary parameter values for these
probability tables are currently chosen on the basis of this
task analysis. These values will be revised on the basis of the
data which will be collected during simulator experiments. The
intention inference delivers for each task node a probability
value that this task is currently being executed by the currently
considered pilot. The tasks with a probability value above a
certain threshold are interpreted as the currently executed tasks
of the pilot. These are the subjective tasks of a pilot, which
are the output of the intention inference module. Thanks to
this approach based on a Bayesian network, we will be in the
position to recognize the pilot’s intentions seen as goals or
tasks.
B. Workload
High, as well as too low workload can inﬂuence the pilots’
performance negatively. Thus, the purpose of the workload
module is to determine in how far the pilot is cognitively
and physically used to capacity. In this module the workload,
of a pilot is described by a multi-resource model which is
comparable to the one of Wickens [44]. The dimensions of our
workload model are Visual perception, Visual processing, Au-
ditory perception, Auditory processing, and Auditory action.
According to this model the pilots’ cognitive and physical ca-
pacities in the different dimensions are limited. The execution
of tasks causes the consumption of some of these capacities,
which leads to an increased workload. The relevant tasks were
identiﬁed and described during a task analysis. The description
of every identiﬁed task is stored in a task pool. The task
description contains, among other things, information about the
workload which is created by a task in the different dimension.
These workload values were collected by interviewing pilots
with a questionnaire. To estimate the current workload of the
pilot, his objective tasks, the tasks he is currently assigned to
are taken into account. The workload values stored in the task
descriptions for each dimension are summed up over the pilot’s
objective tasks. These aggregated workload scores reﬂect the
actual load of the pilot in the different dimensions and are the
output of the workload module.
C. Situation Awareness
Situation awareness (SA) is a state of knowledge which is
the product of a cognitive process, called situation assessment.
During situation assessment, operators interpret available envi-
ronment and system information in the context of their current
goals. Based on SA, operators decide what they are going to
do in a certain situation. Due to high automation, loss of SA
can remain without bad consequences in standard situations but
can lead to accidents in critical situations. Knowing the current
coverage of SA at each moment in time during operation could
help to prevent incidents and accidents caused by incorrect
SA. The purpose of the SA module of the CSI is real-time
SA inference. The basis for the inference process is a formal
situation model and a formal model of SA which represents
the human operators subjective state. Both models consist of
a set of atomic elements. These elements can refer to basic
items like information or more complex things like tasks.
Information elements are linked to the perception level of the
SA while the tasks are more related to the comprehension level.
203
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-390-2
COGNITIVE 2015 : The Seventh International Conference on Advanced Cognitive Technologies and Applications

Figure 3: Description of SA inference for perceptive aspects
The human operator can update the subjective model by, e.g.,
focusing on sources of information. The state of each atomic
element of the SA model can be compared to the state of each
element of the situation model. The approach is visualized
in Figure 3. The comparison allows detecting inconsistencies
between the real situations and situations as they are perceived
by the operator. Currently, SA is focused on the tasks of a
human operator. The tasks of the Situation Model are the
objective tasks of the pilot. The current set of objective tasks
is delivered by an external component, the Task Distribution
module. The active subjective tasks of the SA model are
updated by the Intention inference module. The objective and
the subjective tasks of the human operator are compared. Thus
it can be determined if the operator performs the tasks which
are appropriate for the current situation. With an eye-tracker
it would also be possible to consider the basic information
elements. These elements are then updated in the subjective
model whenever the pilot gazes on the corresponding cockpit
instrument.
IV.
INTEGRATION INTO A-PIMOD ARCHITECTURE
The A-PiMod architecture consists of several new com-
ponents which do not exist in present cockpits. The aim is
to provide further assistance and to improve the interaction
between the human pilots and the automation. The new com-
ponents are Mission Level Situation Determination, Mission
Level Risk Assessment, Cockpit Level Situation Determina-
tion, Task Determination at Cockpit Level, Task Distribution,
Cockpit Level Risk Assessment, Human Machine Multimodal
Interface (HMMI), HMMI Interaction Manager and Crew State
Inference. The Crew State Inference communicates with the
components as shown in Figure 4. Input data is received
from the Mission Level Situation Determination, the HMMI
and the Task Distribution. The output of the CSI module is
aggregated by the Cockpit Level Situation Determination and
then processed by the Cockpit Level Risk Assessment and the
Task Distribution. The CSI also provides input for the HMMI
Interaction Manager
Mission Level Situation Determination delivers context
information like the progress on ﬂight plan, the state of aircraft
systems and environmental data (e.g., weather, ATC). The data
of this component are treated as general observations of context
in the CSI intention inference module.
Figure 4: Crew State Inference module connections to other modules inside
the A-PiMod architecture (connections between most of the other modules
were intentionally left out)
The HMMI handles interactions of the human crew mem-
bers with the cockpit in several modalities. The supported
modalities are conventional buttons, touch, speech and ges-
tures. Additionally, this component tracks the eye movements
of the human crew members. Thus, HMMI delivers the actions
and the gaze information of the pilots which are interpreted
by the CSI.
The Task Distribution component receives, from the Task
Determination at Cockpit Level, the tasks which are pertinent
for the given situation. Every pertinent task is then assigned
to at least one crew member (including automation) which
is capable of performing this task. To elaborate a new task
distribution the component considers the capabilities and the
state of all available crew members, including human pilots
and automation systems. The currently active task distribution
is communicated to the CSI and consists of a set of tasks
for each crew member. The tasks of a set are the so-called
objective tasks of a human operator.
The Cockpit Level Situation Determination aggregates the
states of all crew members. This means it monitors the state
of all automation systems and receives the state of the human
pilots from the CSI component. This information is then
delivered to the Task Distribution component and the Cockpit
Level Risk Assessment component.
Cockpit Level Risk Assessment evaluates the risk for task
distributions. Here, the state of the human crew members and
the state of the automation system is taken into account. Only
if the risk for a task distribution is below a certain threshold
this task distribution can be activated. If there are more than
one possible task distribution available usually the one with
the lowest risk is activated.
The purpose of the HMMI Interaction Manager is to
modify the salience and the modality of the HMMI output
to the human crew members. This means, e.g., if a human
crew member is not aware of some information, the Interaction
Manager can make the information on the display more
salient. If the human crew member currently has a high visual
workload, the Interaction Manager could also switch the output
modality of the information to speech.
204
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-390-2
COGNITIVE 2015 : The Seventh International Conference on Advanced Cognitive Technologies and Applications

V.
CONCLUSION
In this paper, we presented the concept of the A-PiMod
pilot model, which uses a mixed modelling approach (cognitive
and probabilistic) to infer intentions, situation awareness, and
workload of a pilot crew. The pilot model is embedded into
the A-PiMod architecture and relies on the data generated by
other modules. A ﬁrst prototype of the pilot model has been
integrated and the communication with other modules has been
tested within a simulator setting at the German Aerospace
Center. The next steps will be to improve the concept and
the implementation, in order to be ready for a ﬁrst validation
of the promised functions.
ACKNOWLEDGMENT
The A-PiMod project is funded by the European Commis-
sion Seventh Framework Programme (FP7/2007-2013) under
contract number: 605141 Project A-PiMod.
REFERENCES
[1]
R. Amalberti, “Automation in aviation: A human factors perspective,”
in Handbook of aviation human factors. Lawrence Erlbaum Associates
Mahwah, NJ, 1999, pp. 173–192.
[2]
R. Parasuraman and V. Riley, “Humans and automation: Use, misuse,
disuse, abuse,” in Human Factors: The Journal of the Human Factors
and Ergonomics Society, vol. 39, no. 2.
SAGE Publications, 1997, pp.
230–253.
[3]
R. Parasuraman and C. Wickens, “Humans: Still vital after all these
years of automation,” in Human Factors: The Journal of the Human
Factors and Ergonomics Society, vol. 50, no. 3.
Sage Publications,
2008, pp. 511–520.
[4]
L. Bainbridge, “Ironies of automation,” in Automatica, vol. 19, no. 6.
Elsevier, 1983, pp. 775–779.
[5]
H. Sogame and P. Ladkin, “Aircraft accident investigation report
96-5.
japan:
Ministry
of
transport,”
1996.
[Online].
Available:
http://sunnyday.mit.edu/accidents/nag-1.html [retrieved: 1,2015]
[6]
“Applying
pilot
models
for
safer
aircraft.”
[Online].
Available:
http://www.apimod.eu [retrieved: 2,2015]
[7]
A. Mavor et al., Modeling human and organizational behavior: Appli-
cation to military simulations.
National Academies Press, 1998.
[8]
K. Leiden et al., “A review of human performance models for the
prediction of human error,” in Ann Arbor, vol. 1001, 2001, p. 48105.
[9]
J. Rasmussen, “Skills, rules, and knowledge; signals, signs, and sym-
bols, and other distinctions in human performance models,” in Systems,
Man and Cybernetics, IEEE Transactions on, vol. 12, no. 3.
IEEE,
1983, pp. 257–266.
[10]
F. Frische, J.-P. Osterloh, and A. L¨udtke, “Modelling and validating
pilots visual attention allocation during the interaction with an advanced
ﬂight management system,” in Human Modelling in Assisted Trans-
portation.
Springer, 2011, pp. 165–172.
[11]
F. E. Ritter et al., “Techniques for modeling human performance in
synthetic environments: A supplementary review,” DTIC Document,
Tech. Rep., 2003.
[12]
A. Newell, Uniﬁed theories of cognition.
Harvard University Press,
1994.
[13]
J. R. Anderson and C. Lebiere, The atomic components of thought.
Psychology Press, 1998.
[14]
J. R. Anderson, How can the human mind occur in the physical
universe?
Oxford University Press, 2007.
[15]
A. Newell and H. A. Simon, GPS, a program that simulates human
thought.
Defense Technical Information Center, 1961.
[16]
J. F. Lehman, J. E. Laird, and P. S. Rosenbloom, “A gentle introduction
to Soar, an architecture for human cognition,” in Invitation to cognitive
science, vol. 4.
MIT Press, 1996, pp. 212–249.
[17]
K. M. Corker and B. R. Smith, “An architecture and model for cogni-
tive engineering simulation analysis: Application to advanced aviation
automation,” in Proceedings of the AIAA Computing in Aerospace 9
Conference, 1993, pp. 1079–1088.
[18]
B. F. Gore, “Workload as a Performance Shaping Factor for Human
Performance Models,” in Behavioral Representation in Modeling and
Simulation (BRIMS), 2011, p. 276.
[19]
J. R. Anderson, Rules of the mind.
Psychology Press, 2014.
[20]
A. Newell, P. S. Rosenbloom, and J. E. Laird, “Symbolic architectures
for cognition,” DTIC Document, Tech. Rep., 1989.
[21]
J. R. Anderson et al., “An integrated theory of the mind.” in Psycho-
logical review, vol. 111, no. 4.
American Psychological Association,
2004, p. 1036.
[22]
R. E. Wray and R. M. Jones, “Considering Soar as an agent ar-
chitecture,” in Cognition and multi-agent interaction: From cognitive
modeling to social simulation, vol. 33, 2006, pp. 53–78.
[23]
K. M. Corker, “Cognitive models and control: Human and system
dynamics in advanced airspace operations,” in Cognitive engineering
in the aviation domain, vol. 31.
Lawrence Erlbaum Associates, 2000,
pp. 13–42.
[24]
M. A. Freed, “Simulating human performance in complex, dynamic
environments,” Ph.D. dissertation, Northwestern University, 1998.
[25]
W. Zachary, T. Santarelli, J. Ryder, and J. Stokes, “Developing a
multi-tasking cognitive agent using the COGNET/iGEN integrative
architecture,” DTIC Document, Tech. Rep., 2000.
[26]
A. L¨udtke, L. Weber, J.-P. Osterloh, and B. Wortelen, “Modeling
Pilot and Driver Behavior for Human Error Simulation,” in Digital
Human Modeling, ser. Lecture Notes in Computer Science, V. Duffy,
Ed., vol. 5620.
Springer, 2009, pp. 403–412. [Online]. Available:
http://dx.doi.org/10.1007/978-3-642-02809-0 43 [retrieved: 1,2015]
[27]
C. Wickens et al., “Modeling and evaluating pilot performance in
nextgen: Review of and recommendations regarding pilot modeling
efforts, architectures, and validation studies,” NASA Ames Research
Center, Moffett Field, CA, Tech. Rep. NASA/TM-2013-216504, 2013.
[28]
A. L¨udtke and J.-P. Osterloh, “Simulating perceptive processes of pilots
to support system design,” in Human-Computer Interaction–INTERACT
2009.
Springer, 2009, pp. 471–484.
[29]
B. Wortelen, A. L¨udtke, and M. Baumann, “Integrated simulation
of attention distribution and driving behavior,” in Proceedings of the
22nd Annual Conference on Behavior Representation in Modeling &
Simulation, W. G. Kennedy, R. S. Amant, and D. Reitter, Eds. Ottawa,
Canada: BRIMS Society, 2013, pp. 69–76.
[30]
A. L¨udtke, J.-P. Osterloh, T. Mioch, F. Rister, and R. Looije, “Cognitive
modelling of pilot errors and error recovery in ﬂight management tasks,”
in Human Error, Safety and Systems Development.
Springer, 2010,
pp. 54–67.
[31]
M. Hayashi, “Hidden Markov Models for analysis of pilot instrument
scanning and attention switching,” Ph.D. dissertation, Massachusetts
Institute of Technology, 2004.
[32]
D. Donath, “Verhaltensanalyse der Beanspruchung des Operateurs in
der Multi-UAV-F¨uhrung,” Dissertation, Universit¨at der Bundeswehr
M¨unchen, 2012.
[33]
C.
Moebus
and
M.
Eilers,
“Prototyping
Smart
Assistance
with
Bayesian
Autonomous
Driver
Models,”
in
Handbook
of
Research
on
Ambient
Intelligence
and
Smart
Environments:
Trends
and
Perspectives,
N.-Y.
Chong
and
F.
Mastrogiovanni,
Eds.
IGI
Global,
May
2011,
pp.
460–512.
[Online].
Avail-
able:
http://www.igi-global.com/chapter/prototyping-smart-assistance-
bayesian-autonomous/54671 [retrieved: 1,2015]
[34]
G. Agamennoni, J. I. Nieto, and E. M. Nebot, “A bayesian approach
for driving behavior inference,” in 2011 IEEE Intelligent Vehicles
Symposium (IV), no. Iv.
Ieee, 2011, pp. 595–600.
[35]
M.-I. Toma and D. Datcu, “Determining car driver interaction intent
through analysis of behavior patterns,” in Technological Innovation for
Value Creation.
Springer, 2012, pp. 113–120.
[36]
E. Horvitz, J. Breese, D. Heckerman, D. Hovel, and K. Rommelse,
“The Lumiere project: Bayesian user modeling for inferring the
goals
and
needs
of
software
users,”
in
Proceedings
of
the
Fourteenth conference on Uncertainty in artiﬁcial intelligence. Morgan
Kaufmann Publishers Inc., 1998, pp. 256–265. [Online]. Available:
http://dl.acm.org/citation.cfm?id=2074124 [retrieved: 1,2015]
[37]
M. Strohal and R. Onken, “Intent and error recognition as part of a
knowledge-based cockpit assistant,” in Proc. SPIE, vol. 3390, 1998,
205
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-390-2
COGNITIVE 2015 : The Seventh International Conference on Advanced Cognitive Technologies and Applications

pp. 287–299. [Online]. Available: http://dx.doi.org/10.1117/12.304818
[retrieved: 1,2015]
[38]
C.
W.
Pleydell-Pearce,
B.
Dickson,
and
S.
Whitecross,
“Cognition
monitor:
a
system
for
real
time
pilot
state
assessment,”
in
Contemporary
Ergonomics.
Taylor
&
Francis
Group,
2000,
pp.
65
–
69.
[Online].
Available:
http://research-information.bristol.ac.uk/en/publications/cognition-
monitor-a-system-for-real-time-pilot-state-assessment(fbd8abcd-d97e-
4963-b042-c8a5e4a5f5dd).html [retrieved: 1,2015]
[39]
K. S. Moore, “Comparison of Eye Movement Data to Direct Measures
of Situation Awareness for Development of a Novel Measurement Tech-
nique in Dynamic, Uncontrolled Test Environments,” Ph.D. dissertation,
Clemson University, 2009.
[40]
M. Diez et al., “Tracking pilot interactions with ﬂight management sys-
tems through eye movements,” in Proceedings of the 11th International
Symposium on Aviation Psychology, 2001, pp. 1–6.
[41]
D. Donath and A. Schulte, “Behavior Model Based Recognition of
Critical Pilot Workload as Trigger for Cognitive Operator Assistance,”
in Engineering Psychology and Cognitive Ergonomics. Springer, 2009,
pp. 518–528.
[42]
T. C. Hankins and G. F. Wilson, “A comparison of heart rate, eye
activity, EEG and subjective measures of pilot mental workload during
ﬂight.” in Aviation, Space, and Environmental Medicine, vol. 69, no. 4,
1998, pp. 360–367.
[43]
P.
R.
Cohen
and
H.
J.
Levesque,
“Intention
is
choice
with
commitment,”
in
Artiﬁcial
Intelligence,
vol.
42,
no.
2-3,
Mar.
1990,
pp.
213–261.
[Online].
Avail-
able: http://linkinghub.elsevier.com/retrieve/pii/0004370290900555 [re-
trieved: 1,2015]
[44]
C. D. Wickens, “Processing Resources in Attention,” in Varieties of
attention.
Academic Press, 1984, pp. 63–102.
206
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-390-2
COGNITIVE 2015 : The Seventh International Conference on Advanced Cognitive Technologies and Applications

