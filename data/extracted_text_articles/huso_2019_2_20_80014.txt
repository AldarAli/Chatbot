Response Patterns during Child-Robot Interaction of
Children with Cognitive Impairments
Luthfﬁ Idzhar Ismail∗†, Tony Belpaeme∗‡
and Francis wyffels∗
∗Ghent University-imec, IDLab, Technologiepark-Zwijnaarde 126, B-9052 Ghent, Belgium
†Universiti Putra Malaysia, Faculty of Engineering, 43400 Selangor, Malaysia,
‡University of Plymouth, School of Computing, Plymouth PL4 8AA United Kingdom
Email: luthffiidzharbin.ismail@ugent.be
Fazah Akhtar Hanapiah
Universiti Teknologi MARA,
Faculty of Medicine,
47000 Selangor, Malaysia
Abstract—Literature on Human-Robot Interaction reports that
children with cognitive impairments often have engaging interac-
tions with social robots. However, there are hardly any guidelines
on how to design an interaction to achieve particular therapeutic
outcomes. This paper reports on a study in which 20 children
with cognitive impairments interacted with a social robot, with
the aim to assess their responses and their engagement which
eventually impacts on the outcomes we can achieve. The children
were introduced to the robot and had three sessions during which
they played therapeutic games with the robot to improve their
attention skills. The overall pattern of their responses for in the
sessions are reported, showing a reduction in completion time
with each subsequent session. This is indicative of improved
attention. This response pattern might be important in future
behaviour analysis, especially as a measure for social attention
skills, eye contact, and engagement analysis during child-robot
interaction.
Keywords–Robot, Cognitive Impairments, Child-Robot Interac-
tion
I.
INTRODUCTION
Robots have been actively used in recent years to help
children with cognitive and physical disabilities and other
special needs. Research suggest that children positively en-
gage with robots during child-robot interaction, e.g. [1], [2].
According to some carers and teachers, children with cognitive
impairments (hereafter referred to as ”CWCI”) are known to
have difﬁculties with remaining focused during human-human
interaction, which in turn has a negative impact on interactions
with peers, teachers and family members.
Conventional human-human intervention programs and
therapy were proven to be effective to improve their social
communication skills [3]. Teachers and therapists usually rely
on additional tools, such as cards or toys, for their intervention
programs or therapy sessions. Given the reliance on external
props to support the sessions, and the need for focal points to
practice social skills such as joint attention, deictic gaze and
eye contact, it likely that social robotics can play a role here.
In this study, we use the social robot LUCA (as illustrated
in Figure 1) and designed child-robot interaction modules
to help CWCI to improve their social interaction skills. We
hypothesize that their response towards the robot and the
interaction modules shall provide us with more information
and insight with which to design future child-robot interaction
studies. Section 2 discusses the experimental study of our
child-robot interaction, together with a description of each
Figure 1. Figure shows LUCA robot which was build based on OPSORO
robot platform [4].
module. Finally, section 3 shall reports the ﬁnding of the study
which elaborate the pattern of child’s response in each module
for each session of child-robot interaction.
II.
CHILD-ROBOT INTERACTION EXPERIMENT
All experimental procedure has been given ethical approval
on 30th July 2018 from Research Ethics Committee, Universiti
Teknologi MARA (UiTM), Malaysia (REC reference number:
600-IRMI (5/1/6)). In this study, we collaborated with one of
the schools in Putrajaya, Malaysia. This school has 92 children
with special needs. 20 children diagnosed with cognitive
impairments fulﬁlled our inclusion and exclusion criteria as
described in Table I.
Consent to participate in our study was also obtained from
their parents or legal guardian prior to start the experiment.
The protocol of the experiment was clearly explained to the
teachers and therapist. A teacher or therapist would come to the
experimental room with one child at a time. They would knock
on the door, walk into the room and sit down in front of the
robot. All interactions were recorded using ﬁve video cameras
for later analyses. Once the child was seated and ready, the
teacher would ﬂash a card at the robot and the interaction
with the robot was initiated. Each child was exposed to the
robot for 3 consecutive sessions. Each session consists of 5
33
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-725-2
HUSO 2019 : The Fifth International Conference on Human and Social Analytics

TABLE I. INCLUSION AND EXCLUSION CRITERIA FOR ALL PARTICIPANTS
Inclusion criteria
Exclusion criteria
1)Age between 6 to 12 years
1)Child with mutism
2)No evidence of self injury or aggressive behaviour
2)Uncorrected hearing deﬁcit
3)Able to speak in English or Malay
3)Uncorrected vision deﬁcit
4)Diagnosed as having a Cognitive Impairment (level validated by attention
skills via Children Colouring Trail Test: CCTT [5])
4)Unwillingness to participate
5)Able to follow simple instructions in English or Malay
interaction modules. The 5 modules are as below:
•
Module 1: Introduction to the robot
The ﬁrst module aimed to introduce the robot to
the participant. The child was welcomed by LUCA
using simple English language and some low valence
non-verbal behaviour. The text to speech voice was
generated using an online synthesize [6].
•
Module 2: Facial expression game
This module was designed as a facial expression game
and has been designed to help CWCI improve their
attention skills [7], [8]. The dependent variable in this
module is the time taken by the child to complete
the task. In this module, the researcher controlled
the robot and selected a range of different facial
expressions such as happy, sad, angry. The children
were invited by the robot to guess the expression, and
they were allowed a second try if their initial answer
was wrong. If their answer was still incorrect, the
correct answer was given by the robot. The children
were also expected to mimic the expression of the
robot while maintaining eye contact with the robot.
•
Module 3: Song with facial expression game
In this module, a song was added to the facial expres-
sion game in order to encourage the children to play
the facial expression game and make the interaction
more engaging. Some children have some difﬁculties
in distinguishing certain facial expressions. The music
was chosen to match the emotions expressed by the
robot and helped the children guess the facial expres-
sion, next to enhancing their attention span.
•
Module 4: Attention task
This module was developed to measure the attention
skills of the child. These are very important skills, cen-
tral to social interaction, learning and collaboration,
and robots are believed to be able to improve these
skills during Child-Robot Interaction [9], [10]. This
session expected the child to look at a certain shape
pasted on a board placed on the right (for example, an
image of rectangle) and left (for example, an image of
circle) of the robot. The child would need to perform a
“matching task” in which the robot gave an instruction
to look at at a shape (mounted to the left or right of
the robot) and ﬁxate their gaze for 3 seconds. For
example, he/she would be required to look at the
rectangle for 3 seconds.
•
Module 5: Free style interaction
Finally, module 5 was a free style interaction between
the child and the robot. The child was given the chance
to ask questions to the robot. The robot answered, with
answers being typed in on a keyboard by a member of
the research team and spoken by the robot. If children
requested the robot to move, then these actions were
performed when the robot had the capability to do so.
III.
RESULTS
This section reports on the overall response to the child-
robot interaction for children diagnosed with cognitive impair-
ments. Five modules were designed for this study. In module 1,
children were only introduced to the robot. This is necessary in
order to break the ice between the child and the robot [11], [12]
and to assure the following interactions are not inﬂuenced by
the child being unfamiliar with the robot or the study setting.
Neither behaviour nor tasks in module 1 have been evaluated.
Nevertheless, the average response time from all children was
recorded to be around 60 seconds as shown in Fig. 2. In module
2, the overall response pattern showed that children took less
time to complete the tasks in session 2 and 3 as compared
to their average completion time in session 1. This pattern
suggests that their level of concentration and attention skills
has improved, considering they take less time to complete the
modules over the 3 consecutive sessions. This however needs
to be further investigated, as a reduction in completion time
might also be caused by a practice effect.
In module 3, the overall response pattern of the chil-
dren were similar to those in module 2. There was a slight
improvement between session 2 and 3, as the addition of
music in session 3 had a positive impact on task completion
time. Earlier pilots and studies also found that music was an
effective manner to draw children into the interaction [12],
[13]. In module 4, there was only a slight improvement in
the time needed to complete the tasks. Most of the children
needed less time to complete the task in session 2 and 3 as
compared to session 1, which we expected since the task were
uncomplicated. Finally, the results for module 5 were difﬁcult
to generalize since it was an open and free style of interaction.
The pattern for the children’s response in 3 different sessions
were quite scattered. This module can be very useful to gauge
their interest in and attention towards the robot, which serves
as a measure for their focus in social interactions [14], [15].
Their overall response varies in each session. Nevertheless, we
show how unstructured interactions are still able to capture the
attention of the children, with most children engaging with the
robot. Most of the children showed their interest towards the
robot and spent an average approximately 2 minutes in all
session.
IV.
CONCLUSION
We showed that children diagnosed with cognitive impair-
ments respond well to child-robot interaction. Based on our
observations, they spent an enjoyable time interacting with the
LUCA robot. The tasks set by the robot were designed to
uncomplicated and motivated the children to keep interacting
34
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-725-2
HUSO 2019 : The Fifth International Conference on Human and Social Analytics

0
300
S1
S2
S3
Session
Time (s)
Module
M1
M2
M3
M4
M5
Figure 2. Figure shows the overall results of child-robot interaction time for each module in Session 1, 2 and 3.
with the robot, with no children expressing or showing disap-
pointment with the robot. The child-robot interaction modules
hold promise to help children with cognitive impairments to
improve their social interaction skills. While our initial results
are encouraging, further analysis is needed, especially with
regards to improving the children’s attention skills and transfer
to human-human interaction. Time completion task analysis
could be used as a proxy to indicate their improvements
in attention skills in modules 2, 3 and 4 for each session.
Moreover, interaction duration time could also be used as a
proxy to measure their interest in the robot in module 5. This
can be useful, especially for future behavior monitoring by a
therapist or carer. Completion times could provide important
information about the behaviour of children diagnosed with
cognitive impairments (such as eye contact patterns and level
of attention skills) in child-robot interaction.
ACKNOWLEDGMENT
The authors would like to thank all the funds for this
project; Luthfﬁ Idzhar Ismail received a Postgraduate Educa-
tion Fund from Majlis Amanah Rakyat, MARA (MARA REF:
330407445608). This work was also partially funded by the
EU FP7 DREAM project (grant 611391) and Niche Research
Grant Scheme (NRGS):600-RMI/NRGS 5/3 (11/2013). Special
thanks to all members of Research Ethic Committee (REC)
for the ethical approval which was granted from the REC
of Universiti Teknologi MARA, Malaysia (REC reference
number: 600-IRMI (5/1/6)) prior to research commencement.
Moreover, participant’s ofﬁcial consent to participate in this
study were granted from all parents or guardians prior to start
the experiment. Thank you to all of them. The authors also
want to declare that they have no conﬂict of interest in this
project.
REFERENCES
[1]
L. I. Ismail, S. Shamsudin, H. Yussof, F. A. Hanapiah, and N. I.
Zahari, “Robot-based intervention program for autistic children with
humanoid robot nao: initial response in stereotyped behavior,” Procedia
Engineering, vol. 41, 2012, pp. 1441–1447.
[2]
T. Belpaeme, P. Baxter, R. Read, R. Wood, H. Cuay´ahuitl, B. Kiefer,
S. Racioppa, I. Kruijff-Korbayov´a, G. Athanasopoulos, V. Enescu et al.,
“Multimodal child-robot interaction: Building social bonds,” Journal of
Human-Robot Interaction, vol. 1, no. 2, 2013, pp. 33–53.
[3]
J. A. Corbett, M. R. Trimble, and T. C. Nichol, “Behavioral and
cognitive impairments in children with epilepsy: the long-term effects
of anticonvulsant therapy,” Journal of the American Academy of Child
Psychiatry, vol. 24, no. 1, 1985, pp. 17–23.
[4]
T. Vervisch, N. Doms, S. Descamps, C. Vandevelde, S. Verstockt,
J. Saldien et al., “Oto–a diy platform for mobile social robots in
education,” in International Conference on Robotics and Education RiE
2017.
Springer, 2017, pp. 257–262.
[5]
A. M. Llorente, R. G. Voigt, J. Williams, J. K. Frailey, P. Satz, and
L. F. DElia, “Children’s color trails test 1 & 2: test–retest reliability
and factorial validity,” The Clinical Neuropsychologist, vol. 23, no. 4,
2009, pp. 645–660.
[6]
I. Cloud. Text to speech. Accessed on July to August 2018. [Online].
Available: https://text-to-speech-demo.ng.bluemix.net/
[7]
A. G. Pour, A. Taheri, M. Alemi, and A. Meghdari, “Human–robot facial
expression reciprocal interaction platform: case studies on children with
autism,” International Journal of Social Robotics, vol. 10, no. 2, 2018,
pp. 179–198.
[8]
A. Adams and P. Robinson, “An android head for social-emotional
intervention for children with autism spectrum conditions,” in Affective
Computing and Intelligent Interaction.
Springer, 2011, pp. 183–190.
[9]
C.-M. Huang and A. L. Thomaz, “Joint attention in human-robot
interaction.” in AAAI Fall Symposium: Dialog with Robots, 2010.
35
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-725-2
HUSO 2019 : The Fifth International Conference on Human and Social Analytics

[10]
Z. E. Warren, Z. Zheng, A. R. Swanson, E. Bekele, L. Zhang, J. A.
Crittendon, A. F. Weitlauf, and N. Sarkar, “Can robotic interaction
improve joint attention skills?” Journal of autism and developmental
disorders, vol. 45, no. 11, 2015, pp. 3726–3734.
[11]
N. Bee, E. Andr´e, and S. Tober, “Breaking the ice in human-agent
communication: Eye-gaze based initiation of contact with an embodied
conversational agent,” in International Workshop on Intelligent Virtual
Agents.
Springer, 2009, pp. 229–242.
[12]
L. I. Ismail, S. Shamsudin, H. Yussof, F. A. Hanapiah, and N. I.
Zahari, “Estimation of concentration by eye contact measurement in
robot–based intervention program with autistic children,” Procedia
Engineering, vol. 41, 2012, pp. 1548–1552.
[13]
Y.-H. Peng, C.-W. Lin, N. M. Mayer, and M.-L. Wang, “Using a
humanoid robot for music therapy with autistic children,” in Automatic
Control Conference (CACS), 2014 CACS International.
IEEE, 2014,
pp. 156–160.
[14]
K. Dautenhahn, “Robots as social actors: Aurora and the case of
autism,” in Proc. CT99, The Third International Cognitive Technology
Conference, August, San Francisco, vol. 359, 1999, p. 374.
[15]
——, “Methodology & themes of human-robot interaction: A growing
research ﬁeld,” International Journal of Advanced Robotic Systems,
vol. 4, no. 1, 2007, p. 15.
36
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-725-2
HUSO 2019 : The Fifth International Conference on Human and Social Analytics

