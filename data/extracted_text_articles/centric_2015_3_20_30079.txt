Making the System a Relational Partner: Users’ Ascriptions in Individualization-
focused Interactions with Companion-systems  
 
Julia Krüger, Mathias Wahl, Jörg Frommer 
University Clinic for Psychosomatic Medicine and Psychotherapy 
Medical Faculty, Otto von Guericke University 
Magdeburg, Germany 
e-mail: {julia.krueger; mathias.wahl; joerg.frommer}@med.ovgu.de 
 
 
Abstract—Whether 
or 
not 
a 
Companion-system 
is 
experienced as a confidant and empathic assistant depends on 
users’ individual ascriptions to it. An ascription-based 
understanding of users’ experiences in user-companion 
interaction is proposed, which focuses primarily on ascriptions 
of human-like characteristics, intentions, motivations or 
emotions to the system. It is examined with regard to the 
individualization-focused interactions pivotally required for 
the adaptation of a Companion-system to the user. The study is 
based on a Wizard of Oz experiment and subsequently 
conducted in-depth user interviews. By applying qualitative 
content analysis, four categories describing dimensions of 
users’ ascriptions to the system were worked out (nature, 
capabilities, requirements, and relational offer) as well as two 
categories describing users’ reactions based on these 
(adaptation work and self-disclosing behavior). The findings 
are discussed with regard to theories of psychology, philosophy 
and human-robot interaction. Moreover, two needs inherent in 
humans, which seem to be relevant for users’ ascriptions in 
user-companion interaction, are referred to: the need for 
safety and the need to belong.  
Keywords-users’ ascriptions; user experience; Companion-
systems; individualization; human-computer interaction. 
I. 
 INTRODUCTION  
Claims 
of 
individual-centered 
human-computer 
interaction (HCI) culminate in visions like that of technical 
systems adopting the “Companion-metaphor” [1]. Under 
terms like “relational agents”, “sociable robots”, “artificial 
companions” or “Companion-systems”, these systems 
provide 
monitoring, 
personalized 
assistive 
and/or 
companionship services [2]. They shall i.a. be experienced as 
confidants [3] by their users and enable them to form 
emotional and long-term social attachments with them [4]. 
A. Companion-systems  
Following 
the 
understanding 
of 
the 
German 
Transregional Collaborative Research Centre SFB/TRR 62, 
which aims at developing a Companion-technology for 
cognitive technical systems, Companion-systems are defined 
as follows: These are visionary cognitive technical systems 
which adapt their functionality to each individual user by 
considering his preferences, needs, abilities, requirements, 
current emotional state and situation. Hence, they represent 
available, reliable, cooperative and trustworthy empathic 
assistants providing also an emotional dimension to the 
interaction [5][6].  
B. Users’ Ascriptions form Users’ Experiences 
Besides 
a robust 
technical realization 
of 
these 
Companion-features, it seems crucial for the success and 
acceptance of Companion-systems that these features are 
individually experienced by their users. 
It can be assumed that this individual experience is based 
on the user’s interpretations of the implemented system 
characteristics and the system behavior. Following a 
constructivist view (e.g., [7]), these interpretations can be 
understood as individual user’s ascriptions to the system 
[8][9]: The user himself ‘constructs’ his view on the system 
by (consciously, as well as mostly unconsciously) ascribing 
to it and experiences his ascription-based system view as 
‘objective reality’. Consequently, he chooses his behavior in 
reaction to his ascriptions, so that system- and self-related 
experiences are mutually influential. According to this 
understanding, users’ ascriptions can be understood as 
‘interpretation foil’ for individual users’ experiences of 
Companion-systems. By following this perspective, the large 
body of user experience studies may be supplemented, which 
examine relationships 
between 
distinct psychological 
constructs and users’ experiences as summarized overall 
evaluations [10][11].  
It is proposed that in the case of Companion-systems, 
functional and structural ascriptions (like described in mental 
models, e.g., [12]) will supplement or even become 
secondary to anthropomorphic ascriptions of human-like 
mental states, e.g., motives, wishes, aims and feelings [8][9] 
(multiple examples can be found in literature, e.g., already 
[13] regarding ELIZA; [14] regarding robots like “Furbies” 
or “Tamagotchis”): Firstly, because Companion-systems will 
provide social cues to realize their Companion-functionality 
which may trigger the perception of interacting with a social 
actor (this assumption is in line with the theory of social 
response in HCI, e.g., [15]); and secondly, because the 
average user will be unable to explain and predict the system 
behavior on the basis of their complex construction and 
functioning and consequently will draw on ascribing mental 
states to it in order to interact effectively with them 
(Intentional Stance [16]).  
48
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-440-4
CENTRIC 2015 : The Eighth International Conference on Advances in Human-oriented and Personalized Mechanisms, Technologies, and Services

C. Users’ Ascriptions and Self-disclosure in 
Individualization-focused User-Companion Interaction 
This study aims at examining users’ ascriptions which 
arise in user-companion interaction (UCI). Therefore, 
researching initial interaction sequences seems promising. 
Therein, the user gets to know the system for the first time 
and questions like “How can I interact with this system?” or 
“What can I expect from the system?” arise, which are likely 
to imply lots of ascriptions to the system.  
Initial situations in UCI will mostly focus on gathering 
user information to realize individual adaptation to the user 
and fully evolve Companion-functionality. In such sensible 
UCI, positive ascriptions to the system like willingness to 
pursue user’s goals, sensibility and trustworthiness seem to 
be necessary to enable users to cooperatively disclose 
relevant 
information 
without 
feeling 
uncomfortable. 
Especially intimate self-disclosure, which may be necessary 
in usage contexts like, e.g., E-health or E-mental health, may 
entail high-risk information and induce feelings of 
vulnerability [17]. Ascriptions like malice or pursuit of 
dominance may induce such negative emotions and result in 
a decrease of cooperativeness, reactance and even 
communication break-ups. Hence, it can be assumed that 
users’ ascriptions to the system have potential to mediate 
users’ self-disclosing behavior.  
For recommender systems [18] describe a mediation of 
self-disclosing behavior by users’ individual interpretations 
of system characteristics and user experience: Objective 
system aspects are perceived by the user (e.g., recognition of 
differences in recommendation quality based on different 
algorithms) and make up the user experience which in turn 
constitutes the user’s self-disclosing behavior. Regarding the 
so-called ‘subjective system aspects’, solely conscious user’s 
evaluations of certain aspects, namely interaction usability, 
perceived quality and appeal of the system, are considered.   
All in all, besides the large body of research in strategies 
for enhancing users’ self-disclosure (behavior-centered 
approach; for an overview cf. [19]) still only little is known 
about users’ experiences of and motives for self-disclosure to 
personalized systems [20] or experiential factors that mediate 
self-disclosure choices (experience-centered approach). An 
understanding of ascriptions in individualization phases may 
help to explain the often surprising findings in research on 
privacy decision making, namely that decisions on self-
disclosure are made neglecting rational principles [21]. 
This study on users’ ascriptions and their influences on 
users’ behavior using the example of individualization-
focused UCI is guided by the following research questions:  
1. What do users ascribe to a Companion-system asking 
for personal and intimate user information (system-
related experiences)?  
2. How do users experience themselves in reaction to their 
individual ascriptions to it (self-related experiences)?  
The paper is structured as follows: In Section II, the 
research approach will be described. Afterwards, the results 
will be presented in Section III and discussed according to 
theories of psychology, philosophy and human-robot 
interaction in Section IV. In Section V, future work on users’ 
ascriptions in UCI will be outlined. 
II. 
MATERIAL AND METHODS 
Users’ ascriptions are unobservable, highly individual and 
often unconscious. Thus, a qualitative research rationale was 
applied in order to examine them. It aims at describing 
individual meaning making processes and the resulting 
variety and contrasts of phenomena in the material rather 
than at quantifying and statistically generalizing their 
allocation in a population (e.g., [22]). The study bases on in-
depth user interviews conducted subsequently to Wizard of 
Oz 
(WOz) 
experiments, 
which 
were 
analyzed 
interpretatively.  
A. Experimental Basis  
In order to do basic research on UCI, a widely 
standardized WOz experiment was conducted (cf. [23][24]). 
The simulated speech-based interactive dialog system should 
represent a kind of preliminary step towards visionary 
Companion-systems. It was represented by a male machine-
like computer voice and a graphical interface on a computer 
screen (no agent). Some of the leading dialog design 
principles were continuous system-initiative and the 
avoidance of self-references (personal pronouns, active 
forms).  
In the study presented here, the focus was on users’ 
experiences of the first experimental module of the WOz 
experiment, called ‘Initial Dialogue’ (for details cf. 
[23][24]). Therein, personal, and even intimate user 
information were gathered for the purpose of simulating the 
individualization process of a future Companion-system: The 
text “Individualization and personalization for xxx” appears 
on the screen. The user is asked to give and spell his name 
whereupon it is complemented in the text (the display 
remains constant during the ‘Initial Dialogue’). Hereafter, he 
is 
asked 
openly 
for 
self-introduction. 
The 
system 
summarizes all the information relevant for individualization 
and asks for revisal. The aim is to gather information about 
age, place of residence, profession, place of work, family, 
body height, clothing size and shoe size. Still missing 
information is requested on inquiry. Furthermore, users are 
asked about recent events in which they were emotional 
(happy and angry) as well as about hobbies. Finally, some 
questions concerning the use of and former experiences with 
technical devices (which devices are used for what purpose 
in everyday life; exemplification on positive and negative 
experiences) are asked. In case of very short answers, the 
system requested further elaboration. 
All in all, the system design (no self-reference, speech-
based interaction, introduction as personal assistant) and the 
design of the ‘Initial Dialogue’ (extensive standardization, 
minimal visualization, pretension of individualization 
purpose) seem appropriate for this study, because they 
provide openness for any kind of users’ ascriptions. 
49
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-440-4
CENTRIC 2015 : The Eighth International Conference on Advances in Human-oriented and Personalized Mechanisms, Technologies, and Services

B. Interview Design 
Semi-structured interviews were conducted subsequent to 
the WOz experiments. They focused on users’ subjective 
experiences of the simulated system and the interaction with 
it. The interview guide included an initial open narration 
stimulus: Users were asked to report freely on the 
experiment and tell about their thoughts and feelings 
concerning the interaction [8]. For this study, the so-called 
initial narrations evoked by this stimulus were analyzed. 
These were proven to be suitable to elicit spontaneous 
experiential expressions.  
C. Sample  
Following the criterion of theoretical saturation (cf. 
Section III-D; [25]) 31 interviews were analyzed. In order to 
maximize variance, interviews were chosen on the basis of a 
qualitative sample plan [26], which allowed to consider 
sample heterogeneity regarding age (18-28 and >60 years), 
gender and educational level [8]. 
D. Interview Analysis 
The interviews were transcribed according to the 
guidelines of GAT-2 ‘Minimaltranskript’ [27]. The initial 
narratives of the 31 interviews made up 410 transcribed 
pages. 
These 
were 
analyzed 
applying 
methods 
of 
summarizing qualitative content analysis [28]. The analysis 
process was accompanied by regular discussions in a group 
of qualitative researchers. It consisted of three main steps: 
1) Breaking down the Text into Meaning Units 
The initial narratives of at first 16 interviews 
(heterogeneous regarding age, gender and educational level) 
were divided into so-called meaning units (MUs) varying 
from word groups to paragraphs in length. MUs are text 
segments, which are understandable by themselves and 
contain one episode, idea or piece of information [29].  
2) Qualitative Content Analysis, Initial Category System 
MUs representing reflections upon the ‘Initial Dialogue’ 
were further processed. They were paraphrased, generalized 
and reduced using the methods of summarizing qualitative 
content analysis [28]. The resulting condensed MUs were 
grouped according to similarities and differences across all 
16 cases. These groups constituted a first set of subcategories 
(third level categories), which in turn could be arranged into 
main categories representing a higher abstraction level 
(second level categories). According to the research 
questions, these second level categories were assigned to 
system- and self-related experiences (first level categories) 
(cf. Table 1). All first, second and third level categories 
made up an initial category system. 
3) Maximization of Variance, Final Category System 
In order to maximize variance of system- and self-related 
experiences, additional initial narratives were added. One by 
one these were broken down into MUs, which were 
summarized as described above. The reduced MUs of each 
initial narrative were arranged according to the initial 
category system. Categories were reformulated or new ones 
(second and third level) were created during this process if 
necessary. After adding another 15 initial narratives, no 
further substantial increase of variance regarding the 
revealed categories and their contents could be detected 
(theoretical saturation [25]). Thus, the category system was 
final. 
III. 
RESULTS 
The range of users’ system- and self-related experiences 
during the ‘Initial Dialogue’ represented by first and second 
level categories is shown in Table 1. It was desisted from 
listing third level subcategories in favour of describing main 
issues represented by them in Sections III-A and III-B. 
TABLE I.  
OVERVIEW OF THE CATEGORY SYSTEM 
First level 
category 
Second level category  
(No. of third level subcategories) 
system-related 
experiences 
Nature of the system: between man and            
machine (7) 
Capabilities of the system: between impressing and 
frightening (7) 
Requirements by the system: between expectable and 
strange (8) 
Relational offer of the system: between insensitive and 
recognizing (12) 
self-related 
experiences 
Adaptation work of the user: between degrading 
oneself and making oneself available (6) 
Self-disclosing behavior of the user: between 
subjection and control (11) 
A. System-related Experiences 
Users differ in the wealth and depth of their reflections 
on both, the system and themselves. Across all of them, more 
system-related experiences are reported.  
1) Nature of the System: Between Man and Machine 
Ascriptions regarding system’s nature are found in 22 
initial narratives. These ascriptions oscillate between the 
poles ‘machine’ and ‘human-like counterpart’. Descriptions 
are often made by comparing the experienced interaction to 
human-human interaction (HHI). Thereby, system aspects, 
which are far away from HHI (technical sound of system’s 
voice, lack of reciprocity and of visible emotionality) and 
those associated with HHI (speech-based interaction style, 
personal communication contents) can get in conflict with 
each other (“strange to talk about personal things with such 
a machine”, MH [in order to ensure anonymity, initials of 
each user were used]). This mismatch can result in 
experiencing hybrid forms of the system, which combine 
both, human- as well as machine-like aspects (“it isn’t a real 
human being”, SS). The hybrid is experienced as unfamiliar; 
it causes uncertainty and even uncanniness. This can result in 
continually searching for the real nature of the counterpart. 
2) Capabilities of the System: Between Impressing and 
Frightening 
Ascriptions regarding system’s capabilities are found in 
22 initial narratives. A lot of users are impressed and 
astonished by the capabilities and performances of the 
system. Comparing the performance of the system with that 
of other technical systems or with human capabilities leads 
to the ascription of communication abilities to the system 
(“it looks simple but it can do more (…) not only yes or no 
(…) but even that it (…) is able to communicate somewhat”, 
50
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-440-4
CENTRIC 2015 : The Eighth International Conference on Advances in Human-oriented and Personalized Mechanisms, Technologies, and Services

BH). However, there are users, who do not appraise the 
experienced human-like characteristics as generally positive: 
A system which gives the impression of a machine but 
shows unexpected humanly performance seems scary. 
Feelings of discomfort, uncertainty and uneasy skepticism 
can appear. Furthermore, ambivalence regarding system’s 
capabilities results, which is often related to the ascription of 
the ability to abuse confidence to the system. 
3) Requirements by the System: Between Expectable 
and Strange 
Ascriptions regarding the requirements by the system are 
found in 26 initial narratives. System’s requests are mostly 
experienced 
as 
strange, 
unexpected 
and 
surprising. 
Emotional reactions vary from amusement and curious 
excitement regarding the further interaction, to shock, 
uncertainty, scepticism, distrust and discomfort. The latter 
are associated with the missing revelation of assumed aims 
of the system, which (possibly even on purpose) are 
expected to lie hidden in system’s requests (“there I’ll be 
kept in the dark completely”, SP). Furthermore, uncertainty 
emerges from doubts regarding the meaningfulness of 
system’s demands (“as it asked me I thought (…) that it 
really cobbles something individual-specific and when I see 
this retrospectively, I don’t know why it needed this”, CT) or 
ambiguity about system’s expectations regarding content, 
demonstration and extent of user’s answers. Nevertheless, 
system’s requests are sometimes also described as ordinary 
or expectable (“some kind of standard questions so I knew 
something like that would happen”, TB). 
4) Relational Offer of the System: Between Insensitive 
and Recognizing 
Ascriptions regarding the relational offer of the system 
are found in 22 initial narratives. On the one hand, most 
users describe the system’s information collection behavior 
as negative and insensitive: The system is experienced as 
nosy, it exceeds limits of privacy and intimacy and applies 
pressure and compulsion in terms of subjection to get user 
information (“so if I didn’t give the required information 
(…) it sort of refused to go on (…) there I felt very 
constrained (…) yes a little blackmail”, SP). Thereby, it is 
not honestly interested in the user. Some users feel tested and 
assessed by the system, e.g., regarding intelligence. On the 
other hand, there are users experiencing the system as a 
supporting one, which tries to adapt to them. It is 
experienced as being really interested in personally 
recognizing the user and even represents a relational partner 
to one of the users (“at some point you felt like someone is 
really interested in you (…) then little by little you built up 
such a bonding”, FK). 
B. Self-related Experiences 
Besides reflections on self-disclosure choices, reflections 
on users’ general contact to the system and implicit 
fundamental behavioral choices regarding the reaction to it 
appear.  
1) Adaptation Work of the User: Between Degrading 
Oneself and Making Oneself Available 
Reflections upon adaptation towards the system are 
found in 18 initial narratives. The users search for an 
adequate reaction to the nature, requirements, capabilities 
and relational offer ascribed to the system. Here, a global 
focus of adapting to the system to ensure a successful 
interaction can be made out. There are users who doubt or 
even accuse themselves of deficient cognitive abilities or 
failures in anticipating system’s capabilities (“you’re really 
stupid, you just could’ve told everything first”, GA) resulting 
in an insufficient adaptation to the system and an 
unsuccessful interaction in turn. Others describe to 
(passively) get accustomed to the specific characteristics of 
the system or (more actively) invest cognitive and time 
resources for this purpose. Finally, some go to great lengths 
to adapt to the system because they anticipated or recognized 
system deficits. They make their own capabilities available 
to the system and thereby support it to ensure a successful 
interaction, e.g., by adapting content and way of answering 
to the anticipated system’s capabilities (“first of all I thought 
what I could tell, like what understands this machine”, UK). 
2) Self-Disclosing Behavior of the User: Between 
Subjection and Control 
Reflections 
upon self-disclosing behavior in the 
interaction are found in 26 initial narratives. Self-disclosing 
experiences range between conscious or even unconscious 
subjection and feeling independent from the system, i.e., 
having control over self-disclosure. There are users who do 
not reflect upon their disclosure (“I’ve just told this (…) and 
I actually didn’t think about it”, AS). They work off the 
system’s questions obediently or answer them despite 
doubting their meaningfulness. Others answer reluctantly, 
e.g., because they feel pushed, feel personal limits exceeded 
or experience nonspecific aims of the system („I found it 
hard to give this information because I didn’t really 
recognize the meaning“, SP). A few users reduce breadth 
and depth of their answers, e.g., because of distrust regarding 
possible information abuse or absent familiarity with the 
system. In contrast, others answer unhesitatingly, e.g., 
motivated by the imagination of the system as a pleasant 
long-term dialog partner (“well, I could have gabbed with 
it”, SB).   
IV. 
DISCUSSION 
In the following, the findings are compiled and discussed 
according to theories from diverse disciplines. Two human 
needs, which seem to fuel users’ ascriptions in UCI are 
referred to: the need for safety and the need to belong.  
A. Need for Safety as a Motivation for Users’ Ascriptions  
Users’ emotional reactions regarding the experience of 
the system mainly range from uncertainty, discomfort and 
irritation to skepticism, fear, feelings of strangeness and 
weirdness. The experienced misfit and ambiguity of 
ascriptions regarding different system aspects seem to be the 
reason: The system is experienced as an unsettling 
counterpart, neither human nor machine. When specifying it 
with regard to the interplay of system aspects, this finding 
seems to fit in what is described as the Uncanny Valley effect 
51
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-440-4
CENTRIC 2015 : The Eighth International Conference on Advances in Human-oriented and Personalized Mechanisms, Technologies, and Services

in robotics: People’s acceptance of anthropomorphic robots 
increases up to a certain point in which the robot is highly 
realistic but not perfect, than decreases abruptly (the robot ‘is 
stuck in the Uncanny Valley’), and increases again when the 
robot is indistinguishable from reality [30]. The inability to 
merge nature, capabilities and requirements of the system 
into a coherent picture causes lots of negative emotions, 
including first and foremost uncertainty. Negative ascriptions 
and skepticism, discomfort and fear are fueled, when 
difficult relational offers by the system, e.g., pressure and 
enforcement of information disclosure, are experienced 
additionally. 
Because safety needs, including the preference of the 
known over the unknown, are inherent in humans (already 
[31]), users strive to handle these ambiguities and regain 
certainty for interacting effectively and successful with the 
system. By ascribing especially human-like mental states 
(motives, aims, emotions, etc.) known from HHI to their 
unsettling counterpart, they try to make sense of its nature 
and behavior and turn it into a certain, predictable one. This 
can be understood in line with the aforementioned 
Intentional Stance [16]: The user is confronted with a highly 
complex system and (suffering from lack of alternative 
useful explanations) anthropomorphizes it to make its 
behavior predictable and explainable. Thereby, he constructs 
a common interaction situation, namely one comparable to 
HHI, which allows choosing confidently adequate reactions 
to the system. This is in line with a psychological theory of 
anthropomorphism [32]: People tend to see nonhuman 
objects as human-like ones for interacting effectively in their 
environment. This ‘effectance motivation’ triggers access 
and application of anthropocentric knowledge and leads to 
anthropomorphism.  
B. Need to Belong as a Motivation for Users’ Ascriptions 
and Users’ Self-disclosure in UCI 
In their theoretical contribution regarding the design of 
robot companions, [33] consider another human need when 
thinking about the user interacting with such robots – the 
need to belong [34]. It describes the immanent desire in 
humans to establish and sustain relationships. Regarding 
anthropomorphism, [32] describe the wish for social contact 
and affiliation in humans (‘sociality motivation’), which – 
besides the effectance motivation (cf. Section IV-A) – 
facilitates the application of anthropocentric knowledge, too.  
This need to form relationships provides another possible 
explanation for occurring anthropomorphic ascriptions in all 
of the four dimensions worked out in this study: The user 
unconsciously aims at constructing a potential relational and 
therefore at least social, if not human-like partner in the 
system. Thus, he interacts with the system by applying an 
‘as-if-mode’, as if it would be a social, human-like 
counterpart that is able to get in contact with him. 
Results 
regarding 
users’ 
self-related 
experiences 
underline the role of the need to belong in this study. 
Surprisingly, a lot of users disclose requested information 
cooperatively although their ascriptions to the system are 
mostly negative ones. They do not consider terminating the 
communication prematurely and often do not even think 
about dropping cooperativeness. Instead, they reveal 
information despite uncertainty, doubts, skepticism and 
discomfort, rate themselves rather than the system as 
inadequate, and adapt to anticipated system’s expectations 
and capabilities.  
This may be interpreted as the users’ attempt to get and 
stay in contact with the system. They want to understand it as 
well as being understood by it. Occurring ambivalence 
between readily providing information (although this 
exceeds privacy limits) on the one hand and hesitating to 
provide information (because of an unsettling and 
unpredictable counterpart) on the other hand is decided in 
favor of the relationship. In order to apply the calculus 
perspective on privacy decision making (e.g., [35]), this 
could be understood as the user’s acceptance of the costs of 
intimate self-disclosure (including endurance of ambivalence 
and uncertainty) for the benefit of gaining a relational 
partner.  
 
 
Figure 1. Grafical summary. 
 
Figure 1 summarizes users’ tendency to ascribe even 
human-like mental states to a Companion-system, which is 
based on users’ inherent need for safety and need to belong. 
V. 
CONCLUSION AND FUTURE WORK 
This study examined users’ individual ascriptions to a 
WOz-simulated speech-based interactive dialog system 
representing a preliminary stage of visionary Companion-
systems as well as users’ experiences of themselves in 
reaction to it. By referring to users’ need for safety and need 
to belong, the surprisingly high user cooperativeness despite 
often negative ascriptions seem to be explainable. The 
numerous ascriptions of social skills and mental states can be 
interpreted as users’ efforts to transform the system from an 
unsettling into a social, anthropomorphic counterpart 
representing a potential relational partner.  
The presented work represents fundamental research on 
users’ experiences in UCI. However, regarding the 
application-oriented perspective it can be emphasized that 
users tend to disclose even private and intimate information 
if they (independently from specific negative ascriptions to 
it) are enabled to see a relational partner in the Companion-
system. Currently, we are conducting a supplementary study 
on identifying patterns in the variety of users’ ascriptions and 
self-related experiences worked out in this study, which base 
induce
need for safety
need to belong
‚as-if’ the system would be a social
human-like relational partner
user’s ascriptions
52
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-440-4
CENTRIC 2015 : The Eighth International Conference on Advances in Human-oriented and Personalized Mechanisms, Technologies, and Services

on the concept of ‘ideal types’ (cf. [36][37]). After building 
up this typology and assigning each individual user to 
exactly one of the ideal types, information about type-
specific accumulations of characteristics from individual 
users’ background profiles will be taken into account (e.g., 
age, sex, educational level, usage behavior and experiences 
regarding technical devices) to enrich the discussion of 
explanations for the identified patterns. The typology may be 
applicable to the design of Companion-systems, e.g., by 
deriving type-specific dialog strategies, which can foster 
positive ascriptions as well as reduce negative ones.  
Additionally, limitations of this study indicate potentials 
for future research: The work was based on an 
experimentally simulated initial system contact for the 
purpose of individualization. Longitudinal studies are needed 
to show how users’ ascriptions may change in different 
situations during even long term interactions. By applying 
quantitative and mixed methods, influences of ascriptions on 
observable user behavior (here with regard to self-disclosure) 
could be taken into account, too. Furthermore, experiments 
were run with volunteers. Thus, questions of ecological 
validity arise. In fact, motives of system use as well as 
downstream questions, e.g., regarding costs and benefits of 
self-disclosure, 
will 
become 
more 
important 
when 
considering real life UCI scenarios like, e.g., in E-Health. It 
could be hypothesized that being dependent on a 
Companion-system may increase users’ tendency to ascribe 
in order to form a predictable, relational counterpart, as well 
as enhance self-disclosure in the interaction.  
ACKNOWLEDGMENT 
The presented study is performed in the framework of the 
Transregional Collaborative Research Centre SFB/TRR 62 
"A 
Companion-Technology 
for 
Cognitive 
Technical 
Systems" funded by the German Research Foundation 
(DFG). The responsibility for the content of this paper lies 
with the authors.  
REFERENCES 
[1] S. Turkle, “Sociable technologies: Enhancing human 
performance when the computer is not a tool but a 
companion”, in Converging Technologies for Improving 
Human Performance, M. C. Roco and S. B. William, Eds. 
Arlington: NSF-DOC report, pp. 133-140, 2002. 
[2] K. Böhle and K. Bopp, “What a vision: the artificial 
companion. A piece of vision assessment including an expert 
survey”, STI stud., vol. 10, 2014, pp. 155-186, ISSN: 1861-
3675, 
Available 
from: 
http://www.sti-studies.de/ojs/ 
index.php/sti/article/view/148/132 [retrieved: 09, 2015]. 
[3] Y. Wilks, “Foreword”, in Close engagements with artificial 
companions: key social, psychological, ethical and design 
issues, Y. Wilks, Ed., Amsterdam: John Benjamins, pp. xi-xii, 
2010. 
[4] M. Pfadenhauer and C. Dukat, „Künstlich begleitet. Der 
Roboter 
als 
neuer 
bester 
Freund 
des 
Menschen? 
[Accompanied Artificially. The robot as man’s new best 
friend?]“, in Unter Mediatisierungsdruck. Änderungen und 
Neuerungen in heterogenen Handlungsfeldern, T. Grenz and 
G. Möll, Eds. Wiesbaden: Springer Fachmedien, pp. 189-210, 
2014. 
[5] S. Biundo and A. Wendemuth, „Von kognitiven technischen 
Systemen zu Companion-Systemen [From cognitive technical 
systems to Companion-systems]“, KI, vol. 24, 2010, pp. 335-
339, doi: 10.1007/s13218-010-0056-9. 
[6] A. Wendemuth and S. Biundo, “A Companion Technology 
for Cognitive Technical Systems”, Proc. of the 2011 
International Conference on Cognitive Behavioural Systems 
(COST 2011), LNCS, 2012, pp. 89-103, doi: 10.1007/978-3-
642-34584-5. 
[7] E. von Glasersfeld, Radikaler Konstruktivismus: Ideen, 
Ergebnisse, 
Probleme 
[Radical 
Constructivism: 
Ideas, 
Results, Problems]. Frankfurt a. M.: Suhrkamp, 1997. 
[8] J. Lange and J. Frommer, „Subjektives Erleben und 
intentionale Einstellung in Interviews zur Nutzer-Companion-
Interaktion [Subjective experience and intentional stance in 
interviews regarding user-companion interaction]“, Proc. der 
41. GI-Jahrestagung (INFORMATIK 2011), LNI, 2011, p. 
240, ISSN: 1617-5468, ISBN: 978-3-88579-286-4, Available 
from: 
www.user.tu-berlin.de/komm/CD/paper/060332.pdf 
[retrieved: 09, 2015].  
[9] J. Frommer, D. Rösner, J. Lange, and M. Haase, “Giving 
Computers Personality? Personality in Computers is in the 
Eye of the User”, in Coverbal synchrony in human-machine 
interaction, M. Rojc, N. Campbell, Eds. Boca Raton, FL: Crc 
Press, pp. 41- 71, 2013. 
[10] M. Hassenzahl, “User Experience (UX): Towards an 
experiential perspective on product quality”, in Proc. of the 
20th International Conference of the Association Francophone 
d’Interaction Homme-Machine (IHM 2008), ACM, 2008, pp. 
11-15, doi: 10.1145/1512714.1512717. 
[11] E. Karapanos, Modeling Users’ Experiences with Interactive 
Systems. Berlin: Springer, 2013. 
[12] G. C. van der Veer and M. del Carmen Puerta Melguizo, 
“Mental Models”, in The Human-Computer Interaction 
Handbook. Fundamentals, Evolving Technologies 
and 
Emerging Applications, J. A. Jacko and A. Sears, Eds. 
Mahwah, NJ: Lawrence Erlbaum Associates, pp. 52-80, 2003. 
[13] J. Weizenbaum, “ELIZA – a computer programm for the 
study of natural language communication between man and 
machine”. Commun. ACM, vol. 9, 1966, pp. 36-45, doi: 
10.1145/365153.365168. 
[14] S. Turkle, “Authenticity in the age of digital companions”. 
Interact. 
Stud., 
vol. 
8, 
2007, 
pp. 
501-517, 
doi: 
10.1075/is.8.3.11tur. 
[15] C. Nass and Y. Moon, “Machines and mindlessness: social 
responses to computers”. J. Soc. Issues, vol. 56, 2000, pp. 81-
103, doi: 10.1111/0022-4537.00153. 
[16] D. C. Dennett, The Intentional Stance. Cambridge, MA: MIT 
Press, 1987. 
[17] V. J. Derlega, S. Metts, S. Petronio, and S. T. Margulis, Self-
Disclosure. Newbury Park, CA: Sage, 1993. 
[18] B. P. Knijnenburg, M. C. Willemsen, Z. Gantner, Z., H. 
Soncu, and C. Newell, “Explaining the user experience of 
recommender systems”. User Model. User-Adap., vol. 22, 
2012, pp. 441-504, doi: 10.1007/s11257-011-9118-4. 
[19] B. P. Knijnenburg and A. Kobsa, “Helping Users with 
Information Disclosure Decisions: Potential for Adaptation”,  
Proc. of the Eighteenth International Conference on 
Intelligent User Interfaces (IUI 2013), ACM, 2013, pp. 407-
416, doi: 10.1145/2449396.2449448. 
[20] E. van de Garde-Perik, P. Markopoulos, B. de Ruyter, B. 
Eggen, and W. Ijsselsteijn, “Investigating privacy attitudes 
and behavior in relation to personalization”. Soc. Sci. 
Comput. 
Rev., 
vol. 
26, 
2008, 
pp. 
20-43, 
doi: 
10.1177/0894439307307682. 
[21] A. Acquisti and J. Grossklags, “What can behavioral 
economics teach us about privacy?”, in Digital Privacy: 
Theory, Technologies, and Practices, A. Acquisti, S. Gritzalis, 
53
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-440-4
CENTRIC 2015 : The Eighth International Conference on Advances in Human-oriented and Personalized Mechanisms, Technologies, and Services

C. Lambrinoudakis, and S. De Capitani di Vimercati, Eds. 
Boca Raton, FL: Auerbach Publications, pp. 363-377, 2008. 
[22] T. Brüsemeister, Qualitative Forschung. Ein Überblick 
[Qualitative Research. An Overview]. Wiesbaden: VS Verlag 
für Sozialwissenschaften, 2008. 
[23] D. Rösner, J. Frommer, R. Friesen, M. Haase, J. Lange, and 
M. Otto, “LAST MINUTE: a Multimodal Corpus of Speech-
based User-Companion Interactions.” Proc. of the Eighth 
International Conference on Language Resources and 
Evaluation (LREC 2012), ELRA, 2012, pp. 2559-2566, 
ISBN: 
978-2-9517408-7-7, 
Available 
from: 
http://lrec.elra.info/proceedings/lrec2012/pdf/550_Paper.pdf 
[retrieved: 09, 2015].  
[24] J. Frommer, D. Rösner, M. Haase, J. Lange, R Friesen, and 
M. Otto, Project A3: Detection and Avoidance of Failures in 
Dialogues. Wizard of Oz Experiment Operator’s Manual. 
Lengerich: Pabst Science Publishers, 2012. 
[25] J. Morse, “Theoretical saturation”, in Encyclopedia of social 
science research methods, M. Lewis-Beck, A. Bryman, and T. 
Liao, Eds., Thousand Oaks, CA: Sage, pp. 1123-1124, 2004. 
[26] U. Kelle and S. Kluge, Vom Einzelfall zum Typus [From 
Individual Case to Type]. Wiesbaden: VS Verlag für 
Sozialwissenschaften, 2010. 
[27] M. 
Selting 
et 
al., 
“Gesprächsanalytisches 
Transkriptionssystem 2 (GAT 2)“, Gesprächsforschung - 
Online-Zeitschrift zur verbalen Interaktion, vol. 10, 2009, pp. 
353-402, 
ISSN: 
1617-1837, 
Available 
from: 
http://www.gespraechsforschung-ozs.de/heft2009/px-gat2.pdf 
[retrieved: 09, 2015]. 
[28] P. Mayring, Qualitative content analysis. Theoretical 
foundations, basic procedures and software solution. 
Klagenfurth, 2014, Available from: http://nbn-resolving.de/ 
urn:nbn:de:0168-ssoar-395173 [retrieved: 09, 2015]. 
[29] R. Tesch, Qualitative research analysis types and software 
tools. New York: Palmer Press, 1990.  
[30] M. Mori, K. F. MacDorman, and N. Kageki, “The uncanny 
valley [from the field]”. IEEE Robot. Autom. Mag., vol. 19, 
2012, pp. 98–100, doi: 10.1109/MRA.2012.2192811. 
[31] A. Maslow, “A theory of human motivation”. Psychol. Rev., 
vol. 50, 1943, pp. 370-396, doi: 10.1037/h0054346. 
[32] N. Epley, A. Waytz, and J. T. Cacioppo, “On seeing human: a 
three-factor theory of anthropomorphism”. Psychol. Rev., vol. 
114, 2007, pp. 864-886, doi: 0.1037/0033-295X.114.4.864. 
[33] N. C. Krämer, S. Eimler, A. von der Pütten, and S. Payr, 
“Theory of companions: what can theoretical models 
contribute to applications and understanding of human-robot 
interaction?”, Appl. Artif. Intell., vol. 25, 2011, pp. 474-502, 
doi: 10.1080/08839514.2011.587153. 
[34] R. F. Baumeister and M. R. Leary, “The need to belong: 
desire for interpersonal attachments as a fundamental human 
motivation”, Psychol. Bull., vol. 117, 1995, pp. 497-529, doi: 
10.1037/0033-2909.117.3.497. 
[35] H. J. Smith, T. Dinev, and H. Xu, “Information privacy 
research: an interdisciplinary review”. MIS Quart., vol. 35, 
2011, pp. 989-1015, ISSN: 0276-7783. 
[36] M. Weber, “Objectivity in Social Science and Social Policy”, 
in The Methodology of the Social Sciences, E. A. Shils and 
H. A. Finch, Eds. Glencoe, IL: Free Press, 1949, pp. 50-112.  
[37] J. Brewer, “Ideal type”, in The A-Z of Social Research, R. L. 
Miller and J. D. Brewer, Eds., London: Sage, pp. 147–148, 
2003. 
 
54
Copyright (c) IARIA, 2015.     ISBN:  978-1-61208-440-4
CENTRIC 2015 : The Eighth International Conference on Advances in Human-oriented and Personalized Mechanisms, Technologies, and Services

