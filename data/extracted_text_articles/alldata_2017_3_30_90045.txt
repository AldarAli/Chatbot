Empirical Evaluation of Open Government Data Visualisations 
Elena Ornig, Jolon Faichney, Bela Stantic 
School of Information and Communication Technology 
Griffith University 
Gold Coast, Australia 
email: elena.ornig@griffithuni.edu.au 
email: {j.faichney,b.stantic}@griffith.edu.au 
 
 
Abstract—The Open Government Data (OGD) movement has 
seen governments around the world embrace the concept of 
opening their data. However, the large amounts of data 
released have not resulted in wide acceptance of the data by 
end-users. This is partly due to the emphasis on machine-
readability rather than human-usability. Recently, some data 
portals have included visualization techniques to make the 
portals more usable. In this work, we report on user studies 
conducted to evaluate different OGD visualization techniques. 
The techniques were evaluated both quantitatively, through 
recorded tasks, and qualitatively, through a post study survey. 
We found that geographic map visualizations were reported by 
users to provide the highest level of qualitative satisfaction, 
which correlates with the quantitative results requiring the 
shortest time to complete the tasks. This study provides 
insights into empirical evaluation of visualization techniques to 
aid OGD providers in making decisions about the best way to 
present data in their portals. 
Keywords- data visualizations; open government data; empirical 
evaluation;  
I. 
 INTRODUCTION  
The amount of OGD released for public use, reuse, and 
redistribution is rapidly growing. Currently, 18 million OGD 
datasets have been published around the world [1] and 
according to dataportal.org there are 520 registered 
government portals [2]. At the International Open 
Government Dataset Search there are 192 catalogs in 24 
languages representing 43 countries [3]. These numbers 
represent a growing supply of OGD for  users.  However, the 
uptake by users has been limited. One possible cause, which 
we investigate in this paper, is the limited usability of open 
data. 
The primary focus of the OGD movement has been on 
ensuring the release of the data so that it can be accessed. 
Additionally, the desired format of the data is one that is 
machine-readable, in formats such as Comma Separated 
Values (CSV) and eXtensible Markup Language (XML), 
preferably in the most raw and primal forms [20]. The 
motivation is based on transparency, so that the community 
has access to the data in its original form without 
modification. A downside to this motivation is that it is only 
usable by a small percentage of the community, those with 
technical computer skills, such as computer programmers 
and data analysts. The focus on machine-readability has 
limited the data's human-usablility [17]. 
One strategy to increase end-user uptake of OGD is to 
present the data using visualizations [18][19]. Graves and 
Hendler [4] conducted a detailed study on the use of 
visualisations for OGD. Their research focussed on end-
users with some knowledge of data analysis such as 
researchers, journalists, and government data providers and 
consumers. They also identified a user profile of “Common 
Citizen” but did not include them in their study. They 
expressed an interest to investigate the remaining open 
question of how to empower “Common Citizens” and make 
it easier for them to consume OGD. 
In our study, we are particularly interested in those who 
don’t have skills in data analysis, but have an intention to use 
and benefit from open data. 
Since there are many different groups of consumers and 
more than a hundred techniques of data visualisation [7], we 
singled out a group of consumers, defined by Graves et al [4] 
as common citizens to evaluate three different visualizations. 
We planned to find out what can make it easier for common 
citizens to use OGD data. To answer this question we 
conducted a field experiment in order to empirically evaluate 
human-usability of OGD visualisations by common citizens 
with the aim to inform designer and practitioners.  In 
addition we evaluated what stops common citizens to use 
OGD with the aim to inform OGD community.  To conduct 
field experiment we engaged  common citizens in random 
locations, assigning them only if they had no knowledge in 
how to create, modify and manage data visualisations.   
In Section II, we describe the relationship between data, 
visualization, and evaluation. In Section III, we explain the 
methodology used and how it was implemented. In Section 
IV, we report our results and finding.  In Section V, we 
discuss the results significance and implications; our 
assumptions and limitations.  Section VI describes our 
conclusions. 
II. 
BACKGROUND 
Visualization 
is 
an 
effective 
technique 
for 
communication of data, due to our natural ability to 
understand patterns [8]. 
When selecting a visualization technique there are a 
number of considerations: the underlying scientific principles 
of human perception and cognition; design guidelines; and 
the empirical evaluation of the technique. 
One challenge with visualization as a science is that it 
currently does not have a unified general theory [10]. 
35
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-552-4
ALLDATA 2017 : The Third International Conference on Big Data, Small Data, Linked Data and Open Data (includes KESA 2017)

Demiralp [9] identified several causes for a lack of general 
theory. One issue is that visualization works at several 
domains in human perception and cognition. Additionally, 
visualization isn’t necessarily limited to what we perceive, 
but may include an interactive element, which may have a 
significant impact on the success of the visualization.  
Demiralp [9] concludes that the question of how to measure 
and construct effective visualizations in general is an 
unsolved problem.  
In terms of design principles, more work has been done 
and is somewhat well established. Shneiderman [5] 
introduced a type-by-task taxonomy to guide designers: 
overview first; zoom and filter; then details-on-demand [11], 
which has become the extended principles to guide designers 
of visualizations. 
To judge a particular quality of a system or interface 
researchers and practitioners use evaluation, which is the last 
step in the process of the creation of visualizations [12], 
preferably empirically-driven [13].  Evaluation helps to 
understand the visualisation tool, visualisations themselves, 
and the complex process that this tool supports [13], as well 
as its potential and limitations [6]. This process represents 
the relationship between data, visualization, and evaluation. 
III. 
 METHODOLOGY 
To evaluate the usability of open data visualization 
techniques we performed field experiments using the 
DataViva [21] open data web portal. DataViva is a web 
portal for Brazil's open data developed in partnership with 
the MIT Media Lab. Since starting this study, MIT Media 
Lab have also launched the Data USA [22] open data portal, 
which contains updated visualizations. We did not evaluate 
Data USA in this study. 
The field experiment focused on three visualization 
techniques provided by DataViva: TreeMap, Map, and 
Stacked. Examples of these visualizations are shown in 
Figures 1-3.  
 
 
Figure 1. DataViva TreeMap visualization. 
 
Figure 2. DataViva Map visualization. 
 
 
Figure 3. DataViva Stacked visualization. 
 
 
We engaged our users at 7 different locations around 
Gold Coast city, Australia, in public places where Wi-Fi 
access was freely available. To conduct the experiment we 
used a MacBook Air laptop.  DataViva was used to explore 
simple questions on the Brazilian economy.  As a tool for 
video and audio data collection we used Software Debut.  
Our goal was to test at least 10 participants as this is a 
suitable number according to Faulkner [16]. 
To balance the control between observer and the users 
and to balance the trade-offs between generalization, 
precision, and realism [14], the experiment was broken down 
into two stages: preliminary stage and controlled-testing 
stage. 
The preliminary stage included presenting the participant 
with an information sheet about the study and conversational 
questioning to find out what stops common citizens from 
using OGD and concluded with the formal signing of the 
consent form.  
The controlled-testing stage included 5 minutes of device 
and interface familiarization which was followed by 
performance tasks designed as a motivational scenario based 
on an envisaged real situation and setting. Tasks were 
designed to solve real problems with real data. The user’s 
36
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-552-4
ALLDATA 2017 : The Third International Conference on Big Data, Small Data, Linked Data and Open Data (includes KESA 2017)

interaction was captured with screen recording software and 
audio that were later analyzed to calculate completion time. 
We used an unenforced think aloud protocol to support the 
identification of possible usability issues. Users were given 3 
tasks to complete, each using a different visualization 
technique and a different task for that visualization. The 
tasks are shown in Table 1. 
 
TABLE 1. VISUALIZATION TASKS COMPLETED BY 
PARTICIPANTS 
 
Number 
Technique 
Description 
Task 1 
TreeMap 
How many jobs are in Sao Paulo? 
Task 2 
Map 
What is the nominal wage growth 
in Sao Paolo? 
Task 3 
Stacked 
What is the total of monthly wages 
in Sao Paolo? 
 
This was followed by preferential rating to quantify 
user’s opinions for overall assessment of each single 
visualization interface. Finally, the participants were asked a 
single open question: Why do you prefer this particular 
visualization compared to others? 
IV. 
RESULTS 
Our experiment sample was based on 12 users. Their 
average age was 54 years. As shown in Figure 4, 33% had a 
university degree, 42% had a college education and 25% 
were educated at TAFE (a technical training institution).  
 
 
 
Figure 4. Distribution of participants’ occupations. 
 
The time spent per participant to complete the tasks took 
on average 11 minutes, excluding 5 minutes given to 
participants to familiarise with the DataViva interface and 
the time spent to answer an open-ended question.  
At the preliminary stage we approached participants with 
the conversational questioning to find out what stops them 
from using OGD. 83.2% of participants answered that they 
had never heard of OGD; did not know OGD existed; or 
what it means.  However, after their interaction with open 
data, 66.6% had expressed an interest to know more.  
The average time to complete each task was calculated 
and the results are shown in Table 2. The Map visualization 
was the quickest, followed by Stacked, and then TreeMap. 
 
 
TABLE 2. CONSISTANCY BETWEEN TIME PERFORMANCE & 
PREFERABLE CHOICE 
 
Visualizations 
Average time 
per 
participant 
Preferable 
choice 
Map 
1 min 
First  
Stacked 
1min 13sec 
Second  
TreeMap 
1min 19sec 
Last  
 
Participants were asked to rank the visualizations in 
order of preference. The participants were asked the 
question: “What visualization they prefer or perceive as the 
easiest to use and why?” Figure 5 shows the results of the 
ranking.  
 
 
Figure 5. Preferential ranking for each visualization type. 
 
The Map visualization had the most number of first 
choice rankings, it also had the most number of second 
choice rankings, and no participants placed it last. The 
ranking of TreeMap and Stacked were very similar with 
Stacked having one more ranking in second place and hence 
one less ranking last. As a result the order of preference for 
the participants was Map, Stacked, and TreeMap, which 
correlates with the time it took to complete each task as 
shown in Table 2. 
Participants also provided reasons why they gave 
visualizations the particular ranking. The Map visualization 
was chosen because it was perceived as a familiar shape, 
that of a geographic map, and easy to use. 
The Stacked visualisation had contradictory perception. 
Some perceived it as easy to understand and clear. Others 
found it confusing and reported that it “didn’t make sense”.  
Participants that rated the TreeMap first found it easy to 
find information. Those that rated it second stated that it 
was “not clear”. Those that rated it last said it was 
confusing, busy, and more difficult to find information. 
V. 
DISCUSSION 
The field experiments highlighted a number of issues 
with open data usability. Firstly, only 16.6% of participants 
had previously heard of open data. Secondly, TreeMap is a 
very common visualization tool used commonly in data 
37
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-552-4
ALLDATA 2017 : The Third International Conference on Big Data, Small Data, Linked Data and Open Data (includes KESA 2017)

journalism. However, we found that participants had the 
most trouble with it, both in terms of taking the longest time 
to complete the task, and also in response to the open 
question. 
The most significant usability problem with all three 
visualisations was a feature known as the tooltip plugin or 
more commonly as pop-up box.  With all three 
visualizations, the pop-up box was blocking the overview. 
Taking into consideration the extended principles for 
designers of data visualizations: overview first, zoom and 
filter; then details-on-demand [5] we demonstrated that this 
feature was blocking overview with details shown in 
Figures 1-3.  
The problem with the feature is that it appears on a 
mouse rollover. As the user is navigating to interact with the 
visualization, the popup box occludes the area they want to 
interact with. 
We have provided possible solutions to the popup box 
issue for each of the visualizations, shown in Figures 6-8. 
The solution is generally to display the popup box to the 
side. 
Other issues that users reported were difficulty in 
reading titles or headings or the headings not being visible 
at all.  
VI. 
CONCLUSIONS 
The OGD movement is maturing with large quantities of 
data being released by governments around the world. The 
embracing of OGD hasn’t necessarily translated into uptake 
by OGD consumers. We propose that this is because of the 
focus on machine-readability rather than human-usability. 
Recent efforts are focusing on providing interactive 
visualizations of OGD. In this paper, we evaluated one 
OGD portal to identify strengths and weaknesses between 
data visualization techniques, specifically for common 
citizens, which currently hasn’t been investigated in the 
literature. 
Even though our participants were unfamiliar with 
OGD, after a short introduction they were able to answer the 
problems on average in under 2 minutes, showing the 
advantage visualizations have over technical and raw data. 
This serves as a strong argument for OGD portals to provide 
visualizations to increase end-user uptake by common 
citizens. 
Comparing three different types of visualizations, the 
clear preference was for Map visualization which presents 
the data on a geographical map. The basis for Map being the 
greatest preference both qualitatively and quantitatively is 
due to its familiarity to the users. Concrete concepts are 
quicker to grasp than abstract concepts. The TreeMap and 
Stacked visualizations present data more abstractly and 
require a greater conceptual leap for common citizens to 
grasp. 
Therefore to encourage end-user uptake of OGD, 
visualizations should be selected that are concrete and 
familiar to end-users, such as Map visualizations, and to 
avoid more abstract visualizations. Note that visualizations 
such as TreeMap have been designed to address many 
usability and visualization factors, however, we have found 
that for common citizens, concreteness and familiarity are 
more important than other usability factors. 
 
 
Figure 6. Non-occluding popup box for TreeMap visualization. 
 
 
Figure 7. Non-occluding popup box for Map visualization. 
 
 
Figure 8. Non-occluding popup box for Stacked visualizaton. 
 
Our study also identified smaller issues such as popups, 
where a simple and useful feature when poorly implemented 
can grossly impact the effectiveness of a visualization and 
reinforces the need not just for visualizations, but for end-
user testing to verify the effectiveness of the visualizations.  
38
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-552-4
ALLDATA 2017 : The Third International Conference on Big Data, Small Data, Linked Data and Open Data (includes KESA 2017)

Our study compared three visualization techniques. 
Future 
work 
will 
investigate 
broader 
visualization 
techniques and investigate newer data portals such as Data 
USA and a new version of DataViva. Additionally more 
comprehensive tasks can be evaluated that provide greater 
insights into the strengths and weaknesses of different 
techniques and enhance the benefits of each. 
As Demiralp [9] has identified there is currently no 
general unified theory of designing and evaluating 
visualization techniques. We are interested in drawing 
together the science of perception, design principles, and 
empirical 
evaluation 
to 
enhance 
and 
improve 
the 
consumption of OGD. 
  
ACKNOWLEDGMENT 
We would like to acknowledge all participants who 
greatly contributed their time and effort to support this 
project. 
REFERENCES 
 
[1] data.world, “data.world launches to make the world’s data 
easier to find, use, and share.” 11 July, 2016, retrieved: 
March, 
2017. 
[Online]. 
Available: 
https://globenewswire.com/news-
release/2016/07/11/855045/0/en/data-world-Launches-to-
Make-the-World-s-Data-Easier-to-Find-Use-and-Share.html  
[2] Data Portals, “A Comprehensive List of Open Data Portals 
from Around the World”, retrieved: March, 2017. [Online].  
Available: http://dataportals.org/ 
[3] Tetherless World Constellation Linking Open Government 
Data (TWC LOPG), “IOGDS: International OGDset 
Search”. retrieved: March, 2017. [Online]. Available: 
https://logd.tw.rpi.edu/demo/international_dataset_catalog_
search 
[4] A. Graves & J. Hendler, “A study on the use of 
visualizations for OGD”. Information Polity, 19(1, 2), pp. 
73-91, 2014. 
[5] B. Shneiderman, “A Grander Goal: A Thousand-fold 
Increase in Human Capabilities”. Educom Review, 32, 6, 4-
10. HCIL-97-23, Nov-Dec 1997. 
[6] C. Plaisant, “The challenge of information visualization 
evaluation.” In Proceedings of the working conference on 
Advanced visual interfaces (pp. 109-116). ACM, May, 
2004. 
[7] R. Lengler & M. J. Eppler, “Towards a periodic table of 
visualization methods for management.” In IASTED 
Proceedings 
of 
the 
Conference 
on 
Graphics 
and 
Visualization in Engineering (GVE 2007), Clearwater, 
Florida, USA, Jan, 2007. 
[8] C. Ware, “Information visualization: Perception for design”. 
Elsevier. Third edition, 2013. 
[9] C. Demiralp, D. H. Laidlaw, J. J. Van Wijk, and C. Ware, 
“Theories of Visualization—Are There Any?” Brown 
University. Panel discussion. 2 Sep, 2016. retrieved: March, 
2017. 
[Online]. 
Avaialable:  
http://hci.stanford.edu/~cagatay/projects/vismodel/Theories
OfVisualization-Vis11.pdf  
[10] B. Rogowitz, “Visualization Theory: Putting the Pieces 
Together,” IEEE Visualization VizWeek, 29 Oct, 2010. 
retrieved: 
March, 
2017. 
[Online]. 
Available: 
https://sites.google.com/site/bernicerogowitz/theory-of-
visualization  
[11] B. Shneiderman, “The eyes have it: a task by data type 
taxonomy 
for 
information 
visualizations,” 
Visual 
Languages, 1996. Proceedings., IEEE Symposium on, 
Boulder, 
CO, 
1996, 
pp. 
336-343. 
DOI= 
10.1109/VL.1996.545307. 
[12] A. R. Teyseyre and M. R. Campo, “An Overview of 3D 
Software 
Visualization,” 
in 
IEEE 
Transactions 
on 
Visualization and Computer Graphics, vol. 15, no. 1, pp. 
87-105, Jan.-Feb. 2009. Doi: 10.1109/TVCG.2008.86 
[13] H. Lam, E. Bertini, P. Isenberg, C. Plaisant and S. 
Carpendale, 
“Empirical 
Studies 
in 
Information 
Visualization: Seven Scenarios,” in IEEE Transactions on 
Visualization and Computer Graphics, vol. 18, no. 9, pp. 
1520-1536, Sept. DOI=2012.doi: 10.1109/TVCG.2011.279, 
2012. 
[14] Carpendale, S, “Evaluating information visualizations,” (pp. 
19-45). Berlin, Heidelberg: Springer Berlin Heidelberg. 
doi:10.1007/978-3-540-70956-5_2, 2008. 
[15] C. Chen, “Top 10 Unsolved Information Visualization 
Problems.” IEEE Comput. Graph. Appl. 25, 4, pp. 12-16. 
DOI=http://dx.doi.org/10.1109/MCG.2005.91, July, 2005. 
[16] L. Faulkner, “Beyond the five-user assumption: Benefits of 
increased sample sizes in usability testing.” Behavior 
Research Methods, Instruments and Computers, 35(3), 279-
383. doi=10.3758/BF03195514, 2003. 
[17] C. Martin, “Barriers to the OGD Agenda: Taking a 
Multi-Level Perspective,” Policy & Internet, 6(3), 217-240,  
2014.         
[18] N. Shadbolt, et al., “Linked OGD: Lessons from 
data.gov.uk,” IEEE Intelligent Systems, 27(3), 16-24, 2012. 
[19] J. Hendler, J. Holm, C. Musialek, & G. Thomas, “US 
government linked open data: semantic.data.gov,” IEEE 
Intelligent Systems, 27(3), 0025-31, 2012. 
[20] Sunlight Foundation, “Ten principles for opening up gov- 
ernment,” 
2010, 
retrieved: 
March, 
2017. 
[Online]. 
Available: 
http://sunlightfoundation.com/policy/documents/ten-open-
data- principles/   
[21] DataViva. retrieved: March, 2017. [Online]. Available: 
http://dataviva.info. 
[22] Data USA. retrieved:  March, 2017. [Online]. Available: 
https://datausa.io/ 
 
 
 
 
 
39
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-552-4
ALLDATA 2017 : The Third International Conference on Big Data, Small Data, Linked Data and Open Data (includes KESA 2017)

