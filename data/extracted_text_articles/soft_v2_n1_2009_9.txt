101
International Journal On Advances in Software, vol 2 no 1, year 2009, http://www.iariajournals.org/software/
From Supervised to Reinforcement Learning:
a Kernel-based Bayesian Filtering Framework
Matthieu GEIST1,2,3, Olivier PIETQUIN1 and Gabriel FRICOUT2
1IMS Research Group, Sup´elec, Metz, France
2MC Cluster, ArcelorMittal Research, Maizi`eres-l`es-Metz, France
3CORIDA Project-Team, INRIA Nancy - Grand Est, France
{matthieu.geist,olivier.pietquin}@supelec.fr
Abstract—In a large number of applications, engineers have
to estimate a function linked to the state of a dynamic system.
To do so, a sequence of samples drawn from this unknown
function is observed while the system is transiting from state
to state and the problem is to generalize these observations
to unvisited states. Several solutions can be envisioned among
which regressing a family of parameterized functions so as to
make it ﬁt at best to the observed samples. This is the ﬁrst
problem addressed with the proposed kernel-based Bayesian
ﬁltering approach, which also allows quantifying uncertainty
reduction occurring when acquiring more samples. Classical
methods cannot handle the case where actual samples are not
directly observable but only a non linear mapping of them is
available, which happens when a special sensor has to be used or
when solving the Bellman equation in order to control the system.
However the approach proposed in this paper can be extended to
this tricky case. Moreover, an application of this indirect function
approximation scheme to reinforcement learning is presented. A
set of experiments is also proposed in order to demonstrate the
efﬁciency of this kernel-based Bayesian approach.
Index Terms—supervised learning; reinforcement learning;
Bayesian ﬁltering; kernel methods
I. INTRODUCTION
In a large number of applications, engineers have to estimate
values of an unknown function given some observed samples.
For example, in order to have a map of wiﬁ (wireless ﬁdelity)
coverage in a building, one solution would be to simulate
the wave propagation in the building according to Maxwell
equations, which would be intractable in practice. An other
solution is to measure the electromagnetic ﬁeld magnitude in
some speciﬁc locations, and to interpolate between theses ob-
servations in order to build a ﬁeld map which covers the whole
building. This task is referred to as function approximation or
as generalization. One way to solve the problem is to regress
a family of parameterized functions so as to make it ﬁt at best
the observed samples. Lots of existing regression methods can
be found in the literature for a wide range of function families.
Artiﬁcial Neural Networks (ANN) [2] or kernel machines [3],
[4] are popular methods. Yet, usually batch methods are used
(gradient descent for ANN or Support Vector Regression for
kernel machines); that is all the observed samples have to
be known before regression is done. A new observed sample
requires running again the regression algorithm using every
sample.
Online regression describes a set of methods able to in-
crementally improve the regression results as new samples
are observed by recursively updating previously computed
parameters. There exists online regression algorithms using
ANN or kernel machines, yet the uncertainty reduction oc-
curring when acquiring more samples (thus more informa-
tion) is usually not quantiﬁed, as well as with the batch
methods. Bayesian methods are such recursive techniques
able to quantify uncertainty about the computed parameters.
They have already been applied to ANN [5], [6] and, to
some extent, to kernel machines [7], [8]. In this paper is
proposed a method based on the Bayesian ﬁltering framework
[9] for recursively regressing a nonlinear function from noisy
samples. In this framework a hidden state (here the regression
parameter vector) is recursively estimated from observations
(here the samples), while maintaining a probability distribution
over parameters (uncertainty estimation).
Several problems are not usually handled by standard tech-
niques. For instance, actual samples are sometimes not directly
observable but only a non linear mapping of them is available.
This is the case when a special sensor has to be used (e.g.,
measuring a temperature using a spectrometer or a thermocou-
ple). This is also the case when solving the Bellman equation
in a Markovian decision process with unknown deterministic
transitions [10]. This is important for (asynchronous and
online) dynamic programming, and more generally for control
theory. The proposed approach is extended to online regression
of nonlinear mapping of observations. First a quite general
formulation of the problem is described in order to appeal
a broader audience. Indeed the technique introduced below
handles well nonlinearities in a derivative free way and it can
be useful in other ﬁelds. Nevertheless, an application of this
framework to reinforcement learning [11] is also described.
The general outline of the proposed method is as follows.
The parametric function approximation problem as well as
its extension to nonlinear mapped observations case mainly
breaks down in two parts. First, a representation for the
approximated function must be chosen. For example, it can
be an ANN. Notice that this also involves to choose a speciﬁc
structure, e.g., number of hidden layers, number of neurons,
synaptic connections, etc. A kernel representation is chosen in
this paper, because of its expressiveness given by the Mercer
theorem [4]. Moreover a dictionary method [12] allows quasi-

102
International Journal On Advances in Software, vol 2 no 1, year 2009, http://www.iariajournals.org/software/
automatizing the choice of the associated structure (that is
number and positions of kernel basis). Second, an algorithm
to learn the parameters is necessary. For this, the regression
problem is cast in a Bayesian tracking problem [9]. As it will
be shown, it allows handling well nonlinearities, uncertainty
and even non-stationarity.
The next section presents some necessary background:
the dictionary method and Bayesian ﬁltering. The following
sections describe the proposed algorithm for function ap-
proximation [1], its extension to regression from nonlinear
mapping of observations [13] and the application of this
general algorithm to reinforcement learning [14]. All these
algorithms are experimented and compared to the state-of-the-
art, and the last section concludes.
II. BACKGROUND
Before introducing the proposed approach, some back-
bone methods are presented. The ﬁrst one is a dictionary
method [12] based on mathematical signiﬁcation of kernels
and basic linear algebra which allows automatizing the choice
of the structure (number and position of kernels). The sec-
ond one, Bayesian ﬁltering and more precisely Sigma Point
Kalman ﬁltering, is used as the learning part of the proposed
algorithms. First, kernel-based regression is brieﬂy introduced.
A. Kernel-based Regression
A kernel-based regression is used, namely the approxi-
mation is of the form ˆfθ(x) = Pp
i=1 αiK(x, xi) where x
belongs to a compact set X of Rn (all the work is done
in X) and K is a kernel, that is a continuous, symmetric
and positive semi-deﬁnite function. The parameter vector θ
contains the weights (αi)i, and possibly the centers (xi)i and
some parameters of the kernel (e.g., the variance for Gaussian
kernels). These methods rely on the Mercer theorem [4] which
states that each kernel is a dot product in a higher dimensional
space. More precisely, for each kernel K, there exists a
mapping ϕ : X → F (F being called the feature space)
such that ∀x, y ∈ X, K(x, y) = ⟨ϕ(x), ϕ(y)⟩. Thus, any
linear regression algorithm which only uses dot products can
be cast by this kernel trick into a nonlinear one by implicitly
mapping the original space X to a higher dimensional one.
Many approaches to kernel regression can be found in the lit-
erature, the most classical being the Support Vector Machines
(SVM) framework [4]. There are fewer Bayesian approaches,
nonetheless the reader can refer to [7] or [8] for interesting
examples. To our knowledge, none of them is designed to
handle the second regression problem described in this paper
(when observations are nonlinearly mapped).
B. Dictionary
A ﬁrst problem is to choose the number p of kernel functions
required for the regression task and the prior kernel centers.
A variety of methods can be contemplated, the simplest one
being to choose equally spaced kernel fonctions. However
the method described below rests on the mathematical sig-
niﬁcation of kernels and basic algebra, and is thus well
motivated. By observing that although the feature space F
is a (very) higher dimensional space, ϕ(X) can be a quite
smaller embedding, the objective is to ﬁnd a set of p points
in X such that
ϕ(X) ≃ Span {ϕ(˜x1), . . . , ϕ(˜xp)}
(1)
This method is iterative. Suppose that samples x1, x2, . . .
are sequentially generated. At time k, a set
Dk−1 = (˜xj)mk−1
j=1
⊂ (xj)k−1
j=1
(2)
of mk−1 elements is available where by construction feature
vectors ϕ(˜xj) are approximately linearly independent in F. A
sample xk is then uniformly sampled from X, and is added to
the dictionary if ϕ(xk) is linearly independent on Dk−1. To
test this, weights a = (a1, . . . , amk−1)T have to be computed
so as to verify
δk =
min
a∈Rmk−1

mk−1
X
j=1
ajϕ(˜xj) − ϕ(xk)

2
(3)
Formally, if δk = 0 then the feature vectors are linearly depen-
dent, otherwise not. Practically an approximate dependence is
allowed, and δk is compared to a predeﬁned threshold ν de-
termining the quality of the approximation (and consequently
the sparsity of the dictionary). Thus the feature vectors will
be considered as approximately linearly dependent if δk ≤ ν.
By using the kernel trick and the bilinearity of dot products,
equation (3) can be rewritten as
δk =
min
a∈Rmk−1
n
aT ˜Kk−1a − 2aT ˜kk−1(xt) + K(xk, xk)
o
(4)
where

˜Kk−1

i,j = K(˜xi, ˜xj)
(5)
is a mk−1 × mk−1 matrix and

˜kk−1(x)

i = K(x, ˜xi)
(6)
is a mk−1 × 1 vector. If δk > ν, xk = ˜xmk is added to the
dictionary, otherwise not. Equation (4) admits the following
analytical solution
(
ak = ˜K−1
k−1˜kk−1(xk)
δk = K(xk, xk) − ˜kk−1(xk)T ak
(7)
Notice that the matrix ˜K−1
k
can be computed efﬁciently. If
δk ≤ ν no point is added to the dictionary, and thus ˜K−1
k
=
˜K−1
k−1. If xk is added to the dictionary, one can write the matrix
˜Kk by blocs:
˜Kk =

˜Kk−1
˜kk−1(xk)
˜kk−1(xk)T
K(xk, xk)

(8)
By using the partitioned matrix inversion formula, its inverse
is incrementally computed:
˜K−1
k
= 1
δk

δk ˜K−1
k−1 + akaT
k
−ak
−aT
k
1

(9)

103
International Journal On Advances in Software, vol 2 no 1, year 2009, http://www.iariajournals.org/software/
where ak and δk are the given analytical solution to prob-
lem (4). This bounds the computational cost for the kth
sample by O(m2
k). Thus this approach allows computing
sequentially and incrementally an approximate basis of ϕ(X).
The dictionary method is brieﬂy sketched in Algorithm 1,
nevertheless see [12] for more details and theoretical analysis
of the properties of this approach.
Algorithm 1: Dictionary computation
inputs : a set of N samples randomly selected from X,
sparsiﬁcation parameter ν
outputs: a dictionary D
Initialization;
D1 = {x1};
Dictionary computation;
for k = 1, 2, . . . N do
Observe sample xk;
Compute approximate dependence:
δk =
min
a∈Rmk−1

mk−1
X
j=1
ajϕ(˜xj) − ϕ(xk)

2
if δk > ν then
Add xk to the dictionary: Dk = Dk−1 ∪ {xk}
else
Let the dictionary unchanged: Dk = Dk−1
Thus, by choosing a prior on the kernel to be used, and
by applying this algorithm to a set of points (x1, . . . , xN)
randomly sampled from X, a sparse set of good candidates
to the kernel regression problem is obtained. This method is
theorically well founded, easy to implement, computationally
efﬁcient and it does not depend on kernels nor space topology.
Notice that, despite the fact that this algorithm is naturally
online, this dictionary cannot be built (straightforwardly) while
estimating the parameters, since the hyper-parameters of the
chosen kernels (such as mean and deviation for Gaussian
kernels) will be parameterized as well (which leads to a
nonlinear parameterization). If the samples used for regression
are known beforehand, they can be used to construct the dic-
tionary. However, for online regression, samples are generally
not known beforehand. Knowing bounds on X is sufﬁcient to
compute a dictionary.
C. Bayesian Filtering
Bayesian ﬁltering was originally designed to track the state
of a stochastic dynamic system from observations (e.g., track-
ing the position of a plane from radar measures). When used
as a learner for function approximation, the parameter vector is
the hidden state to be tracked. As it will be shown below, this
parameter vector is modeled as a random variable, whereas it
is generally deterministic. However, this allows handling non-
stationary regression problems, and even if stationary, it helps
avoiding local minima. Indeed, the process noise (to be deﬁned
below) then plays a role quite similar to simulated annealing.
A comprehensive survey of Bayesian ﬁltering is given in [9].
1) Paradigm: The problem of Bayesian ﬁltering can be
expressed in its state-space formulation; suppose that the
dynamic of a system and the associated generated observations
are driven by the following equations:
(
sk+1 = fk(sk) + vk
yk = gk(sk) + nk
(10)
The objective is to sequentially infer the hidden state sk given
the observations y1, . . . , yk = y1:k. The state evolution is
driven by the possibly nonlinear mapping fk and the process
noise vk (centered and of variance Pvk). The observation yk is
a function of the state sk, corrupted by an observation noise
nk (centered and of variance Pnk). To do so, the posterior
density (of state over past observations) is recursively updated
as new observations arrive by making use of the Bayes rule
and of the dynamic state-space model of the system (10).
Such a Bayesian ﬁltering approach can be decomposed
in two steps, which crudely consists in predicting the new
observation generated by the system given the current approx-
imated model, and then correcting this model according to the
accuracy of this prediction, given the new observation. The
ﬁrst stage is the prediction step. It consists in computing the
following distribution:
p(Sk|Y1:k−1) =
Z
S
p(Sk|Sk−1)p(Sk−1|Y1:k−1)dSk−1
(11)
It is the prior distribution of current state conditioned on past
observations up to time k − 1. It is a projection forward in
time of the posterior at time k − 1, p(Sk−1|Y1:k−1) by using
the process model represented by p(Sk|Sk−1) which depends
on fk. For example, if the evolution function is linear and the
noise is Gaussian, the distribution of Sk|Sk−1 is Gaussian of
mean fk−1(Sk−1) and of variance Pvk−1:
Sk|Sk−1 ∼ N

104
International Journal On Advances in Software, vol 2 no 1, year 2009, http://www.iariajournals.org/software/
mappings are nonlinear (but the noises are still Gaussian), a
ﬁrst solution is to linearize them around the state: it is the
principle of the Extended Kalman Filter (EKF), and sufﬁcient
statistics are still propagated through linear transformations.
Another approach is the Sigma Point Kalman Filter (SPKF)
framework [6]. The basic idea is that it is easier to approximate
a probability distribution than an arbitrary nonlinear function.
It is based on the unscented transform [16], which is now
described (the sigma-point designation can be seen as a
generalization of the unscented transform).
2) The Unscented Transform: The problem of approxi-
mating Bayesian ﬁltering when evolution and observation
equations are not linear can be expressed as follows: given
ﬁrst and second order moment of a random variable, compute
ﬁrst and second order moments of a nonlinear mapping of this
random variable. The unscented transform addresses this issue
by deterministically sampling the distribution using its mean
and variance.
Let’s abstract from previous sections and their notations.
Let X be a random vector, and let Y be a mapping of X. The
problem is to compute mean and covariance of Y knowing
the mapping and ﬁrst and second order moments of X. If
the mapping is linear, the relation between X and Y can be
written as Y = AX where A is a matrix of ad hoc dimension.
In this case, required mean and covariance can be analytically
computed: E[Y ] = AE[X] and E[Y Y T ] = AE[XXT ]AT .
If the mapping is nonlinear, the relation between X and Y
can be generically written as:
Y = f(X)
(15)
A ﬁrst solution would be to approximate the nonlinear map-
ping, that is to linearize it around the mean of the random
vector X. This leads to the following approximations of the
mean and covariance of Y :
E[Y ] ≈ f (E[X])
(16)
E[Y Y T ] ≈ (∇f (E[X])) E[XXT ] (∇f (E[X]))T
(17)
This approach is the basis of Extended Kalman Filtering
(EKF) [17], which has been extensively studied and used in
past decades. However it has some limitations. First it cannot
handle non-derivable nonlinearities. It requires to compute the
gradient of the mapping f, which can be quite difﬁcult even if
possible. It also supposes that the nonlinear mapping is locally
linearizable, which is unfortunately not always the case and
can lead to quite bad approximations, as exempliﬁed in [16].
The basic idea of the unscented transform is that it is easier
to approximate an arbitrary random vector than an arbitrary
nonlinear function. Its principle is to sample deterministically
a set of so-called sigma-points from the expectation and the
covariance of X. The images of these points through the
nonlinear mapping f are then computed, and they are used
to approximate statistics of interest. It shares similarities with
Monte-Carlo methods, however here the sampling is deter-
ministic and requires less samples to be drawn, nonetheless
allowing a given accuracy [16].
The original unscented transform is now described more
formally (some variants have been introduced since, but the
basic principle is the same). Let n be the dimension of X. A
set of 2n + 1 sigma-points is computed as follows:
x0 = ¯X
j = 0
(18)
xj = ¯X +
p
(n + κ)PX

j
1 ≤ j ≤ n
(19)
xj = ¯X −
p
(n + κ)PX

n−j
n + 1 ≤ j ≤ 2n
(20)
as well as associated weights:
w0 =
κ
n + κ and wj =
1
2 (n + κ)∀j > 0
(21)
where ¯X is the mean of X, PX is its variance matrix, κ is
a scaling factor which controls the accuracy of the unscented
transform [16], and (
p
(n + κ)PX)j is the jth column of the
Cholesky decomposition of the matrix (n + κ)PX. Then the
image through the mapping f is computed for each of these
sigma-points:
yj = f(xj),
0 ≤ j ≤ 2n
(22)
The set of sigma-points and their images can ﬁnally be used
to compute ﬁrst and second order moments of Y , and even
PXY , the covariance matrix between X and Y :
¯Y ≈ ¯y =
2n
X
j=0
wjyj
(23)
PY ≈
2n
X
j=0
wj (yj − ¯y) (yj − ¯y)T
(24)
PXY ≈
2n
X
j=0
wj (xj − ¯x) (yj − ¯y)T
(25)
where ¯x = x0 = ¯X.
3) Sigma Point Kalman Filtering: The unscented transform
having been presented, the Sigma-Point Kalman Filtering,
which is an approximation of Bayesian ﬁltering for nonlinear
mapping based on unscented and similar transforms (e.g.,
central differences transform, see [6] for details), is shortly
described.
SPKF and classical Kalman equations are very similar.
The major change is how to compute sufﬁcient statistics
(directly for Kalman, through sigma points for SPKF). Al-
gorithm 2 sketches a SPKF update based on the state-space
formulation (10), and using the standard Kalman notations:
sk|k−1 denotes a prediction, sk|k an estimate (or correction),
Ps,y a covariance matrix, ¯nk a mean and k is the discrete
time index. The principle of each update is as follow. First,
the prediction step consists in predicting the current mean
and covariance for the hidden state given previous estimates
and using the evolution equation. From this and using the
observation equation, the current observation is predicted.
This implies to use the unscented transform if the mappings
are nonlinear. Then mean and covariance of the hidden state
are corrected using the current observation and some system

105
International Journal On Advances in Software, vol 2 no 1, year 2009, http://www.iariajournals.org/software/
statistics (the better the prediction, the lesser the correction).
Update (or equivalently correction step) is made according
to the so-called Kalman gain Kk which depends on statistics
computed thanks to the unscented transform and using system
dynamics. Notice also that being online this algorithm must
be initialized with some priors ¯s0|0 and P0|0.
The reader can refer to [6] for details. More precise al-
gorithms will be given later, when developing speciﬁc ap-
proaches. Generally speaking, the computational complexity
of such an update is O(|S|3), where |S| is the dimension of
the state space. However, it will be shown that a square com-
putational complexity is possible for the approach developed
in next sections.
Algorithm 2: SPKF Update
inputs : ¯sk−1|k−1, Pk−1|k−1
outputs: ¯sk|k, Pk|k
Sigma-points computation;
Compute deterministically sigma-point set Sk−1|k−1
from ¯sk−1|k−1 and Pk−1|k−1;
Prediction Step;
Compute sigma-point set Sk|k−1 from fk(Sk−1|k−1, ¯nk)
and process noise covariance;
Compute ¯sk|k−1 and Pk|k−1 from Sk|k−1;
Correction Step;
Observe yk;
Compute sigma-point set Yk|k−1 = gk(Sk|k−1, ¯vk);
Compute ¯yk|k−1, Pyk and Psyk from Sk|k−1, Yk|k−1 and
observation noise covariance;
Kk = PsykP −1
yk ;
¯sk|k = ¯sk|k−1 + Kk(yk − ¯yk|k−1);
Pk|k = Pk|k−1 − KkPykKT
k ;
III. SUPERVISED LEARNING
The approach presented in this section addresses the prob-
lem of nonlinear function approximation. The aim here is to
approximate a nonlinear function f(x), x ∈ X, where X is a
compact set of Rn, from noisy samples
(xk, yk = f(xk) + nk)k
(26)
where k is the time index and nk is the observation random
noise, by a function ˆfθ(x) parameterized by the vector θ. The
rest of this paper is written for a scalar output, nevertheless
Bayesian ﬁltering paradigm allows an easy extension to the
vectorial output case.
The hint is to cast this regression problem in a state-space
formulation. The parameter vector is considered as the hidden
state to be infered. It is thus modeled as a random variable. As
a process model, a random walk is chosen, which is generally a
good choice if no more information is available. First it allows
handling non-stationarity, but it can also help to avoid local
minima, as discussed before. The observation equation links
the observations (noisy samples from the function of interest)
to the parameterized function. Notice that even if the function
of interest is not noisy, it does not necessarily exist in the
function space spanned by the parameters, so the observation
noise is also structural. This gives the following state-space
formulation :
(
θk+1 = θk + vk
yk = ˆfθk(xk) + nk
(27)
As announced in Section I, the principle of the propose
approach is to choose a (nonlinear) kernel representation of
the value function, to use the dictionary method to automate
the choice of the structure, and to use a SPKF to track the
parameters.
A. Parameterization
The approximation is of the form
ˆfθ(x) =
p
X
i=1
αiKσi(x, xi)
(28)
where θ = [(αi)p
i=1, (xi)p
i=1, (σi)p
i=1]T
and Kσi(x, xi) = exp(−∥x − xi∥2
2σ2
i
)
The problem is to ﬁnd the optimal number of kernels and a
good initialisation for the parameters (extension to other types
of kernel is quite straightforward). As a preprocessing step,
the dictionary method of Section II-B is used. First a prior
σ0 is put on the Gaussian width (or alternatively on ad hoc
parameters if another kernel is considered, e.g., degree and bias
for a polynomial kernel). Then N random points are sampled
uniformly from X and used to compute the dictionary. Thus
a set of p points D = {x1, . . . , xp} is obtained such that
ϕσ0(X) ≃ Span {ϕσ0(x1), . . . , ϕσ0(xp)}
(29)
where ϕσ0 is the mapping corresponding to the kernel Kσ0.
Notice that even if the sparsiﬁcation procedure is ofﬂine,
the algorithm (the regression part) is online. Moreover, no
training sample is needed for this preprocessing step, but only
classical prior which is anyway needed for the Bayesian ﬁlter
(σ0), one sparsiﬁcation parameter ν and bounds for X. These
requirements are not too restrictive.
Let q be the number of parameters. Given the chosen
parameterization of equation (28), there is p parameters for
the weights, p parameters for Gaussian standard deviations
and np parameters for Gaussian centers: q = (2 + n)p.
B. Prior
As for any Bayesian approach (and more generally any
online method) a prior (an initialization) has to be put on the
(supposed Gaussian) parameter distribution:
θ0 ∼ N(¯θ0, Σθ0)
(30)
where mean and covariance matrix are deﬁned as
(¯θ0 = [α0, . . . , α0, D, σ0, . . . , σ0]T
Σθ0 = diag([σ2
α0, . . . , σ2
α0, σ2
µ0, . . . , σ2
µ0, σ2
σ0, . . . , σ2
σ0])
(31)

106
International Journal On Advances in Software, vol 2 no 1, year 2009, http://www.iariajournals.org/software/
The operator diag applied to a column vector gives a diagonal
matrix. In these expressions, α0 is the prior mean on kernel
weights, D is the dictionary computed in the preprocessing
step, σ0 is the prior mean on kernel deviation, and σ2
α0, σ2
µ0,
σ2
σ0 are respectively the prior variance on kernel weights,
centers and deviations. All these parameters (except the dic-
tionary) have to be set up beforehand. Notice that ¯θ0 ∈ Rq
and Σθ0 ∈ Rq×q. A prior has also to be put on noises:
v0 ∼ N(0, Rv0) where Rv0 = σ2
v0Iq, Iq being the identity
matrix, and n0 ∼ N(0, Rn0), where Rn0 = σ2
n0 is a scalar.
C. Artiﬁcial Process Noise
Another issue is to choose the artiﬁcial process noise.
Formally, since the target function is stationary, there is
no process noise. However introducing an artiﬁcial process
noise can strengthen convergence and robustness properties
of the ﬁlter. Choosing this noise is still an open research
problem. Following [6] two types of artiﬁcial process noise are
considered (the observation noise is chosen to be constant).
The ﬁrst one is a Robbins-Monro stochastic approximation
scheme for estimating innovation. That is the process noise
covariance is set to
Rvk = (1−αRM)Rvk−1+αRMKk(yk− ˆf¯θk|k(xk))2KT
k (32)
Here αRM is a forgetting factor set by the user, and Kk is the
Kalman gain obtained during the Bayesian ﬁlter update.
The second type of noise provides for an approximate
exponentially decaying weighting past data. Its covariance is
set to a fraction of the parameters covariance, that is
Rvk = (λ−1 − 1)Pk|k
(33)
where λ ∈]0, 1] (1 − λ ≪ 1) is a forgetting factor similar to
the one from the recursive least-squares (RLS) algorithm.
D. Tracking Parameters
A Sigma Point Kalman ﬁlter update (which updates ﬁrst
and second order moments of the random parameter vector)
is applied at each time step, as a new training sample (xk, yk)
is available. The evolution equation being linear, the update
algorithm does not involve the computation of a sigma-point
set in the prediction step as in Algorithm 2. The proposed
approach is fully described in Algorithm 3.
First, the dictionary has to be computed and priors have
to be chosen. Then, as each input-output couple (xk, yk)
is observed, the parameter vector mean and covariance are
updated. First is the prediction phase. As the parameter vector
evolution is modeled as a random walk, the prediction of
the mean at time k is equal to the estimate of this mean at
time k − 1 (that is ¯θk|k−1 = ¯θk−1|k−1), and the parameters
covariance is updated thanks to the process noise covariance.
Then, the set of 2q + 1 sigma-points Θk|k−1 corresponding to
the parameters distribution must be computed using ¯θk|k−1 and
Pk|k−1, as well as associated weights W (see Section II-C2):
Θk|k−1 =
n
θ(j)
k|k−1,
0 ≤ j ≤ 2q
o
(34)
W = {wj,
0 ≤ j ≤ 2q
}
(35)
Notice that each of these sigma-points corresponds to a
particular parameterized function. Each of these functions is
evaluated in xk, the current observed input, which forms the
set of images of the sigma-points:
Yk|k−1 =
n
y(j)
k|k−1 = ˆfθ(j)
k|k−1(xk), 0 ≤ j ≤ 2q
o
(36)
It can be seen as an approximated sampled prior distribution
over observations. This sigma-point set and its image are then
used to compute the prediction of the observation as well as
some statistics necessary to the computation of the Kalman
gain:
¯yk|k−1 =
2q
X
j=0
wjy(j)
k|k−1
(37)
Pθyk =
2q
X
j=0
wj(θ(j)
k|k−1 − ¯θk|k−1)(y(j)
k|k−1 − ¯yk|k−1)
(38)
Pyi =
2q
X
j=0
wj

y(j)
k|k−1 − ¯yk|k−1
2
+ Pnk
(39)
Finally, the Kalman gain can be computed, and the estimated
mean and covariance can be updated:
Kk = PθykP −1
yk
(40)
¯θk|k = ¯θk|k−1 + Kk

107
International Journal On Advances in Software, vol 2 no 1, year 2009, http://www.iariajournals.org/software/
Algorithm 3: Direct regression algorithm
Compute dictionary;
∀i ∈ {1 . . . N}, xi ∼ UX ;
Set X = {x1, . . . , xN} ;
D =Compute-Dictionary (X,ν,σ0) ;
Initialisation;
Initialise ¯θ0, P0|0, Rn0, Rv0;
for k = 1, 2, . . . do
Observe (xk, yk);
SPKF update;
Prediction Step;
¯θk|k−1 = ¯θk−1|k−1;
Pk|k−1 = Pk−1|k−1 + Pvk−1;
Sigma-points computation ;
Θk|k−1 =
n
θ(j)
k|k−1,
0 ≤ j ≤ 2q
o
;
W = {wj,
0 ≤ j ≤ 2q
} ;
Yk|k−1 =
n
y(j)
k|k−1 = ˆfθ(j)
k|k−1(xk), 0 ≤ j ≤ 2q
o
;
Compute statistics of interest;
¯yk|k−1 = P2q
j=0 wjy(j)
k|k−1;
Pθyk = P2q
j=0 wj(θ(j)
k|k−1 − ¯θk|k−1)(y(j)
k|k−1 − ¯yk|k−1);
Pyi = P2q
j=0 wj

y(j)
k|k−1 − ¯yk|k−1
2
+ Pnk;
Correction step;
Kk = PθykP −1
yk ;
¯θk|k = ¯θk|k−1 + Kk

108
International Journal On Advances in Software, vol 2 no 1, year 2009, http://www.iariajournals.org/software/
Fig. 3.
Conﬁdence (uniform distribution).
Fig. 4.
Conﬁdence (normal distribution).
in kernel-based regression methods). A typical conﬁdence
interval is illustrated on Figure 3. The dotted line, solid
line and the crosses represent respectively the cardinale sine,
the conﬁdence interval (indeed the standard deviation, which
corresponds to a conﬁdence interval if Gaussian noise is
assumed) and the observations. It can be particularly useful
when the regression is used in a control framework, where
this conﬁdence approach can be used to take more cautious
decisions (see [18] for example). It can be also useful for
active learning, which aims at selecting costly samples such
as gaining the more possible new information.
In Figure 3, the samples used to feed the regressor are
sampled uniformly from X, and thus the associated conﬁdence
interval has an approximately constant width. However, a
regressor which handles uncertainty should do it locally. This
is indeed the case for the proposed algorithm. In Figure 4, the
distribution of samples is Gaussian zero-centered. It can be
seen that the conﬁdence interval is much larger where samples
are less frequent (close to the bounds). So the computed
conﬁdence intervals make sens.
IV. EXTENSION TO THE CASE OF NONLINEARLY MAPPED
OBSERVATIONS
The problem addressed here is slightly different from the
one in Section III. The function of interest is not directly
observed, but only a nonlinear (and possibly non-derivable)
mapping of it is available, the mapping being known analyt-
ically. Moreover, some of the involved nonlinearities can be
just observed. This is the case when a special sensor has to
be used (e.g., measuring a temperature using a spectrometer
or a thermocouple). This is also the case when solving
the Bellman equation in a Markovian decision process with
unknown deterministic transitions, which will be developped
in Section V.
More formally, let x = (x1, x2) ∈ X = X 1 × X 2 where
X 1 (resp. X 2) is a compact set of Rn (resp. Rm). Let t :
X 1 × X 2 → X 1 be a nonlinear transformation (transitions in
case of dynamic systems) which will be observed. Let g be a
nonlinear mapping such that g : f ∈ RX → gf ∈ RX×X 1. The
aim here is to approximate sequentially the nonlinear function
f : x ∈ X → f(x) = f(x1, x2) ∈ R from samples

109
International Journal On Advances in Software, vol 2 no 1, year 2009, http://www.iariajournals.org/software/
of the form
ˆfθ(x) =
p
X
i=1
αiKσ1
i (x1, x1
i )Kσ2
i (x2, x2
i ), with
(46)
θ = [(αi)p
i=1,

110
International Journal On Advances in Software, vol 2 no 1, year 2009, http://www.iariajournals.org/software/
Recall that this analytical expression is only used to generate
observations in this experiment but it is not used in the
regression algorithm. Let the nonlinear mapping g be
gf

111
International Journal On Advances in Software, vol 2 no 1, year 2009, http://www.iariajournals.org/software/
(a) Approximation of f(x).
(b) Nonlinear mapping calculated from ˆfθ(x).
Fig. 6.
Approximated functions.
Fig. 7.
RMSE (nonlinear mapping).
test error
Method
ˆf
g ˆ
f
%DV/SVs
Proposed Alg.
2.45 × 10−1
4.5 × 10−2
2.6%
KRLS
1.5 × 10−2
7%
SVR
5.5 × 10−2
60%
TABLE III
COMPARATIVE RESULTS.
nonlinear mapping is the same as considered before. RMSE
results of Algorithm 4 are compared to Kernel Recursive Least
Squares (KRLS) and Support Vector Regression (SVR). For
the proposed approach, the approximation is computed from
nonlinear mapping of observations. For KLRS and SVR, it is
computed from direct observations. Notice that SVR is a batch
method, that is it needs the whole set of samples in advance.
The target of the regressor is indeed gf, and the same order of
magnitude is obtained (however much nonlinearities are intro-
duced for this method and the representation is more sparse).
The RMSE on ˆf is slightly higher for this contribution, but this
can be mostly explained by the invariances (as bias invariance)
induced by the nonlinear mapping.
V. APPLICATION TO REINFORCEMENT LEARNING
The work presented in this section is a rather direct applica-
tion of the general algorithm proposed in Section IV. However,
before presenting the so-called Bayesian Reward Filter, the
reinforcement learning paradigm is brieﬂy introduced. Notice
that the notion of state used in this section is different from
the one used in Section II-C, despite same name and notation.
Here the state is the state of a dynamic system to be controlled
and not a parameter vector. Moreover it is directly observable
and there is no need to infer it.
A. Reinforcement Learning Paradigm
Reinforcement learning (RL) [11] is a general paradigm
in which an agent learns to control a dynamic system only
through interactions. A feedback signal is provided to this
agent as a reward information, which is a local hint about the
quality of the control. Markov Decision Processes (MDP) are
a common framework to solve this problem. A MDP is fully
described by the state space that can be explored, the action
set that can be chosen by the agent, a family of transition
probabilities between states conditioned by the actions and
a set of expected rewards associated to transitions. This is
further explained in Section V-B. In this framework, at each
time step k, the system adopts a state sk. According to this,
the agent can chose an action ak, which leads to a transition
to state sk+1 and to the obtention of a reward rk, the agent
objective being to maximize the future expected cumulative
rewards. Here the knowledge of the environment is modelled
as a Q-function which maps state action pairs to the expected
cumulative rewards when following a given associated policy
after the ﬁrst transition. The proposed approach is model-free,
no model of transitions and reward distributions is (directly or
explicitly) learnt or known.

112
International Journal On Advances in Software, vol 2 no 1, year 2009, http://www.iariajournals.org/software/
Solutions exist for the RL problem with discrete state and
action spaces. However they generally do not scale up very
well and cannot handle continuous state and/or action spaces.
A wide variety of function approximation schemes have thus
been applied to reinforcement learning (see [11] as a starting
point). This is known as the generalization problem, and it is
proposed here to handle it with a Bayesian ﬁltering approach.
The idea to use Bayesian ﬁltering for reinforcement learning is
not novel, but it has been surprisingly little studied. In [19] a
modiﬁcation of the linear quadratic Gaussian Kalman ﬁlter
model is proposed, which allows the on-line estimation of
optimal control (which is off-line for the classical one). In
[20] Gaussian processes are used for reinforcement learning.
This method can be understood as an extension of the Kalman
ﬁlter to an inﬁnite dimensional hidden state (the Gaussian
process), but it can only handle (optimistic) policy-iteration-
like update rules (because of the necessary linearity of the
observation equation), contrarily to the proposed contribution,
which can be seen as a nonlinear extension of the parametric
case developed in [20] to a nonlinear and value-iteration-like
scheme. In [21] a Kalman ﬁlter bank is used to ﬁnd the
parameters of a piecewise linear approximation of the value
function.
B. Problem Statement
A Markov Decision Process (MDP) consists of a state
space S, an action space A, a Markovian transition proba-
bility p : S × A → P(S) and a bounded reward function
r : S × A × S → R. A policy is a mapping from state to
action space: π : S → A. At each time step k, the system is
in a state sk, the agent chooses an action ak = π(sk), and the
system is then driven in a state sk+1 following the conditional
probability distribution p(.|sk, ak). The agent receives the
associated reward rk = r(sk, ak, sk+1). Its goal is to ﬁnd
the policy which maximizes the expected cumulative rewards,
that is the quantity Eπ[P
k∈N γkr(Sk, Ak, Sk+1)|S0 = s0] for
every possible starting state s0, the expectation being over the
state transitions taken upon executing π, where γ ∈ [0, 1[ is a
discount factor.
A classical approach to solve this optimization problem is
to introduce the Q-function deﬁned as:
Qπ(s, a) =
Z
S
p(z|s, a)

r(s, a, z) + γQπ(z, π(z))

dz (54)
It is the expected cumulative rewards by taking action a in state
s and then following the policy π. The optimality criterion
is to ﬁnd the policy π∗ (and associated Q∗) such that for
every state s and for every policy π, maxa∈A Q∗(s, a) ≥
maxa∈A Qπ(s, a). The optimal Q-function Q∗ satisﬁes the
Bellman’s equation:
Q∗(s, a) =
Z
S
p(z|s, a)

r(s, a, z) + γ max
b∈A Q∗(z, b)

dz
(55)
In the case of discrete and ﬁnite action and state spaces, the
Q-learning algorithm provides a solution to this problem. Its
principle is to update a tabular approximation of the optimal
Q-function after each transition (s, a, r, s′):
ˆQ(s, a) ← ˆQ(s, a)+α

r + γ max
b∈A
ˆQ(s′, b) − ˆQ(s, a)

(56)
where α is a learning rate. An interesting fact is that the
Q-learning is an off-policy algorithm, that is it allows to
learn the optimal policy (from the learned optimal Q-function)
while following a suboptimal one, given that it is sufﬁciently
explorative. The proposed contribution can be seen as an
extension of this algorithm to a Bayesian ﬁltering framework
(however with other advantages). See [11] for a comprehensive
introduction to reinforcement learning, or [22] for a more
formal treatment.
The reward is what is observed, the Q function is what
is searched, and both are linked by the Bellman equation.
Suppose that the Q-function is parameterized (either linearly
or nonlinearly) by a vector θ. The aim is to ﬁnd a good
approximation ˆQθ of the optimal Q-function Q∗ by observing
transitions (s, a, r, s′). This reward regression problem is cast
into a state-space representation. For an observed transition
(sk, ak, rk, s′
k), it is written as:
(
θk+1 = θk + vk
rk = ˆQθk(sk, ak) − γ maxa∈A ˆQθk(s′
k, a) + nk.
(57)
Here vk is the artiﬁcial process noise and nk the centered
observation noise including all the stochasticity of the MDP.
The framework is thus posed, but is far from being solved.
The observation equation is nonlinear and even non-derivable
(because of the max operator), that is why classical methods
such as the standard Kalman ﬁlter cannot be used. Formally,
the process noise is null, nevertheless introducing an artiﬁcial
noise can improve the stability and convergence performances
of the ﬁlter, as discussed before. This can be seen as a
special case of the algorithm of the previous section, which is
developed next.
C. Algorithm
As before Gaussian kernels are chosen, and their mean and
deviation are considered as parameters:
ˆQθ(s, a) =
p
X
i=1
αiKσs
i (s, si)Kσa
i (a, ai)
(58)
with Kσx
i (x, xi) = exp

113
International Journal On Advances in Software, vol 2 no 1, year 2009, http://www.iariajournals.org/software/
functions t and g being given by:
t : S × A → S
(59)
s, a 7→ s′
and
g : RS×A → RS×A×S
(60)
Q 7→

s, a, s′ 7→ Q(s, a) − γ max
b∈A Q(s′, b)

Thus Algorithm 4 can be specialized for reinforcement learn-
ing, which gives the Bayesian Reward Filter summarized in
Algorithm 5.
Algorithm 5: Bayesian Reward Filter
Compute dictionary;
∀i ∈ {1 . . . N}, [si, ai]T ∼ US×A;
Set X = {[s1, a1]T , . . . , [sN, aN]T } ;
D =Compute-Dictionary (S × A,ν,σ0) ;
Initialisation;
Initialise ¯θ0, P0|0, Rn0, Rv0;
for k = 1, 2, . . . do
Observe (sk, ak, s′
k, rk);
SPKF update;
Prediction Step;
¯θk|k−1 = ¯θk−1|k−1;
Pk|k−1 = Pk−1|k−1 + Pvk−1;
Sigma-points computation ;
Θk|k−1 =
n
θ(j)
k|k−1,
0 ≤ j ≤ 2q
o
;
W = {wj,
0 ≤ j ≤ 2q
} ;
Rk|k−1 =
n
r(j)
k|k−1 = ˆQθ(j)
k|k−1(sk, ak) −
γ maxb∈A ˆQθ(j)
k|k−1(s′
k, b), 0 ≤ j ≤ 2q
o
;
Compute statistics of interest;
¯rk|k−1 = P2q
j=0 wjr(j)
k|k−1;
Pθrk = P2q
j=0 wj(θ(j)
k|k−1 − ¯θk|k−1)(r(j)
k|k−1 − ¯rk|k−1);
Pri = P2q
j=0 wj

r(j)
k|k−1 − ¯rk|k−1
2
+ Pnk;
Correction step;
Kk = PθykP −1
yk ;
¯θk|k = ¯θk|k−1 + Kk

114
International Journal On Advances in Software, vol 2 no 1, year 2009, http://www.iariajournals.org/software/
Fig. 8.
Wet-chicken.
To sum up, the Gaussian prior on parameterization is chosen
such that:











σx(j)
0
∝ x(j)max − x(j)min
µα0 = 0 and σα0 ∝ rmax
1−γ
σµx(j)
0
∝ x(j)max−x(j)min
(ns+na)√p
σσx(j)
0
∝ σx(j)
0
(61)
2) Wet-Chicken: In the wet-chicken problem (inspired by
[24]), a canoeist has to paddle on a river until reaching a
waterfall. It restarts if it falls down. Rewards increase linearly
with the proximity of the waterfall, and drop off for falling.
Turbulences make the transition probabilistic. More formally,
the state space is S = [0, 10] (10 being the waterfall position),
the action space A = [−1, 1] (continuously from full backward
padding to full forward padding), the transition is s′ = s+a+c
with c ∼ N(0, σc), σc = 0.3 and the associated reward is
equal to r = s′
10. If s′ ≥ 10 the canoeist falls, the associated
reward is r = −1 and the episode ends.
To test the proposed framework, random transitions are
uniformly sampled and used to feed the ﬁlter: at each time
step k, a state sk and an action ak are uniformly sampled
from S × A, and used to generate a (random) transition to s′
k,
with associated reward rk, and the transition (sk, ak, s′
k, rk) is
the input of the algorithm. The results are shown on Figure 8.
For each run of the algorithm and every 250 samples, the
expected cumulative rewards for the current policy has been
computed as an average of cumulative rewards over 1000
episodes which were randomly (uniform distribution) initiated
(thus the average is done over starting states and stochasticity
of transitions). Notice that the lifetime of the agent (the
duration of an episode) was bounded to 100 interactions with
its environment. Then a two dimensional histogram of those
averaged cumulative rewards is computed over 100 different
runs of the algorithm. In other words, the distribution of
cumulative rewards over different runs of the algorithm as
a function of the number of observed transitions is shown.
The bar on the right shows the percentages associated to the
histogram.
The optimal policy has an averaged cumulative rewards of
6. It can be seen on Figure 8 that the proposed algorithm
can learn near optimal policies. After 1000 samples some of
policies can achieve a score of 5 (84% of the optimal policy),
which is achieved by a majority of the policies after 3000
samples. After 7000, very close to optimal policies were found
in almost all runs of the algorithm (the mode of the associated
distribution is at 5.85, that is 98% of the optimal policy).
To represent the approximated Q-function, 7.7 ± 0.7 kernel
functions were used, which is relatively few for such a problem
(from a regression perspective).
Two remarks of interest have to be made on this benchmark.
First, the observation noise is input-dependant, as it models
the stochasticity of the MDP. Recall that here a constant
observation noise has been chosen. Secondly, the noise can
be far to Gaussianity. For example, in the proximity of the
waterfall it is bimodal because of the shift of reward. Recall
that the proposed ﬁlter assumes Gaussianity of noises. Thus it
can be concluded that the proposed approach is quite robust,
and that it achieves good performance considering that the
observations were totally random (off-policy aspect).
3) Mountain Car: The second problem is the mountain-
car task. A underpowered car has to go up a steep mountain
road. The state is 2-dimensional (position and velocity) and
continuous, and there are 3 actions (forward, backward and
zero throttle). The problem full description is given in [11].
A null reward is given at each time step, and r = 1 is given
when the agent reaches the goal.
A ﬁrst problem is to ﬁnd a parameterization for this task.
The proposed one is adapted for continuous problems, not
hybrid ones. But this approach can be easily extended to
continuous states and discrete actions tasks. A simple solution
consists in having a parameterization for each discrete action,
that is a parametrization of the form θ = [θa1, θa2, θa3] and
an associated Q-function Qθ(s, a) = Qθa(s). But it can be
noticed that for a ﬁxed state and different actions the Q-values
will be very close. In other words Q∗(s, a1), Q∗(s, a2) and
Q∗(s, a3) will have similar shapes, as functions over the state
space. Thus consider that the weights will be speciﬁc to each
action, but the kernel centers and deviations will be shared
over actions. More formally the parameter vector is
θ = [(αa1
i )p
i=1, (αa2
i )p
i=1, (αa3
i )p
i=1, (sT
i )p
i=1, ((σs
i )T )p
i=1]T
(62)
the notation being the same as in the previous sections.
As for the wet-chicken problem, the ﬁlter has been fed with
random transitions. Results are shown on Figure 9, which is
a two-dimensional histogram similar to the previous one. The
slight difference is that the performance measure is now the
“cost-to-go” (the number of steps needed to reach the goal).
It can be linked directly to the averaged cumulative rewards,
however it is more meaningful here. For each run of the
algorithm and every 250 samples, the expected cost-to-go for
the current policy has been computed as an average of 1000
episodes which were randomly initiated (average is only done
over starting states here, as transitions are deterministic). The

115
International Journal On Advances in Software, vol 2 no 1, year 2009, http://www.iariajournals.org/software/
Fig. 9.
Mountain car.
lifetime of the agent was bounded to 1000 interactions with
its environment. The histogram is computed over 100 runs.
The optimal policy has an averaged cost-to-go of 55. It can
be seen on Figure 9 that the proposed algorithm can ﬁnd near
optimal policies. After 1500 samples most of policies achieve
a cost-to-go smaller than 120. After 6000 samples, policies
very close to the optimal one were found in almost all runs
of the algorithm (the mode of the associated distribution is
at 60). To represent the approximated Q-function, 7.5 ± 0.8
kernel functions were used, which is relatively few for such a
problem (from a regression perspective).
This problem is not stochastic, however informative rewards
are very sparse (which can cause the Kalman gain to converge
too quickly), transitions are nonlinear and rewards (that is
observations) are binary. Despite this, the proposed ﬁlter
exhibits good convergence. Once again it can be concluded
that the proposed approach achieves good results considering
the task at hand.
4) Comparison to Other Methods: For now the proposed
algorithm treats the different control tasks as regression prob-
lems (learning from random transitions), thus it is ill com-
parable to state-of-the-art reinforcement learning algorithms
which learns from trajectories. Nevertheless it is argued that
the quality of learned policies is comparable to state-of-the
art methods. Measuring this quality depends on the problem
settings and on the measure of performance, however the
Bayesian reward ﬁlter ﬁnds very close to optimal policies.
See [24] for example.
In most approaches, the system is controlled while learning,
or for batch methods observed samples come from a subopti-
mal policy. In the proposed experiments, totally random tran-
sitions are observed. However for the mountain-car problem it
is often reported that at least a few hundreds of episodes are
required to learn a near-optimal policy (see for example [11]),
and each episode may contain from a few tens to hundreds
steps (this depends on the quality of the current control). In
the proposed approach a few thousands of steps have to be
observed in order to obtain a near optimal policy. This is
roughly the same order of magnitude for convergence speed.
VI. CONCLUSION
A Bayesian approach to online nonlinear kernel regression
with a preprocessing sparse structure automatization procedure
has been proposed. This method has proven to be effective
(from a RMSE point of view) on a simple cardinale sine
regression problem. This example demonstrated that the pro-
posed approach compares favorably with the state-of-the-art
methods and it illustrated how the uncertainty of generalization
is quantiﬁed.
An approach allowing to regress a function of interest f
from observations which are obtained through a nonlinear
mapping of it has also been proposed as an extension. The
regression is still online, kernel-based, nonlinear and Bayesian.
This method has proven to be effective on an artiﬁcial problem
and reaches good performance although much more nonlinear-
ities are introduced.
Finally the proposed approach has been specialized into a
general Bayesian ﬁltering framework for reinforcement learn-
ing. By observing rewards (and associated transitions) the ﬁlter
is able to infer a near-optimal policy (through the parame-
terized Q-function). It has been tested on two reinforcement
learning benchmarks, each one exhibiting speciﬁc difﬁculties
for the algorithm. This off-policy Bayesian reward ﬁlter has
been shown to be efﬁcient on these two continuous tasks.
However, this paper did not demonstrated all the poten-
tialities of the proposed framework. The Bayesian ﬁltering
approach allows to derive uncertainty information over es-
timated Q-function which can be used to handle the so-
called exploration-exploitation dilemma, in the spirit of [25]
or [26]. This could allow to speed-up and to enhance learn-
ing. This uncertainty information can also be useful from a
regression perspective if used for active learning. Moreover,
the partial observability problem (the issue of non-directly
observable state in RL) can be quite naturally embedded in
such a Bayesian ﬁltering framework, as the Q-function can be
considered as a function over probability densities. Finally, the
observation noise arising in the Bayesian Reward Filter is not
purely white if the Markovian decision process has stochastic
transition probabilities. The whiteness of this noise being a
necessary assumption for the derivation of the sigma-point
Kalman ﬁlter, which is a baseline of the proposed framework,
this aspect should be investigated further.
ACKNOWLEDGMENT
Olivier Pietquin thanks the R´egion Lorraine and the Eu-
ropean Community (CLASSiC project, FP7/2007-2013, grant
agreement 216594) for ﬁnancial support.
REFERENCES
[1] M. Geist, O. Pietquin, and G. Fricout, “A Sparse Nonlinear Bayesian
Online Kernel Regression,” in Proceedings of the Second IEEE Inter-
national Conference on Advanced Engineering Computing and Appli-
cations in Sciences (AdvComp 2008), vol. I, Valencia (Spain), October
2008, pp. 199–204.
[2] C. M. Bishop, Neural Networks for Pattern Recognition.
New York,
NY, USA: Oxford University Press, 1995.

116
International Journal On Advances in Software, vol 2 no 1, year 2009, http://www.iariajournals.org/software/
[3] B. Scholkopf and A. J. Smola, Learning with Kernels: Support Vector
Machines, Regularization, Optimization, and Beyond. Cambridge, MA,
USA: MIT Press, 2001.
[4] V. N. Vapnik, Statisical Learning Theory.
John Wiley & Sons, Inc.,
1998.
[5] L. Feldkamp and G. Puskorius, “A signal processing framework based
on dynamic neural networks with application to problems in adaptation,
ﬁltering, and classiﬁcation,” in Proceedings of the IEEE, vol. 86, no. 11,
1998, pp. 2259–2277.
[6] R. van der Merwe, “Sigma-Point Kalman Filters for Probabilistic Infer-
ence in Dynamic State-Space Models,” Ph.D. dissertation, OGI School
of Science & Engineering, Oregon Health & Science University, April
2004.
[7] C. M. Bishop and M. E. Tipping, “Bayesian Regression and Clas-
siﬁcation,” in Advances in Learning Theory: Methods, Models and
Applications, vol. 190.
OS Press, NATO Science Series III: Computer
and Systems Sciences, 2003, pp. 267–285.
[8] J. Vermaak, S. J. Godsill, and A. Doucet, “Sequential Bayesian Kernel
Regression,” in Advances in Neural Information Processing Systems 16.
MIT Press, 2003.
[9] Z. Chen, “Bayesian Filtering : From Kalman Filters to Particle Filters,
and Beyond,” Adaptive Systems Lab, McMaster University, Tech. Rep.,
2003.
[10] R. Bellman, Dynamic Programming, 6th ed.
Dover Publications, 1957.
[11] R.
S.
Sutton
and
A.
G.
Barto,
Reinforcement
Learning:
An Introduction (Adaptive
Computation
and
Machine Learning),
3rd
ed.
The
MIT
Press,
March
1998.
[Online].
Available:
http://www.cs.ualberta.ca/ sutton/book/the-book.html
[12] Y. Engel, S. Mannor, and R. Meir, “The kernel recursive least squares
algorithm,” IEEE Transactions on Signal Processing, vol. 52, pp. 2275–
2285, 2004.
[13] M. Geist, O. Pietquin, and G. Fricout, “Online Bayesian Kernel Re-
gression from Nonlinear Mapping of Observations,” in Proceedings of
the 18th IEEE International Workshop on Machine Learning for Signal
Processing (MLSP 2008), no. a53, Cancun (Mexico), October 2008, pp.
309–314.
[14] ——, “Bayesian Reward Filtering,” in Proceedings of the European
Workshop on Reinforcement Learning (EWRL 2008), ser. Lecture Notes
in Artiﬁcial Intelligence, S. G. et al., Ed.
Lille (France): Springer
Verlag, June 2008, vol. 5323, pp. 96–109.
[15] R. E. Kalman, “A New Approach to Linear Filtering and Prediction
Problems,” Transactions of the ASME–Journal of Basic Engineering,
vol. 82, no. Series D, pp. 35–45, 1960.
[16] S. J. Julier and J. K. Uhlmann, “Unscented Filtering and Nonlinear
Estimation,” in Proceedings of the IEEE, vol. 92, no. 3, March 2004,
pp. 401–422.
[17] D. Simon, Optimal State Estimation: Kalman, H Inﬁnity, and Nonlinear
Approaches, 1st ed.
Wiley & Sons, August 2006.
[18] A. L. Strehl, L. Li, and M. L. Littman, “Incremental model-based
learners with formal learning-time guarantees,” in 22nd Conference on
Uncertainty in Artiﬁcial Intelligence, 2006, pp. 485–493.
[19] I. Szita and A. L˝orincz, “Kalman Filter Control Embedded into the
Reinforcement Learning Framework,” Neural Comput., vol. 16, no. 3,
pp. 491–499, 2004.
[20] Y. Engel, “Algorithms and Representations for Reinforcement Learning,”
Ph.D. dissertation, Hebrew University, April 2005.
[21] C. W. Phua and R. Fitch, “Tracking Value Function Dynamics to
Improve Reinforcement Learning with Piecewise Linear Function Ap-
proximation,” in ICML 07, 2007.
[22] D. P. Bertsekas, Dynamic Programming and Optimal Control, 3rd ed.
Athena Scientiﬁc, 1995.
[23] M. A. Carreira-Perpinan, “Mode-Finding for Mixtures of Gaussian
Distributions,” IEEE Transsactions on Pattern Analalysis and Machine
Intelligence, vol. 22, no. 11, pp. 1318–1323, 2000.
[24] D. Schneegass, S. Udluft, and T. Martinetz, “Kernel Rewards Regres-
sion: an Information Efﬁcient Batch Policy Iteration Approach,” in
AIA’06: Proceedings of the 24th IASTED international conference on
Artiﬁcial intelligence and applications.
Anaheim, CA, USA: ACTA
Press, 2006, pp. 428–433.
[25] R. Dearden, N. Friedman, and S. J. Russell, “Bayesian Q-learning,”
in Fifteenth National Conference on Artiﬁcial Intelligence, 1998, pp.
761–768. [Online]. Available: http://www.cs.bham.ac.uk/ rwd/
[26] A. L. Strehl, L. Li, E. Wiewiora, J. Langford, and M. L. Littman, “PAC
Model-Free Reinforcement Learning,” in 23rd International Conference
on Machine Learning (ICML 2006), Pittsburgh, PA, USA, 2006, pp.
881–888. [Online]. Available: http://paul.rutgers.edu/ strehl/

