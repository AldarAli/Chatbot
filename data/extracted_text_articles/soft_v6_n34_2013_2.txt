237
International Journal on Advances in Software, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/software/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Data Minability Evaluation by Compression – An Experimental Study
Dan A. Simovici, Dan Pletea, and Saaid Baraty
University of Massachusetts Boston,
Boston, USA,
{dsim,dpletea,sbaraty}@ cs.umb.edu
Abstract—The effectiveness of compression algorithms is increas-
ing as the data subjected to compression contains patterns which
occur with a certain regularity. This basic idea is used to detect
the existence of regularities in various types of data ranging from
market basket data to undirected graphs. The results are quite
independent of the particular algorithms used for compression
and offer an indication of the potential of discovering patterns
in data before the actual mining process takes place.
Keywords-data mining; lossless compression; LZW; market basket
data; patterns; Kronecker product.
I.
INTRODUCTION
Our goal is to show that compression can be used as a tool
to evaluate the potential of a data set of producing interesting
results in a data mining process. The basic idea that data that
contains patterns that occur with a certain regularity will be
compressed more efﬁciently compared to data that has no such
characteristics. Thus, a pre-processing phase of the mining
process should allow to decide whether a data set is worth
mining, or compare the interestingness of applying mining
algorithms to several data sets.
Since compression is generally inexpensive (and certainly
less expensive than mining algorithms), and compression
methods are well-studied and understood, pre-mining using
compression will help data mining analysts to focus their
efforts on mining resources that can provide a highest payout
without an exorbitant cost.
Compression has received lots of attention in the data min-
ing literature. As observed by Mannila [14], data compression
can be regarded as one of the fundamental approaches to data
mining [14], since the goal of the data mining is to “compress
data by ﬁnding some structure in it”.
The role of compression developing parameter-free data
mining algorithms in anomaly detection, classiﬁcation and
clustering was examined in [8]. The size C(x) of a compressed
ﬁle x is as an approximation of Kolmogorov complexity [4]
and allows the deﬁnition of a pseudo-distance between two
ﬁles x and y as
d(x, y) =
C(xy)
C(x) + C(y),
where xy is the ﬁle obtained by concatenating x and y. Note
that this is not the common deﬁnition of a pseudo-distance
(see, for example [21]); instead, it is simply a numerical
evaluation of the similarity of the ﬁles x and y; its minimal
value is obviously equal to 0.5.
Further advances in this direction were developed in [9],
[10] and [23]. A Kolmogorov complexity-based dissimilarity
was successfully used to texture matching problems in [3]
which have a broad spectrum of applications in areas like
bioinformatics, natural languages. and music. Compression
algorithms are used in the actual mining process to handle
data mining explorations that return huge sets of results by
extracting those results that actually are representative of the
data set (see, for example [19], [22]).
Our goal in this paper is to show that compression can be
used for assessing the interestingness of applying an actual data
mining process. In other words, to evaluate the minability of
a data set using compression. We justify experimentally this
idea by evaluating data sets that have different characteristics
and sources.
In general, data mining is task-oriented and the mining
process entails seeking speciﬁc patterns. Thus, our assessment
of minability will not necessarily help identify patterns of
interest; instead, it will signal that such patterns may exist
and will invite to further exploration.
There are two broad classes of compression algorithms:
lossy compression, that reduces signiﬁcantly data but does not
allow the full inverse transformation, from compressed data to
the original data, and lossless compression, that achieves data
reduction and can be completely reversed. We illustrate the
use of lossless compression in pre-mining data by focusing on
several distinct data mining processes: ﬁles with frequent pat-
terns, frequent item sets in market basket data, and exploring
similarity of graphs.
The LZW (Lempel-Ziv-Welch) algorithm was introduced in
1984 by T. Welch in [24] and is among the most popular com-
pression techniques. The algorithm does not need to check all
the data before starting the compression and the performance
is based on the number of the repetitions and the lengths of
the strings and the ratio of 0s/1s or true/false at the bit level.
There are several versions of the LZW algorithm. Popular
programs (such as Winzip or the zip function of MATLAB) use
variations of the LZW compression. These algorithms work
both at the bit level and at the character level.
An important role in evaluating concentrations of values
in various probability distributions is played by the notion
of entropy, Namely, if p = (p1, . . . , pn) is a probability
distribution with pi ⩽ 0 and Pn
i=1 pi = 1, the entropy of

238
International Journal on Advances in Software, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/software/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
this distribution is
H(p) =
n
X
i=1
pi log2
1
pi
.
It is well-known (see [6], [13]) that the maximum value of the
entropy is obtained when
p1 = p2 = · · · = pn = 1
n
and this value is log2 n. The minium value is 0 and this occurs
when there exists pi with pi = 1 and pj = 0 for j ̸= i,
1 ⩽ j ⩽ n. The entropy helps us to evaluate the diversity of the
values assumed by a random variable: the more concentrated
these values are the lower the entropy.
After examining compressibility of binary strings in Sec-
tion II we explore several experimental settings that provide
strong empirical evidence of the correlation between compres-
sion ratio and the existence of hidden patterns in data. In
Section III we discuss the compressibility of sequences of sym-
bols produced by various generative mechanisms. Section IV is
dedicated to the compressibility of adjacency matrix for graphs
relative to the entropy of distribution of subgraphs. Finally, in
Section V, we examine the compressibility of ﬁles that contain
market basket data sets. This paper is an extension of our
contribution [1].
II.
PATTERNS IN STRINGS AND COMPRESSION
An alphabet is a ﬁnite and non-empty set whose elements
are referred to as symbols. Let A∗ be the set of sequences on
the alphabet A. We refer to these sequences as words or strings.
The length of a string w is denoted by |w|. The null string on
A is denoted by λ and we deﬁne A+ as A+ = A∗ −{λ}. The
subsets of A∗ are referred to as languages over A.
If w ∈ A∗ can be written as w = utv, where u, v ∈ A∗ and
t ∈ A+, we say that the pair (t, m) is an occurrence of t in
w, where m is the length of u.
The occurrences (x, m) and (y, p) are overlapping if p <
m + |x| and m < p + |y|. If this is the case, m < p and
p+|y| > m+|x| then there is a proper sufﬁx of x that equals
a proper preﬁx of y. If x is a word such that the sets of its
proper preﬁxes and its proper sufﬁxes are disjoint, there are
no overlapping occurrences of x in any word.
The number of occurrences of a string x in a string w is de-
noted by nx(w). Clearly, we have P{na(w) | a ∈ A} = |w|
for any symbol a ∈ A. The prevalence of x in w is the number
fx(w) =
nx(w)·|x|
|w|
which gives the ratio of the characters
contained in the occurrences of t relative to the total number
of characters in the string.
The result of applying a compression algorithm C to a string
w ∈ A∗ is denoted by C(w) and the compression ratio is the
number
CRC(w) = |C(w)|
|w|
.
We shall use the binary alphabet B
=
{0, 1} and the
LZW algorithm, the compression algorithm of the package
java.util.zip, or the zip function of MATLAB.
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
1.2
1.4
Proportion of 1s
Compression Ratio
 
 
CR for 500K bits
CR for 100K bits
CR for 10K bits
Figure 1.
Baseline CRjZIP Behavior
We generated random strings of bits (0s and 1s) and
computed the compression ratio for strings with a variety of
symbol distributions. A string w that contains only 0s (or only
1s) achieves a very good compression ratio of CRjZIP (w) =
0.012 for 100K bits and CRjZIP = 0.003 for 500K bits, where
jZIP denotes the compression algorithm from the package
java.util.zip. Figure 1 shows, as expected, that the worst
compression ratio is achieved when 0s and 1s occur with equal
frequencies.
For strings of small length (less than 104 bits) the compres-
sion ratio may exceed 1 because of the overhead introduced
by the algorithm. However, when the size of the random
string exceeds 106 bits this phenomenon disappears and the
compression ratio depends only on the prevalence of the bits
and is relatively independent on the size of the ﬁle. Thus, in
Figure 1, the curves that correspond to ﬁles of size 100K bits
and 500K bits overlap. We refer to the compression ratio of
a random string w that contains n0(w) zeros and n1(w) ones
as the baseline compression ratio for this distribution of bits.
We created a series of binary strings ϕt,m which have a
minimum guaranteed number m of occurrences of patterns
t ∈ {0, 1}k, where 0 ⩽ m ⩽ 100. The compression baselines
for ﬁles containing the patterns 01, 001,0010, and 00010 are
shown in Table I.
TABLE I.
BASELINE COMPRESSION RATIO FOR FILES CONTAINING A
MINIMUM GUARANTEED NUMBER OF PATTERNS
Pattern
Proportion of 1s
Baseline
01
50%
1.007
001
33%
0.934
0010
25%
0.844
00010
20%
0.779
Speciﬁcally, we created 101 ﬁles ϕ001,m for the pattern 001,
each containing 100K bits and we generated similar series
for t ∈ {01, 0010, 00010}. In the case of the 001 pattern
the baseline is established at 0.934, and after the prevalence
exceeds 20% the compression ratio drops dramatically. Results
of the experiment for 001 are shown in Table II. In Figure 2
we show that similar results hold for all patterns mentioned

239
International Journal on Advances in Software, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/software/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
TABLE II.
PATTERN ’001’ PREVALENCE VERSUS THE COMPRESSION
RATIO CRjZIP
Prevalence of
CRjZIP
’001’ pattern
0%
0.93
10%
0.97
20%
0.96
30%
0.92
40%
0.86
50%
0.80
60%
0.72
70%
0.62
80%
0.48
90%
0.31
95%
0.19
100%
0.01
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
1.2
1.4
Pattern prevalence
Compression ratio
 
 
CR for 01
CR for 001
CR for 0010
CR for 00010
Figure 2.
Dependency of Compression Ratio on Pattern Prevalence
above.
III.
COMPRESSIBILITY OF LANGUAGES AND SEQUENCES
Sequences or sets of sequences of symbols are often sub-
jected to data mining processes and identifying those se-
quences that contain interesting patterns before the actual
mining process may be computationally signiﬁcant.
We begin by examining the well-known sequence called the
Thue-Morse sequence [2] that has many applications rang-
ing from crystal physics [16], counter synchronization [25],
metrology [7], [11], and chess playing [15], as well as in game
theory, fractals and turtle graphics, chaotic dynamical systems,
etc.
This sequence contains patterns but not repetitions.
Deﬁnition 3.1: Let n ∈ N be a natural number. The Thue-
Morse sequence sn = s0s1 · · · sn is a word over the alphabet
{0, 1} deﬁned as:
si =



1
if i has an odd number of 1s
in its binary representation
0
otherwise,
for 0 ⩽ i ⩽ n.
TABLE III.
THE COMPRESSION RATIO CRjZIP (S2k) FOR
THUE-MORSE SEQUENCES
k
|seq2k |
CRjZIP (seq2k )
5
32
34
8
256
4.625
10
1024
1.226
12
4096
0.328
14
16384
0.0932
15
32768
0.0542
16
65536
0.0322
17
131072
0.0208
18
262144
0.0151
19
524288
0.012
20
1048576
0.010
21
2097152
0.010
22
4194304
0.009
For example, we have
s16 = (0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0).
It is clear that if m, n ∈ N and m ⩽ n, sm is a preﬁx of sn.
Thus, the successive Thue-Morse sequences deﬁne an inﬁnite
sequence.
An equivalent method for deﬁning the Thue-Morse sequence
is by starting with 0 and concatenating the complement of the
sequence obtained so far. This procedure yields 0, then 01,
0110, 01101001, and so on. It is known (see [18], for example)
that the Thue-Morse sequence is a cube-free sequence, that is,
the sequence does not contain substrings of the form www.
We generated the Thue-Morse sequences and stored this
sequence of 0s and 1s at the bit level. By using the zip
compression utility from the java.util.zip package the
compression ratios shown in Table III were obtained.
For small values of k, the sequence is incompressible due
to the overhead produced by the compression process. As
Table III and Figure 3 show, for k big enough (2k ⩾ 2000)
the sequence becomes compressible and the compression ratio
reaches a low value (of less than 1%) for Thue-Morse se-
quences longer than 4, 000, 000 characters. Since the Thue-
Morse sequence s2k has equal number of 0s and 1s for
any value of k and its compression ratio is well below the
baseline compression ratio established for sequences of bits
in Section II, we can conclude that even in the absence of
repetitions, compression can be used for the detections of
patterns.
In a series of experiments involving generative grammars we
examined the compressibility of language fragments generated
by these grammars. A generative grammar, or in short, a
grammar is deﬁned as a 4-tuple G = (AN, AT , S, P), where
AN and AT are non-empty, ﬁnite and disjoint sets referred
to as the non-terminal and the terminal alphabet, respectively,
S ∈ AN is the initial symbol of the grammar G, and P is a
ﬁnite set of pairs of the form (α, β), where α ∈ (AN ∪ AT )+
and β ∈ (AN ∪ AT )∗. A pair (α, β) ∈ P is a production of
the grammar G. Productions are used for rewriting words over
AN ∪ AT . Namely, if γ, δ ∈ (AN ∪ AT )∗, γ = γ1αγ2, and
δ = γ1βγ2 for some production (α, β) ∈ P, we write γ ⇒
G δ.
The reﬂexive and transitive closure of the binary relation ⇒
G
is denoted by “
∗⇒
G ”. The language generated by G is the set

240
International Journal on Advances in Software, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/software/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
10
12
14
16
18
20
22
0
0.2
0.4
0.6
0.8
1
1.2
1.4
k
compression ratio
Figure 3.
Compression Ratio Behavior of Thue-Morse Sequence
L(G) = {x ∈ A∗
T | S
∗⇒
G }.
Grammars are used as generative devices that produce
languages over their terminal alphabet. Chomsky’s hierarchy
(see [17] or [20]) deﬁnes four classes of grammars based on
the complexity of their productions. In turn, these classes of
grammars, deﬁne a strict hierarchy of classes of languages
L3 ⊂ L2 ⊂ L1 ⊂ L0, where L3 is the class of regular
languages, L2 is the class of context-free languages, L1 is
the class of context-sensitive languages, and L0 is the class
of recursively enumerable languages. It is worth noting that
the classes L3 and L2 collapse on languages over one-symbol
alphabet. In other words, if L is a language over an one-symbol
alphabet, then L ∈ L2 implies L ∈ L3.
We evaluate the compressibility of a language L over an
alphabet A by considering the increasing sequence of ﬁnite
languages S(L) = (L0, L1, . . . , Ln, . . .), where Ln consists of
the ﬁrst n words of L in lexicographic order, computing the
compression ratios CRjZIP (Ln), and examining the depen-
dency of this ratio on n.
We examinine comparatively the compressibility of the
languages L1 = {ww | w ∈ {0, 1}∗} (a context-sensitive
language) versus the compressibility of a similar language
L2 = {wwR
|
w ∈ {0, 1}∗} (a context-free language)
which has a simpler structure. Here, the word wR is the
reversal of the word w and is deﬁned as λR = λ and
(ai1 · · · ain)R = ain · · · ai1.
The results shown in Figures 4 and 5 show that L2, the less
complex language has a better (lower) compression ratio, and
therefore, higher compressibility.
Similar results are obtained when comparing the compress-
ibility of the context-sensitive languages Lexp and Lprime over
the one-symbol alphabet {a} deﬁned by
Lexp
=
{a2n | n ∈ N},
Lprime
=
{ap | p is a prime number}.
The reference [17] (see Chapter 1, section 2) contains speciﬁc
grammars developed for both languages. Namely, the grammar
for Lexp has 6 productions, while the second grammar that
generates Lprime has 42 productions. As expected, experi-
0
2
4
6
8
10
12
x 10
4
−0.85
−0.8
−0.75
−0.7
−0.65
−0.6
−0.55
−0.5
−0.45
−0.4
−0.35
size of data set for {ww | w ∈ {0,1}*}
log of compression ratio
Figure 4.
Compression Ratio Behavior of the language L1
0
2
4
6
8
10
12
x 10
4
−0.9
−0.85
−0.8
−0.75
−0.7
−0.65
−0.6
−0.55
−0.5
log of compression ratio
size of data set for {wwR | w ∈ {0,1}*}
Figure 5.
Compression Ratio Behavior of the language L2
ments summarized in Figure 6 show that the Lexp is more
compressible than Lprime which has a rather complex gener-
ating process.
These results suggest that the compressibility of languages is
related to the complexity of the generative process that produce
them. This will be the object of further investigations.
IV.
RANDOM INSERTION AND COMPRESSION
For a matrix M ∈ {0, 1}u×v denote by ni(M) the number
of entries of M that equal i, where i ∈ {0, 1}. Clearly, we
have n0(M) + n1(M) = uv.
For a random variable V which ranges over the set of
matrices {0, 1}u×v let νi(V) be the random variable whose
values equal the number of entries of V that equal i, where
i ∈ {0, 1}.
Let A ∈ {0, 1}p×q be a 0/1 matrix and let
B :

B1
B2
· · ·
Bk
p1
p2
· · ·
pk

,
be a matrix-valued random variable where Bj ∈ Rr×s, pj ⩾ 0
for 1 ⩽ j ⩽ k, and Pk
j=1 pj = 1.

241
International Journal on Advances in Software, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/software/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
0
1
2
3
4
5
6
x 10
5
−6.5
−6
−5.5
−5
−4.5
−4
← Lexp
size of data set
log of compression ratio
← Lprime
Figure 6.
Compression Ratios of Languages Lexp and Lprime
Deﬁnition 4.1: The random variable A ← B obtained by
the insertion of B into A is given by
A ⊗ B =



a11B
. . .
a1nB
...
...
...
am1B
. . .
amnB


 ∈ Rmr×ns
In other words, the entries of A ← B are obtained by
substituting the block aijBℓ with the probability pℓ for aij
in A.
Note that this operation is a probabilistic generalization of
Kronecker’s product for if
B :

B1
1

,
then A ← B has as its unique value the Kronecker product
A ⊗ B.
The expected number of 1s in the insertion A ← B is
E[ν1(A ← B)] = n1(A)
k
X
j=1
n1(Bj)pj
When n1(B1) = · · · = n1(Bk) = n, we have E[ν1(A ←
B)] = n1(A)n.
In the experiment that involves insertion, we used a matrix-
valued random variable such that n1(B1) = · · · = n1(Bk) =
n. Thus, the variability of the values of A ← B is caused
by the variability of the matrices B1, . . . , Bk which can be
evaluated using the entropy of the distribution of B,
H(B) = −
k
X
j=1
pj log2 pj.
We expect to obtain a strong positive correlation between the
entropy of B and the degree of compression achieved on the
ﬁle that represents the matrix A ← B, and the experiments
support this expectation.
In a ﬁrst series of experiments, we worked with a matrix
A ∈ {0, 1}106×106 and with a matrix-valued random variable
B :

B1
B2
B3
p1
p2
p3

,
where Bj ∈ {0, 1}3×3, and n1(B1) = n1(B2) = n1(B3) = 4.
Several probability distributions were considered, as shown
in Table IV. Values of A ← B had 1062∗32 = 101124 entries.
In Table IV, we had 39% 1s and the baseline compression
rate for a binary ﬁle with this ratio of 1s is 0.9775. We
also computed the correlation between the CRjZIP and the
Shannon entropy of the probability distribution and obtained
the value 0.98 for the insertion of a matrix-valued random
variable having three values.
In Table V, we did the same experiment but with 4 different
matrices of format 4 × 4. An even stronger correlation (0.99)
was observed between CRjZIP and the Shannon entropy of
the probability distribution.
TABLE IV.
INSERTION OF A THREE-VALUED RANDOM VARIABLE,
ENTROPY AND COMPRESSION RATIOS
Probability
Compression
Entropy
distribution
Ratio
p1
p2
p3
0
1
0
0.33
0
1
0
0
0.33
0
0
0
1
0.33
0
0.9
0.1
0
0.51
0.46
0.8
0
0.2
0.61
0.72
0
0.3
0.7
0.7
0.88
0.2
0.2
0.6
0.77
1.37
0.6
0.2
0.2
0.74
1.37
0.15
0.35
0.5
0.78
1.44
0.49
0.25
0.26
0.77
1.5
0.33
0.33
0.34
0.79
1.58
TABLE V.
INSERTION OF A FOUR-VALUED RANDOM VARIABLE,
ENTROPY AND COMPRESSION RATIOS
Probability
Compression
Entropy
distribution
Ratio
p1
p2
p3
p4
0
1
0
0
0.23
0
0.4
0
0.2
0.4
0.53
1.52
0.45
0.12
0.22
0.21
0.61
1.83
0.3
0.1
0.2
0.4
0.65
1.84
0.2
0.2
0.2
0.4
0.69
1.92
0.25
0.25
0.25
0.25
0.69
2
The relationship between the compression ratio CRjZIP
and the Shannon entropy of the probability distribution of
the inserted random variable is shown in Figure 7 for both
experiments.
This experiment reconﬁrms that data that contains patterns
can be better compressed than randomly generated ﬁles and
that the compressibility is less pronounced when the diversity
of these patterns increases.

242
International Journal on Advances in Software, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/software/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
Shannon entropy
Compression ratio
 
 
Insertion of 3 matrices
Insertion of 4 matrices
Figure 7.
Evolution of CRjZIP and Shannon Entropy for Insertions
Next, we examine the compressibility of binary square
matrices and its relationship with the distribution of principal
submatrices. An m × m principal submatrix of a matrix
A ∈ Rn×n is the matrix A[I] deﬁned by a non-empty m-
element subset I of the set {1, . . . , n} and is obtained by
selecting entries of A of the form aij, where i, j ∈ I.
We mention that the principal submatrices of the adjacency
matrix of a graph correspond to the adjacency matrices of the
subgraphs of that graph. The patterns in a graph are captured
in the form of frequent isomorphic subgraphs.
A binary square matrix is compressed by ﬁrst vectorizing
the matrix and then compressing the resulting binary sequence.
There is a strong correlation between the compression ratio of
the adjacency matrix of a graph and the frequencies of the
occurrences of isomorphic subgraphs of it. Speciﬁcally, the
lower the compression ratio is, the higher are the frequencies
of isomorphic subgraphs and hence the worthier is the graph
for being mined.
Let Gn be an undirected graph having {v1, . . . , vn} as its
set of nodes. The adjacency matrix of Gn, AGn ∈ {0, 1}n×n
is deﬁned as
(AGn)ij =
1
if there is an edge between vi and vj in Gn
0
otherwise.
We denote with CRC(AGn) the compression ratio of the
adjacency matrix of graph Gn obtained by applying the com-
pression algorithm C.
Let S
= {i1, . . . , ik} be a subset of {1, . . . , n}. The
principal submatrix AGn[S] is the adjacency matrix of the
subgraph of Gn which consists of the nodes with indices in
S along with those edges that connect these nodes. We denote
by Pn(k) the collection of all subsets of {1, 2, . . . , n} of size
k where 2 ⩽ k ⩽ n. We have |Pn(k)| =

243
International Journal on Advances in Software, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/software/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Figure 8.
Standard deviation vs. average of the CRjZIP (AGn) for a number
of different permutations of nodes for the same graph. The horizontal axis is
labelled with the number of edges of the graph.
Figure 9.
Standard deviation vs. average of the HP (Gn, k) of a number
of different permutations of nodes for the same graph. The horizontal axis is
labelled with the number of edges of the graph. Each curve corresponds to
one value of k.
Table VI contains the correlation between CRjZIP (AGn)
and HP (Gn, k) calculated for the 560 randomly generated
graphs for each value of k.
TABLE VI.
CORRELATIONS BETWEEN CRjZIP (AGn) AND
HP (Gn, k)
k
Correlation
3
0.92073175
4
0.920952812
5
0.919256573
V.
FREQUENT ITEMS SETS AND COMPRESSION RATIO
A market basket data set consists of a multiset T of
transactions. Each transaction T is a subset of a set of items
I = {i1, . . . , iN}. The multiplicity of a transaction T in the
multiset T is denoted by m(T).
A transaction is described by its characteristic N-tuple t =
(t1, . . . , tN), where
tk =
1
if ik ∈ T.
0
otherwise,
for 1
⩽
k
⩽
N. The length of a transaction T
is
|T| = PN
k=1 tk, while the average size of transactions is
P{|T | | T in T}
|T|
.
n = 60 and k = 3
n = 60 and k = 4
n = 60 and k = 5
Figure 10.
Plots of average CRjZIP (AGn) (CMP RTIO) and average
HP (Gn, k) (DIST ENT) for randomly generated graphs Gn of equal number
of edges with respect to the number of edges.
The support of a set of items K of the data set T is the
number
supp(K) = |{T ∈ T | K ⊆ T}|
|T|
.
The set of items K is s-frequent if supp(K) > s.
The study of market basket data sets is concerned with
the identiﬁcation of association rules. A pair of item sets
(X, Y ) is an association rule, denoted by X
→ Y . Its
support, supp(X → Y ) equals supp(X) and its conﬁdence
conf(X → Y ) is deﬁned as
conf(X → Y ) = supp(X ∪ Y )
supp(X)
.
Using the artiﬁcial transaction ARMiner generator described
in [5], we created a basket data set.

244
International Journal on Advances in Software, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/software/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Transactions
are
represented
by
sequences
of
bits
(t1, · · · , tN).
The
multiset
T
of
M
transactions
was
represented as a binary string of length MN obtained by
concatenating the strings that represent transactions.
We generated ﬁles with 1000 transactions, with 100 items
available in the basket, adding up to 100K bits.
For data sets having the same number of items and trans-
actions, the efﬁciency of the compression increases when
the number of patterns is lower (causing more repetitions).
In an experiment with an average size of a frequent item
set equal to 10, the average size of a transaction equal to
15, and the number of frequent item sets varying in the
set {5, 10, 20, 30, 50, 75, 100, 200, 500, 1000}, the compres-
sion ratio had a signiﬁcant variation ranging between 0.20
and 0.75, as shown in Table VII. The correlation between
the number of patterns and the compression ratio was 0.544.
Although the frequency of 1s and baseline compression ratio
were roughly constant (at 0.75), the number of patterns and
compression ratio were correlated.
TABLE VII.
NUMBER OF ASSOCIATION RULES AT 0.05 SUPPORT
LEVEL AND 0.9 CONFIDENCE
Number of
Frequency
Baseline
Compr.
Number of
Patterns
of 1s
compression
ratio
rules
5
16%
0.75
0.20
9,128,841
10
17%
0.73
0.34
4,539,650
20
17%
0.73
0.52
2,233,049
30
17%
0.76
0.58
106,378
50
19%
0.75
0.65
2,910,071
75
18%
0.75
0.67
289,987
100
18%
0.75
0.67
378,455
200
18%
0.75
0.70
163
500
18%
0.75
0.735
51
1000
18%
0.75
0.75
3
Further, there was a strong negative correlation (-0.92)
between the compression ratio and the number of association
rules indicating that market basket data sets that satisfy many
association rules are very compressible.
VI.
CONCLUDING REMARKS
Compression ratio of a ﬁle can be computed fast and easy,
and in many cases offers a cheap way of predicting the exis-
tence of embedded patterns in data. Thus, it becomes possible
to obtain an approximative estimation of the usefulness of an
in-depth exploration of a data set using more sophisticated and
expensive algorithms.
The presence of patterns in strings leads to a high degree of
compression (that is, to low compression ratios). Thus, a low
compression ratio for a ﬁle indicates that the mining process
may produce interesting results. Compressibility however, does
not guarantee that a sequence contains repetitions. Strings that
are free of repetitions but contain patterns can display a high
degree of compressibility as shown by the well-known Thue-
Morse binary string.
The use of compression as a measure of minability is
illustrated on a variety of paradigms: graph data, market basket
data, etc. Compression has been applied in bioinformatics as
a tool for reducing the size of immense data sets that are
generated in the genomic studies. Furthermore, specialized
algorithms were developed that mine data in compressed
form [12].
Our current work shows that identifying compressible areas
of human DNA by comparing the compressibility of certain
genomic regions is a useful tool for detecting areas where the
gene replication mechanisms are disturbed (a phenomenon that
occurs in certain genetically based diseases).
AKNOWLEDGEMENTS
This work was supported by a Science and Technology grant
from the President of the University of Massachusetts.
The authors appreciate the remarks of the anonymous re-
viewers who contributed to the improvement of this paper.
REFERENCES
[1]
D. Simovici, D. Pletea, and S. Baraty.
Evaluating data minability
through compression - an experimental study. In Proceedings of DATA
ANALYTICS, pages 97–102. Think Mind Digital Library, 2012.
[2]
J.-P. Allouche and J. Shallit.
The ubiquitous Prouhet-Thue-Morse
sequence. In Sequences and their Applications, pages 1–16. Springer
London, 1999.
[3]
B. J. L. Campana and E. J. Keogh.
A compression based distance
measure for texture. In SDM, pages 850–861, 2010.
[4]
R. Cilibrasi and P. M. B. Vit`anyi. Clustering by compression. IEEE
Transactions on Information Theory, 51:1523–1545, 2005.
[5]
L. Cristofor.
The ARMiner project, 2000, Univ. of Massachusetts
Boston. http://www.cs.umb.edu/
∼laur/ARMiner.
[6]
I. Csisz´ar and J. K¨orner. Information Theory - Coding Theorems for
Discrete Memoryless Systems. (second edition). Cambridge University
Press, Cambridge, UK, 2011.
[7]
L. Jin, K. L. Parthasarathy, T. Kuyel, R. L. Geiger, and D. Chen.
High-performance adc linearity test using low-precision signals in non-
stationary environments. In Proceedings 2005 IEEE International Test
Conference, pages 1182–1191, 2005.
[8]
E. Keogh, S. Lonardi, and C. A. Ratanamahatana. Towards parameter-
free data mining. In Proc. 10th ACM SIGKDD Intnl Conf. Knowledge
Discovery and Data Mining, pages 206–215. ACM Press, 2004.
[9]
E. Keogh, S. Lonardi, C. A. Ratanamahatana, L. Wei, S. Lee, and
J. Handley. Compression-based data mining of sequential data. Data
Mining and Knowledge Discovery, 14:99–129, 2007.
[10]
E. J. Keogh, L. Keogh, and J. Handley. Compression-based data mining.
In Encyclopedia of Data Warehousing and Mining, pages 278–285.
2009.
[11]
T. Kuyel D. Chen L. Jin, K. Parthasarathy and R. Geiger. Accurate
testing of analog-to-digital converters using low linearity signals with
stimulus error identiﬁcation and removal.
IEEE Transactions on
Instrumentation and Measurement, 54:1188–1199, 2005.
[12]
P. R. Loh, M. Baym, and B. Berger. Compressive genomics. Nature
Biotechnology, 30:627–630, 2012.
[13]
D. J. C. McKay. Information Theory, Inference, and Learning Algo-
rithms. Cambridge University Press, Cambridge, UK, 2004.
[14]
H. Mannila.
Theoretical frameworks for data mining.
SIGKDD
Exploration, 1:30–32, 2000.
[15]
M. Morse and G. A. Hedlund. Unending chess, symbolic dynamics, and
a problem in semigroups. Duke Mathematical Journal, 11:1–7, 1944.
[16]
L. Youran R. Ricklund, S. Mattias. The Thue-Morse aperiodic crystal,
a link between the Fibonacci quasicrystal and the periodic crystal.
International Journal of Modern Physics B, 1:121–132, 1987.
[17]
A. Salomaa. Formal Languages. Academic Press, New York, 1973.
[18]
A. Salomaa. Jewels of Formal Language Theory. Computer Science
Press, Rockville, Maryland, 1981.

245
International Journal on Advances in Software, vol 6 no 3 & 4, year 2013, http://www.iariajournals.org/software/
2013, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
[19]
A. Siebes, J. Vreeken, and M. van Leeuwen. Items sets that compress. In
Proceedings of SIAM International Conference on Data Mining, pages
393–404. SIAM, 2006.
[20]
D. A. Simovici and R. L. Tenney.
Formal Language Theory with
Applications. World Scientiﬁc, Singapore, 1999.
[21]
D. A. Simovici and C. Djeraba. Mathematical Tools for Data Mining.
(second edition) Springer, London, 2014.
[22]
J. Vreeken, M. van Leeuwen, and A. Siebes. KRIMP: mining items
that compress. Data Mining and Knowledge Discovery, 23:169–214,
2011.
[23]
L. Wei, J. Handley, N. Martin, T. Sun, and E. J. Keogh. Clustering
workﬂow requirements using compression dissimilarity measure.
In
ICDM Workshops, pages 50–54, 2006.
[24]
T. Welch. A technique for high performance data compression. IEEE
Computer, 17:8–19, 1984.
[25]
R. Yarlagadda and J. Hershey. Counter synchronization using the Thue-
Morse sequence and psk.
IEEE Transactions on Communications,
32:947–977, 1984.

