Action Development and Integration in a Humanoid iCub Robot 
How Language Exposure and Self-Talk Facilitate Action Development 
 
Tobias Leugger 
Laboratory of Intelligent Systems 
École Polytechnique Fédérale de Lausanne 
EPFL-STI-IMT-LIS, Station 11  
CH-1015, Lausanne, Switzerland 
tobias.leugger@epfl.ch 
Stefano Nolfi 
Institute of Cognitive Science and Technologies, 
National Research Council (CNR-ISTC) 
Via S. Martino della Battaglia, 44 
00185, Roma, Italy 
stefano.nolfi@istc.cnr.it
 
 
Abstract— One major challenge in evolutionary/developmental 
robotics is constituted by the need to identify design principles 
that allow robots to acquire progressively more complex action 
skills by integrating them into their existing behavioral 
repertoire. In this paper, we present a novel method that 
address this objective, the theoretical background behind the 
proposed methodology, and the results obtained in a series of 
experiments in which a simulated iCub robot develops lower-
level and then integrated higher-level action skills. Moreover, 
we illustrate how the development of integrated action skill is 
facilitated by language exposure and self-talk.  
Keywords-developmental 
robotics; 
action 
integration; 
language and action; self-talk.  
I. 
 INTRODUCTION 
The acquisition of new behavioral skills and the ability to 
progressively expand our behavioral repertoire represents 
one key aspect of human intelligence and a fundamental 
capacity for robots companion, i.e., robots that should 
cooperate with humans in everyday environments [1]. 
Unfortunately the issue of how robots can acquire new action 
skills by integrating them into their existing behavioral 
repertoire 
still 
represent 
an 
open 
challenge 
for 
evolutionary/developmental robotics [1-3].  
In this paper we provide a model validated through a 
series of experiments that demonstrates how a simulated 
humanoid robot can be trained incrementally for the ability 
to develop lower-level and then higher-level goal directed 
action skills (i.e., action capabilities that enable the robot to 
achieve a given desired goal in varying environmental 
conditions).  
The first assumption behind our approach is that 
behavioral and cognitive processes in embodied agents 
should be conceived as dynamical processes with a multi-
level and multi-scale organization [4]. This means that 
behavior (and cognitive skills) are: (i) dynamical processes 
originating from the continuous interaction between the 
robot and the physical and social environment, and (ii) 
display a multi-level and multi-scale organization in which 
the combination and interaction between lower-level 
behaviors, lasting for limited time duration, give rise to 
higher-level behaviors, extending over longer time scale and 
in which higher-level behaviors later affect lower-level 
behaviors and the robot/environmental interaction from 
which they arise. This assumption implies that a behavioral 
unit does not necessarily correspond to a dedicated control 
unit or modules of the robot’s neural controller. Moreover, it 
implies that the development of additional and higher-level 
behavior can occur through the recombination and re-use of 
pre-existing motor skills even when these skills do not 
correspond to separated physical entities but rather to 
processes 
that 
ultimately 
emerge 
from 
the 
robot/environmental interactions. 
The second theoretical assumption behind our approach 
is that the concurrent development of cognitive and social 
skills 
(with 
particular 
reference 
to 
early 
language 
comprehension skills and language mediated interactions 
skills) might represent an important prerequisite for the 
development of action skills and vice versa [5]. Indeed, as 
originally hypothesized by Vygotsky [6-7], we believe that 
human language does not only play a communicative 
function but also constitutes a cognitive tool that 
facilitate/enable the development of other capabilities, 
including action capabilities. A comprehensive discussion of 
this hypothesis and of the implications of this idea for 
developmental robotics is provided by Mirolli and Parisi [8] 
that constitutes one of the main source of inspiration of the 
work described in this paper.  
On the basis of these theoretical assumptions we studied 
how a robot provided with a non-modular neural controller 
can be trained for the ability to produce a series of lower-
level elementary actions through a form of trial and error 
learning. Moreover we studied how such robot can later be 
trained for the ability to perform high-level integrated actions 
by re-using and re-combing previously learned skills. Such 
training process can potentially be extended to the 
acquisition of still higher-level action capabilities generated 
through the combination and re-use of previously acquired 
higher-level skills. 
The acquisition of action skills at all level of organization 
is realized by enriching the robots’ sensory state with 
symbolic linguistic inputs that allow the robot to more easily 
learn the affordance of different categorical contexts as well 
as to disambiguate between contexts affording multiple 
actions (for a related approach see [9]).  
The acquisition of action skills at higher (non-
elementary) level of organization is realized by also 
24
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-218-9
COGNITIVE 2012 : The Fourth International Conference on Advanced Cognitive Technologies and Applications

enriching the robot’s sensory state with a linguistic 
description of the elementary actions that can be re-used to 
generate new integrated behavior capabilities (for a related 
approach see [10]). Moreover, the acquisition of higher-level 
skills is realized by providing the robot with a neural 
architecture that allows it to develop and exploit a form of 
self-talk. By talking to ourselves or self-talk we refer to the 
ability to self-generate the linguistic stimulation produced by 
other agents, as both children and adult human beings do 
both externally (as in private speech) and internally (as in 
inner speech [7,11]). 
In section II, we describe the experimental scenario and 
in section III, the obtained results. Finally, in section IV, we 
draw our conclusions. 
II. 
EXPERIMENTAL SCENARIO 
A simulated humanoid iCub robot has been trained for the 
ability to display low-level (elementary) behaviors and 
higher-level behaviors by combining and integrating the 
previously acquired low-level behaviors. 
A. The robot and the environment 
The iCub is a humanoid robot developed at IIT as part of 
the EU project RobotCub [12]. It has 53 motors that move 
the head, arms and hands, waist, and legs. From the sensory 
point of view, the iCub is equipped with digital cameras, 
gyroscopes and accelerometers, microphones, force/torque 
sensors, tactile sensors. In the experiment reported in this 
paper, the sensors and actuators located on the head, the right 
arm and on the legs have not been used. The experiments 
have been carried out by using the simulator developed at 
our lab by Gianluca Massera, Tomassino Ferrauto and 
others. The simulator reproduces as accurately as possible 
the physics and the dynamics of the robot and 
robot/environment interaction, and is based on the Newton 
Game Dynamics open-source physics engine.  
The robot is located in front of a table containing a red 
object (Figure 1). The object is a sphere with a mass of 200 
grams and a diameter of 7 cm with a flattened base to 
prevent it from rolling away. A green spot indicates the 
target location in which the object should be moved. At the 
beginning of each trial the target object is placed in one of 
four possible areas (10 cm to the front, back, left, and right 
respectively to a point 30 cm in front and 10 cm to the left of 
the robot). To increase robustness the position of the object 
is randomly moved between [-2, 2] cm from the four points 
indicated above. The target spot is randomly placed within 
two areas located 10cm left or right with respect to the front 
of the robot at a height of 25cm above the table and at a 
distance of 25 cm from the robot.  
B. The robot’s neural controller 
The controller of the robot is constituted by an artificial 
neural network (Figure 2) that receives proprioceptive input 
from torso and from the left-arm/hand and exteroceptive 
input from the visual system, the tactile sensors, and from the 
linguistic input units that encode the labels provided by a 
caretaker (see below). The network produces as output the 
desired states of the joints of the torso and of the right 
arm/hand and an output that determines the focus of 
attention. 
 
Figure 1. The simulated robot and the environment. 
More specifically, the focus output unit binarily encodes 
whether the visual system of the robot is paying attention to 
the red object or to the green target, the 2 torso motor 
neurons encode the desired angular position of the rotation 
and extension/flexion Degree of Freedoms (DOFs) of the 
torso, the 7 arm motor neurons encode the desired angular 
position of the 7 DOFs of the left-arm and of the wrist, and 
the 3 fingers motor neurons indicate the extension/flexion of 
thumb, the opposition of the thumb with respect to the other 
fingers, and the extension/flexion of all other fingers (i.e. to 
simplify the model all fingers joints are actuated through 
only 3 motor neurons). 
All the arm and hand joints are allowed to move in the 
full range of motion possible on the physical iCub. The yaw 
and pitch torso joints are both limited to a range of [-10,40]  
degrees to eliminate un-desirable postures.  
The 3 position sensors indicate the relative position of the 
red ball or of the green target (depending on what the robot is 
paying attention to) with respect to the left-hand along the 
three orthogonal axis, the 12 propriosensors encode the 
current position of the torso, left-arm, and fingers joints, the 
6 tactile sensors encode the activation state of the tactile 
sensors located on the left-hand palm and on the tips of the 5 
fingers, finally the 4 linguistic inputs locally encode whether 
the caretaker produced the “reach”, “grasp”, “open”, or 
“move” linguistic label.  
The state of the robots’ sensors, the state of the neural 
controller, the desired state of the robots actuators, and the 
state of the robot and of the environment are updated every 
step, i.e., every 50 milliseconds.  
C. The training algorithm 
The architecture of the neural controller is fixed. The 
connection weights, biases, and time constants are encoded 
as free parameters and trained thorough an evolutionary 
robotics method [13]. This method has been chosen since it 
is one of the most simple yet effective way to train a robot on 
the basis of a distal reward (i.e., for the ability to display 
behaviors producing a desired outcome without specifying 
25
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-218-9
COGNITIVE 2012 : The Fourth International Conference on Advanced Cognitive Technologies and Applications

how such behaviors should be realized) and since it does not 
put constraints on the architecture of the robots’ controller 
and/or on the type of parameters that can be subjected to the 
training process.  
 
Figure 2. The architecture of the robot’s neural controller during the first 
training phase. The neurons are grouped in clusters. Arrows between two 
clusters indicate full connectivity between the two corresponding groups of 
neurons. 
The initial population consists of 100 randomly generated 
genotypes that encode the connection weights and the biases 
of 100 corresponding neural controllers. Each parameter is 
encoded by 16 bits, and normalized in the range [-1.0, 1.0] in 
the case of connection weights and biases, and in the range 
[0.0, 1.0] in the case of time constants. Each genotype is 
translated into a corresponding neural controller and 
evaluated as described below. The top 20 genotypes are 
allowed to reproduce by generating 5 offspring (1 unvaried 
and 4 varied copies). Variations are introduced by randomly 
flipping 0.005% and 0.04% of the bits, during the first and 
second training phase respectively. The selection of the top 
genotypes is realized on the overall performance of the robot, 
i.e., the sum of the rewards obtained by the robot during its 
entire lifetime. 
The training is realized incrementally. During the first 
training phase, the robot is trained for the ability to display 
four lower-levels actions: REACH (i.e., bring the hand over 
an object), OPEN (i.e., open the fingers and align the palm to 
face downward), GRASP (i.e., close the fingers around the 
object), MOVE (i.e., move the object toward a target 
destination). The evaluation of candidate solutions is realized 
during 16 trials --- 4 trials for each of the four actions with 
the object placed in four different areas. At the beginning of 
each trial the posture of the robot is initialized in a position 
that enable the robot to potentially display the desired action 
also without possessing the other related skills (e.g., far from 
the object in the case of REACH, with the fingers closed in 
the case of OPEN, near and over the object in the case of 
GRASP, with the object in the hand in the case of MOVE). 
To force the robot to develop robust solutions the initial 
posture of the robot at the beginning of each trial is varied 
within 16 different alternatives. During each trial the robot is 
rewarded for the ability to achieve the goal of the current 
elementary action.  The robot receives a small reward every 
time step on the basis of extent to which its current state 
approximate the desired state and a significant reward when 
the goal has been accomplished. In the case of REACH 
actions, the goal consists in bringing the palm of the left 
hand within 6cm from the top part of the object. The goal of 
the OPEN actions consists in stretching out all the fingers 
while aligning the palm downward and while keeping the 
palm within a distance of 6 cm from the top part of the 
object. In the case of GRASP actions, the goal consists in 
closing the fingers around the object (i.e., reducing the 
distance between the barycenter of the object and the 
centroid of the tip of the thumb, the tip of the pinky, and the 
center of the palm below a threshold). The goal of MOVE 
actions consists of moving the object within 6cm from the 
target location. The overall performance (fitness) of each 
individual robot is computed by calculating the harmonic 
mean of the scores obtained during trials involving the 
execution of different actions.  
During the second training phase, the robot is trained for 
the ability to perform integrated actions, such as MOVE-TO-
TARGET (i.e., moving a distant object from its location to a 
target location), by combining and integrating over time the 
previously acquired elementary action skills. During this 
second phase each robot is evaluated for 8 trials. At the 
beginning of each trial the object and the target spot are 
randomly initialized within the areas described above. The 
posture of the robot is initialized so that the position of the 
left-hand is far from the object and from the target location.  
The goal of this integrated action is the same of the 
MOVE elementary action: moving the object to within 6cm 
of the target location. Due to the different initial conditions, 
however, the realization of this goal requires the execution of 
an integrated sequence of actions. The robot receives a 
significant reward when this goal is accomplished and 
smaller rewards when the object has been lifted and when 
the robot correctly focuses its attention on the object and on 
the target location before and after the object is lifted, 
respectively. 
 To study the role of language exposure and self-talk four 
series of experiments have been carried out in four 
experimental conditions described in the following section. 
For more details see [14].  
III. 
RESULTS 
In this section, we describe the results obtained during the a 
training phase, in which the robot is trained for the ability to 
display the low-level (elementary) behaviors (Section A), 
and during a second training phase in which the robot is 
trained for the ability to display the higher-level integrated 
behavior (Section B-D). For the second phase we report the 
results obtained in: (i) a control experimental condition (C), 
(ii) a language exposure condition (LE) in which the robot 
receives from the caretaker the label that indicates the 
elementary action that is appropriate in the current context 
during part of the trials, (iii) a self-talk condition (ST) in 
which the robot is allowed to self-generate the labels of the 
elementary actions to be executed, and (iv) a learning to self-
talk condition (LST) in which the robot is allowed to self-
26
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-218-9
COGNITIVE 2012 : The Fourth International Conference on Advanced Cognitive Technologies and Applications

generate the labels during part of the trials and to anticipate 
the labels produced by the caretaker during the remaining 
trials. The first and second training phases have been 
replicated 10 times for each experimental condition. 
A. Acquisition of elementary action skills 
The training of the elementary behaviors reached optimal 
performance within 1500 generations in 9 out of 10 
replications of the experiment. By optimal performance we 
mean robots displaying successful behavior in 16 out of 16 
trials. The fastest and the slowest successful replications 
reached optimal performance at generation 246 and 711. The 
best individual of the worst replication (the only one that did 
not achieved optimal performance) was successful in 15 out 
of 16 trials.  
B. Acquisition of integrated action skills 
In the second phase, the robot was trained for the ability to 
display the MOVE-TO-TARGET high-level behavior that 
consists in moving a distant object from its current location 
to a target location indicated by a green spot. Since the 
initial posture of the robot’s arm/hand is far from the object, 
the production of this higher-level behavior can be realized 
by re-using, combining, and integrating the REACH, 
OPEN, GRASP, and MOVE behaviors acquired previously.  
To provide the robots with the computational resources 
necessary for combining and integrating the elementary 
action skills we provided their neural controller with four 
additional continuous time internal neurons receiving and 
projecting connections from and to the block of 15 internal 
neurons and from themselves (Figure 3). Moreover, the new 
block of internal neurons receives connections from high-
level linguistic input neurons (only “move-to-target” in the 
case of the experiment reported in this paper).  
 
Figure 3. The architecture of the robot’s neural controller during the second 
training phase (C and LE conditions). 
The initial population of candidate solutions is generated 
by using the genome of the best 10 individuals of the 10 
replications of the experiment described above in which the 
robot were trained for the ability to display the lower-level 
actions. The values corresponding to the newly added 
connection 
weights, 
biases, 
and 
time-constant 
were 
randomly generated and subjected to the training process. 
The values corresponding to the pre-existing connection 
weights were kept constant during the second training phase. 
The training process is continued for 100 generations. 
The state of the high-level linguistic input is activated, 
the state of the four lower-level linguistic inputs is set to a 
null value.  
By post-evaluating the best robots of each generation of 
each replication for 40 trials we observed that the average 
performance are rather low and no individual successfully 
produce the integrated behavior in all trials (see Figure 4, 
condition C). The best individual successfully displays the 
integrated behavior in 30 out of 40 trials. 
 
Figure 4. Percentage of successful trials for the best robots trained in the 
four experimental conditions (see text). Each robot has been post-evaluated 
for 40 trials during which it did not received any linguistic input from the 
caretaker. Each boxplot shows the percentage of successful trials displayed 
by the best 10 robots obtained in the 10 corresponding replications of the 
experiment. Whiskers expand to the minimum and maximum with outliers 
marked as +. 
C. How language exposure facilitates action development 
In a second experimental condition we studied whether the 
availability of linguistic inputs produced by a caretaker, that 
specify the label of the elementary behavior that is 
appropriate in any given circumstance, facilitates the 
acquisition of the integrated behavior. Given the nature of 
the integrated behavior, the following sequence of labels is 
provided: REACH, OPEN, GRASP, and MOVE. For 
practical reasons, the caretaker has been simulated through a 
software routine that analyzes the state of the robot and of 
the environment and determines the point over time in 
which the current label has to be substituted with the next 
label. More specifically, caretakers start to produce the 
“reach” label and switch to the next label as soon as the 
distance between the top part of the object and the left-palm 
of the robot decrease below 6 cm. Then, the “open” label is 
produced until all the fingers are extended sufficiently, the 
palm is horizontally oriented over the object, and the 
distance between the palm and the top-part of the object is 
below 6cm. Then, the “grasp” label is produced until 3 of 
the tactile sensors are in contact with the object. Finally, the 
“move” label is produced until the end of the trial.  
To force the robot to develop an ability to produce the 
integrated behavior also autonomously, i.e., without the 
27
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-218-9
COGNITIVE 2012 : The Fourth International Conference on Advanced Cognitive Technologies and Applications

support of the caretaker, the artificial caretaker produced the 
linguistic input only during even trials (i.e., the state of the 
four linguistic inputs is always null during odd trials).  
The linguistic inputs provided by the caretaker thus 
enrich the robots’ sensory information during half of the 
trials. The way in which the performance of the robot is 
evaluated, as well as all other parameters, is the same of the 
experiments described in the previous sub-section.   
The analysis of the trained robots indicates that optimal 
performance are obtained in 4 out of 10 replications of the 
experiment. The performance obtained by post-evaluating 
the best trained individuals for 40 trials without linguistic 
labels and the comparison with the performance obtained in 
the control (C) condition (Figure 4, condition LE) shows 
how the exposure to linguistic inputs enables the robot to 
achieve 
better 
performance, 
although 
only 
in 
few 
replications.  
D. How self-talk  facilitates action development 
In the third and fourth experimental conditions, we 
investigated whether the possibility to self-talk, i.e. the 
possibility to self-generate over time the linguistic labels 
associated to the elementary actions, can facilitate the 
development of the integrated behavior. 
To enable the robots to develop a form of inner-speech 
we extended the neural architecture used for the acquisition 
of the elementary actions in a different manner (Figure 5). 
More specifically, we added a layer of four neurons that 
receive and project connections from and to the layer of 15 
internal neurons, and receive connections from themselves 
and from the higher-level linguistic input units (only the 
“move-to-target” unit in the case of the experiments reported 
in this paper). These four neurons are used to self-generate 
linguistic labels (i.e. vector of binary values encoding the 
labels “reach”, “open”, “grasp”, and “move”) that are then 
used to set the activation of the four lower-level linguistic 
inputs. This is realized by activating the linguistic input 
corresponding to the most activated linguistic output and by 
setting to a null value the activation states of the other 
linguistic inputs. All other parameters are kept the same as in 
previous experimental conditions. 
As in the case of the experiments performed in the two 
conditions illustrated above, the value of the additional 
connection weights and biases are initialized to a random 
value and subjected to the training process. All other 
parameters are identical to those used in the experiments 
described in the other conditions. 
More specifically, we studied two self-talk conditions. In 
the ST condition, the robots never received linguistic labels 
from the caretaker and always rely on the self-generated 
labels.  
The analysis of the results obtained in this condition 
indicates that robots displaying successful behaviors in all 
trials are obtained in 8 out of 10 replications of the 
experiment. The performance obtained in the post-evaluation 
test (Figure 4, ST condition) are significantly better than the 
performance obtained in the language exposure (LE) and 
control condition (C). Overall the obtained results thus 
indicates that the possibility to self-talk strongly facilitates 
the development of the integrated behavior.  
 
 
Figure 5. The architecture of the robot’s neural controller during the second 
training phase (ST and LST conditions). 
In the ST conditions the robots are not rewarded directly 
for the ability to self-talk but only for the ability to produce 
the integrated behavior. In the fourth and last condition 
(LST) we rewarded the robot for the ability to self-generate 
and to predict the linguistic labels produced by the caretaker 
during even trial and for the ability to produce the integrated 
behavior by self-generating the linguistic labels during odd 
trials. The aim of the experimental condition was that to 
verify whether an explicit training to self-talk, realized 
through the attempt to anticipate the caretaker linguistic 
behavior, can facilitate the acquisition of the integrated 
behavior.  
To enable the development of an ability to anticipate the 
caretaker behavior we reward the robot with a big score 
every time it self-generates the new label 1-5 steps earlier 
than the caretaker and a smaller reward every time it self-
generate the new label 6-20 steps earlier than the caretaker.  
The analysis of the performance of individuals during the 
training process indicate that the best individuals achieve 
optimal performance in 9 out of 10 replications. Moreover, 
the analysis of the results obtained by post-evaluating the 
best individuals for 40 trials without linguistic inputs (see 
Figure 4, LST condition)  indicates that the possibility to 
self-talk combined with an explicit training to self-talk 
produce better result with respect to the ST condition.  
 
E. Generalization and integration strategies 
To verify the generalization capabilities of trained robots 
and to compare generalization performance for agents 
trained in the four experimental conditions we post-evaluated 
the performance of trained robots by placing the object in 
7x7 different positions uniformly distributed over a 35x35 
cm2 area and by varying the initial position of the arm for 
each object position within four different alternative 
postures. As can be seen in Figure 6, overall, the robots 
display rather good generalization capabilities. That is 
performance decrease only slightly with respect to the case 
in which the robots have been post-evaluated in the same 
condition experienced during the training process. A two-
28
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-218-9
COGNITIVE 2012 : The Fourth International Conference on Advanced Cognitive Technologies and Applications

tailed Mann-Whitney U Test indicate that the LST condition 
is significantly better than the other three conditions and that 
the ST condition is significantly better than the C condition. 
 
Figure 6. Percentage of successful trials for the best robots of the four 
experimental conditions post-evaluated for 49 different object positions. 
The comparison of the behavior produced during even 
and odd trials in the LST conditions (in which the robot 
receives and does not receive the linguistic labels from the 
caretaker, respectively) indicates that 6 of the 10 robots of 
the LST condition that have a better generalized performance 
when they operate autonomously than when they receive the 
linguistic inputs from the caretaker. Moreover, during self-
talk all the 6 robots are also faster in performing the 
integrated behavior.  The significance of the difference has 
been evaluated on the basis of  a two-tailed Mann-Whitney 
U Test. This result indicates that robots are capable of 
developing better strategies than those conveyed by the 
caretaker.  
IV. 
DISCUSSION AND CONCLUSION 
In this paper, we demonstrated how a simulated iCub robot 
can acquire multiple goal-oriented action skills through an 
incremental training process in which it first develop lower-
level (elementary) actions and then higher-level integrated 
behaviors by combining and integrating previously acquired 
lower-level action skills. Overall, the obtained results 
represent one of the first demonstrations of how a relatively 
complex robot can acquire and display multiple behavioral 
skills and can expand its behavioral repertoire (for a related 
work see [15]). 
The behavioral skills developed by our robots are not 
simply elements or objects but rather dynamical processes 
that originate from the robot/environmental interaction (and 
in some cases also from the interaction with the social 
environment constituted by the caretaker). They are flexible 
entities that are able to achieve the appropriate goal in 
varying robot/environmental circumstances. Similarly, the 
way in which these processes are combined to generate 
higher-level action skill is realized in a manner that is fluid 
and flexible enough to achieve the appropriate goal in 
varying environmental circumstances. 
One important aspect that characterizes the model 
presented is that it relies on a non-modular controller, i.e., it 
does not require a control system divided into modules (as 
for example in [16]) and does not assume a one-to-one 
correspondence between modules and behaviors. This aspect 
is particularly important from a developmental point of view, 
since it allows the development of behaviors that emerge 
from the interaction between the robot and the environment 
and from the interaction between previously developed 
control mechanisms that are responsible for the generation of 
other action skills. Moreover, this aspect facilitates the 
development of behavioral capabilities that are more suitable 
to be recombined and integrated. To appreciate this point we 
should consider that the development of low-level action 
capabilities (such as “REACH” and “GRASP”) through the 
use of different control modules will likely end up with the 
exhibition of behaviors based on different equivalent 
postures that are hard to combine to produce integrated 
behavior. The realization of the lower-level behaviors 
through the same neural controller, instead, leads to the 
production of more similar behaviors that are more ready to 
be integrated.  
A second important aspect that characterizes the model 
proposed is constituted by the key role played by language 
mediated social interactions with particular reference to 
language exposure and self-talk. 
For what concern language exposure, the availability of 
linguistic labels such as “grasp” and “move” during the 
acquisition of low-level actions facilitate the acquisition of 
an ability to discriminate the categorical contexts affording 
specific actions. Moreover, as previously demonstrated by 
[17] in a study conducted on a similar experimental scenario, 
the availability of linguistic inputs indicating the current 
appropriate action allows the robot to overcome the problem 
caused by the need to handle robot/environmental contexts 
affording multiple actions. Finally, the availability of 
linguistic inputs indicating the sequence and the timing with 
which lower-level actions should be concatenated to generate 
new high-level behaviors facilitates the acquisition of an 
ability to produce integrated actions also autonomously, i.e. 
without linguistic inputs. 
For what concerns self-talk, the possibility to self-
generate internal states analogous to the linguistic inputs 
produced 
by 
the 
caretaker 
strongly 
facilitates 
the 
development of integrated behaviors and leads to robust 
solutions that generalize well also in new environmental 
circumstances. This can be explained by considering that the 
combination of language exposure and self-talk facilitates 
the re-use of previously developed action skills.  An 
additional facilitation effect can be gathered by explicitly 
training the robot to anticipate the linguistic inputs provided 
by the caretaker.  
The model proposed also allows the robot to develop 
more effective strategies with respect to those demonstrated 
by the caretaker. Indeed, the training method proposed 
constitutes a form of socially assisted individual learning that 
on one hand allows the robot to exploit the social feedback to 
facilitate the discovery of effective solutions, but that on the 
other hand, leaves the robot free to improve its current 
solution also with respect to the strategy illustrated by the 
caretaker.   
 
29
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-218-9
COGNITIVE 2012 : The Fourth International Conference on Advanced Cognitive Technologies and Applications

ACKNOWLEDGMENT 
This research work was supported by the ITALK project 
(EU, ICT, Cognitive Systems and Robotics Integrated 
Project, grant n. 214668).  
REFERENCES 
[1] Schaal S. (2007). The new robotics: towards human-centered 
machines," HFSP Journal, vol. 1, no. 2, pp. 115-126. 
[2] Weng J.J., McClelland J., Pentland A., Sporns O., Stockman 
I., Sur M. and Thelen E. (2001). Autonomous mental 
development by robots and animals. Science, 291:599–600. 
[3] Bongard J. (2008) Behavior Chaining: Incremental Behavior 
Integration for Evolutionary Robotics, Artificial Life XI, MIT 
Press, Cambridge, MA. 
[4] Nolfi S. (2006). Behaviour as a complex adaptive system: on 
the role of self-organization in the development of individual 
and collective behaviour. ComplexUs, 2 (3-4): 195-203. 
[5] Cangelosi A., Metta G., Sagerer G., Nolfi S., Nehaniv C, 
Fischer K., Tani J., Belpaeme T., Sandini G., Fadiga L., 
Wrede B., Rohlfing K., Tuci E., Dautenhahn K., Saunders J. 
and Zeschel A. (2010). Integration of action and language 
knowledge: A roadmap for developmental robotics. IEEE 
Transactions on Autonomous Mental Development, (2) 3: 
167-195 
[6] Vygotsky L.S. (1962). Thought and language, MIT Press, 
Cambridge, MA. 
[7] Vygotsky L.S. (1978). Mind in society. Cambridge, MA: 
Harvard University Press. 
[8] Mirolli M. and Parisi D. (2011). Towards a Vygotskyan 
Cognitive Robotics: The Role of Language as a Cognitive 
Tool, New Ideas in Psychology, vol. 9, pp. 298-311. 
[9] Yamashita Y. and Tani J. (2008). Emergence of functional 
hierarchy in a multiple timescale neural network model: a 
humanoid robot experiment. PLoS Computational Biology, 
Vol.4, Issue.11, e1000220.  
[10] Zhang Y. and Weng J. (2007). Task transfer by a 
developmental robot. IEEE Transactions on Evolutionary 
Computation, (11) 2: 226-248. 
[11] Diaz R. and Berk L.E., ed. (1992), Private speech: From 
social interaction to self regulation, Erlbaum, New Jersey, NJ 
[12] Sandini G., Metta G., and Vernon D. (2004). Robotcub: An 
open framework for research in embodied cognition. 
International Journal of Humanoid Robotics, 8(2), 18-31. 
[13] Nolfi S. and Floreano D. (2000). (2000). Evolutionary 
Robotics: The Biology, Intelligence, and Technology of Self-
Organizing Machines. Cambridge, MA: MIT Press/Bradford 
Books. 
[14] Leugger T. (2012). Development of Integrated Behaviour in a 
Simulated Humanoid Robot: Exploiting Language Assisted 
Training and Self Talk. Masters Thesis. School of Computer 
and Communication Sciences. Ecole Polytechnique Federale 
de Lausanne, Switzerland. 
[15] Tani J., Nishimoto R. and Paine R.W. (2008). Achieving 
'organic compositionality' through self-organization: Reviews 
on brain-inspired robotics experiments", Neural Networks, 
21:584-603. 
[16] Brooks R. (1986). A robust layered control system for a 
mobile robot. IEEE J of Robotics and Automation, 2(1):14–
23. 
[17] Massera G., Tuci E., Ferrauto T. and Nolfi S. (2010). The 
facilitatory role of linguistic instructions on developing 
manipulation 
skills, 
IEEE 
Computational 
Intelligence 
Magazine, (5) 3: 33-42. 
 
 
 
30
Copyright (c) IARIA, 2012.     ISBN:  978-1-61208-218-9
COGNITIVE 2012 : The Fourth International Conference on Advanced Cognitive Technologies and Applications

