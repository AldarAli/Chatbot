Automatic Discrimination of Voluntary and Spontaneous Eyeblinks. 
The use of the blink as a switch interface 
 
Shogo Matsuno, Minoru Ohyama, Shoichi Ohi 
Graduate School of Information Environment 
Tokyo Denki University 
Tokyo, Japan 
e-mail:12jkm25@ms.dendai.ac.jp 
Kiyohiko Abe, Hironobu Sato 
Department of Network and Multi-Media Engineering 
Kanto Gakuin University 
Kanagawa, Japan 
e-mail:abe@kanto-gakuin.ac.jp 
 
 
Abstract— This paper proposes a method to analyze 
the automatic detection and discrimination of eyeblinks for use 
with a human-computer interface. When eyeblinks are 
detected, the eyeblink waveform is also acquired from a change 
in the eye aperture area of the subject by giving a sound signal. 
We compared voluntary and spontaneous blink parameters 
obtained by experiments, and we found that the trends of 
the subjects for important feature parameters could be sorted 
into three types. As a result, the realization of automatic 
discrimination of voluntary and spontaneous eye blinking can 
be expected. 
 Keywords— computer interface; automatic discrimination; 
voluntary eye blink; spontaneous eye blink 
I. 
 INTRODUCTION 
The relation between eyeblinks and cognitive states has 
been pointed out by psychological experiments [1][2]. 
Realization of automatic discrimination of spontaneous and 
voluntary eyeblinks is desired. However, it is difficult to 
automate the detection of eyeblinks or the discrimination of 
kinds of eyeblinks, because the generation of eyeblinks has 
large variations between individuals. Therefore, until now, 
the detection or discrimination of eyeblinks has been carried 
out manually in most cases. Now, researchers are advancing 
their studies by aiming at automation of the detection and 
discrimination of eyeblinks while bearing a computer 
interface in mind. 
A technique using the electro-oculogram (EOG) has been 
proposed as an automatic detection method for voluntary 
eyeblinks [3][4]. The EOG method consists of sticking 
an electrode on the skin near the eyeball and detecting 
changes in cornea-retina potential. However, adopting 
the EOG method as a common interface is accompanied with 
the difficulty of directly equipping the skin with an electrode 
and exclusively using a special machine. 
In this research, the videotape recording (VTR) method is 
adopted, whereby eyeblinks are measured from a video 
image. Until recently, automatic blink detection using 
the VTR method was difficult, because the accuracy was 
lowered by a shortage of sampling points. Then, a technique 
for using an interlaced picture was incorporated, which 
divided the field that others had proposed. Hence, even if we 
use an ordinary NTSC video camera, we can obtain twice as 
much time resolution as with natural NTSC video. In this 
research, we tried to detect a blink waveform by analyzing 
interlaced pictures taken with an ordinary NTSC video 
camera and then conducting a discrimination experiment on 
spontaneous eyeblinks and voluntary eyeblinks. In this 
experiment, we performed an estimation experiment on 
automatic detection of eyeblinks and extraction experiments 
on the shape feature parameters of eyeblink waveforms (two-
pattern condition). The results are reported herein.  
 
II. 
AUTOMATIC DETECTION OF EYEBLINK WAVEFORM 
BY VTR METHOD 
If the time evolution of the eyeblink process is correctly 
measurable, it is possible to express an eyeblink as 
a waveform (a so-called eyeblink waveform). In order to 
identify the kind of eyeblink that has occurred, sampling 
the eyeblink waveform is first necessary. The typical 
techniques for sampling eyeblink waveforms are the EOG 
method and the VTR method. The former technique involves 
sticking an electrode on the skin near the eyeball, and it 
acquires the eyeblink waveform by recording the changes in 
cornea-retina 
potential. 
Until 
now, 
this 
has 
been 
the technique proposed for automatic methods to detect 
voluntary eyeblinks. However, the EOG method needs an 
exclusive apparatus, and the skin must be directly equipped 
with an electrode. For that reason, it is unsuitable for 
a simple interface. Moreover, there exists the disadvantage of 
easily mixing in noise from a living body. 
On the other hand, the VTR method is suitable for a 
computer interface because it is a non-contact technique and 
has great flexibility. However, the process of change is 
difficult for a typical NTSC video camera to capture, because 
an eyeblink is a high-speed operation. Therefore, analysis 
using a high-speed camera has been attempted [5]. Computer 
interfaces using eyeblinks have been proposed in several 
papers [6][7][8]. Eyeblink switches were devised by using 
two different types of hard instructions in these techniques. 
For example, in [7], a subject closed an eye for a blink time 
of over 200 ms but, in [8], double-blinking was used for 
a mouth clicking switch. We aimed for a method to lower 
the stress of the subject. Eyeblink waveforms exhibit large 
individual variations, so automatic detection with multiple 
persons as the target is difficult. However, we have 
developed an algorithm such as that described below.  
 
433
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

 
 
The first step is to analyze video images of the near-eye 
area to obtain the changes in eye aperture area (Fig. 1). 
The data include the eyeblink waveform. This step 
incorporates an algorithm used in previous research [9]. This 
technique shows that we can detect a change in eye aperture 
area while sampling at 1/60 s by using interlaced NTSC 
video images divided into field images. 
 The next step is to differentiate the area data obtained. 
We smooth the derivative of the aperture area, and then we 
take the absolute value. The coordinates of the maximums 
are determined by using a second differentiation. Fig. 2 
shows 
the 
coordinates 
of 
these 
maximums. 
Here, 
the eyeblink operation (namely, the closing and opening 
phases) corresponds to maximums with large values. 
However, the result of this step contains much noise. In other 
words, 
the 
maximums 
due 
to 
small 
changes 
in 
the coordinates are from noise. Therefore, we needed to 
remove the noise by using the k-means method for optimal 
partition clustering. We set the number of clusters to two, 
because the purpose was separation of the required data part 
from the noise part. We applied the evaluation function 







k
i
C
x
i
b
i
c
x
J
1
2||
||


 where Jb represents the objective function, C is the dataset, 
x is a data value, and c is the cluster centroid. The k-means 
process is repeated until Jb is minimized, and then we  
 
 
terminate the clustering process. Fig. 3 shows the clustering 
result. Of the clusters obtained, the lower cluster is the noise 
cluster, and the upper cluster is the required data cluster. We 
then plotted the coordinates of the retrieved maximums on 
the diagram that records the changes of eye aperture area 
(Fig. 4). 
There are times when a strange movement of the eyes is 
noted in the middle of a blink and maximums are recorded in 
more than three places. Then, if a clustering process is 
performed, it will not be able to remove superfluous points 
and will not be able to pick up the eyeblink waveform 
normally. Hence, we need to remove the superfluous points. 
We solve this problem by removing as noise the points that 
were recorded later, if the recording was continuous over 
a short period of time. 
As a result of these processes, we obtain maximums in 
the middle of the eyeblink waveform. We can distinguish 
closing and opening by evaluating the derivative of the eye 
aperture area at that point. In other words, if the derivative at 
the point is negative, the point exists in the closing phase. 
Conversely, if the derivative is positive, the point exists in 
the opening phase. We can determine the eyeblink starting 
point and the eye closing point based on this. Moreover, we 
can determine the duration of a single eyeblink waveform. 
 
 
 
Figure 1. Changes of opening eye aperture area. 
Figure 2. Coordinates of speed maximums. 
 
 
 
Figure 3. Result of clustering. 
Figure 4. Coordinates of retrieved speed maximums an diagram that 
records. 
hanges of eye aperture area. 
434
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

III. 
SHAPE FEATURE PARAMETERS OF VOLUNTARY 
EYEBLINK 
Blinking can be categorized as voluntary, reflex, or 
spontaneous [10]. A voluntary eyeblink occurs consciously 
and is performed at the order of the experimenter. A reflex 
eyeblink occurs due to external stimulation such as photic 
stimulation or auditory stimulation. A blink that occurs 
unconsciously is called a spontaneous eyeblink. In this paper, 
we 
intend 
to 
discriminate 
between 
voluntary 
and 
spontaneous blinks. Spontaneous eyeblink waveforms have 
various amplitude patterns and greater individual unevenness 
than other kinds of eyeblink waveforms. Nevertheless, 
a voluntary eyeblink causes the eyelids to close almost 
completely. Moreover, there is relatively little variation in 
the recorded eyeblink waveforms of the same person. Thus, 
based on the literature [11], we focused on the parameters of 
two types of eye blinking. The parameters are the amplitudes 
in the closing and opening phases as well as the duration. 
Fig. 5 models a single blink. In this figure, the amplitude of 
the closing phase (Acl) is measured from the blink start 
point (Ps) to the eye aperture area minimum (Pmin) between 
the closing phase end point (Psb) and the opening phase start 
point (Peb). The eyeblink duration (Dur) is the number of 
samples from the blink start point (Ps) to the blink end 
point (Pe). 
 
IV. 
EXPERIMENTS 
A. System Outline 
The hardware for our experimental system includes one 
digital camcorder, which acquires pictures near the eye, and 
a personal computer, which conducts image analysis and 
eyeblink waveform analysis. Although the camera can also 
take high-definition (HD) pictures, the standard-definition 
(SD) picture was used for the experiment. In addition to 
ordinary 
indoor 
lighting 
(incandescent 
lighting), 
the experiment used LED lighting (LPL: LED VL-300C 
L26811).  
 
 
TABLE I.  RESULTS OF AUTOMATIC DETECTION OF EYEBLINKS. 
 
Eyeblinks 
[number] 
Error [number] 
Agreement 
rate [%] 
oversight 
misdetection 
Subject 1 
12 
0 
0 
100.0 
Subject 2 
4 
0 
0 
100.0 
Subject 3 
4 
2 
0 
50.0 
Subject 4 
2 
0 
0 
100.0 
Subject 5 
4 
0 
0 
100.0 
Total 
26 
2 
0 
92.3 
 
 
Incandescent lighting was set when taking the moving 
images. Then, LED lighting was installed at a distance of 
about 60 cm from the front of the subject’s face. Subjects 
would sit on a chair at a distance of about 20 cm from 
the video camcorder, and the back of the head was lightly 
supported with a device to prevent the head from shaking. 
The video camera was placed in front of and below 
the subject`s head. Then, the camera magnified and took 
pictures around the subject’s left eye. The image format is 
SD video, so the resolution is 720 pixels by 480 lines with 
16:9 aspect ratio and the refresh rate is 30 frames per second 
(NTSC). These experiments were performed on the naked 
eye, so eye glasses were not allowed during the filming in 
the experiments. 
 
B. Automatic Detection of Eyeblink Waveform 
We conducted an evaluative experiment for the automatic 
detection of the eyeblink waveform by using the algorithm 
described in Section II. This experiment was used for 
preprocessing purposes and to extract the shape feature 
parameters of the eyeblink waveform. Five people were 
included in this experiment: four men in their twenties and 
one man in his thirties. 
1) Method: Before the experiment, the subjects were 
first instructed to “pay attention to the mark that was 
installed on the upper part of the camcorder” and told “you 
do not have to resist the unconscious urge to blink.”  
Rest time was provided for about 1 min prior to beginning 
the experiment. Once the experiment began, moving images 
were taken for 10 s. Changes in the eye aperture area were 
obtained from this moving picture by image analysis, and 
automatic detection of eyeblink waveforms was achieved by 
using the proposed algorithm. 
In this experiment, the obtained waveforms mixed some 
voluntary eyeblinks with many spontaneous eyeblinks, 
because the subjects were not instructed to blink. In addition, 
the instances of automatic detection were validated by 
comparing the data with those of real blinks observed 
visually. 
 
 
Figure 5.  Shape feature parameters of eyeblink. 
435
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

TABLE 2.  RESULTS OF SHAPE FEATURE PARAMETER EXTRACTION 1 
(SPONTANEOUS EYEBLINKS). 
 
Blinks 
[number] 
Amplitude 
(Closing) 
[pixels] 
Amplitude 
(Opening) 
[pixels] 
Duration 
[ms] 
Subject A 
19 
14589 
14964 
335.83 
Subject B 
25 
13024 
12076 
490.67 
Subject C 
27 
15830 
15190 
356.67 
Subject D 
3 
15562 
12635 
422.17 
Subject E 
30 
5043 
2962 
197.67 
Subject F 
15 
10138 
10453 
556.67 
Subject G 
3 
9397 
9352 
394.3 
Subject H 
2 
11006 
10720 
358.3 
Subject I 
21 
11799 
10474 
506.3 
Subject J 
16 
10039 
10318 
534.3 
 
 
TABLE 4.  RESULTS OF SHAPE FEATURE PARAMETER EXTRACTION 2 
(SPONTANEOUS EYEBLINKS). 
 
Blinks 
[number] 
Amplitude 
(Closing) 
[pixels] 
Amplitude 
(Opening) 
[pixels] 
Duration 
[ms] 
Subject K 
16 
12276 
12353 
346.9 
Subject L 
13 
10077 
9909 
371.8 
Subject M 
10 
7503 
6506 
370.0 
Subject N 
13 
7366 
5859 
421.8 
Subject O 
19 
11775 
8085 
384.2 
 
 
2) Result: Table 1 shows the number of automatic 
detections and the number of confirmed eyeblinks for each 
of the subjects. The overall agreement rate of these blink 
numbers is 92.3%. This result was satisfactory enough for 
preprocessing. Subject 3 produced a low detection rate; 
however, a factor in this result is the number of oversights 
in detection. This factor makes little difference, because 
the problem can be solved by promoting re-input if 
the algorithm is implemented as the interface. 
 
 
 
TABLE 3.  RESULTS OF SHAPE FEATURE PARAMETER EXTRACTION 1 
(VOLUNTARY EYEBLINKS). 
 
Blinks 
[number] 
Amplitude 
(Closing) 
[pixels] 
Amplitude 
(Opening) 
[pixels] 
Duration 
[ms] 
Subject A 
8 
19249 
23379 
545.8 
Subject B 
9 
14016 
13281 
1238 
Subject C 
9 
16354 
15908 
396.1 
Subject D 
9 
16581 
15155 
762.8 
Subject E 
9 
8712 
5320 
214.6 
Subject F 
9 
12482 
11434 
737.0 
Subject G 
9 
12370 
12688 
474.0 
Subject H 
9 
10459 
11045 
785.2 
Subject I 
9 
19724 
17667 
588.8 
Subject J 
9 
13581 
12741 
498.2 
 
 
TABLE 5.  RESULTS OF SHAPE FEATURE PARAMETER EXTRACTION 2 
(VOLUNTARY EYEBLINKS). 
 
Blinks 
[number] 
Amplitude 
(Closing) 
[pixels] 
Amplitude 
(Opening) 
[pixels] 
Duration 
[ms] 
Subject K 
9 
14196 
14388 
353.7 
Subject L 
9 
12227 
12070 
381.6 
Subject M 
9 
13044 
12396 
787.0 
Subject N 
9 
19737 
19397 
653.7 
Subject O 
9 
12316 
11288 
807.4 
 
 
 
C. Shape Feature Parameter Extraction 1 (Experiment 1) 
Based on the results of the evaluative experiment of 
Section IV-B, we performed an experiment to extract 
the shape feature parameters described in Section III for 
eyeblink waveforms. Ten people were included in this 
experiment: nine men and one woman in their twenties.  
These subjects were all different persons from those in 
the abovementioned preprocessing experiment. 
 
 
 
436
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

 
 
 
1) Method: As in the experiment described above, 
the subjects were instructed before the experiment to “pay 
attention to the mark that was installed on the upper part of 
the camcorder” and told “you do not have to resist 
the unconscious urge to blink.” In addition, the subjects in 
this experiment were told, “you must always blink when 
you hear the signal.” This instruction makes it possible to 
distinguish voluntary eyeblinks from spontaneous eyeblinks. 
The signal was sounded randomly at intervals of from 4 s to 
8 s by using a digital timer. A rest time of about 1 min was 
provided prior to the task. Once the experiment began, 
moving images were taken for approximately 1 min. 
2) Result: Fig. 6 shows an example of the acquired 
waveforms (changes in eye aperture area). Moreover, 
Tables 2 and 3 list the shape feature parameters that were 
obtained in this experiment. Those for spontaneous 
eyeblinks are listed in Table 2, and those for voluntary 
eyeblinks are listed in Table 3. The value of each parameter 
is the mean for the detected blinks.  
D. Shape Feature Parameter Extraction 2 (Experiment 2) 
In addition to the experiment described in Section IV-C, 
an experiment was carried out in another room. Five of 
the subjects who generated the data presented in this section 
differed from those who participated in the experiments of 
Sections IV-B and IV-C (four men and one woman in their 
twenties). 
1) Method: Basically, the experimental method was 
the same as that described in Section IV-C-1. However, 
there were two differences. The light of the sun was not 
evident in the room, and the camcorder was moved to 
a position directly in front of the subject’s face. 
2) Result: Tables 4 and 5 list the shape feature 
parameters that were obtained in this experiment. Like 
Tables 2 and 3, these tables each list statistics for 
spontaneous or voluntary eyeblinks, respectively. The value 
of each parameter is the mean for detected blinks. 
 
 
 
 
 
 
 
 
 
V. 
DISCUSSION 
First, we were able to obtain waveform data regardless of 
the influence of sunlight, because we compared the data of 
Experiment 1 with that of Experiment 2 and there was no 
great difference between the two sets of data. 
We discuss the results on the basis of the assumption 
mentioned above. Fig. 6 shows an example of the acquired 
waveforms (changes in eye aperture area). In addition, 
Figs. 7–9 show the patterns of waveform pairs for three types 
of subject. To enable comparisons between a pair of eyeblink 
waveform patterns, these plots were normalized using the 
pixels of the eye aperture area at the first sampling point of 
the first field image. When we reviewed the average values 
in Tables 2–5, the shape feature parameters of the voluntary 
eyeblinks recorded were nearly all greater than those of the 
spontaneous ones. This was a common trait found with all 
subjects. Admittedly, the recorded amplitude of the closing 
phase for Subject H in Experiment 1 was an exception, 
because so few spontaneous eyeblinks were found during 
the experiment. However, this voluntary parameter is greater 
than the spontaneous one if the length of time taken for 
the experiment is added. Hence, it is difficult for 
discrimination methods to use the rate of amplitude change 
because there is an absence of variety in the patterns. 
A significant difference existed in the blink durations of 
the majority of subjects. In particular, Subjects B and H of 
Experiment 1 had values recorded for voluntary blinks that 
were twice those recorded for spontaneous blinks. Subject E 
had a small difference in duration but large differences in 
the amplitudes of the opening and closing phases. Subject J 
had a duration for spontaneous blinks that was longer than 
that for voluntary ones. The reasons are as follows: Subject J 
was observed to produce many blinks with larger ocular 
movements than the other subjects. When we accumulated 
the numbers of blinks, a voluntary eyeblink was counted 
whenever the subject responded to the signal heard. 
Consequently, it is thought that among the spontaneous 
eyeblinks 
were 
included 
some 
voluntary 
eyeblinks. 
Moreover, the change of eye aperture area by ocular 
movement is a factor that extends the duration. 
 
 
 
 
 
 
Figure 6.  Example of eyeblink waveform. 
437
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

Figure 7. Waveforms recorded for the type of subject with a large 
difference in duration between voluntary and spontaneous blinks. 
 
 
Next, we classified the subjects on the basis of three 
patterns in the shape feature parameters (Figs. 7–9): 
1) subjects with a large difference in duration between 
voluntary and spontaneous blinks (e.g., Subjects B and D), 
2) subjects with a large difference in amplitude between 
voluntary and spontaneous blinks (e.g., Subjects A and I), 
and 3) subjects with both characteristics mentioned above 
(e.g., Subject M). There are meaningful differences between 
these patterns in parameters that reveal each large difference. 
Hence, we expect that automatic discrimination of 
voluntary eyeblinks can be realized by using a machine that 
has learned to weight the parameters that exhibit large 
differences. In addition, we think that classifying patterns 
would become more accurate if the velocities of the eye 
opening and closing phases were included among 
the parameters.  
 
VI. 
CONCLUSION 
We examined automatic discrimination of voluntary and 
spontaneous eyeblinks, which is a problem that has been 
awaiting solutions in various kinds of blink research. So far, 
eyeblink detections have needed to use the EOG method or 
a high-speed camera. However, this paper has proposed 
a detection algorithm by using ordinary interlaced NTSC 
video images divided into field images. This algorithm can 
automatically pick up every eyeblink waveform from 
the changes in the eye aperture area, which were obtained 
from an SD moving image by means of image analysis. In 
our experiments to evaluate this algorithm, the average rate 
of detection was 92.3%. 
Further, we conducted experiments on shape feature 
parameter extraction using an audio signal and our proposed 
algorithm. In this experiment, we instructed the subjects to 
blink voluntarily in order to yield an eyeblink waveform of 
both voluntary and spontaneous blinks. We instructed 
the subjects by saying, “you must always blink when you 
hear a signal.”  
 
 
 
Figure 8.  Waveforms recorded for the type of subject with a large difference 
in amplitude between voluntary and spontaneous blinks. 
Figure 9. Waveforms recorded for the type of subject with both 
characteristics mentioned above. 
 
 
After that, we analyzed the tendencies of the shape 
feature parameters, and three facts became known. First, 
the parameters of voluntary eyeblinks are greater than those 
of spontaneous ones. Second, a large difference in the blink 
duration is usually observed, even apart from these 
parameters. Third, a different tendency for large feature 
parameters is observed with every subject. However, we 
knew that the rate of amplitude change would be difficult to 
use as a shape feature parameter. 
In particular, we newly observed subjects who exhibited 
both of the different patterns found in Experiment 2. In other 
words, we understood that we could classify subjects using 
three types; namely, 1) subjects with a large difference in 
duration between voluntary and spontaneous blinks, 
2) subjects with a large difference in amplitude between 
voluntary and spontaneous blinks, and 3) subjects with both 
characteristics mentioned above.  
From the above results, we showed the possibility of 
discriminating 
voluntary 
eyeblinks 
from 
spontaneous 
eyeblinks by using shape feature parameters such as 
the eyeblink duration and the amplitudes of the opening and 
closing phases. Moreover, we confirmed the ability to 
process these parameters in a uniform manner, because we 
could obtain some tendencies from the parameters even 
438
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

when the environmental conditions, such as the lighting or 
the subjects, were changed. 
We now wish not only to classify more accurately using 
other parameters such as velocity but also to classify subjects 
automatically using the three patterns. We are aiming for 
real-time discrimination of voluntary eyeblinks and 
spontaneous eyeblinks. 
REFERENCE 
 
[1] J. A. Stern, L. C. Walrath, and R. Goldstein, “The endogenous 
eyeblink,” Psychophysiology, vol. 21, no. 1, Jan. 1984, 
pp. 22–33. 
[2] K. Tanabe, “Eyeblink activity during identification of 
Katakana characters viewed through a restricted visual field,” 
IEICE Trans. Fundam. Electron. Commun. Comput. Sci., 
vol. E87-A, no. 8, 2004, pp. 2189–2191. 
[3] J. Hori, K. Sakano, and Y. Saitoh, “Development of 
communication 
supporting 
device 
controlled 
by 
eye 
movements and voluntary eye blink,” Proc. IEEE Conf. Eng. 
Med. Biol. Soc., vol. 6, 2004, pp. 4302–4305.  
[4] O. Tetuya, K. Hironori, and K. Masashi, “Development of 
an input operation of the communication tool using voluntary 
eye blink,” Papers of Technical Meeting on Medical and 
Biological Engineering, IEE Japan, vol. 6, 2006, pp. 1–4. 
[5] K. Ohzeki and B. Ryo, “Video analysis for detecting eye 
blinking using a high-speed camera,” 40th Asilomar Conf. 
Signals, Systems and Computers (ACSSC ’06), 2006, 
pp. 1081–1085. 
[6] K. Grauman, M. Betke, J. Gips, and G. R. Bradski, 
“Communication via eye blinks — detection and duration 
analysis in real time,” Proc. IEEE CS Conf. Computer Vision 
and Pattern Recognition (CVPR 2001), vol. 1, 2001, 
pp. 1010–1017. 
[7] A. Krolak and P. Strumillo, “Vision-based eye blink 
monitoring system for human-computer interfacing,” Proc. 
Conf. Human System Interactions, 2008, pp. 994–998. 
[8] D. O. Gorodnichy, “Second order change detection, and its 
application to blink-controlled perceptual Interfaces,” Proc. 
IASTED Conf. Visualization, Imaging, and Image Processing, 
2003, pp.140–145. 
[9] A. Kiyohiko, O. Shoichi, and O. Minoru, “Automatic method 
for measuring eye blinks using split-interlaced images,” Proc. 
Conf. Human-Computer Interactions, Part I  (HCII 2009), 
LNCS 5610, 2009, pp. 3–11. 
[10] F. VanderWerf, P. Brassinga, D. Reits, M. Aramideh, and 
B. Ongerboer de Visser, “Eyelid movements: Behavioral 
studies of blinking in humans under different conditions,” 
J. Neurophysiol., vol. 89, 2003, pp. 2784–2796. 
[11] B. Matteo, A. Rocco, B. Gregori., D. Belvisi, D. Ottaviani, 
et al., “Voluntary, spontaneous and reflex blinking in patients 
with clinically probable progressive supranuclear palsy,” 
Brain, vol. 132, no. 2, 2009, pp. 502–510. 
 
 
439
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-250-9
ACHI 2013 : The Sixth International Conference on Advances in Computer-Human Interactions

