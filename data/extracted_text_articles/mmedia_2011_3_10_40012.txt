Indexing Support Vector Machines for Efﬁcient top-k Classiﬁcation
Giuseppe Amato, Paolo Bolettieri, Fabrizio Falchi, Fausto Rabitti, Pasquale Savino
ISTI-CNR
Pisa, Italy
Email: g.amato@isti.cnr.it, p.bolettieri@isti.cnr.it, f.falchi@isti.cnr.it, f.rabitti@isti.cnr.it, p.savino@isti.cnr.it
Abstract—This paper proposes an approach to efﬁciently
execute approximate top-k classiﬁcation (that is, identifying the
best k elements of a class) using Support Vector Machines,
in web-scale datasets, without signiﬁcant loss of effectiveness.
The novelty of the proposed approach, with respect to other
approaches in literature, is that it allows speeding-up several
classiﬁers, each one deﬁned with different kernels and kernel
parameters, by using one single index.
Keywords-Image Classiﬁcation; Support Vector Machines;
Similarity Searching.
I. INTRODUCTION
The classiﬁcation problem is typically deﬁned as follows.
Given a set of classes c1, . . . , cn, and an object o, the
classiﬁcation problem is to decide which classes o belongs
to. In this paper, on the other hand, we address the classi-
ﬁcation problem from an Information Retrieval perspective.
Let c1, . . . , cn be n classes, and DS a very large dataset
(for instance the World Wide Web) of unclassiﬁed objects.
Given a class ci, we want to retrieve the k objects of DS
having the highest likelihood to belong to ci. In this case,
the class ci can be considered as a query, and the best k
objects that belong to ci as the answer to the query. We
call top-k classiﬁcation this formulation of the classiﬁcation
problem.
Support Vector Machines (SVMs) [1], are widely used to
perform automatic supervised classiﬁcation. The aim of this
paper is to propose a strategy that, given a set of SVM
classiﬁers deﬁned for a set of classes c1, . . . , cn, and a
dataset DS, executes top-k classiﬁcation very efﬁciently.
More speciﬁcally, we do not address the problem of learn-
ing classiﬁers, for which several solutions already exist as
mentioned in Section III. Rather, given an SVM classiﬁer
for any class ci, and a dataset DS, our aim is to efﬁciently
search in DS for the best k objects that belong to ci.
As we will see, we propose an approximated approach.
That is, our approach returns an imprecise result, compared
to the result that would have been obtained by performing
a sequential scan of the entire dataset DS and applying
the available classiﬁers to every object. However, the ex-
periments will show a small imprecision, compared to an
improvement of efﬁciency of orders of magnitude.
The rest of the paper is organized as follows. First we
discuss related work. Next we brieﬂy introduce the SVM. In
Sections IV and V, we present the top-k classiﬁcation, while
in Section VI, the index structure used in the experiments.
Finally in Sections VII and VIII, we describe the settings of
the experiments and the analysis of the results.
II. RELATED WORK
Efﬁcient top-k classiﬁcation techniques were proposed in
[2], [3], by leveraging on the property that instances in the
feature space lie on a hypersphere. This approach is able
to use one index for various classes obtained using Support
Vector Machines and built using the same kernel. In [4], the
authors propose a method for efﬁcient top-k classiﬁcation
based on boosting. In [5], the authors propose an efﬁcient
method for retrieving the instances closest to the separating
hyperplane (the most ambiguous instances) to support active
relevance feedback.
The novelty of the proposed approach, with respect to
other methods existing in literature, is that it allows sup-
porting and speeding-up the use of several classiﬁers, each
one deﬁned with different kernels and/or kernel parameters,
by using one single index in the input space 1 of the dataset.
III. INTRODUCTION TO SVM
An SVM [1] builds classiﬁers by learning from a training
set that is composed of both positive and negative examples.
In many cases, in order to be able to separate element that
belong to the class from those that do not belong to the class,
it is convenient to map vectors, representing elements, in an
higher dimensional vector space using a mapping function
Φ(·). Omitting several theoretical details (see [1] for more
information), the learning phase determines a vector ω such
that the decision function
f(o) =< ω, Φ(o) > +b
(1)
is able to optimally classify most of the training set
examples ( < ω, Φ(o) > is the dot product between vectors
ω and Φ(o)). When the decision function is positive it
indicates that an object belongs to the class. A popular
learning algorithm is the kernel-based version of the adatron
algorithm.
The SVM literature often call input space the space where
objects are deﬁned, and feature space the space where
1the space in which objects are originally represented (see Section III).
56
MMEDIA 2011 : The Third International Conferences on Advances in Multimedia
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-129-8

objects are mapped by Φ(·). We will use this terminology
in the reminder of the paper.
SVM methods do not deﬁne the mapping function ex-
plicitly, but use the properties of the kernel functions to
perform learning and classiﬁcation. A kernel function K,
deﬁned as K(oi, oj) =< Φ(oi)T , Φ(oj) >, computes the
dot-product of oi and oj in the feature space. There are
simple kernel functions that easily compute the dot-product
of objects mapped in very high or even inﬁnite dimensional
spaces without even knowing the actual mapping functions.
It can be proven [1] that the kernel-based decision func-
tion deﬁned above, can be also represented in the dual form
f(o) =
∑
(oi,yi)∈T
yiαiK(o, oi) + b
(2)
where oi are the element of the training set and yi is 1
or to −1 according to the fact that the training object oi
is a positive or a negative sample of the class to learn. In
this formulation, the learning phase consists in ﬁnding the
parameters αi, which basically determine the contribution
of each example oi of the training set to the solution of the
learning problem, rather than the vector ω. Most of the αi,
obtained in the training phase, will be equal to 0. So, in
order to compute the decision function f, we only need to
maintain the training objects for which the αi are greater
than 0. These objects are the support vectors.
IV. TOP-k CLASSIFICATION
Let fc be a decision function deﬁned according to Equa-
tion 2 for the class c. The value fc(o) indicates the degree
of membership of the object o to the class c. Large positive
values indicate high membership of o to c; large negative
values indicate that o does not belong to c; values close to
zero indicate uncertainty.
The top-k classiﬁcation problem can also be formulated
as follows. Given a decision function fc and a dataset DS,
retrieve the k objects o1, . . . , ok in DS for which fc(oi), i =
1 . . . k, is larger than when applied to any other object in
DS. More formally:
Deﬁnition: 1: Let DS be a dataset of objects, c a class,
and fc the decision function for c. We deﬁne
top–k(DS, c) ={o1, . . . , ok ∈ DS |
∀o ∈ (DS\{o1, . . . , ok}),
fc(o) ≤ fc(oi), i = 1, . . . , k}
A. Approximate top-k classiﬁcation
Clearly, top−k(DS, c) can be computed with a sequential
scan of the whole dataset. However, this is very inefﬁcient
when DS is very large. Suppose we have a set of candidates
CS ⊆ DS for class c. Then, top-k(CS, c) is an approxima-
tion of top-k(DS, c). However, consider that if CS is chosen
carefully, top-k(CS, c) will not necessarily differ very much
from top-k(DS, c). For instance, if CS = top-k(DS, c),
then top-k(CS, c) =top-k(DS, c). According to this, given
CS, approximate top-k classiﬁcation can be performed by
applying the decision function to the objects of CS rather
than all objects in DS. Provided that CS is much smaller
that DS (#CS ≪ #DS2), this process will be much more
efﬁcient than exhaustively classifying all objects in DS.
In the next section, we will propose a strategy to obtain
CS, by using techniques of nearest neighbors searching, in
such a way that approximation will be highly accurate and
CS is much smaller than DS.
V. TOP-k CLASSIFICATION BY MEANS OF NEAREST
NEIGHBORS SEARCH
The training set Tc for a class c consists of positive
and negative examples. Let us denote as PTc and NTc
respectively the positive and negative training objects (Tc =
PTc
∪ NTc). As discussed in Section III, the learning phase
identiﬁes the αs parameters for the decision function, and
implicitly the support vectors (the training vectors whose αi
are strictly greater than 0). Let us denote as PSVc ⊆ PTc
and NSVc ⊆ NTc respectively the positive and negative
support vectors identiﬁed after the training of the classiﬁer
for a class c. The decision function given by Equation 2 can
be rewritten as
fc(o) =
∑
p∈P SVc
αpK(o, p) −
∑
n∈NSVc
αnK(o, n) + b
The formula above just uses the support vectors and
disregards the elements of the training set whose αs are
0, since they do not provide any contribution to fc.
From the deﬁnition of fc given above, it is easy to see that
the objects o of the dataset that are very similar, according
to K, to several positive support vectors and are dissimilar
to several negative support vectors, have higher chances to
return an high value when fc is applied to them. In fact,
the kernel K can be seen as a similarity function. That
is, K returns large values when the two compared objects
are similar and small values when the two objects are not
similar. This suggests a strategy to select from DS a subset
of promising candidates for c. In short, we can ﬁrst search
the objects of the dataset that are closer, according to K to
each positive support vector. Then, we apply the decision
function fc only to the selected candidates to ﬁnd the best
k matches.
More formally, the candidate set CSc ⊆ DS for class c
can be obtained as
CSc =
∪
p∈P SVc
NNK(p, s, DS)
(3)
where NNK(p, s, DS), is a nearest neighbors query,
which returns the s objects of DS most similar to p,
2we use # to indicate the cardinality of a set
57
MMEDIA 2011 : The Third International Conferences on Advances in Multimedia
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-129-8

according to kernel K, for some s much smaller than the
size of DS.
The selection of the k best objects matching the class c
can now be obtained by applying fc only to objects in CSs,
which is signiﬁcantly smaller than DS.
VI. USING AN INDEX STRUCTURE IN THE INPUT SPACE
Several scalable techniques can be found in literature to
efﬁciently process nearest neighbors search queries [6], [7].
However, even if, leveraging on these techniques, Equation
3 can be computed very efﬁciently, there are two reasons
why it is not practical to build an index using K directly
(that is in the feature space):
1) Access methods for similarity search typically rely on
the fact that the similarity (the kernel K in our case)
can be expressed in terms of a distance (dissimilarity)
function, which should satisfy some speciﬁc proper-
ties, like for instance the metric postulates. A distance
function can always be derived from a kernel K.
However, such function, in many cases is not suitable
to be used to build an efﬁcient index structure. In fact,
given that the kernel K compares elements in an high
dimensional feature space, the underlying distribution
of distances might not be convenient and we might
incur in the curse of dimensionality [8].
2) We would like to support several classiﬁers for the
same dataset, each recognizing a different class. Dif-
ferent classiﬁers might require different kernels and
different kernel parameters (that is, different similarity
functions). If we succeed to ﬁnd a suitable distance
function for a certain kernel K and we create an index
with this distance, we are bound to the speciﬁc kernel
K and its kernel parameters (for instance different σs
in the case of the RBF kernel), so we are bound to
a speciﬁc classiﬁer. To support several classiﬁers, we
would need several indexes, each for a speciﬁc kernel
and kernel parameters.
Next section shows how to solve the above problems by
building one single index in the input space to serve various
kernels, provided that they satisfy some conditions.
A. Kernels that allow using a single index in the input space
Instead of deriving a distance function from a kernel,
in many cases it is possible deﬁne a kernel in terms of a
convenient distance function as follows:
K(o1, o2) = g(d(o1, o2))
(4)
where d : D × D → R is a distance function between
objects of the input space D.
Many widely used kernels can be deﬁned using Equation
4. For instance, if d is the Euclidean distance (L2), with
g(x) = e− x2
2σ2 we obtain the RBF kernel; with g = −xβ,
for β > 0, we obtain the power kernel; when d is the L1
distance, with g(x) = e−γx we obtain the Laplacian kernel.
Other examples exist.
Let R = d(D, D), R ⊆ R, that is R is the set of possible
values that d can give for any arbitrary pair o1, o2 ∈ D.
If g is monotonously decreasing over R, then the k objects
closest to o, with respect to d, are the k objects most similar
to o in the feature space, with respect to K.
In other words, given any kernel K deﬁned accord-
ing to Equation 4, with a monotonous decreasing g and
the same distance d,we have that3 NNK(p, s, DS)
=
NNd(p, s, DS).
This implies that the most similar objects to a support
vector in the feature space induced by K, are exactly the
objects closest to the same support vector, in the input space.
Therefore, we can use just one single access method deﬁned
using d and built in the input space, to search for the nearest
neighbors in the feature space induced by a large class of
kernels deﬁned in terms of d. This, as discussed in Section
V, also gives us the possibility of identifying the subset of
promising candidates just working in the input space, for the
same class of kernels4.
Note also that the gs, that produce the RBF, Laplacian,
and power kernels, are all monotonously decreasing in R+.
Therefore, in this case, any d, which always returns positive
values, allows this technique to be used.
VII. EXPERIMENT SETTINGS
Tests of the proposed techniques were executed on a
single 2.4GHz Core 2 Quad CPU, using the CoPhIR dataset
[11]. CoPhIR consists of 106 millions images, taken from
Flickr, described by MPEG-7 visual descriptors. In the tests
we used the ﬁrst set of one millions images taken from
CoPhIR. The access method used to efﬁciently search for
objects close to the support vectors, in the input space,
is the MI-File [12] (Metric Inverted File). The MI-File is
a disk maintained index, based on inverted ﬁles, which
supports efﬁcient and scalable approximate similarity search
on data represented in metric spaces. To deﬁne the kernel
for the support vector machine, according to Equation 4,
we used g(x) = e− x2
2σ2 , and as d we used a combination
of MPEG-7 distance functions [13]. We trained the support
vector machine to recognize 5 different classes: churches,
pyramids, seascapes, paintings, and temples. We used a
standard kernel-based adatron with cross-validation, to learn
3Here we abuse with the notation, NNK gives the most similar, while
NNd gives the closest ones.
4Note that kernels used with SVM must be positive deﬁnite [1] or
conditionally positive deﬁnite [9]. Therefore when a kernel K is obtained
from Equation 4, we must ﬁrst prove this. However, in many common
cases this is true. Consider that, [10], in Theorem 12, shows that when
g = e−tx, for all t > 0, K is positive deﬁnite iff d is negative deﬁnite
and symmetric. Note that the Eucledian distance is negative deﬁnite and
symmetric. In fact, given that the RBF Kernel is positive deﬁnite, then d2,
when d is the Eucledian distance, is negative deﬁnite and symmetric and,
as consequence of Theorem 11 in [10], also d (the Eucledian distance) is
negative deﬁnite and symmetric.
58
MMEDIA 2011 : The Third International Conferences on Advances in Multimedia
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-129-8

Figure 1. Quality of the of the approximate Top-k classiﬁcation varying the
number of nearest neighbors (s) retrieved for each support vector, according
to Equation 3, for various values of k. Here, for brevity, we just show the
results when the query is class Pyramids. Similar results were obtained for
the other classes.
these classes. The set of candidates CSc for a class c was
obtained according to Equation 3. NNd(p, s, DS) searches
were executed efﬁciently using the MI-File populated with
all objects in the dataset according to distance d. The number
s of nearest neighbors to each support vector ranged from 10
to 100, that is respectively 100.000 and 10.000 times smaller
than the size of the dataset. Approximate Top-k(DS, c) was
executed computing Top-k(CSc, c) for various values of k
ranging from 10 up to 200.
VIII. ANALYSIS OF THE EXPERIMENTS
It is important to stress that this paper does not propose a
new classiﬁcation technique. Rather, given an SVM classiﬁer
built using standard tools, we propose a technique to perform
top-k classiﬁcation much faster than exhaustively classify
all objects of a huge dataset. In this respect, given a classi-
ﬁer, our experiments aim at comparing the techniques that
we propose, for efﬁcient approximate top-k classiﬁcation,
against the exhaustive solution for top-k classiﬁcation, which
solve the top-k problem by sequentially and systematically
classifying all objects of the dataset.
The evaluation of the quality of the top-k classiﬁcation
results consists of two parts: 1) an objective and quantitative
evaluation of the error introduced by the use of the approx-
imate classiﬁcation and 2) a subjective evaluation based on
real user feedback.
Both evaluations required to perform an exhaustive clas-
siﬁcation (sequential classiﬁcation of the entire dataset), that
was compared with the proposed approximate technique.
The objective evaluation was carried out by computing the
measures of recall and error on the position [6]. More pre-
cisely, given a class c, the recall at k, Rk, is the percentage
of the best k objects retrieved by the approximate method
that also appear in the best k identiﬁed by the exhaustive
classiﬁcation as belonging to c.
The error in the position at k (EPk) measures the quality
of the ranking obtained by the approximate method with
respect to the exhaustive one. It gives the average shifting
of elements in the rank in percentage with respect to the
size of the dataset.
More formally, recall at k is
Rk = #(Sk ∩ SA
k )
#Sk
(5)
and the error on position at k is
EPk =
∑
o∈SA
k |OX(o) − SA
k (o)|
#SA
k · #X
,
(6)
where Sk and SA
k are the k best matches to c found
respectively by the exhaustive classiﬁcation of the entire
dataset and by the our approximate method. OX is the
ordering of the entire dataset X with respect to the decision
function fc for class c. For example, if o1 is the most
appropriate object that belongs to the class c, o2 is the
second and o3 is the third, OX(o1) = 1, OX(o2) = 2
and OX(o3) = 3. SA
k (o) is the position of o in the rank of
k best matches found by the approximate classiﬁcation.
The subjective evaluation, based on user feedbacks was
performed by asking 5 students to blindly judge the results
obtained with the exhaustive and approximate classiﬁcation.
To compare the two results we used the precision at k
measure deﬁned as follows:
Pk = #(Sk ∩ Sc)
#Sk
(7)
where Sk is the result obtained by either the approximate
or the exhaustive classiﬁcation method, and Sc is the set of
images correctly classiﬁed for c. Sk ∩ Sc was obtained by
asking the users to select the correct results in Sk (blindly
for exhaustive and approximate classiﬁcation). Precision at
k tells us the percentage of the k retrieved elements that
belong to the class c according to the user judgement.
A. Approximate vs exhaustive classiﬁcation
We ﬁrst performed experiments to see how, according
to Equation 3, the choice of the number s of retrieved
nearest neighbor dataset objects to a support vector affects
the quality of the approximation. Results are reported in
Figure 1. We varied s from 10 to 100. For brevity, in the
Figure we report results just for Pyramids. However, similar
considerations can be made for the other classes. We can see
that, in the chosen range of s, recall increases with s and
saturates when s is around 90. On the other hand, the error
59
MMEDIA 2011 : The Third International Conferences on Advances in Multimedia
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-129-8

0
0,1
0,2
0,3
0,4
0,5
0,6
0,7
0,8
0,9
1
0
50
100
150
200
Precision
k
Churches
Exact
Appr.
0
0,1
0,2
0,3
0,4
0,5
0,6
0,7
0,8
0,9
1
0
50
100
150
200
Precision
k
Temples
Exact
Appr.
0
0,1
0,2
0,3
0,4
0,5
0,6
0,7
0,8
0,9
1
0
50
100
150
200
Precision
k
Pyramids
Exact
Appr.
0
0,1
0,2
0,3
0,4
0,5
0,6
0,7
0,8
0,9
1
0
50
100
150
200
Precision
k
Painting
Exact
Appr.
0
0,1
0,2
0,3
0,4
0,5
0,6
0,7
0,8
0,9
1
0
50
100
150
200
Precision
k
Seascapes
Exact
Appr.
Figure 2.
Precision of the approximate Top-k classiﬁcation and the exact Top-k classiﬁcation, for various values of k, as judged by users.







	













	
	







Figure 3.
Recall and EP of the approximate top-k classiﬁcation for
several classes as queries, when s is ﬁxed to 100, for various values of
k, considering the exact top-k classiﬁcation the ground truth.
on position decreases rapidly when s increases. Very small
error values are already obtained when s is 60. This means
that, recall increases and missed objects are also substituted
by better objects when s increases.
Let us now discuss the quality of the approximate classi-
ﬁcation when s is ﬁxed to 100, and we vary the number k of
objects retrieved for a class. Results of these experiments are
shown in Figure 3. For each class considered (i.e. churches,
temples, pyramids, paintings, and seascapes) we plot the
recall and the EP vs the number of best matches k to a class.
We note that both recall and EP are inﬂuenced by k: worse
results are typically obtained when k increases. However,
we can see that the reduction of the recall is in many cases
minimal with respect to the increase of k, while the increase
of the error on the position is more evident. This means that
on average the approximate classiﬁcation strategy is always
able to ﬁnd the same percentage of correct objects (almost
stable recall), even if missed correct objects are replaced by
worse objects (worsening EP). For instance, let us consider
the class pyramid. Recall is around 0.9 for k = 10 and it
goes to 0.75 when k = 200. That is, 1 out of 10 images is
missed when we retrieve 10 objects, while a bit less than 3
out of 10 images are missed when we retrieve 200. The error
in position is almost 0 when k = 10 and it is also negligible
until k = 100. Thus, the ordering is practically maintained in
the approximate result. When k increases more, the quality
of ranking degrades. For instance, in case of k = 200, the
error in position is about 0.00003. That is with a dataset of
1 million objects the average shift was of 30 positions, with
respect to the exact rank.
We should also consider that the time required to perform
exhaustive classiﬁcation of the entire dataset, for a given
class was 39 minutes, on average. Good approximate clas-
siﬁcation of the same dataset can be obtained on average in
1.5 minutes,thus the approximate classiﬁcation is more than
one order of magnitude faster than exhaustive classiﬁcation.
B. User evaluation
Results discussed above were obtained comparing ap-
proximate classiﬁcation against exhaustive classiﬁcation al-
gorithms, using the exhaustive classiﬁcation as a ground
truth. However, generally even the exhaustive classiﬁcation
presents some imprecisions, which can be evaluated when
users are called to judge the result, or by using real ground
truths. As we will see in the following, surprisingly, users
60
MMEDIA 2011 : The Third International Conferences on Advances in Multimedia
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-129-8

do not see much difference between exact and approximate
results. This means that the degradation from exact to
approximate classiﬁcation is purely mathematical, and it is
not signiﬁcantly perceived by users.
The test discussed in this section evaluates the difference
of quality between the exhaustive top-k execution and the
approximate top-k execution, as perceived by real users. To
obtain this, we asked 5 students to blindly judge the results,
obtained by the exhaustive and approximate classiﬁcation,
by selecting the good and the wrong images. Based on this,
we computed the average precision using Equation 7. Results
are shown in Figure 2 for various choices of the number of
best matches k.
It can be seen that generally there is no signiﬁcant
difference between the precision of the exhaustive and the
approximate classiﬁcation, even thought the approximate
classiﬁcation is much faster. In fact, precision measured for
the exhaustive and approximate classiﬁcation has practically
the same trend when k varies. In addition, generally the
difference in precision, between the exhaustive and the
approximate classiﬁcation, is smaller than 10%.
A separate discussion is needed for the Seascape classiﬁer.
In the experiments discussed in previous section, results for
the Seascape class were worse than all the other classes
(see Figure 3). In fact, recall was always below 0.4. On the
other hand, the user perceived precision of the approximate
classiﬁcation is very high and always above 0.8. It can
be seen, also, that the exhaustive classiﬁcation has also a
precision above 0.9 in most cases. When the approximate
classiﬁcation is used, missed images are always substituted
by other images that are deemed to be still good by human
evaluators, offering an high precision. Therefore, approxi-
mation makes sense also in this case.
It is also worth mentioning that in one case of the tested
classes, the approximate classiﬁcation performed even better
than the exhaustive one. In fact, it can be seen that for the
Pyramids class, the curve of the approximate classiﬁcation
is always higher than that of the exhaustive one. This, we
believe, is a further proof that no signiﬁcant information is
actually lost during the approximation: the lost information
is mainly noisy information.
IX. CONCLUSIONS
Science is becoming data-dominated. New data-intensive
computing paradigms are emerging that differ from the tra-
ditional techniques, where Big Data was not a fundamental
issue [14]. We have presented an approximate technique
for efﬁciently executing top-k classiﬁcation tasks on very
large datasets. The proposed technique is some orders of
magnitude faster with respect to exhaustive classiﬁcation and
the accuracy of approximate results is very high.
The peculiarity of the proposed technique is that it is able
to use one single index built in the input space to support top-
k classiﬁcation tasks on several classes deﬁned using various
kernels and kernel parameters. We discussed the properties
that the kernel has to satisfy so that they can be used with
the proposed technique and we have seen that many widely
used kernels are in fact included.
ACKNOWLEDGEMENTS
This work was partially supported by the VISITO Tus-
cany POR CREO FESR 2007-2013 project, funded by the
Regione Toscana.
REFERENCES
[1] N. Cristianini and J. Shawe-Taylor,
An Introduction to
Support Vector Machines and Other Kernel-based Learning
Methods, Cambridge University Press, March 2000.
[2] A. Qamra and E. Y. Chang, “Using pivots to index for support
vector machine queries,” in CVDB ’05, New York, NY, USA,
2005, pp. 59–64, ACM.
[3] N-Panda and E. Y. Chang, “Exploiting geometry for support
vector machine indexing,” in Proceedings of SIAM Interna-
tional Data Mining Conference, SDM, 2005, pp. 322–333.
[4] S. Litayem, A. Joly, and N. Boujemaa, “Interactive objects
retrieval with efﬁcient boosting,”
in Proceedings of ACM
Multimedia, 2009, pp. 545–548.
[5] M. Crucianu, D. Estevez, V. Oria, and J.P. Tarel, “Speeding
up active relevance feedback with approximate knn retrieval
for hyperplane queries,” Int. J. Imaging Syst. Technol., vol.
18, no. 2-3, pp. 150–159, 2008.
[6] P. Zezula, G. Amato, V. Dohnal, and M. Batko, Similarity
Search - The Metric Space Approach, vol. 32 of Advances in
Database Systems, Springer, 2006.
[7] H. Samet, Foundations of Multidimensional and Metric Data
Structures, Morgan Kaufmann Publishers Inc., San Francisco,
CA, USA, 2005.
[8] K. S. Beyer, J. Goldstein, R. Ramakrishnan, and U. Shaft,
“When is ”nearest neighbor” meaningful?,” in ICDT ’99, Pro-
ceedings. 1999, vol. 1540 of LNCS, pp. 217–235, Springer.
[9] S. Boughorbel, J.P. Tarel, and N. Boujemaa, “Conditionally
positive deﬁnite kernels for svm based image recognition,” in
IEEE ICME 2005. July 2005, pp. 113–116, IEEE.
[10] C. Cortes, P. Haffner, and M. Mohri,
“Rational kernels:
Theory and algorithms,” J. Mach. Learn. Res., vol. 5, pp.
1035–1062, 2004.
[11] P. Bolettieri, A. Esuli, F. Falchi, et al., “Enabling content-
based image retrieval in very large digital libraries,” in Second
Workshop on VLDL, 2009, pp. 43–50.
[12] G. Aamato and P. Savino, “Approximate similarity search in
metric spaces using inverted ﬁles,” in InfoScale ’08. 2008,
pp. 1–10, ICST.
[13] M. Batko, F. Falchi, C. Lucchese, et al., “Building a web-
scale image similarity search system,” Multimedia Tools and
Applications, vol. 47, no. 3, pp. 599–629, 2009.
[14] T. Hey, S. Tansley, and K. Tolle, Eds., The Fourth Paradigm
- Data Intensive Scientiﬁc Discovery, Microsoft Res., 2009.
61
MMEDIA 2011 : The Third International Conferences on Advances in Multimedia
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-129-8

