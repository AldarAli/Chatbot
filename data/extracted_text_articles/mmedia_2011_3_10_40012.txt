Indexing Support Vector Machines for EfÔ¨Åcient top-k ClassiÔ¨Åcation
Giuseppe Amato, Paolo Bolettieri, Fabrizio Falchi, Fausto Rabitti, Pasquale Savino
ISTI-CNR
Pisa, Italy
Email: g.amato@isti.cnr.it, p.bolettieri@isti.cnr.it, f.falchi@isti.cnr.it, f.rabitti@isti.cnr.it, p.savino@isti.cnr.it
Abstract‚ÄîThis paper proposes an approach to efÔ¨Åciently
execute approximate top-k classiÔ¨Åcation (that is, identifying the
best k elements of a class) using Support Vector Machines,
in web-scale datasets, without signiÔ¨Åcant loss of effectiveness.
The novelty of the proposed approach, with respect to other
approaches in literature, is that it allows speeding-up several
classiÔ¨Åers, each one deÔ¨Åned with different kernels and kernel
parameters, by using one single index.
Keywords-Image ClassiÔ¨Åcation; Support Vector Machines;
Similarity Searching.
I. INTRODUCTION
The classiÔ¨Åcation problem is typically deÔ¨Åned as follows.
Given a set of classes c1, . . . , cn, and an object o, the
classiÔ¨Åcation problem is to decide which classes o belongs
to. In this paper, on the other hand, we address the classi-
Ô¨Åcation problem from an Information Retrieval perspective.
Let c1, . . . , cn be n classes, and DS a very large dataset
(for instance the World Wide Web) of unclassiÔ¨Åed objects.
Given a class ci, we want to retrieve the k objects of DS
having the highest likelihood to belong to ci. In this case,
the class ci can be considered as a query, and the best k
objects that belong to ci as the answer to the query. We
call top-k classiÔ¨Åcation this formulation of the classiÔ¨Åcation
problem.
Support Vector Machines (SVMs) [1], are widely used to
perform automatic supervised classiÔ¨Åcation. The aim of this
paper is to propose a strategy that, given a set of SVM
classiÔ¨Åers deÔ¨Åned for a set of classes c1, . . . , cn, and a
dataset DS, executes top-k classiÔ¨Åcation very efÔ¨Åciently.
More speciÔ¨Åcally, we do not address the problem of learn-
ing classiÔ¨Åers, for which several solutions already exist as
mentioned in Section III. Rather, given an SVM classiÔ¨Åer
for any class ci, and a dataset DS, our aim is to efÔ¨Åciently
search in DS for the best k objects that belong to ci.
As we will see, we propose an approximated approach.
That is, our approach returns an imprecise result, compared
to the result that would have been obtained by performing
a sequential scan of the entire dataset DS and applying
the available classiÔ¨Åers to every object. However, the ex-
periments will show a small imprecision, compared to an
improvement of efÔ¨Åciency of orders of magnitude.
The rest of the paper is organized as follows. First we
discuss related work. Next we brieÔ¨Çy introduce the SVM. In
Sections IV and V, we present the top-k classiÔ¨Åcation, while
in Section VI, the index structure used in the experiments.
Finally in Sections VII and VIII, we describe the settings of
the experiments and the analysis of the results.
II. RELATED WORK
EfÔ¨Åcient top-k classiÔ¨Åcation techniques were proposed in
[2], [3], by leveraging on the property that instances in the
feature space lie on a hypersphere. This approach is able
to use one index for various classes obtained using Support
Vector Machines and built using the same kernel. In [4], the
authors propose a method for efÔ¨Åcient top-k classiÔ¨Åcation
based on boosting. In [5], the authors propose an efÔ¨Åcient
method for retrieving the instances closest to the separating
hyperplane (the most ambiguous instances) to support active
relevance feedback.
The novelty of the proposed approach, with respect to
other methods existing in literature, is that it allows sup-
porting and speeding-up the use of several classiÔ¨Åers, each
one deÔ¨Åned with different kernels and/or kernel parameters,
by using one single index in the input space 1 of the dataset.
III. INTRODUCTION TO SVM
An SVM [1] builds classiÔ¨Åers by learning from a training
set that is composed of both positive and negative examples.
In many cases, in order to be able to separate element that
belong to the class from those that do not belong to the class,
it is convenient to map vectors, representing elements, in an
higher dimensional vector space using a mapping function
Œ¶(¬∑). Omitting several theoretical details (see [1] for more
information), the learning phase determines a vector œâ such
that the decision function
f(o) =< œâ, Œ¶(o) > +b
(1)
is able to optimally classify most of the training set
examples ( < œâ, Œ¶(o) > is the dot product between vectors
œâ and Œ¶(o)). When the decision function is positive it
indicates that an object belongs to the class. A popular
learning algorithm is the kernel-based version of the adatron
algorithm.
The SVM literature often call input space the space where
objects are deÔ¨Åned, and feature space the space where
1the space in which objects are originally represented (see Section III).
56
MMEDIA 2011 : The Third International Conferences on Advances in Multimedia
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-129-8

objects are mapped by Œ¶(¬∑). We will use this terminology
in the reminder of the paper.
SVM methods do not deÔ¨Åne the mapping function ex-
plicitly, but use the properties of the kernel functions to
perform learning and classiÔ¨Åcation. A kernel function K,
deÔ¨Åned as K(oi, oj) =< Œ¶(oi)T , Œ¶(oj) >, computes the
dot-product of oi and oj in the feature space. There are
simple kernel functions that easily compute the dot-product
of objects mapped in very high or even inÔ¨Ånite dimensional
spaces without even knowing the actual mapping functions.
It can be proven [1] that the kernel-based decision func-
tion deÔ¨Åned above, can be also represented in the dual form
f(o) =
‚àë
(oi,yi)‚ààT
yiŒ±iK(o, oi) + b
(2)
where oi are the element of the training set and yi is 1
or to ‚àí1 according to the fact that the training object oi
is a positive or a negative sample of the class to learn. In
this formulation, the learning phase consists in Ô¨Ånding the
parameters Œ±i, which basically determine the contribution
of each example oi of the training set to the solution of the
learning problem, rather than the vector œâ. Most of the Œ±i,
obtained in the training phase, will be equal to 0. So, in
order to compute the decision function f, we only need to
maintain the training objects for which the Œ±i are greater
than 0. These objects are the support vectors.
IV. TOP-k CLASSIFICATION
Let fc be a decision function deÔ¨Åned according to Equa-
tion 2 for the class c. The value fc(o) indicates the degree
of membership of the object o to the class c. Large positive
values indicate high membership of o to c; large negative
values indicate that o does not belong to c; values close to
zero indicate uncertainty.
The top-k classiÔ¨Åcation problem can also be formulated
as follows. Given a decision function fc and a dataset DS,
retrieve the k objects o1, . . . , ok in DS for which fc(oi), i =
1 . . . k, is larger than when applied to any other object in
DS. More formally:
DeÔ¨Ånition: 1: Let DS be a dataset of objects, c a class,
and fc the decision function for c. We deÔ¨Åne
top‚Äìk(DS, c) ={o1, . . . , ok ‚àà DS |
‚àÄo ‚àà (DS\{o1, . . . , ok}),
fc(o) ‚â§ fc(oi), i = 1, . . . , k}
A. Approximate top-k classiÔ¨Åcation
Clearly, top‚àík(DS, c) can be computed with a sequential
scan of the whole dataset. However, this is very inefÔ¨Åcient
when DS is very large. Suppose we have a set of candidates
CS ‚äÜ DS for class c. Then, top-k(CS, c) is an approxima-
tion of top-k(DS, c). However, consider that if CS is chosen
carefully, top-k(CS, c) will not necessarily differ very much
from top-k(DS, c). For instance, if CS = top-k(DS, c),
then top-k(CS, c) =top-k(DS, c). According to this, given
CS, approximate top-k classiÔ¨Åcation can be performed by
applying the decision function to the objects of CS rather
than all objects in DS. Provided that CS is much smaller
that DS (#CS ‚â™ #DS2), this process will be much more
efÔ¨Åcient than exhaustively classifying all objects in DS.
In the next section, we will propose a strategy to obtain
CS, by using techniques of nearest neighbors searching, in
such a way that approximation will be highly accurate and
CS is much smaller than DS.
V. TOP-k CLASSIFICATION BY MEANS OF NEAREST
NEIGHBORS SEARCH
The training set Tc for a class c consists of positive
and negative examples. Let us denote as PTc and NTc
respectively the positive and negative training objects (Tc =
PTc
‚à™ NTc). As discussed in Section III, the learning phase
identiÔ¨Åes the Œ±s parameters for the decision function, and
implicitly the support vectors (the training vectors whose Œ±i
are strictly greater than 0). Let us denote as PSVc ‚äÜ PTc
and NSVc ‚äÜ NTc respectively the positive and negative
support vectors identiÔ¨Åed after the training of the classiÔ¨Åer
for a class c. The decision function given by Equation 2 can
be rewritten as
fc(o) =
‚àë
p‚ààP SVc
Œ±pK(o, p) ‚àí
‚àë
n‚ààNSVc
Œ±nK(o, n) + b
The formula above just uses the support vectors and
disregards the elements of the training set whose Œ±s are
0, since they do not provide any contribution to fc.
From the deÔ¨Ånition of fc given above, it is easy to see that
the objects o of the dataset that are very similar, according
to K, to several positive support vectors and are dissimilar
to several negative support vectors, have higher chances to
return an high value when fc is applied to them. In fact,
the kernel K can be seen as a similarity function. That
is, K returns large values when the two compared objects
are similar and small values when the two objects are not
similar. This suggests a strategy to select from DS a subset
of promising candidates for c. In short, we can Ô¨Årst search
the objects of the dataset that are closer, according to K to
each positive support vector. Then, we apply the decision
function fc only to the selected candidates to Ô¨Ånd the best
k matches.
More formally, the candidate set CSc ‚äÜ DS for class c
can be obtained as
CSc =
‚à™
p‚ààP SVc
NNK(p, s, DS)
(3)
where NNK(p, s, DS), is a nearest neighbors query,
which returns the s objects of DS most similar to p,
2we use # to indicate the cardinality of a set
57
MMEDIA 2011 : The Third International Conferences on Advances in Multimedia
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-129-8

according to kernel K, for some s much smaller than the
size of DS.
The selection of the k best objects matching the class c
can now be obtained by applying fc only to objects in CSs,
which is signiÔ¨Åcantly smaller than DS.
VI. USING AN INDEX STRUCTURE IN THE INPUT SPACE
Several scalable techniques can be found in literature to
efÔ¨Åciently process nearest neighbors search queries [6], [7].
However, even if, leveraging on these techniques, Equation
3 can be computed very efÔ¨Åciently, there are two reasons
why it is not practical to build an index using K directly
(that is in the feature space):
1) Access methods for similarity search typically rely on
the fact that the similarity (the kernel K in our case)
can be expressed in terms of a distance (dissimilarity)
function, which should satisfy some speciÔ¨Åc proper-
ties, like for instance the metric postulates. A distance
function can always be derived from a kernel K.
However, such function, in many cases is not suitable
to be used to build an efÔ¨Åcient index structure. In fact,
given that the kernel K compares elements in an high
dimensional feature space, the underlying distribution
of distances might not be convenient and we might
incur in the curse of dimensionality [8].
2) We would like to support several classiÔ¨Åers for the
same dataset, each recognizing a different class. Dif-
ferent classiÔ¨Åers might require different kernels and
different kernel parameters (that is, different similarity
functions). If we succeed to Ô¨Ånd a suitable distance
function for a certain kernel K and we create an index
with this distance, we are bound to the speciÔ¨Åc kernel
K and its kernel parameters (for instance different œÉs
in the case of the RBF kernel), so we are bound to
a speciÔ¨Åc classiÔ¨Åer. To support several classiÔ¨Åers, we
would need several indexes, each for a speciÔ¨Åc kernel
and kernel parameters.
Next section shows how to solve the above problems by
building one single index in the input space to serve various
kernels, provided that they satisfy some conditions.
A. Kernels that allow using a single index in the input space
Instead of deriving a distance function from a kernel,
in many cases it is possible deÔ¨Åne a kernel in terms of a
convenient distance function as follows:
K(o1, o2) = g(d(o1, o2))
(4)
where d : D √ó D ‚Üí R is a distance function between
objects of the input space D.
Many widely used kernels can be deÔ¨Åned using Equation
4. For instance, if d is the Euclidean distance (L2), with
g(x) = e‚àí x2
2œÉ2 we obtain the RBF kernel; with g = ‚àíxŒ≤,
for Œ≤ > 0, we obtain the power kernel; when d is the L1
distance, with g(x) = e‚àíŒ≥x we obtain the Laplacian kernel.
Other examples exist.
Let R = d(D, D), R ‚äÜ R, that is R is the set of possible
values that d can give for any arbitrary pair o1, o2 ‚àà D.
If g is monotonously decreasing over R, then the k objects
closest to o, with respect to d, are the k objects most similar
to o in the feature space, with respect to K.
In other words, given any kernel K deÔ¨Åned accord-
ing to Equation 4, with a monotonous decreasing g and
the same distance d,we have that3 NNK(p, s, DS)
=
NNd(p, s, DS).
This implies that the most similar objects to a support
vector in the feature space induced by K, are exactly the
objects closest to the same support vector, in the input space.
Therefore, we can use just one single access method deÔ¨Åned
using d and built in the input space, to search for the nearest
neighbors in the feature space induced by a large class of
kernels deÔ¨Åned in terms of d. This, as discussed in Section
V, also gives us the possibility of identifying the subset of
promising candidates just working in the input space, for the
same class of kernels4.
Note also that the gs, that produce the RBF, Laplacian,
and power kernels, are all monotonously decreasing in R+.
Therefore, in this case, any d, which always returns positive
values, allows this technique to be used.
VII. EXPERIMENT SETTINGS
Tests of the proposed techniques were executed on a
single 2.4GHz Core 2 Quad CPU, using the CoPhIR dataset
[11]. CoPhIR consists of 106 millions images, taken from
Flickr, described by MPEG-7 visual descriptors. In the tests
we used the Ô¨Årst set of one millions images taken from
CoPhIR. The access method used to efÔ¨Åciently search for
objects close to the support vectors, in the input space,
is the MI-File [12] (Metric Inverted File). The MI-File is
a disk maintained index, based on inverted Ô¨Åles, which
supports efÔ¨Åcient and scalable approximate similarity search
on data represented in metric spaces. To deÔ¨Åne the kernel
for the support vector machine, according to Equation 4,
we used g(x) = e‚àí x2
2œÉ2 , and as d we used a combination
of MPEG-7 distance functions [13]. We trained the support
vector machine to recognize 5 different classes: churches,
pyramids, seascapes, paintings, and temples. We used a
standard kernel-based adatron with cross-validation, to learn
3Here we abuse with the notation, NNK gives the most similar, while
NNd gives the closest ones.
4Note that kernels used with SVM must be positive deÔ¨Ånite [1] or
conditionally positive deÔ¨Ånite [9]. Therefore when a kernel K is obtained
from Equation 4, we must Ô¨Årst prove this. However, in many common
cases this is true. Consider that, [10], in Theorem 12, shows that when
g = e‚àítx, for all t > 0, K is positive deÔ¨Ånite iff d is negative deÔ¨Ånite
and symmetric. Note that the Eucledian distance is negative deÔ¨Ånite and
symmetric. In fact, given that the RBF Kernel is positive deÔ¨Ånite, then d2,
when d is the Eucledian distance, is negative deÔ¨Ånite and symmetric and,
as consequence of Theorem 11 in [10], also d (the Eucledian distance) is
negative deÔ¨Ånite and symmetric.
58
MMEDIA 2011 : The Third International Conferences on Advances in Multimedia
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-129-8

Figure 1. Quality of the of the approximate Top-k classiÔ¨Åcation varying the
number of nearest neighbors (s) retrieved for each support vector, according
to Equation 3, for various values of k. Here, for brevity, we just show the
results when the query is class Pyramids. Similar results were obtained for
the other classes.
these classes. The set of candidates CSc for a class c was
obtained according to Equation 3. NNd(p, s, DS) searches
were executed efÔ¨Åciently using the MI-File populated with
all objects in the dataset according to distance d. The number
s of nearest neighbors to each support vector ranged from 10
to 100, that is respectively 100.000 and 10.000 times smaller
than the size of the dataset. Approximate Top-k(DS, c) was
executed computing Top-k(CSc, c) for various values of k
ranging from 10 up to 200.
VIII. ANALYSIS OF THE EXPERIMENTS
It is important to stress that this paper does not propose a
new classiÔ¨Åcation technique. Rather, given an SVM classiÔ¨Åer
built using standard tools, we propose a technique to perform
top-k classiÔ¨Åcation much faster than exhaustively classify
all objects of a huge dataset. In this respect, given a classi-
Ô¨Åer, our experiments aim at comparing the techniques that
we propose, for efÔ¨Åcient approximate top-k classiÔ¨Åcation,
against the exhaustive solution for top-k classiÔ¨Åcation, which
solve the top-k problem by sequentially and systematically
classifying all objects of the dataset.
The evaluation of the quality of the top-k classiÔ¨Åcation
results consists of two parts: 1) an objective and quantitative
evaluation of the error introduced by the use of the approx-
imate classiÔ¨Åcation and 2) a subjective evaluation based on
real user feedback.
Both evaluations required to perform an exhaustive clas-
siÔ¨Åcation (sequential classiÔ¨Åcation of the entire dataset), that
was compared with the proposed approximate technique.
The objective evaluation was carried out by computing the
measures of recall and error on the position [6]. More pre-
cisely, given a class c, the recall at k, Rk, is the percentage
of the best k objects retrieved by the approximate method
that also appear in the best k identiÔ¨Åed by the exhaustive
classiÔ¨Åcation as belonging to c.
The error in the position at k (EPk) measures the quality
of the ranking obtained by the approximate method with
respect to the exhaustive one. It gives the average shifting
of elements in the rank in percentage with respect to the
size of the dataset.
More formally, recall at k is
Rk = #(Sk ‚à© SA
k )
#Sk
(5)
and the error on position at k is
EPk =
‚àë
o‚ààSA
k |OX(o) ‚àí SA
k (o)|
#SA
k ¬∑ #X
,
(6)
where Sk and SA
k are the k best matches to c found
respectively by the exhaustive classiÔ¨Åcation of the entire
dataset and by the our approximate method. OX is the
ordering of the entire dataset X with respect to the decision
function fc for class c. For example, if o1 is the most
appropriate object that belongs to the class c, o2 is the
second and o3 is the third, OX(o1) = 1, OX(o2) = 2
and OX(o3) = 3. SA
k (o) is the position of o in the rank of
k best matches found by the approximate classiÔ¨Åcation.
The subjective evaluation, based on user feedbacks was
performed by asking 5 students to blindly judge the results
obtained with the exhaustive and approximate classiÔ¨Åcation.
To compare the two results we used the precision at k
measure deÔ¨Åned as follows:
Pk = #(Sk ‚à© Sc)
#Sk
(7)
where Sk is the result obtained by either the approximate
or the exhaustive classiÔ¨Åcation method, and Sc is the set of
images correctly classiÔ¨Åed for c. Sk ‚à© Sc was obtained by
asking the users to select the correct results in Sk (blindly
for exhaustive and approximate classiÔ¨Åcation). Precision at
k tells us the percentage of the k retrieved elements that
belong to the class c according to the user judgement.
A. Approximate vs exhaustive classiÔ¨Åcation
We Ô¨Årst performed experiments to see how, according
to Equation 3, the choice of the number s of retrieved
nearest neighbor dataset objects to a support vector affects
the quality of the approximation. Results are reported in
Figure 1. We varied s from 10 to 100. For brevity, in the
Figure we report results just for Pyramids. However, similar
considerations can be made for the other classes. We can see
that, in the chosen range of s, recall increases with s and
saturates when s is around 90. On the other hand, the error
59
MMEDIA 2011 : The Third International Conferences on Advances in Multimedia
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-129-8

0
0,1
0,2
0,3
0,4
0,5
0,6
0,7
0,8
0,9
1
0
50
100
150
200
Precision
k
Churches
Exact
Appr.
0
0,1
0,2
0,3
0,4
0,5
0,6
0,7
0,8
0,9
1
0
50
100
150
200
Precision
k
Temples
Exact
Appr.
0
0,1
0,2
0,3
0,4
0,5
0,6
0,7
0,8
0,9
1
0
50
100
150
200
Precision
k
Pyramids
Exact
Appr.
0
0,1
0,2
0,3
0,4
0,5
0,6
0,7
0,8
0,9
1
0
50
100
150
200
Precision
k
Painting
Exact
Appr.
0
0,1
0,2
0,3
0,4
0,5
0,6
0,7
0,8
0,9
1
0
50
100
150
200
Precision
k
Seascapes
Exact
Appr.
Figure 2.
Precision of the approximate Top-k classiÔ¨Åcation and the exact Top-k classiÔ¨Åcation, for various values of k, as judged by users.







	













	
	







Figure 3.
Recall and EP of the approximate top-k classiÔ¨Åcation for
several classes as queries, when s is Ô¨Åxed to 100, for various values of
k, considering the exact top-k classiÔ¨Åcation the ground truth.
on position decreases rapidly when s increases. Very small
error values are already obtained when s is 60. This means
that, recall increases and missed objects are also substituted
by better objects when s increases.
Let us now discuss the quality of the approximate classi-
Ô¨Åcation when s is Ô¨Åxed to 100, and we vary the number k of
objects retrieved for a class. Results of these experiments are
shown in Figure 3. For each class considered (i.e. churches,
temples, pyramids, paintings, and seascapes) we plot the
recall and the EP vs the number of best matches k to a class.
We note that both recall and EP are inÔ¨Çuenced by k: worse
results are typically obtained when k increases. However,
we can see that the reduction of the recall is in many cases
minimal with respect to the increase of k, while the increase
of the error on the position is more evident. This means that
on average the approximate classiÔ¨Åcation strategy is always
able to Ô¨Ånd the same percentage of correct objects (almost
stable recall), even if missed correct objects are replaced by
worse objects (worsening EP). For instance, let us consider
the class pyramid. Recall is around 0.9 for k = 10 and it
goes to 0.75 when k = 200. That is, 1 out of 10 images is
missed when we retrieve 10 objects, while a bit less than 3
out of 10 images are missed when we retrieve 200. The error
in position is almost 0 when k = 10 and it is also negligible
until k = 100. Thus, the ordering is practically maintained in
the approximate result. When k increases more, the quality
of ranking degrades. For instance, in case of k = 200, the
error in position is about 0.00003. That is with a dataset of
1 million objects the average shift was of 30 positions, with
respect to the exact rank.
We should also consider that the time required to perform
exhaustive classiÔ¨Åcation of the entire dataset, for a given
class was 39 minutes, on average. Good approximate clas-
siÔ¨Åcation of the same dataset can be obtained on average in
1.5 minutes,thus the approximate classiÔ¨Åcation is more than
one order of magnitude faster than exhaustive classiÔ¨Åcation.
B. User evaluation
Results discussed above were obtained comparing ap-
proximate classiÔ¨Åcation against exhaustive classiÔ¨Åcation al-
gorithms, using the exhaustive classiÔ¨Åcation as a ground
truth. However, generally even the exhaustive classiÔ¨Åcation
presents some imprecisions, which can be evaluated when
users are called to judge the result, or by using real ground
truths. As we will see in the following, surprisingly, users
60
MMEDIA 2011 : The Third International Conferences on Advances in Multimedia
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-129-8

do not see much difference between exact and approximate
results. This means that the degradation from exact to
approximate classiÔ¨Åcation is purely mathematical, and it is
not signiÔ¨Åcantly perceived by users.
The test discussed in this section evaluates the difference
of quality between the exhaustive top-k execution and the
approximate top-k execution, as perceived by real users. To
obtain this, we asked 5 students to blindly judge the results,
obtained by the exhaustive and approximate classiÔ¨Åcation,
by selecting the good and the wrong images. Based on this,
we computed the average precision using Equation 7. Results
are shown in Figure 2 for various choices of the number of
best matches k.
It can be seen that generally there is no signiÔ¨Åcant
difference between the precision of the exhaustive and the
approximate classiÔ¨Åcation, even thought the approximate
classiÔ¨Åcation is much faster. In fact, precision measured for
the exhaustive and approximate classiÔ¨Åcation has practically
the same trend when k varies. In addition, generally the
difference in precision, between the exhaustive and the
approximate classiÔ¨Åcation, is smaller than 10%.
A separate discussion is needed for the Seascape classiÔ¨Åer.
In the experiments discussed in previous section, results for
the Seascape class were worse than all the other classes
(see Figure 3). In fact, recall was always below 0.4. On the
other hand, the user perceived precision of the approximate
classiÔ¨Åcation is very high and always above 0.8. It can
be seen, also, that the exhaustive classiÔ¨Åcation has also a
precision above 0.9 in most cases. When the approximate
classiÔ¨Åcation is used, missed images are always substituted
by other images that are deemed to be still good by human
evaluators, offering an high precision. Therefore, approxi-
mation makes sense also in this case.
It is also worth mentioning that in one case of the tested
classes, the approximate classiÔ¨Åcation performed even better
than the exhaustive one. In fact, it can be seen that for the
Pyramids class, the curve of the approximate classiÔ¨Åcation
is always higher than that of the exhaustive one. This, we
believe, is a further proof that no signiÔ¨Åcant information is
actually lost during the approximation: the lost information
is mainly noisy information.
IX. CONCLUSIONS
Science is becoming data-dominated. New data-intensive
computing paradigms are emerging that differ from the tra-
ditional techniques, where Big Data was not a fundamental
issue [14]. We have presented an approximate technique
for efÔ¨Åciently executing top-k classiÔ¨Åcation tasks on very
large datasets. The proposed technique is some orders of
magnitude faster with respect to exhaustive classiÔ¨Åcation and
the accuracy of approximate results is very high.
The peculiarity of the proposed technique is that it is able
to use one single index built in the input space to support top-
k classiÔ¨Åcation tasks on several classes deÔ¨Åned using various
kernels and kernel parameters. We discussed the properties
that the kernel has to satisfy so that they can be used with
the proposed technique and we have seen that many widely
used kernels are in fact included.
ACKNOWLEDGEMENTS
This work was partially supported by the VISITO Tus-
cany POR CREO FESR 2007-2013 project, funded by the
Regione Toscana.
REFERENCES
[1] N. Cristianini and J. Shawe-Taylor,
An Introduction to
Support Vector Machines and Other Kernel-based Learning
Methods, Cambridge University Press, March 2000.
[2] A. Qamra and E. Y. Chang, ‚ÄúUsing pivots to index for support
vector machine queries,‚Äù in CVDB ‚Äô05, New York, NY, USA,
2005, pp. 59‚Äì64, ACM.
[3] N-Panda and E. Y. Chang, ‚ÄúExploiting geometry for support
vector machine indexing,‚Äù in Proceedings of SIAM Interna-
tional Data Mining Conference, SDM, 2005, pp. 322‚Äì333.
[4] S. Litayem, A. Joly, and N. Boujemaa, ‚ÄúInteractive objects
retrieval with efÔ¨Åcient boosting,‚Äù
in Proceedings of ACM
Multimedia, 2009, pp. 545‚Äì548.
[5] M. Crucianu, D. Estevez, V. Oria, and J.P. Tarel, ‚ÄúSpeeding
up active relevance feedback with approximate knn retrieval
for hyperplane queries,‚Äù Int. J. Imaging Syst. Technol., vol.
18, no. 2-3, pp. 150‚Äì159, 2008.
[6] P. Zezula, G. Amato, V. Dohnal, and M. Batko, Similarity
Search - The Metric Space Approach, vol. 32 of Advances in
Database Systems, Springer, 2006.
[7] H. Samet, Foundations of Multidimensional and Metric Data
Structures, Morgan Kaufmann Publishers Inc., San Francisco,
CA, USA, 2005.
[8] K. S. Beyer, J. Goldstein, R. Ramakrishnan, and U. Shaft,
‚ÄúWhen is ‚Äùnearest neighbor‚Äù meaningful?,‚Äù in ICDT ‚Äô99, Pro-
ceedings. 1999, vol. 1540 of LNCS, pp. 217‚Äì235, Springer.
[9] S. Boughorbel, J.P. Tarel, and N. Boujemaa, ‚ÄúConditionally
positive deÔ¨Ånite kernels for svm based image recognition,‚Äù in
IEEE ICME 2005. July 2005, pp. 113‚Äì116, IEEE.
[10] C. Cortes, P. Haffner, and M. Mohri,
‚ÄúRational kernels:
Theory and algorithms,‚Äù J. Mach. Learn. Res., vol. 5, pp.
1035‚Äì1062, 2004.
[11] P. Bolettieri, A. Esuli, F. Falchi, et al., ‚ÄúEnabling content-
based image retrieval in very large digital libraries,‚Äù in Second
Workshop on VLDL, 2009, pp. 43‚Äì50.
[12] G. Aamato and P. Savino, ‚ÄúApproximate similarity search in
metric spaces using inverted Ô¨Åles,‚Äù in InfoScale ‚Äô08. 2008,
pp. 1‚Äì10, ICST.
[13] M. Batko, F. Falchi, C. Lucchese, et al., ‚ÄúBuilding a web-
scale image similarity search system,‚Äù Multimedia Tools and
Applications, vol. 47, no. 3, pp. 599‚Äì629, 2009.
[14] T. Hey, S. Tansley, and K. Tolle, Eds., The Fourth Paradigm
- Data Intensive ScientiÔ¨Åc Discovery, Microsoft Res., 2009.
61
MMEDIA 2011 : The Third International Conferences on Advances in Multimedia
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-129-8

