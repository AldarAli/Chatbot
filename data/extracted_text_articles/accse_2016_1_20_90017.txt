Towards Drone-Assisted Large-Scale Disaster
Response and Recovery
Michael Spranger, Florian Heinke, Sven Becker and Dirk Labudde
University of Applied Sciences Mittweida
Mittweida, Germany
Email: {name.surname}@hs-mittweida.de
Abstract—Major damaging events with many victims or
environmental contamination hazards, such as natural
disasters, airplane crashes, train accidents or terroristic
acts, are shocking events to society. Fast and comprehensive
information acquisition of event sites ensures appropriate
and safe actions for rescue forces and investigators, and
increases resilience of our society with respect to such
disorders in the long term. The use of unmanned aerial
vehicles, so-called drones, for gathering as much infor-
mation as possible about the event site to support the
whole resilience cycle is a fast and safe way to elucidate
such unknown environments. Therefore, an application
framework is currently developed by the authors, which
aims at supporting decision makers with respect to targeted
and safe management of rescue teams and the fast locating
of victims, as well as 3D spatiotemporal modeling and
simulation of events for forensic purposes. In this work, we
present a process chain for 3D reconstruction of event sites
using aerial photogrammetry and open source software.
Keywords–forensic; unmanned aerial vehicle; resilience
engineering; open source; 3D reconstruction
I.
INTRODUCTION
On March the 24th 2015, Germanwings ﬂight
4U9525 crashed in the French Alps after a suici-
dal co-pilot intentionally initiated a controlled ten
minutes lasting descent. All 150 people were killed
instantly. Located in remote mountainous terrain
difﬁcult to access by vehicles, the disaster site is
quickly reachable only by helicopter which posed
major logistic problems to rescue forces and inves-
tigators, especially in the ﬁrst hours after the crash.
Many other similar scenarios, such as natural disas-
ters, pile-ups, train accidents or terroristic acts, pose
analogous problems to personnel, whereas locating
survivors and bodies, obstructive and dangerous
pieces of debris, and sources of toxic or hazardous
chemicals is a major concern in this respect. Thus it
is of major importance to generate a general picture
based on available information about the disaster
site–not only with regards to lifesaving, but strategic
resource planning as well. However, especially in
the ﬁrst minutes and hours, such information is
difﬁcult to obtain [1] [2].
In this work, we present the concepts of a general
framework for drone-assisted 3D reconstruction and
mapping of disaster sites. Drones (or unmanned
aerial vehicles, UAVs) are small and effective plat-
forms for accessing and imaging locations that are
difﬁcult or impossible to reach quickly by personnel
or helicopters, and are thus of great potential in
providing a fast mapping of disaster sites, as well
as locations of potential survivors [1] [3]. Addi-
tional drone payloads, such as thermal imaging sys-
tems, pollutant sensors, and automated GPS-assisted
navigation systems, yield an even wider range of
applicability and potential in resilience engineering
in general (please see the work of Colomina and
Molina [4], as well as Horsman [5] for an extensive
overview).
Based on a study presented by P¨uschel et al. [6],
which aimed at 3D reconstruction of tourism objects
by combining aerial and terrestrial photogrammetry
using drones, we designed concepts for disaster site
3D reconstruction and mapping by means of open
source software based on aerial images acquired by
drones. In this paper, we ﬁrst provide background
and motivation for said reconstruction processes (see
Section II), present the photogrammetry software in
question and introduce a strategy for drone-assisted
3D disaster site reconstruction, including results ob-
tained by proof-of-concept testing of such. Finally,
in Section III, we brieﬂy introduce our drone system
5
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-481-7
ACCSE 2016 : The First International Conference on Advances in Computation, Communications and Services

and its conceptual application to the crash site of
Flight 4U9525, demonstrate the proposed strategy’s
performance in application to the Mittweida water
tower which we chose as our object of initial studies,
and conclude by prospects of future work (Section
IV).
II.
3D DISASTER SITE RECONSTRUCTION
An important task in forensic disaster response
and recovery is to gather and process informa-
tion about event sites (such as crime or disaster
scenes) quickly and safely in order to support action
planning and well-directed employment of rescue
teams on the one hand, and eventually provide data
for eventual spatiotemporal reconstruction of such
an event on the other. There are various methods
for gaining such valuable information, including
terrestrial and helicopter- or drone-based large-scale
imaging, videotaping or laser scanning. The eval-
uation of the data obtained by mentioned means
and subsequent 3D reconstruction can be realized
by utilizing various software packages, either li-
censed, free or open-source. Although cutting edge
in quality and performance, laser scanning systems
are costly with respect to operation, maintenance
and acquisition (including specialized software for
data processing). In contrast, open source software
can pose cost efﬁcient alternatives. We here propose
a pipeline of open source software for 3D event site
reconstruction based on drone-assisted imaging.
The ﬁrst open source software package within
the reconstruction process is Visual Structure from
Motion (VisualSFM) [7]. Providing both a user
interface, as well as command line access for batch
integration, VisualSFM is utilized for calculating
3D point clouds of multi-image photographs of an
object or area based on Wu’s scale-invariant feature
transform GPU algorithm [8]. In addition there are
further algorithms implemented in VisualSFM, such
as Clustering Views for Multi-view Stereo (CMVS)
and Patch-based Multi-view Stereo (PMVS) [9] to
cluster and condense calculated point clouds. In the
process, CMPMVS [10] is utilized to reﬁne ob-
tained 3D point clouds, reconstruct object surfaces
and compute object textures. The downstream soft-
ware to VisualSFM and CMPMVS are MeshLab
[11] and Blender [12]. MeshLab can be used for
post-processing and editing reconstructed surfaces
or reﬁning them from computed point clouds if
necessary. Furthermore, multiple object meshes can
be aligned and uniﬁed to one single mesh. Blender
is a 3D graphics and animation software and can
be used to import object (.obj) ﬁles generated in
MeshLab and edit, add, merge, measure, (re-)texture
and render 3D objects.
Using this pipeline, valid reconstruction results
can only be obtained if underlying images are of
good quality and, equally important, coherent in
perspective. More precisely, images are required to
capture the entire scene from all general view angles
including perspective overlaps that ensure determin-
ing virtual camera positions in the reconstructed 3D
point space and, hence, proper object reconstruction.
Therefore, analogous to results presented in [13],
drone-assisted 3D site reconstruction is of best qual-
ity using this software pipeline if the area in question
is captured at a circular ﬂight path from a drone cir-
cumnavigating the area. Note that computation time
and memory usage are determined by the surface
reconstruction process, growing exponentially by the
number of images, image resolution, and identiﬁed
object points.
Initially, we tested the concept using sequences of
pseudo-aerial images obtained from Google Earth.
Images were retrieved along virtual circular ﬂight
paths and eventually used for 3D site reconstruc-
tion. A schematic of the proposed reconstruction
workﬂow is shown in Figure 1 including an example
reconstruction of the Germanwings Flight 4U9525
crash site based on 25 pseudo-aerial images [14].
III.
DRONE-BASED IMAGE ACQUISITION
A. The Drone in Use: Technical Aspects and Capa-
bilities
The drone used in our study is a MikroKopter
MK-ARF Okto XL 6S12, an eight-blade rotary wing
drone for multi-purpose utilization. In our set-up the
MK-ARF Okto XL 6S12 has a maximum slant range
of 4,000 m and a maximum ceiling of 5,000 m above
sea level. With fully charged batteries and optimal
weather conditions, the drone achieves a maximum
ﬂight time of about 45 minutes. Besides present
weather conditions, maximum ﬂight time is reduced
6
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-481-7
ACCSE 2016 : The First International Conference on Advances in Computation, Communications and Services

by hardware additionally mounted to the drone, such
as a ﬁxed SLR (single lens reﬂex) camera. Further-
more, the drone is equipped with a CMOS (comple-
mentary metal-oxide-semiconductor) camera whose
video feed can be received and post-processed on
the ground. Although not-movable around the yaw
axes, both the CMOS and SLR camera mount can be
pitched and rolled. In combination with automated
pre-planned waypoint ﬂight and point-of-interest
focusing capabilities, as well as automated camera
triggering, the user is thus able to obtain images
made in-ﬂight at pre-planned positions and altitudes.
Waypoints and trigger events can be set-up and
uploaded to the drone using the maintenance and
control software MikroKopter-Tool V2.12a.
B. A Strategy for Drone-assisted Disaster Site
Imaging
As discussed in Section II, 3D object recon-
struction by means of VisualSFM using a sequence
of images is only feasible if these are recorded
along a circular track around the object. Hence,
drone-assisted terrain and disaster site reconstruc-
tion utilizing this reconstruction strategy requires an
analogous object-camera geometry in the recording
process. Therefore, a drone is programmed to ﬂy a
nearly circular path around the center of the region
of interest, whereas the center is constantly focused
by the camera. The appropriate image sequence
can eventually be obtained in-ﬂight. As elucidated
above, the MK Okto XL 6S12 is capable to realize
such a pre-planned ﬂight proﬁle.
With respect to extensive dimensions of some
disaster sites (i. e., the crash site of Flight 4U9525 is
about 380 m × 500 m), covering the entire region of
interest utilizing a single circular path is unfeasible
or even impossible to achieve due to range and ﬂight
time limitations. To circumvent these restrictions,
a given region of interest can be split in a set
of smaller overlapping circles in a straightforward
manner (see Figure 2A and B). Obtained image
sequences are used for 3D reconstruction of corre-
sponding smaller circular areas which are eventually
assembled to a single uniﬁed model of the region of
interest using MeshLab (Figure 2C and D). Image
capturing and site reconstruction can thus also be
conducted in parallel. In addition, Figure 2E shows
a section of the MikroKopter-Tool’s waypoint ﬂight
planner employed to the Flight 4U9525 crash site.
In this virtual scenario, the drone is programmed
to take off at a clearing (waypoint two, P2) which
can be reached either by helicopter or by vehicle
via an unpaved mountain trail and proceeds around
the POI (waypoint one, P1) in a circular manner
described by 25 waypoints, whereas the camera is
pointed towards the POI throughout the ﬂight. Blue
circles visualize three additional circular ﬂight paths
required for reconstructing almost the entire crash
site from corresponding 3D models.
C. Application
The proposed 3D reconstruction strategy was
tested on the Mittweida water tower and its close
surroundings. The tower is 38 m high and consists
of two sections with varying widths of about 10
and 16 m. Featuring large dimensions, small de-
tails and free space in its surroundings, it poses a
suitable object to test reconstruction capabilities in
combination with varying camera-object geometries,
camera settings and image resolutions. In Figure 3A,
a model of the tower and its surrounding area is
shown. Here, the drone was programmed to ﬂy a
circle with 50 m radius at an altitude of 50 m above
ground level. Images were extracted from recorded
HD video material every single second, resulting to
111 images, and processed as proposed. Although
major details are discernible, smaller features with
a size of about less than one meter are difﬁcult
to identify. On a standard desktop machine (eight
3.5 GHz CPUs, 32 GB RAM, GeForce 750 GTX
Titan), the 3D reconstruction process required about
1.5 to 2 hours of computation time and 1.5 GB
of disk space. Hence, with this set-up, obtained
models are only suitable for fast mapping of larger
areas. Interestingly, a model computed from only 28
images (four seconds per image) shows only minor
discrepancies in quality compared to the model
computed from 111 images. Said model is shown
in Figure 3B. In addition, computation time for this
model is only 20 minutes.
In the reconstruction process of the tower basis
the drone was programmed to ﬂy at three meters
above ground level with a distance of ten meters
from the tower. The obtained model is shown in
7
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-481-7
ACCSE 2016 : The First International Conference on Advances in Computation, Communications and Services

Figure 1.
A: Schematic workﬂow of the proposed 3D disaster site
reconstruction strategy using aerial images by means of open source
software. Obtained images are input to VisualSFM [7], whereas 3D
points for initial object/area reconstruction are computed. CMPMVS
[10] is utilized next for point enrichment and mesh construction
which is eventually reﬁned or merged with other meshes using
MeshLab [11]. Blender [12] is utilized for further reﬁnement and
post-processing. Best reconstruction results are obtained if images are
recorded on a circular ﬂight path around the object/area of interest. B:
3D model reconstructed from pseudo-aerial images of Flight 4U9525
crash site retrieved from Google Earth [14]. White dots indicate
virtual camera locations computed by VisualSFM.
Figure 3C. Here, sixty 24 megapixel images were
recorded at an angular offset of 6◦ using interval
triggering. Time and disk space demands are signif-
icantly larger for this set-up (6.5-8 hours, 53 GB disk
A
B
E
C
D
Figure 2.
3D reconstruction strategy for extensive disaster sites. A
and B: 3D-reconstruction of a large disaster site is realized by merging
computed meshes obtained from image sequences of shorter circular
ﬂights covering the entire area. C and D: Overlapping ﬂight paths
ensure proper mesh alignment and uniﬁcation. E: Screen capture of
a planned circular ﬂight in MikroKopter-Tool V2.12a shown for the
Flight 4U9525 crash site [14] as the region of interest. Throughout the
ﬂight, the camera is pointed towards the point of interest (P1). Three
additional overlapping circular ﬂights (indicated by blue circles) are
required to reconstruct the area based on obtained image sequences.
space). Computational demands are accompanied
with a high degree in object and surface detail, even
for small objects (< 10 cm), suitable for detailed
reconstruction and mapping of smaller areas.
In summary, camera-object geometry, image resolu-
tion and the number of considered images has to be
chosen in accordance to the features of the target
object/area and the problem to address. Especially
computational time demands have to be taken into
account during ﬂight planing.
8
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-481-7
ACCSE 2016 : The First International Conference on Advances in Computation, Communications and Services

Figure 3.
A and B: 3D model of the Mittweida water tower (height:
38 m) obtained from 111 respectively 28 images extracted from HD
video. C: 3D model of the tower basis (width: 10 m) generated from
sixty 24 megapixel images.
IV.
CONCLUSION AND FUTURE WORK
Fast reconstruction of disaster sites can be of
great value in disaster response and recovery. The
presented work is focused on the conceptual design
of such a 3D disaster site reconstruction strategy
by means of open-source software based on aerial
images obtained by drones. Models generated from
images obtained during ﬂight show that model qual-
ity is greatly dependent on camera-to-POI geometry
and image resolution. It is further pointed out that
even the resolution of images extracted from high
deﬁnition video is not always sufﬁcient for detailed
disaster scene reconstruction. Although lower image
resolutions lead to coarse models, time demands
for automated model calculation are relatively small
and obtained level of detail can be sufﬁcient for
fast mapping processes, which makes these mod-
els adequate for the exploration of large disaster
sites and providing support to the rescue forces
in response planning. High resolution models are
achieved by using high resolution images (e. g.,
24 megapixels), whereas computational demands
increase signiﬁcantly.
Future work requires the recording of more aerial
drone-based images and evaluation of generated
meshes, whereas the conceptual strengths and weak-
nesses are ought to be identiﬁed and veriﬁed. Here,
the focus lies on the quantiﬁcation of reconstruction
error. Furthermore, specialized payloads such as
thermal imaging systems shall be considered in the
future.
REFERENCES
[1]
H. Bendea et al., “Low cost UAV for post-disaster assessment,”
The International Archives of the Photogrammetry, Remote
Sensing and Spatial Information Sciences, vol. 37, no. Part
B, 2008, pp. 1373–1379.
[2]
Y. Naidoo, R. Stopforth, and G. Bright, “Development of an
UAV for search & rescue applications,” in AFRICON, 2011.
IEEE, 2011, pp. 1–6.
[3]
L. Barazzetti et al., “3D scanning and imaging for quick
documentation of crime and accident scenes,” in SPIE Defense,
Security, and Sensing.
International Society for Optics and
Photonics, 2012, pp. 835 910–835 910.
[4]
I. Colomina and P. Molina, “Unmanned aerial systems for
photogrammetry and remote sensing: A review,” ISPRS Journal
of Photogrammetry and Remote Sensing, vol. 92, 2014, pp.
79–97.
[5]
G. Horsman, “Unmanned aerial vehicles: A preliminary analy-
sis of forensic challenges,” Digital Investigation, vol. 16, 2016,
pp. 1–11.
9
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-481-7
ACCSE 2016 : The First International Conference on Advances in Computation, Communications and Services

[6]
H. P¨uschel, M. Sauerbier, and H. Eisenbeiss, “A 3D model
of Castle Landenberg (CH) from combined photogrammetric
processing of terrestrial and UAV-based images,” Int. Arch.
Photogramm. Remote Sens. Spat. Inf. Sci, vol. 37, 2008, pp.
93–98.
[7]
M. Ziegler, E. G¨ulch, and P. Rawiel, “3D-Rekonstruktion von
Objekten mittels Structure-from-Motion aus einer photogram-
metrischen Aufnahme mit den Programmen VisualSFM und
CMPMVS,” in DGPF Tagungsband, vol. 23.
[8]
C.
Wu,
“SiftGPU
Implementation
of
Scale
In-
variant
Feature
Transform.”
[Online].
Available:
http://cs.unc.edu/ ccwu/siftgpu/, retrieved: April 8, 2016
[9]
Y. Furukawa and J. Ponce, “Accurate, Dense, and Robust
Multiview Stereopsis,” IEEE Transactions on Pattern Analysis
and Machine Intelligence,, vol. 32, no. 8, Aug 2010, pp. 1362–
1376.
[10]
M. Jancosek and T. Pajdla, “Multi-view reconstruction preserv-
ing weakly-supported surfaces,” in 2011 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2011, pp.
3121–3128.
[11]
MeshLab. [Online]. Available: http://meshlab.sourceforge.net/,
retrieved: April 8, 2016
[12]
Blender.
[Online].
Available:
https://www.blender.org/,
retrieved: April 8, 2016
[13]
P. L. Falkingham, “Acquisition of high resolution three-
dimensional models using free, open-source, photogrammetric
software,” Palaeontologia Electronica, vol. 15, no. 1, 2012,
p. 15.
[14]
Google Earth, retrieved: January 19, 2016, ’Germanwings
Flight 4U9525 crash site’ 44◦16’48.46” N 6◦26’18.84” E,
Image recording date: March 29, 2015.
10
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-481-7
ACCSE 2016 : The First International Conference on Advances in Computation, Communications and Services

