An Evaluation of Blended Learning Components of the Cisco Network Academy 
Using a Rasch Model 
Kevin Sealey (Author) 
Curtin University 
Perth, Australia 
kevin.sealey@ozemail.com.au 
 
 
 
 
 
 
Abstractâ€”The blended model of e-learning espoused by the 
Network Academy curriculum involves the interaction of 
students with its various components - online content, 
laboratory exercises, simulations, online assessment, texts and 
most importantly instructors. These components were evaluated 
from the student viewpoint using two surveys and data from the 
final online examination.  The analysis used a Rasch 
probabilistic model. This was done in the West Australian, and 
in the case of the online examination, an Australia-wide context. 
 
Keywords-curriculum evaluation; blended learning; Rasch 
model, student 
I  
INTRODUCTION 
This is part of a wider study to evaluate the Network 
Academy curriculum in secondary and post-secondary 
academies in Western Australia, looking at the intended, 
implemented, achieved and perceived curriculum [1][2] 
from the point of view of various stakeholders - students, 
instructors, employers and Cisco.  
Considerable research has been carried out into the value 
of the Network Academy in various educational settings 
[3][4].  In this study students were asked to respond to two 
online surveys [5], one at the beginning of their course and 
another later in their course.  For the first survey, the 175 
respondents comprised 12 secondary students (7%), 109 
Technical and Further Education (TAFE) students (62%) 
and 54 university students (31%).  135 of these students 
(76%) studied full time and 42 part time (24%).  20% of 
these students (36) were in full time employment, 38% in 
part time employment (66) and 42% not employed (73).  In 
the second survey, the 93 respondents were comprised of 13 
secondary (14%), 56 TAFE (60%) and 24 university (26%) 
students.  The responses to the final online examination for 
the Exploration curriculum for the first semester, 2010, were 
also analysed.  This involved 750 students drawn from 
academies across Australia. 
Analyses of surveys and examination were performed 
using a Rasch probabilistic model, which yields precise, 
interval-level measures of both person (ability) and item 
(difficulty). This model was used to investigate if it could 
give additional insight beyond that derived from standard 
analysis. 
II 
WHY DO STUDENTS STUDY THE CURRICULUM? THE 
INTENDED CURRICULUM 
It was anticipated that students would take the Network 
Academy course for a wide variety of reasons, chiefly 
centred on their current and future study goals and their 
employment considerations 
In order to gain further insight into the survey responses, 
the data was analysed using RUMM2030[6].  The summary 
of the analysis indicates that the data fits the Rasch model 
[7].  Both the reliability indices and the power of test-of-fit 
indicate that the data was suitable for analysis using this 
approach.   
TABLE 1.   
FIRST SURVEY RESULTS 
Survey 
Item 
Description 
Location 
(Difficulty to Endorse) 
Fit 
Residual 
(Rasch 
Model Fit) 
Standard Error 
of Estimate 
7 
Graduation 
-0.28 
0.24 
0.087 
8 
Further 
Education 
0.008 
-0.204 
0.086 
9 
Job 
-0.662 
0.24 
0.098 
10 
Current Job 
0.55 
2.211 
0.074 
11 
Interest 
0.714 
1.057 
0.081 
12 
Peers 
1.529 
2.294 
0.082 
13 
Practical 
-0.798 
-0.206 
0.107 
14 
Theory 
-0.852 
-0.913 
0.112 
15 
Certification 
-0.209 
0.389 
0.082 
338
ICNS 2011 : The Seventh International Conference on Networking and Services
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-133-5

 
 
Figure 1. Intended curriculum 
 
 
Table 1 shows the item details for the first survey.  This 
displays the difficulty to endorse (Location), the test of fit 
(Fit Residual) and the standard error of the measurement of 
the Location.  The unit of measurement for Location is logit 
(log odds).  In this context, the more negative the Location 
for an item the easier it is for the respondent to endorse the 
body of the item.  That is, the more negative the item's 
Location the stronger is the agreement of the respondent to 
the item's statement.  The Fit Residual, as listed here, 
measures the correspondence of the data with the model.  It 
represents the standardized difference between the actual 
data and the values calculated using the item estimates and 
the mathematical model.  It is considered acceptable fit if 
the Fit Residual lies between +2 and -2.  Values between 2 
and 2.5 and between -2 and -2.5 are considered marginal.  
Values above 2.5 and below -2.5 are considered to represent 
poor fit to the model.  The standard error  estimates are 
meant to indicate the degree of uncertainty in the Location 
values for a particular item.  It is considered a bonus for this 
method of analysis that each item has its own estimate of 
error.  With classical test theory the standard error is 
calculated to be the same for all items. 
It can be seen from Table 1 that the items vary 
significantly in difficulty to endorse, and that most items fit 
the model, with the exception of items 4 and 6, which have 
only marginal fit.  If the data is presented as a bubble plot 
(Fig. 1), the differences between the items become more 
evident. 
In this plot difficulty to endorse is plotted horizontally and 
fit residual vertically.  The width of the bubbles represents 
the standard error (each bubble's width is approximately 
shown as double the value of the standard error). 
It is evident that the strongest intentions of the students 
were to gain theoretical and practical knowledge and to 
enhance their employment prospects.  To a lesser extent, the 
opportunity to progress towards graduation and certification 
and also to prepare for further education were seen to be 
important.  Pursuing the course as a matter of interest was 
not well endorsed.  The fact that three quarters of the cohort 
were not employed means that the responses to the item 
asking whether they thought that the course would help in 
their current employment was less reliable.  This item did not 
have a good fit to the model.  The influence of peers on their 
decision to take the course was not endorsed, and the 
relevant item was not a good fit to the model.  Perhaps this 
reflects the large age range and variability of maturity and 
experience of the cohort. Table 2 shows the results for the 
analysis of the second survey.  These results are used to 
investigate the implemented and perceived curriculum. 
III 
HOW DO STUDENTS STUDY THE IMPLEMENTED 
CURRICULUM? 
Subjecting the data from the second student survey to 
analysis using RUMM2030 indicated that the data had an 
excellent fit to the model. Once again, graphical 
representation of the data indicates significant differences in 
endorsement of the items by students.  Although the item 
data is shown in the one table, different groups of items 
represented particular features of the analysis.  
graduation
furthered
job
current job
interest
peers
practical
theory
certification
-3
-2
-1
0
1
2
3
-3
-2
-1
0
1
2
3
fit residual
difficulty to endorse (logits)
339
ICNS 2011 : The Seventh International Conference on Networking and Services
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-133-5

TABLE 2.  
SECOND SURVEY RESULTS 
Item 
Description 
Location 
(Difficulty to 
endorse) 
Fit 
Residual 
Standard 
error 
Item 
Description 
Location 
(Difficulty to 
endorse) 
Fit Residual 
Standard 
error 
5 
Online 
-1.513 
0.057 
0.137 
37 
Explain Solutions 
-1.141 
0.399 
0.146 
6 
Texts 
0.264 
0.194 
0.114 
38 
Teamwork 
-1.933 
1.15 
0.127 
7 
Lecture 
-0.19 
-0.345 
0.125 
39 
Work/Others 
-1.479 
3.446 
0.125 
8 
Labs 
-1.594 
-0.783 
0.13 
40 
Learn/Others 
-1.791 
2.655 
0.127 
9 
Packet Tracer 
-0.27 
5 
0.121 
41 
Self Paced 
-2.235 
1.156 
0.134 
10 
Tests 
-0.336 
-0.022 
0.124 
42 
Relate/Contexts 
0.977 
0.092 
0.112 
11 
Cases 
0.509 
0.087 
0.109 
43 
Interest 
0.692 
-0.199 
0.126 
12 
Others 
0.318 
0.089 
0.121 
44 
Ability Match 
0.292 
-0.308 
0.135 
13 
Explanation 
-0.298 
-0.156 
0.125 
45 
Control/Learning 
0.206 
-0.212 
0.124 
14 
Icg 
0.72 
0.128 
0.114 
46 
Use Feedback 
0.036 
-0.064 
0.127 
15 
Other Networks 
0.567 
0.082 
0.108 
47 
Understanding 
0.071 
-0.222 
0.127 
16 
Online 
-0.042 
0.001 
0.165 
48 
Relevant 
-0.351 
-0.116 
0.134 
17 
Packet Tracer 
-0.837 
0.234 
0.199 
49 
Important 
-0.029 
-0.252 
0.122 
18 
Tests 
-0.309 
-0.002 
0.175 
50 
Weaknesses 
0.005 
0.044 
0.119 
19 
Discussions 
-0.614 
-0.214 
0.19 
51 
Explain 
0.198 
-0.152 
0.104 
20 
Labs 
-0.776 
-0.285 
0.199 
52 
Pace 
0.408 
-0.311 
0.122 
21 
Texts 
-0.001 
-0.058 
0.188 
53 
Examples 
0.488 
-0.37 
0.124 
22 
Lectures 
-0.447 
-0.495 
0.179 
54 
Involvement 
0.524 
-0.221 
0.134 
23 
Quest/Contrib 
0.298 
0.397 
0.144 
55 
Texts 
0.88 
0.213 
0.116 
24 
Presentation 
1.863 
0.126 
0.135 
56 
Effective 
0.09 
-0.395 
0.107 
25 
Presentation 
1.815 
0.142 
0.13 
57 
Prepared 
-0.581 
-0.35 
0.141 
26 
Unprepared 
1.198 
0.741 
0.132 
58 
Clear Course 
0.524 
-0.161 
0.125 
27 
Worked/ Others 
0.181 
0.614 
0.132 
59 
Good Answers 
-0.602 
-0.73 
0.131 
28 
Mentor 
1.543 
0.155 
0.149 
60 
Difficulty 
0.281 
-0.213 
0.129 
29 
Extra-Curricular 
1.88 
0.134 
0.141 
61 
Activities 
-1.395 
-1.619 
0.147 
30 
Challenged 
-0.99 
1.534 
0.225 
62 
Cases 
0.413 
-0.106 
0.145 
31 
Discuss Ideas 
0.872 
-0.117 
0.126 
63 
Working/ Others 
0.187 
0.104 
0.124 
32 
Give Opinions 
1.104 
0.054 
0.124 
64 
Well Matched 
-0.025 
-0.118 
0.147 
33 
Inst Questions 
0.823 
0.164 
0.125 
65 
Understanding 
-0.277 
-0.12 
0.143 
34 
Ideas Used 
1.894 
-0.003 
0.124 
66 
Available 
-1.682 
0.428 
0.137 
35 
Quest/Contrib 
0.246 
0.276 
0.118 
67 
Attitude 
0.369 
-0.172 
0.111 
36 
Problem 
Solving 
-0.999 
0.664 
0.166 
 
 
 
 
 
 
 
340
ICNS 2011 : The Seventh International Conference on Networking and Services
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-133-5

 
Figure 2. Implemented Curriculum 
 
The items relating to student implementation of the 
curriculum (items 5 to 15) were concerned with the 
strategies found most effective by the students to proceed 
through the curriculum.  The results of this analysis are 
shown in Fig 2.  The fit residuals and standard errors 
indicate that all items have a good fit and that there are 
broadly three groups of items when comparing aspects 
relating to students' difficulty in endorsement.  Clearly the 
online curriculum and the laboratory exercises were the 
most important resources for the students.  The second most 
important group of resources included quizzes and chapter 
tests, simulations (Packet Tracer), explanations by the 
instructor and the lecture presentations by the instructor.  
The least important resources included texts, working with 
other students, case studies, working with other networks 
outside the laboratory and interactive course guides.  This 
rating of importance of resources is confirmed by class 
observation.  It is interesting to note that the online 
assessment resources are rated as highly as the interaction 
with the instructor.  Interactive Course Guides were not 
commonly used by students as they were a recent innovation 
and targeted chiefly at the instructors. 
Another group of items (items 23 to 37) were related to 
the students' engagement in the class.  This could be 
considered to be another aspect of how the students 
implemented the curriculum. The most strongly endorsed 
items (most negative Location) relate to the degree of 
academic challenge of the curriculum.  Students were 
challenged by the course and frequently engaged in problem 
solving and explained solutions to their classmates.  To a 
lesser extent students worked with others, asked questions 
and made contributions to the class.  It is notable that 
students considered that they always prepared well for the 
classes.  The other group of items could indicate that 
students rarely made unsolicited contributions to the class 
and did not work with other members of the class outside of 
scheduled class times. 
A third group of items (items 38 to 42) indicated the 
learning style adopted by the students. These items suggest 
that students embraced the self-paced aspect of the 
curriculum and that when working in groups they felt that 
teamwork was important..  However, the material learnt in 
class was not well related to the non-class situation.  Items 
relating to working with and learning from others were 
poorly fitting items and thus no conclusions may be drawn 
from their values. 
IV. 
HOW DO STUDENTS FEEL ABOUT THE 
CURRICULUM? THE PERCEIVED CURRICULUM 
When asked to rate the effectiveness of the components 
of the curriculum (items 16 to 22), the students' responses 
indicated that they perceived all of the components as 
effective.  Once again the fit residuals indicate a good fit to 
the model.  The fact that all items have a negative Location 
shows that all components were strongly endorsed as being 
effective.  The spread of the Location of the items is smaller 
than for those items discussed previously.  It may be 
inferred that the laboratory exercises and the simulations 
with Packet Tracer and classroom discussions were 
perceived as the most effective components of the course.  
The lectures, online assessment and online curriculum were 
in turn more effective than the texts. 
On the other hand, some aspects of the curriculum were 
not endorsed as strongly.  These items (items 43 to 45) 
online
texts
lecture
labs
packet tracer
tests
cases
others
explanations
icg
other networks
-3
-2
-1
0
1
2
3
-3
-2
-1
0
1
2
3
fit residual
difficulty to endorse (logits)
341
ICNS 2011 : The Seventh International Conference on Networking and Services
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-133-5

appear to indicate that the students' perception of the 
curriculum materials was that they did not match their ability 
and interests, and that very little was left to the students' 
initiative as far as progression through the course was 
concerned.  In effect, the Cisco course materials and the 
order in which they are presented is very much controlled by 
the instructors and course developers. 
As part of the online aspect of the curriculum, the online 
assessment materials form an integral part of the package.  
The responses to items 46 to 50 indicated that the 
assessments are viewed as well matched to the curriculum, 
and an important resource in the overall package.  The 
feedback delivered with the assessment results and the use of 
assessment to discover weaknesses and generally help with 
the understanding of course material are not always 
perceived as helpful.  
Instructors are an integral part of this blended e-learning 
initiative. The role and effectiveness of instructors, as 
perceived by the students, is an important aspect of the 
curriculum evaluation.  The items relating to students' 
perception of the instructors (items 51 to 62) fit the model 
well and there is some significant variation in difficulty to 
endorse the statement in the item.  The class activities 
chosen by the instructor were strongly endorsed.  The 
preparation of the instructor and his/her response to in-class 
questions were well regarded.  The pace at which the course 
was conducted, the student involvement in the class, the 
number and quality of the examples chosen by the instructor, 
the teaching effectiveness, the clarity of the scope of the 
course, the difficulty level of the course materials and the 
case studies used by the instructor were not endorsed.  The 
printed material of the course was least strongly endorsed. 
The final group of questions (items 63 to 67) relate to the 
overall perception of the curriculum.  The most strongly 
endorsed perception was that the instructor was freely 
available for consultation.  The course content was well 
matched to the abilities and expectations of the students and 
led to increase understanding of course content.  The value 
of working with other students was less strongly endorsed.  
The overall attitude to coming to class probably reflects the 
challenging nature of the curriculum materials. 
V. 
WHAT DO STUDENTS GET OUT OF THE 
CURRICULUM?  THE ACHIEVED CURRICULUM 
An analysis of the response from all of the students in 
Australia for the final Exploration examination (Semester 
one, 2010) were analysed using RUMM2030.  All except 
two of the 120 items in the item bank returned an acceptable 
level of fit to the model.  An analysis such as this might lead 
to a modification or removal of the poorly fitting items, 
which in the current situation merely add 'noise' to the 
assessment of student ability. 
The person fit as estimated in the first analysis indicated 
that students' responses conformed well to the model.  Those 
students with perfect scores (ten students) were classified as 
having a poor fit to the model, since the ability estimate 
could not be made.  Only one other student in the group of 
750 had a poor fit to the model.  
The ability (Location) frequency graph (Fig 3) shows 
abrupt changes at 0, 0.4, 1.1 and 1.5.  If these are used to 
signify boundaries of different ability groups, then those 
students above 1.5 (raw score 95) could be awarded an A, 
those between 1.1 (raw score 90) and 1.5 a B, those between 
.4 (raw score 70) and 1.1 a C, those between 0 (raw score 50) 
and .4 a D and those below 0 an E.  Examination of the 
frequency distributions for each of the module tests suggests 
a very similar distribution of abilities and this suggests that 
this could be a feasible approach to awarding grades. 
When managing the assessment for the class, the 
instructor can stipulate the form with which the students are 
confronted when they access the test.  The data analysed for 
the Australian cohort consists of responses to three forms, 
each comprising a subset of the 120 items in the item pool.  
Rasch analysis of the data provides a means of comparing 
the ability estimates made with the different forms.  The 
'equating tests' option of RUMM2030 provides a graphical 
indication of the relative difficulty of different tests which 
are made up of different selections of items from a pool of 
calibrated items.  The resulting graph for the three forms of 
the final module examination is displayed in Figure 4 and 
this shows that the ability measure (Location) over the whole 
range of scores is essentially independent of the form used.  
Students are not disadvantaged by being assigned one form 
rather than another. 
 
 
Figure 3. Ability Frequency Distribution 
342
ICNS 2011 : The Seventh International Conference on Networking and Services
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-133-5

 
 
Figure 4. Comparison of Different Forms 
 
VI. 
CONCLUSION 
The study indicates that Rasch analysis is appropriate 
for an in-depth evaluation of the components comprising 
the blended model of e-learning used in the Network 
Academy curriculum.  Such an evaluation may be useful in 
refining the various components, guiding the relative 
importance given to particular components by the different 
stakeholders and modifying student recruitment strategies 
of educational institutions. 
The chief expectations for students were that the 
curriculum would enhance their practical skills and 
theoretical knowledge as well as improve their prospects 
for employment.  The online content and laboratory 
exercises, online assessment and instructor centred 
activities provided the means by which the students 
progressed through the program.  The importance of the 
instructor's role is evident in the students' perception of this 
style of e-learning.  Analysis of the online examinations 
shows that they are a fair and reliable tool for assessing 
student ability.  It is clear, however, that a Rasch analysis 
of the data adds value to the procedure. 
REFERENCES 
[1] 
Hartley, M. S., Treagust, D. F., & Ogunniyi, M. B. (2008). 
The application of a CAL strategy in science and 
mathematics for disadvantaged Grade 12 learners in South 
Africa.  International Journal of Educational Development, 
28, 596-611. 
[2] 
Van den Akker, J. (1998).  The science curriculum: 
Between ideals and outcomes.  In B.J. Fraser & K. G. Tobin 
(Eds.), International handbook of science education. (pp. 
421-427). Dordrecht, The Netherlands: Kluwer Academic 
Press. 
 
 
[3] 
Cakir, H., Bichelmeyer, B., Dennis, A., Bunnage, J. C., 
Duffy, T., Oncu, S., et al. (2006). Value of the CCNA 
program: perspectives on satisfaction and applicability from 
CCNA and comparison group students Cisco Networking 
Academy Evaluation Project. Bloomington, IN: Indiana 
University 
[4] 
Randall, M. H., & Zirkle, C. J. (2005). Information 
technology student-based certification in formal education 
settings: who benefits and what is needed. Journal of 
Information Technology Education, 4, 287-306.  
[5] 
The first student survey is at www.tinyurl.com/ccna-
int-out and the second survey for students is at 
www.tinyurl.com/ccna-final-out 
[6]  Sheridan, B. (2009). Rasch Unidemensional Measurement 
Model (Version 2030). Duncraig, Western Australia: Rumm 
Laboratory Pty Ltd. Retrieved from www.rummlab.com 
[7]  Bond, T. G., & Fox, C. M. (2007). Applying the Rasch 
Model: fundamental measurement in the human sciences 
(second ed.). London: Lawrence Erlbaum Associates, 
Publishers. 
 
343
ICNS 2011 : The Seventh International Conference on Networking and Services
Copyright (c) IARIA, 2011.     ISBN: 978-1-61208-133-5

