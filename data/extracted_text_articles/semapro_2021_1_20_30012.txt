An Extensible Semantic Data Fusion Framework for Autonomous Vehicles 
Efstratios Kontopoulos, Panagiotis Mitzias, 
Konstantinos Avgerinakis, Pavlos Kosmides 
Catalink Limited 
Nicosia, Cyprus 
email: {e.kontopoulos, pmitzias, koafgeri, 
pkosmidis}@catalink.eu 
Nikos Piperigkos, Christos Anagnostopoulos, Aris S. 
Lalos 
Industrial Systems Institute 
Athena Research Center 
Athens, Greece 
email: {piperigkos, anagnostopoulos, lalos}@isi.gr
 
Nikolaos Stagakis, Gerasimos Arvanitis, Evangelia I. Zacharaki, Konstantinos Moustakas 
Electrical and Computer Engineering 
University of Patras 
Patras, Greece 
email: {nick.stag, arvanitis, moustakas}@ece.upatras.gr 
ezachar@upatras.gr 
 
 
Abstract‚ÄîFully autonomous vehicles may still be an elusive 
goal, however, research in the deployment of relevant Artificial 
Intelligence technologies in the domain is rapidly gaining 
traction. A key challenge lies in the fusion of all the diverse 
information from the various sensors on the vehicle and its 
environment. In this context, ontologies and semantic 
technologies can effectively address this challenge by 
semantically fusing heterogeneous pieces of information into a 
uniform Knowledge Graph. This paper presents CASPAR, an 
extensible semantic data fusion platform for autonomous 
vehicles. Two use case scenarios are also presented that 
demonstrate the framework‚Äôs versatility. 
Keywords-Autonomous 
vehicles; 
ontologies; 
knowledge 
graphs; semantic data fusion; AI. 
I. 
 INTRODUCTION 
Although fully autonomous vehicles are still an elusive 
goal, research in the field is rapidly gaining traction, with 
relevant studies estimating the value of the automotive AI 
market at a little over $10.5 billion by 2025 [1]. Consequently, 
most major automotive manufacturers are increasingly 
investing in the field. 
The key challenge in this domain lies in the fact that AI 
systems operating in such dynamic settings must deal 
effectively with large volumes of streaming data generated by 
the various sensors on the vehicle (e.g., camera, radar, Light 
Detection and Ranging ‚Äì LiDAR, Global Positioning System 
‚Äì GPS, etc.) as well as by the vehicle‚Äôs environment (e.g., 
pervasive inputs, weather data, other vehicles, etc.). The 
fusion of all this diverse information is, thus, a non-trivial and 
highly error-prone process.   
In this context, semantic technologies and, most 
prominently, ontologies seem like a natural fit for 
semantically fusing heterogeneous pieces of information into 
a uniform knowledge representation model, i.e., a Knowledge 
Graph (KG). This paper presents ongoing work on an 
extensible semantic data fusion framework for autonomous 
vehicles, called CASPAR [2]. The framework is part of a 
larger platform being developed within the context of the 
CPSoSaware EU-funded project [3]. 
The rest of the paper is structured as follows: Section 2 
gives an overview of related work on deploying semantic 
technologies in the domain of autonomous and connected 
vehicles. Section 3 presents the proposed approach, describing 
the architecture, input sources, as well as the semantic data 
fusion component. Section 4 presents two use case scenarios 
and their evaluation, and, finally, Section 5 concludes the 
paper with some final remarks and directions for future work. 
II. 
RELATED WORK 
It was roughly 20 years ago that the issue of data 
heterogeneity in the automotive industry was gradually 
emerging as a critical challenge, and the first approaches 
proposed the addition of a semantic layer on top of the lower-
level sensors and systems of the vehicle. In collaboration with 
the German car manufacturer Audi AG, the authors in [4] 
proposed an ontology for representing the various parts and 
sensors of a car. Other early approaches adopted ontology-
based representation of the context and the situations 
surrounding the vehicle, like, e.g., the road network and all 
detected objects in the scene, an estimation of the behaviours 
of other traffic participants, as well as the mission goal of the 
own vehicle [5]. 
In the same context, the works presented in [6]-[8] propose 
ontology-based representations of road intersections and road 
infrastructures that could serve the basis for traffic models and 
systems that could predict conflicts between vehicles reaching 
the same intersection.  
Extending the scope beyond representing vehicle- and 
sensor-related aspects, other approaches adopt a user-centred 
view of the world, also considering aspects like the mental and 
physiological state of the driver [9][10] or their grip force and 
alcohol density [11]. 
In more recent works, bigger players entered the game, 
and more holistic Advanced Driver Assistance Systems 
5
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-888-4
SEMAPRO 2021 : The Fifteenth International Conference on Advances in Semantic Processing

(ADAS) were proposed, utilizing a wider range of (the now 
more mature) semantic technologies. [12] and [13] present 
intelligent decision-making systems, as part of an ADAS, for 
assisting autonomous vehicles in making appropriate 
decisions during certain cases. The systems consist of an 
ontology-based KG, as well as a set of Semantic Web Rule 
Language (SWRL) rules for representing traffic regulations 
and spatiotemporal relationships between entities. In both 
works, thorough evaluations regarding semantic reasoning 
and result-set retrieval times are conducted, but, arguably, the 
respective sizes of the KGs are rather small. 
The authors in [14] present a more standards-oriented 
approach, proposing the Vehicle Signal and Attribute 
Ontology (VSSo) that is based on the Sensor, Observation, 
Sample, and Actuator (SOSA) ontology [15] for representing 
sensor measurements, on the Vehicle Signal Specification 
(VSS) [16] for representing domain-pertinent aspects (i.e., 
vehicle signals), and on the Web of Things principles [17] for 
defining technology and protocol-independent interactions 
with Web Things. This combination facilitates the 
decorrelation from automotive standards, enabling the 
collection and analysis of sensor data coming from vehicles of 
different models and brands, and allows integrating car data 
with data coming from other Internet-of-Things (IoT) sources 
from the Web. As suggested by the authors, VSSo can form 
the basis for various applications like car fleet monitoring, car 
trajectory mining, contextual representation of a car and 
interaction between any car and web services. 
Compared to the existing approaches presented above, our 
framework does not ingest raw sensor measurements into the 
KG, but instead adds the higher-level outputs generated by 
analyses performed by other components at a lower level, like, 
e.g., Driver Monitoring Systems (DMSs), driver‚Äôs wearables, 
and visual odometers. This approach offers richer insights 
about various aspects of the vehicle and the driver, like, e.g., 
the system health or the driver‚Äôs state during a driving session. 
III. 
PROPOSED APPROACH 
This section presents our proposed approach, focusing on 
the architecture, input sources, as well as the CASPAR 
semantic data fusion component. 
A. Architecture 
An overview of the system encompassing the semantic 
data fusion framework is presented in Figure 1. Adopting the 
microservice methodology [18], we defined a set of 
independent, replicable services that collaboratively fulfil the 
system‚Äôs functionality. For the communications among 
services, we deployed RabbitMQ [19], a popular open-source 
message broker that is scalable and industry-ready. 
The system components in the monitoring layer 
periodically collect and analyze data related to the driver, the 
vehicle and its surroundings. Five monitoring components 
(DSO, LeGO, CL, DMS and OFE) - all introduced in the next 
subsection - are currently integrated. However, the modularity 
provided by the microservice approach, coupled with the 
straightforward system design, enables the integration of 
third-party data sources (e.g., weather or traffic condition 
reports) with minimum effort. 
 
Figure 1.  Overview of the system architecture. 
The outputs and observations produced by the monitoring 
layer are communicated to the semantic data fusion layer via 
a dedicated RabbitMQ exchange. At this stage, they are 
mapped to ontology concepts, resulting in a unified 
Knowledge Graph (KG), which is instantiated in a Resource 
Description Framework (RDF) triplestore by the CASPAR 
component, which is further described in a next subsection. 
B. Input Sources 
1) Odometry Algorithms: The quantitative trajectory 
evaluation of odometry algorithms is an issue which has been 
examined thoroughly by the research community. A few 
metrics have been proposed over the last years, of which the 
Absolute Trajectory Error (ATE) and the Relative Pose Error 
(RPE) are the most popular. More specifically, let us assume 
that the output of a Simultaneous Localization and Mapping 
(SLAM) algorithm, thus the estimated trajectory is a set of n 
distinct poses Pi ‚àà SE3, where SE3 is the Special Euclidean 
space of rigid body transformations in three dimensional 
space. Each element of this space can be expressed in the 
form of a 4x4 matrix: 
ùõ≠ = #ùëÖ
ùëá
0
1( 
where R ! SO3 is the rotation part, T ! R3 is the translation part, 
and SO3 is the special orthogonal group that contains the 
rotations. Accordingly, the ground truth trajectory is consisted 
of n Gi ! SE3 poses in an arbitrary coordinate system. 
In order to compute the ATE, which gives us the total 
consistency of the algorithm, we have to align the two 
trajectories using an algorithm like Horn‚Äôs method [20]. 
Consequently, the ATE error matrix Ei for each of the n 
estimated poses, can be computed by the following equation: 
ùê∏! ‚à∂= ùê∫!
"#ùê¥ùëÉ! 
where A is the alignment matrix. Usually, we use the Root 
Mean Square Error (RMSE) of ATE which is calculated as 
follows [21]: 
6
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-888-4
SEMAPRO 2021 : The Fifteenth International Conference on Advances in Semantic Processing

ùê∏$%&' ‚à∂= /1
ùëõ 1‚Äñùëá!‚Äñ(
)
!*#
 
The RPE is a metric which indicates the accuracy of the 
algorithm over a specific time step. Let us assume that we 
have a common time step ‚àÜ for both the algorithm and the 
ground truth trajectory, the RPE matrix Ri‚àÜ can be calculated 
by the following equation [21]: 
ùëÖ!
+ ‚à∂= (ùê∫!
"#ùê∫!,+)"#(ùëÉ!
"#ùëÉ!,+) 
The RMSE for the translation of RPE is calculated as in the 
case of ATE. 
The odometry algorithms used in this paper are presented 
below:  
Direct Sparse Odometry (DSO) [22] is a state-of-the-art 
monocular visual odometry solution, relying on the Camera 
sensor. Contrast to most related methods, it features the 
combination of Sparse+Direct: it optimizes the photometric 
error defined directly on images, without exploiting any 
geometric prior, using the so-called keypoints from some 
keyframes. One of the main benefits of keypoints is their 
robustness to photometric variations.  In addition, the main 
drawback of adding geometry priors is the introduction of 
correlations between geometry parameters, which render a 
statistically consistent, joint optimization in real time 
infeasible. DSO provides a strategy for keyframes and 
keypoints management, which leads to a windowed 
optimization problem, solved by Gauss-Newton (GN) 
method. Only the most useful frames out of consecutives 
frames are kept (tracking). And then, some active points are 
determined in order to estimate the pose of the vehicle using 
GN.   
LeGO-LOAM [23] is a lightweight and ground-optimized 
LiDAR odometry solution, which introduces a two-step 
Levenberg Marquardt (LM) optimization for pose estimation. 
As shown in Figure 2, before the feature extraction module, 
the point cloud from LiDAR is being processed by the 
segmentation module. 
 
Figure 2.  LeGO-LOAM system overview. 
The segmentation model is responsible for creating 
clusters of points from the point cloud. More specifically, it 
assigns three related values to individual 3D points: (a) label 
as a ground or segmented point, (b) column and row index in 
the depth image (created by projecting all points to the image 
plane), (c) depth value. Feature extraction is responsible for 
categorizing the points from segmentation to either planar or 
edge features. And finally, the pose is estimated by 
performing LM optimization using these two groups of 
vehicles between consecutive LiDAR scans. 
Cooperative Localization (CL) is expected to further 
improve the positioning accuracy of the Localization sub-
system of Connected and Autonomous Vehicles. Vehicles, 
apart from the advanced sensors of LiDAR, Camera, etc., 
benefit from direct V2V communication and exchange of rich 
information, for increased perception and scene analysis 
ability. Graph Laplacian processing [24][25] enables the 
fusion of heterogeneous measurements from vehicles in a 
linear and compact way, contributing to efficient location 
estimation. It makes use of connectivity representation of 
collaborating vehicles, along with the inter-vehicular 
measurements (noisy distances, angles, and positions) 
provided by the Perception sub-system, in order to formulate 
a linear least-squares estimation problem. Combined with the 
Extended Kalman Filter, it also exploits the motion properties 
of vehicles, significantly increasing location accuracy [26]. 
Note that CL could be useful for mitigating GPS location 
spoofing cyberattacks [27]. 
2) Driver Monitoring System (DMS): Our DMS module 
captures frontal facial images of the driver to assess fatigue 
levels based on the activity of the eyes. The driver‚Äôs 
drowsiness is measured based on two metrics, the Eye Aspect 
Ratio (EAR) [28] and the PERcentage of Eye CLOSure 
(PERCLOS) [29]. To obtain the facial landmarks we use the 
Dlib toolkit, which provides us with 68 facial landmarks 
characterizing various facial features, such as eyes, nose, 
mouth, etc. From those 68 points, 12 points correspond to the 
eyes. We use these landmarks to calculate the ratio of the 
vertical and horizontal lines defined by the eclipse that is 
fitted to the eye. This ratio is computed as: 
ùê∏ùê¥ùëÖ = ‚Äñùëù( ‚àí ùëù-‚Äñ + ‚Äñùëù. ‚àí ùëù/‚Äñ
2‚Äñùëù# ‚àí ùëù0‚Äñ
 
Typical values indicating eyelid closure were determined 
at EAR < 0.2 in [28]. In our tests, we found that an EAR 
threshold closer to 0.25 performs better in this simulation 
context. 
The PERCLOS measure is defined as the percent of the 
time the eyelid occludes the pupil (EAR < 0.25) within a K-
second moving window, where K can be tuned by the user. 
Typical values of K are around 60 seconds. Therefore, 
PERCLOS is calculated as: 
 
In literature, the values that have been suggested as 
representative of low drowsiness state are typically under the 
0.25% PERCLOS and 70% or 80% (known as PERCLOS70 
and PERCLOS80, respectively) for high drowsiness [30]. 
3) Occupancy Factor Estimation (OFE): The Occupancy 
Factor (OF) is an empirical metric, extracted by analysing the 
point cloud of the scene that has been acquired by the LiDAR 
device. It indicates how ‚Äúclear and open‚Äù the road is beyond 
7
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-888-4
SEMAPRO 2021 : The Fifteenth International Conference on Advances in Semantic Processing

the driver‚Äôs field of view. Based on this value, the road‚Äôs 
condition can be separated into three categories: (a) Safe road 
without objects, (b) Road with small obstacles (e.g., potholes) 
or cars at a far distance from the vehicle, (c) Road with a lot 
of traffic, parked cars, etc. 
OF is estimated via point cloud processing. In more detail, 
after the acquisition of the point cloud by the LiDAR, a 
geometry processing technique is applied to estimate the 
saliency map of the point cloud scene [31]. 
The saliency map extraction assigns at each vertex of the 
point cloud a value based on its distinctiveness (geometrical 
importance). To visualize the saliency map of the point cloud, 
we quantize the range of value into 64 classes which we map 
to 64 colours, as presented in Figure 3. The lowest saliency 
values correspond to deep blue, while the highest values 
correspond to deep red. Vertices that lie in totally flat areas 
take the lowest value (as not being salient), while vertices 
lying in very sharp corners take the highest value. 
 
Figure 3.  Example point cloud showcasing our color mapping. 
The next step addresses the spatial scene analysis. 
Specifically, we are interested in the segmentation of the road. 
For estimating OF, we take into consideration only these set 
of vertices, denoted as N below, that: (a) belong to the region 
of the road and (b) correspond to the lowest saliency value 
(i.e., totally flat area of the road). Finally, OF is estimated as 
the sum of the inverse norm2 distance between each vertex, of 
the previously aforementioned set of vertices, and the point v1 
(0,0,0) that represents the centre of the LiDAR sensor. 
 
The higher the OF value, the less occupied the road for 
driving. 
C. Semantic Data Fusion 
The integration of inputs (see previous subsection) to the 
unified KG is handled by CASPAR (Structured Data Semantic 
Exploitation Framework), our domain-agnostic tool for the 
automated retrieval and fusion of structured data from 
disparate sources into domain-specific semantic models, 
facilitating the discovery of new knowledge along with the 
extraction of actionable insights.  
CASPAR is based on the ontology population principles 
presented in recent works of ours [32][33]. In a nutshell, the 
tool administers a set of interconnected mechanisms for 
transforming data into knowledge, represented in a machine-
interpretable 
and 
exploitable 
format 
(RDF). 
These 
mechanisms incorporate: (a) the automated acquisition of 
structured data from user-defined sources (APIs, databases, 
messaging buses, etc.), (b) the mapping of input data fields to 
semantic entities (concepts, relationships, etc.), (c) the 
semantic fusion and population of knowledge into a semantic 
repository, (d) the semantic enrichment of existing knowledge 
from external Linked Open Data repositories, and, (e) the 
application of rule-based semantic reasoning to unveil 
underlying or generate new knowledge. 
For the purposes of this work, the SOSA ontology [15] 
serves as the core semantic model for describing sensors and 
their observations, the studied features of interest and the 
observed properties. As described in the next section, the core 
model is populated with the outputs generated by the input 
sources, i.e., other analysis components in the CPSoSaware 
project architecture. 
 
IV. 
SCENARIOS AND RESULTS 
This section presents two use case scenarios applying the 
proposed architecture and semantic data fusion component. 
Evaluation results are also discussed. 
A. Scenario #1: Evaluate the Robustness of Odometry 
Algorithms 
In the first scenario we rely on an end-to-end testing 
framework, based on the CARLA open-source urban driving 
simulator [34], for generating synthetic sensory data and 
evaluating the three aforementioned odometry algorithms 
against different weather and lighting conditions. Each 
algorithm uses  a different modality and our purpose is to 
study the effect of the changing conditions on the efficiency 
of each algorithm. 
 
Figure 4.  Excerpt of the ATE and RPE observations submitted to 
CASPAR. 
Based on the architecture described above, ATE and RPE 
measurements are sent via the message bus to the CASPAR 
8
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-888-4
SEMAPRO 2021 : The Fifteenth International Conference on Advances in Semantic Processing

semantic data fusion framework and are ingested into the KG. 
Indicatively, 1226 observations were submitted for a driving 
simulation of 126 seconds. Figure 4 displays an excerpt of the 
observations, while Figure 5 illustrates the representation of 
the same sample observations in Graffoo format [35] fused 
inside the KG. As seen in the latter figure, no conflict 
resolution considerations are raised, since SOSA facilitates 
the explicit association of observations to the respective 
sources via 
sosa:madeBySensor, as well as the 
representation 
of 
different 
observed 
properties 
via 
sosa:observedProperty. 
 
Figure 5.  Excerpt of the ATE and RPE observations submitted to 
CASPAR. 
After the population of the KG is complete, useful insights 
regarding the performance of the algorithm can be extracted. 
Figure 6 displays such an example, where the LiDAR-based 
odometry algorithm (LeGO-LOAM) presents better results 
with regards to RPE and seems to be more robust, constituting 
thus a better candidate in conditions similar to the specific 
simulation session.  
 
Figure 6.  Performance comparison of LeGO vs DSO for a simulation 
session. 
More specifically, as illustrated in Figure 6, the LiDAR 
odometer outperforms the visual odometer in terms of the 
relative drift between two consecutive poses, which depicts an 
indicative use case in which the reduced environmental light 
(night) resulted in the downgrade of the DSO performance. 
Additionally, LIDAR‚Äôs robustness has been also pointed 
out in [23]. Specifically, vision-based methods are sensitive to 
illumination and viewpoint changes. On the contrary, LiDAR 
functions well even at night and the high resolution of many 
3D point-clouds permits the capture of the fine details of an 
environment at long ranges, over a wide aperture. 
However, it must be noted that the superiority of LiDAR 
has only been identified in a specific set of scenarios. In the 
future, we plan to extend the set of scenarios and include more 
use cases (e.g., road bumps, sudden breaks, dynamic objects 
etc.), in order to further examine the robustness of the 
algorithms. 
B. Scenario #2: Calculate Risk Levels during a Driving 
Session 
In the second scenario, our objective is to inform the driver 
about potential risks during a driving session. We focus on 
two factors: The driver‚Äôs drowsiness and the free available 
space of the road. For this purpose, two components have been 
developed (see also Figure 1): (a) the Driver Monitoring 
System (DMS) component, and (b) the Occupancy Factor 
Estimation (OFE) component. 
 
Figure 7.  Driving simulation setup for integrating DMS and OEF. 
For the evaluation of our implementation, we integrated 
the DMS with CARLA [34], whose spectacularly 
photorealistic graphics provide an immersive driving 
experience. The simulator provides the flexibility to design a 
variety of driving scenarios under different states of driver‚Äôs 
drowsiness and different conditions of the road (e.g., the state 
of the traffic), in a safe environment for the operator who tests 
the implementation. The setup of the integration (see Figure 
7) uses the Logitech G29 steering wheel for enhancing the 
driving sense, as well as a static web camera that captures the 
face of the driver in real-time. 
Similar to the previous scenario, the DMS and OFE 
modules submit their observations, namely the PERCLOS and 
OF measurements, to CASPAR via RabbitMQ. However, an 
upgrade compared to scenario #1 entails a set of rules (see 
Table I) for calculating the risk levels during the simulated 
driving session. Risk level 1 corresponds to a ‚Äúlow risk‚Äù 
driving situation, where the driver is focused and drives 
carefully in a full open-eyed state (without any observed 
drowsiness). Moreover, the road is free from other vehicles, 
providing thus an unobstructed area for driving. On the other 
hand, risk level 3 corresponds to a ‚Äúhigh risk‚Äù driving situation 
where the driver demonstrates intense drowsiness, as 
identified by the facial analysis of the DMS component, with 
intense drowsiness and/or the unobstructed area of the road is 
9
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-888-4
SEMAPRO 2021 : The Fifteenth International Conference on Advances in Semantic Processing

restricted (due to obstacles, a lot of traffic, small-ranged road, 
etc). 
TABLE I.  
SET OF RULES FOR CALCULATING THE RISK LEVEL 
 
PERCLOS        
< 0.25 
PERCLOS >= 
0.25 & <0.7 
PERCLOS     
>= 0.7 
OF > 280 
Low risk (1) 
Be aware (2) 
High risk (3) 
OF <= 280 & 
>200 
Low risk (1) 
Be aware (2) 
High risk (3) 
OF <= 200 
Be aware (2) 
High risk (3) 
High risk (3) 
 
After the KG is populated through CASPAR, according to 
the approach described before (see Figure 5), the above ruleset 
is executed in the form of a respective SPARQL query ‚Äúon-
top‚Äù of the KG. The result is a risk level report, as illustrated 
in Figure 8. Outputs like this can constitute parts of reports, 
e.g., after traffic accidents.  
 
Figure 8.  Output graph indicating the risk levels during a driving session. 
Observing Table I and Figure 8, we see that when 
PERCLOS is higher than 0.7 (i.e., intense drowsiness), the 
risk level is always equal to 3 (i.e., high risk), independently 
of the value of OF. On the other hand, when PERCLOS is 
lower than 0.25, then the risk level is 1 (i.e., low risk), and 
correspondingly when the PERCLOS ranges from 0.25 to 0.7, 
the risk level is 2 (i.e., be aware). In these cases, the risk level 
is changed (level up) only when OF is lower than 200 
indicating that the driver has to draw extra attention. 
V. 
CONCLUSION 
This paper presented CASPAR, a semantic data fusion 
framework for autonomous vehicle that is part of a larger 
platform in the context of an EU-funded project. The inputs to 
CASPAR constitute analysis results generated by other 
components in the platform and, this way, higher-level and 
richer insights can be derived regarding various aspects of the 
vehicle and the driver. In this context, the two scenarios 
presented in the paper demonstrate the framework‚Äôs 
functionality and versatility in the domain. 
However, this is largely still a work-in-progress. Thus, our 
next steps involve testing the semantic data fusion framework 
in a wider variety of simulation scenarios involving more 
sources of information (e.g., steering frequency, weather info, 
biometrics, etc.) and more challenging conditions (e.g., 
dynamic objects, reduced visibility, sudden braking, etc.). 
This would also entail extending the rule-base accordingly. A 
parallel future direction also involves considering the 
extraction of real-time analytics and insights, which, thus far, 
was not possible due to challenges in the scalability and 
performance of the triplestores we considered. 
ACKNOWLEDGMENT 
This work has received funding from the European 
Union‚Äôs Horizon 2020 research and innovation programme 
under Grant Agreement No 871738 - CPSoSaware: 
Crosslayer cognitive optimization tools & methods for the 
lifecycle support of dependable CPSoS. 
REFERENCES 
[1] Automotive Artificial Intelligence Market by Offering 
(Hardware, Software), Technology (Deep Learning, Machine 
Learning, Computer Vision, Context Awareness and Natural 
Language Processing), Process, Application and Region - 
Global 
Forecast 
to 
2025. 
Available 
at: 
https://www.marketsandmarkets.com/Market-
Reports/automotive-artificial-intelligence-market-
248804391.html. [retrieved: August, 2021] 
[2] CASPAR homepage. Available at: https://caspar.catalink.eu/. 
[retrieved: August, 2021] 
[3] CPSoSaware H2020 project homepage. Available at: 
https://cpsosaware.eu/. [retrieved: August, 2021] 
[4] A. Maier, H. P. Schnurr, and Y. Sure, ‚ÄúOntology-based 
information integration in the automotive industry‚Äù In 
International Semantic Web Conference, Springer, Berlin, 
Heidelberg, Oct. 2003, pp. 897-912. 
[5] S. Vacek, T. Gindele, J. M. Zollner, and R. Dillmann, 
‚ÄúSituation classification for cognitive automobiles using case-
based reasoning‚Äù In 2007 IEEE Intelligent Vehicles 
Symposium, IEEE Press, June 2007, pp. 704-709. 
[6] B. Hummel, W. Thiemann, and I. Lulcheva, ‚ÄúScene 
understanding of urban road intersections with description 
logic‚Äù In Dagstuhl Seminar Proceedings, Schloss Dagstuhl-
Leibniz-Zentrum fr Informatik, 2008.  
[7] R. Regele, ‚ÄúUsing ontology-based traffic models for more 
efficient decision making of autonomous vehicles‚Äù In 4th 
International Conference on Autonomic and Autonomous 
Systems (ICAS'08), IEEE Press, March 2008, pp. 94-99. 
[8] M. H√ºlsen, J. M. Z√∂llner, and C. Weiss, ‚ÄúTraffic intersection 
situation description ontology for advanced driver assistance‚Äù 
In 2011 IEEE Intelligent Vehicles Symposium (IV), IEEE 
Press, June 2011, pp. 993-999. 
[9] M. Feld and C. M√ºller, ‚ÄúThe automotive ontology: managing 
knowledge inside the vehicle and sharing it between cars‚Äù In 
Proceedings of the 3rd International Conference on Automotive 
User Interfaces and Interactive Vehicular Applications, Nov. 
2011, pp. 79-86. 
[10] M. Madkour and A. Maach, ‚ÄúOntology-based context 
modeling for vehicle context-aware services‚Äù Journal of 
Theoretical and Applied Information Technology, vol. 34(2), 
pp. 158-166, 2011. 
[11] S. Kannan, A. Thangavelu, and R. Kalivaradhan, ‚ÄúAn 
intelligent driver assistance system (i-das) for vehicle safety 
modelling using ontology approach‚Äù International Journal of 
UbiComp, vol. 1(3), pp. 15-29, 2010. 
[12] A. Armand, D. Filliat, and J. Iba√±ez-Guzman, ‚ÄúOntology-based 
context awareness for driving assistance systems‚Äù In 2014 
IEEE intelligent vehicles symposium proceedings, IEEE Press, 
June 2014, pp. 227-233. 
10
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-888-4
SEMAPRO 2021 : The Fifteenth International Conference on Advances in Semantic Processing

[13] L. Zhao, R. Ichise, S. Mita, and Y. Sasaki, ‚ÄúCore Ontologies 
for Safe Autonomous Driving‚Äù In International Semantic Web 
Conference (Posters & Demos), Oct. 2015. 
[14] B. Klotz, R. Troncy, D. Wilms, and C. Bonnet, ‚ÄúVSSo: The 
Vehicle Signal and Attribute Ontology‚Äù In SSN@ ISWC, Oct. 
2018, pp. 56-63. 
[15] A. Haller et al., ‚ÄúThe modular SSN ontology: A joint W3C and 
OGC standard specifying the semantics of sensors, 
observations, sampling, and actuation‚Äù Semantic Web, vol. 
10(1), pp. 9-32, 2019. 
[16] Vehicle 
Signal 
Specification: 
https://genivi.github.io/vehicle_signal_specification/. 
[retrieved: August, 2021] 
[17] M. Kovatsch, R. Matsukura, M. Lagally, T. Kawaguchi, K. 
Toumura, and K. Kajimoto, ‚ÄúWeb of Things (WoT) 
Architecture‚Äù W3C Recommendation 9 April 2020. Available 
at: https://www.w3.org/TR/wot-architecture/Overview.html. 
[retrieved: August, 2021] 
[18] S. Newman, ‚ÄúBuilding microservices: designing fine-grained 
systems‚Äù O'Reilly Media Inc, 2015. 
[19] Œú. Rostanski, Œö. Grochla, and Œë. Seman, ‚ÄúEvaluation of 
highly available and fault-tolerant middleware clustered 
architectures using RabbitMQ‚Äù In 2014 federated conference 
on computer science and information systems, IEEE Press, 
Sept. 2014, pp. 879-884. 
[20] Œí. Œö. Horn, ‚ÄúClosed-form solution of absolute orientation 
using unit quaternions‚Äù Josa a, vol. 4(4), pp. 629-642, 1987. 
[21] D. Prokhorov, D. Zhukov, O. Barinova, K. Anton, and A. 
Vorontsova, ‚ÄúMeasuring robustness of Visual SLAM‚Äù In 16th 
International Conference on Machine Vision Applications 
(MVA), IEEE Press, May 2019, pp. 1-6. 
[22] J. Engel, V. Koltun, and D. Cremers, ‚ÄúDirect sparse odometry‚Äù 
IEEE transactions on pattern analysis and machine intelligence, 
vol. 40(3), pp. 611-625, 2017. 
[23] T. Shan and B. Englot, ‚ÄúLego-loam: Lightweight and ground-
optimized LiDAR odometry and mapping on variable terrain‚Äù 
In 2018 IEEE/RSJ International Conference on Intelligent 
Robots and Systems (IROS), IEEE Press, Oct. 2018, pp. 4758-
4765. 
[24] N. Piperigkos, A. S. Lalos, K. Berberidis, and C. 
Anagnostopoulos ‚ÄúCooperative multi-modal localization in 
connected and autonomous vehicles‚Äù In 2020 IEEE 3rd 
Connected and Automated Vehicles Symposium (CAVS), 
IEEE Press, Nov. 2020, pp. 1-5.  
[25] N. Piperigkos, A. S. Lalos, and K. Berberidis, ‚ÄúGraph based 
cooperative localization for connected and semi-autonomous 
vehicles‚Äù In 2020 IEEE 25th International Workshop on 
Computer Aided Modeling and Design of Communication 
Links and Networks (CAMAD), IEEE Press, Sept. 2020, pp. 1-
6.  
[26] N. Piperigkos, A. S. Lalos, and K. Berberidis, ‚ÄúGraph 
Laplacian Extended Kalman Filter for Connected and 
Automated Vehicles Localization‚Äù In 2021 4th IEEE 
International Conference on Industrial Cyber-Physical Systems 
(ICPS), IEEE Press May 2021, pp. 328-333. 
[27] C. Vitale et al., ‚ÄúCARAMEL: results on a secure architecture 
for connected and autonomous vehicles detecting GPS 
spoofing 
attacks‚Äù 
EURASIP 
Journal 
on 
Wireless 
Communications and Networking, vol. 2021(1), pp. 1-28, 
2021. 
[28] F. You, X. Li, Y. Gong, H. Wang, and H. Li, ‚ÄúA real-time 
driving drowsiness detection algorithm with individual 
differences consideration‚Äù IEEE Access, vol 7, pp. 179396-
179408, 2019. 
[29] D. F. Dinges and R. Grace, ‚ÄúPERCLOS: A valid 
psychophysiological measure of alertness as assessed by 
psychomotor vigilance‚Äù US Department of Transportation, 
Federal Highway Administration, Publication Number 
FHWA-MCRT-98-006, 1998. 
[30] S. T. Lin, Y. Y. Tan, P. Y. Chua, L. K. Tey, and C. H. Ang, 
‚ÄúPerclos threshold for drowsiness detection during real 
driving‚Äù Journal of Vision, vol. 12(9), pp. 546-546, 2012. 
[31] G. Arvanitis, A. S. Lalos, and K. Moustakas, ‚ÄúRobust and fast 
3-D saliency mapping for industrial modeling applications‚Äù 
IEEE Transactions on Industrial Informatics, vol. 17(2), pp. 
1307-1317, 2020. 
[32] E. Kontopoulos, P. Mitzias, M. Riga, and I. Kompatsiaris, ‚ÄúA 
Domain-Agnostic Tool for Scalable Ontology Population and 
Enrichment from Diverse Linked Data Sources‚Äù In 
DAMDID/RCDL, Oct. 2017, pp. 184-190. 
[33] M. Riga, P. Mitzias, E. Kontopoulos, and I. Kompatsiaris,  
‚ÄúPROPheT‚ÄìOntology Population and Semantic Enrichment 
from Linked Data Sources‚Äù In International Conference on 
Data Analytics and Management in Data Intensive Domains, 
Springer, Cham, Oct. 2017, pp. 157-168. 
[34] Œë. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, 
‚ÄúCARLA: An open urban driving simulator‚Äù In Conference on 
robot learning (PMLR), Oct. 2017, pp. 1-16. 
[35] R. Falco, A. Gangemi, S. Peroni, D. Shotton, and F. Vitali. 
‚ÄúModelling OWL ontologies with Graffoo.‚Äù In European 
Semantic Web Conference, pp. 320-325. Springer, Cham, 
2014.  
 
11
Copyright (c) IARIA, 2021.     ISBN:  978-1-61208-888-4
SEMAPRO 2021 : The Fifteenth International Conference on Advances in Semantic Processing

