A Virtual Shopping System Based on Room-scale Virtual Reality
Chunmeng Lu
Graduate School of Information,
Production and Systems
Waseda University
Kitakyushu, Japan
Email: lcm0113@163.com
Jiro Tanaka
Graduate School of Information,
Production and Systems
Waseda University
Kitakyushu, Japan
Email: jiro@aoni.waseda.jp
Abstract—In the Virtual Reality (VR) environment, the user needs
to input information and achieves interaction with virtual objects.
At present, most VR systems provide some input devices, such
as keyboard and controller. However, using such devices is not
intuitive, especially in the case of VR shopping system. In the real
world, we use our hands to handle objects. In virtual world, using
hand gestures to interact with VR shopping store will provide us
more intuitive VR shopping experience. According to the needs of
the room-scale VR shopping activities, we have introduced a new
gesture classiﬁcation for the gesture set, which has three levels
to classify hand gestures based on the characteristics of gestures.
We have focused on the gestures in level 3. In our research, we
have built a room-scale VR shopping system and applied the new
hand gesture set for the interaction in the VR shopping system.
Keywords–Room-scale Virtual Reality; Gesture set; Gesture
classiﬁcation.
I.
INTRODUCTION
Virtual Reality makes it possible for the user to interact
with the virtual environment as if he is in the real world. It is
a kind of illusion of “being there” [1].
With the development of the computer graphics, 3D
technology and electrical engineering, Head Mount Display
(HMD) of VR has been gradually improved in the past
few years. Some technology companies have introduced their
simple and easy-to-use VR devices for the consumer market,
such as HTC Vive and Oculus Rift.
A. VR shopping
People can navigate in the virtual environment through a
HMD. Shopping is one of the important activities in our daily
life. People have already been familiar with e-commerce or
online shopping. We can extend online shopping to the virtual
environment.
Over the past decades, many VR shopping environments
have been presented. Some works aimed at improving VR
shopping experience and some works researched on the inter-
action in virtual shopping environments. Bhatt [2] presented
a theoretical framework which showed how to attract cus-
tomers through a website with the three factors: interactivity,
immersion and connectivity. Chen et al. [3] presented a Virtual
Reality Modeling Language - Based (VRML- Based) virtual
shopping mall. They analyzed personal behavior of customer
in the Virtual Shopping Mall System. They also explored the
application of intelligent agent in shopping guidance. Lee et
al. [4] designed a virtual interactive shopping environment and
analyzed whether the interface of virtual interface had positive
affects. Verhulst et al. [5] presented a VR user study. In this
study, they applied VR store as a tool to ﬁnd whether the user
in store had intention to buy the non-standard food. Speicher
et al. [6] introduced a Virtual Reality Shopping Experience
model. Their model had three parts: customer satisfaction, task
performance and user preference.
The previous researches have shown some good features
of VR shopping. Thus, some retail companies and online
shopping companies have become interested in VR shopping.
IKEA company [7] presented a room-scale VR environment,
in which the user could view a virtual kitchen and interact
with the furnitures. Another example comes from inVRsion
[8], which provided a virtual supermarket shopping system,
Shelfzone VR. In the future, there will be more applications
on VR shopping.
B. Room-scale VR and Hand Gesture
When a user is moving his view in virtual world with an
HMD, he cannot move his physical body in real world. Thus,
there is a huge gap between sensorial moving and physical
moving. It will reduce the immersion of VR greatly. This gap
will also cause motion sickness for some people [6] [9]. If
user’s walking is synchronous in both virtual world and real
world, the experience is much better.
Using controllers in the VR shopping environment is not
immersive enough. Users will feel a gap when using controllers
to catch the virtual objects which are similar to real objects.
Besides, the amount and functions of buttons in the controllers
are limited, which limits the interaction when using controllers
in the room-scale VR environment. To achieve VR shopping
activities, we can use the buttons to design interaction methods
in the VR shopping system. Comparing with controllers, using
hand gestures can improve the immersion of VR shopping
experience and also provide the rich interaction vocabulary
for the VR shopping system.
Hand gestures have been widely used in human-computer
interface. Gesture-based interaction provides a nature, intuitive
communication between people and devices. People use 2D
multi-touch gestures to interact with devices like smart phone
and computer in the daily life. 3D hand gestures can be used
by some devices equipped with camera or depth sensor. The
most important problem in hand gesture interaction is how to
make computers understand the meaning of hand gestures [10].
Wachs et al. [11] summarized the requirements of hand-gesture
interfaces and the challenges when applying hand gestures in
different applications. Yves et al. [12] presented a framework
191
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

for 3D visualization and manipulation in an immersive space.
Their works can be used in AR and VR systems. Karam et al.
[13] used depth camera and presented a two-hand interactive
menu to improve efﬁciency. These previous researches show
that hand gestures have many possibilities for human-computer
interaction ﬁeld.
In Section II, we introduce our goal and approach of our
study. In Section III, we show the system design, including VR
environment, gesture set and gesture classiﬁcation. In Section
IV, we show the hardware in our system. In Section V, we
explain how to achieve gesture recognition in our system. In
Section VI, we show how to use the gestures in room-scale VR.
In Section VII, we show the preliminary evaluation. In Section
VIII, we introduce some related works. Finally, in Section IX,
we show our conclusion.
II.
GOAL AND APPROACH
In this research, we aim to present a new hand gesture
set which is suitable for room-scale VR shopping system to
replace the controllers. We introduce a new gesture classiﬁca-
tion to make the gesture set more structural. We apply room-
scale shopping system to provide a immersive virtual shopping
environment, which is a simulation of physical shopping
store. In the room-scale VR shopping environment, the user
can walk around in his room to view the virtual shopping
environment through an HMD. We design the new gestures
for the room-scale VR shopping system. The user can interact
with VR environment by the natural hand gestures but not the
controllers. We introduce a gesture classiﬁcation for gestures,
which has three levels to classify hand gestures based on the
characteristics of gestures. Summarizing the hand gestures,
we get a new hand gesture set especially for room-scale VR
shopping activities. The hand gesture set will improve the
convenience and immersion of the room-scale VR shopping
system.
We use VR devices to build the room-scale VR shopping
system. In the system, there are two sensor stations installed
in the room. The two sensor stations create a walking area for
the user. When moving in the walking area, user’s motion will
be captured by the sensor stations. System will get rotation
and three-dimensional coordinates of the HMD worn by user.
The view in virtual environment will move synchronously
with the HMD. Then we use depth sensor to recognize the
hand gestures. In the virtual environment, the user can see his
virtual hands moving synchronously with his physical hands.
The system can realize the special gestures when the user
performing gestures near the depth sensor, and achieve the
interaction with virtual environment.
III.
SYSTEM DESIGN
The virtual shopping system is composed of a room-scale
VR shopping environment and the gesture-based interaction
system.
A. Room-scale VR Shopping Environment
In order to achieve VR shopping activities, we design a VR
shopping store as the shopping environment, which is similar
to the stores in the real world. We place some desks, shelves
and goods in the VR shopping store, as shown in Figure 1.
In the room-scale VR shopping system, we use HTC Vive
as the room-scale VR device, as shown in Figure 2. We use
Leap Motion [14] as the depth sensor to recognize the hand
gesture. Leap Motion is stuck on the HMD.
Figure 1. A VR store
Figure 2. HMD and depth sensor
We prepare an empty area in the real room. The empty
area is included in a 3D space. We use two tracking sensors
of HTC Vive to capture the motion and rotation of HMD in
the 3D space when the user using the VR shopping system.
As shown in Figure 3, the length of 3D space is 4 meters, the
width of 3D space is 3 meters and the height of 3D space is
2 meters. The 3D space contains walking area of the room.
Figure 3. 3D space and walking area
In the room-scale VR shopping environment, there is also a
virtual walking area, as shown in Figure 4. The virtual walking
area is same with the area in real room. As the VR shopping
store is larger than our real room, the user can change the
virtual walking area when view the whole VR shopping store.
B. Gesture Set
In our research, we need to design a series of hand gestures
specially for the room-scale VR shopping activities. The hand
gestures must provide a natural and suitable interaction for the
user and the system. According to the particular activities in
room-scale VR shopping system, we design these 14 gestures
in our system:
192
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

Figure 4. Walking area in VR environment
•
(1) pointing, (2) holding, (3) OK gesture, (4) No
gesture, (5) push/pull, (6) rotation, (7) drag, (8) wav-
ing, (9) click, (10) zoom in/out, (11) opening/closing,
(12) grab, (13) two-ﬁngers scroll/swipe, (14) changing
area.
These gestures combine a new gesture set for room-scale
VR shopping system.
C. Gesture Classiﬁcation
We present a new gesture classiﬁcation to classify the hand
gestures based on their different characteristics. The gesture
classiﬁcation has three levels:
•
Level 1: Core static hand postures are divided into the
level 1. In level 1, gestures are just hand shape without
hand motion. The classic example is pointing gesture.
•
Level 2: Dynamic palm motions are divided into the
level 2. In level 2, we just care about the palm
movement. We don’t care about the shapes of ﬁngers.
The classic examples are pull and push.
•
Level 3: Combination hand gestures are divided into
the level 3. Combination hand gestures combine the
features of level 1 and level 2 gestures. In level 3, we
care about the motion and shapes of ﬁngers and the
motion of palm.
In VR shopping environment, the hand gestures are divided
into different levels. In the ﬁgures of level 2 and level 3, the
red arrows mean the ﬁngers movement trends.
The classiﬁcation method will make the gesture set more
structural. This classiﬁcation method provides a structure that
can also be used in other gesture sets in different VR systems.
Based on the systematic structure, researchers can design
suitable gestures for their VR systems.
1) Level 1 Gestures: In our system, we use level 1 gestures
to give feedback to system. Level 1 gestures are static signals
for shopping system. We do not need to care about the motion
of ﬁngers or hands. System just needs to detect the hand
shapes. Figure 5 shows two level 1 gestures.
(a) OK gesture
(b) NO gesture
Figure 5. OK and NO gestures positive or negative feedback to system
2) Level 2 Gestures: Level 2 gestures are palm motions.
After choosing a virtual object, the user can use level 2 gestures
to control or interact with it. Figure 6 shows two level 2
gestures and Table I shows the functions of level 2 gestures.
(a) Push gesture
(b) Pull gesture
(c) Waving gesture
Figure 6. Level 2 gestures: push, pull and waving
TABLE I. FUNCTION OF GESTURES IN LEVEL 2
Level
Gesture
Function
2
Push/pull
Push or pull a virtual object with a hand.
2
Waving
Make virtual object return to the original
position.
3) Level 3 Gestures: Gestures in level 3 are hand gestures
that combine ﬁnger shapes and hand motions. These gestures
are complex and combine the features of level 1 and level 2
gestures.
Designing a suitable and convenient gesture set for user
determines whether the user could have an immersive VR
shopping experience. Level 1 and level 2 gestures are simple
and a little weak. Thus, level 3 gesture set is the focus of our
research.
In level 3, we need to recognize the hand shapes and detect
the motions of the ﬁngers and hands at the same time.
The gestures have different usage. Thus, we need to
introduce a classiﬁcation for level 3 gestures. There is the
classiﬁcation:
•
The core gesture: pointing gesture
•
Gestures for interacting with virtual object: (1) grab
gesture, (2) hold gesture, (3) drag gesture, (4) rotation
gesture, (5) zoom in/out gesture;
•
Gestutes for interacting with menu: (1) click gesture,
(2) scroll/swipe gesture, (3) opening/closing gesture;
•
Gesture for interacting with space: change area gesture
In the gesture set, pointing gesture is the most important
gesture, because we need to choose a target object or button
with the pointing gesture before any interaction, as shown in
Figure 7 and Table II.
TABLE II. FUNCTION OF POINTING GESTURE
Level
Gesture
Function
3
Pointing
Point a virtual object with index ﬁnger.
Some gestures are mainly used to interact with virtual
objects in the VR shopping store, such as moving a virtual
193
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

Figure 7. Pointing gesture
object. We design these gestures to achieve it: grab gesture,
hold gesture, drag gesture, rotation gesture, and zoom in/out
gesture, as shown in Figure 8 and Table III.
(a) Grab
(b) Holding
(c) Drag
(d) Rotation
(e) Zoom in
(f) Zoom out
Figure 8. Gestures for interacting with objects
TABLE III. FUNCTIONS OF GESTURES FOR INTERACTING WITH OBJECT
Level
Gesture
Function
3
Grab
Make object move close to hand and grab it
with hand.
3
Holding
Hold a virtual object on one hand.
3
Drag
Move virtual object freely with a hand.
3
Rotation
Rotate a virtual object when viewing it with
a hand.
3
Zoom in/out
Make a virtual object show a larger or
smaller size using relative motion of thumb
and index ﬁnger.
In some cases, we need to interact with menu to achieve
shopping activities. We design these gestures: click gesture,
scroll/swipe gesture, opening/closing gesture, as shown in
Figure 9 and Table IV.
In room-scale VR shopping system, the user can walk in
the real walking area in his own room. However, the room-
scale walking area is always smaller than the VR shopping
store. Thus, we need to design a gesture for the user to change
area in the room-scale VR shopping store. Figure 10 and
(a) Click
(b) Scroll/swipe
(c) Opening
(d) Closing
Figure 9. Gestures for interacting with menu
TABLE IV. FUNCTIONS OF THE GESTURES FOR INTERACTING WITH MENU
Level
Gesture
Function
3
Click gesture
Click the buttons with index ﬁnger.
3
Scroll/swipe
Using two ﬁngers gestures to control menus
in user interface.
3
Opening
Open ﬁve ﬁngers to open the dashboard.
3
Closing
Close ﬁve ﬁngers to close the dashboard.
Table V show the changing area gesture. When performing
the changing area gesture, the user needs to extend his index
ﬁnger and thumb ﬁnger. In the system, when the user wants
to change area in the VR shopping store, he can point at a
position on the virtual ﬂoor with the special hand gesture and
use thumb to click the index ﬁnger to tell system that he wants
to move there.
Figure 10. Gesture for changing area in VR environment
TABLE V. FUNCTION OF THE GESTURE FOR CHANGING AREA
Level
Gesture
Function
3
Changing area
Use index ﬁnger to point a new position on
the ﬂoor and make the thumb click the index
ﬁnger, then the view in VR will move to the
new position.
IV.
GESTURE RECOGNITION
In our system, we use Leap Motion as the depth sensor
to track the users’ hands. Leap Motion can track the joints,
ﬁngertips and palm center of user’s hands. Meantime, Leap
Motion can record the positions of these important points of
user’s hands in every frame.
With the original position data, we can use machine learn-
ing method to recognize the hand shapes. Then combining
194
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

the hand shapes and motions, we will achieve recognizing
the gestures that we design for the room-scale VR shopping
system.
A. Hand Shape Recognition
At ﬁrst, we need to conﬁrm how many hand shapes that
we need to recognize. In some cases, several hand gestures
have the same hand shape. For example, drag gesture, holding
gesture and rotation gesture have the same hand shape. Their
differences are the motions of palm. As we have introduced
all the 14 gestures we design in Section III, we summary the
following hand shapes that we will recognize, as shown in
Figure 11.
Besides, the system also needs to recognize whether the
user’s hands just move naturally without interaction intention.
So, we need to recognize the natural hand shape. Therefor,
there are 9 hand shapes that we need to realize.
Here is the relationship between 14 hand gestures and 9
hand shapes:
1)
OK hand shape (including 1 gestures): OK gesture.
2)
Pointing hand shape (including 3 gestures): pointing
gesture, NO gesture, click gesture.
3)
Extending
hand
shape(including
5
gestures):
pull/push gesture, wave gesture, holding gesture,
drag gesture, rotation gestures gesture.
4)
Grab hand shape (including 1 gestures): grab gesture.
5)
Zoom hand shape (including 1 gestures): zoom in/out
gesture.
6)
Scroll/swipe hand shape (including 1 gestures):
scroll/swipe gesture.
7)
Opening/closing hand shape (including 1 gestures):
opening/closing gesture.
8)
Changing area hand shape (including 1 gestures):
changing area gesture
9)
Natural hand shape: the hand shape when the user
move hands without interaction intention
Then we apply Support Vector Machine (SVM) method in
our system to realize these nine hand shapes. So, we need the
multi-label classiﬁcation method in our system. We use open
source software, libsvm-3.22 in our system [15]. There are
four steps for multi-label classiﬁcation:
•
data collection
•
data normalization and scale
•
model training
•
predicting
1) Data Collection: In a hand, we will capture the end-
points of bones and palm center as “key points” to describe
the hand structure [16]. As shown in Figure 12, we will track
all the position data of the key points in every frame in VR
environment.
2) Data Normalization and Scale: We calculate other key
point positions relative to palm center. Data normalization
follows these steps:
•
Move positions to make palm center on the origin
coordinate.
•
Rotate the points to make palm parallel to the x-z axis
plane.
•
Rotate the points around the y coordinate axis to make
the palm point the –z axis.
•
Scale the data to [-1, 1].
(a) OK hand shape
(b) Pointing hand shape
(c) Extending hand shape
(d) Grab hand shape
(e) Zoom hand shape
(f) Scroll/swipe
(g) Opening/closing
(h) Changing area
(i) Natural hand shape
Figure 11. The nine hand shapes
Figure 12. Key Points: two blue points represent the palm center and the
wrist joint; red points represent the endpoints of bones in hand
195
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

3) Model Training: There are three steps to train the data
and get a classiﬁer model:
•
Capture 50 groups coordinates of the key points for
every hand shape.
•
Normalize and scale the data and get 9 groups training
sets.
•
Through training sets, get the multi-label classiﬁer.
4) Predicting: After getting the classiﬁer, we will use it in
VR shopping system to recognize the nine hand shapes. When
the user viewing the room-scale VR shopping store, the user
moves his hands freely. Depth sensor tracks the hands and
gets a group of original data set in every frame. Then, with
the classiﬁer, we will get the predicting result. The predicting
result will tell the system which hand shape that the user is
performing. If the hand shape is natural hand shape, system
will not give feedback; if getting other hand shapes, system
will respond to the user’s interaction intention.
B. Motion Detection
After knowing the hand shape, the system needs motion
detection because gestures are deﬁned by both hand shape and
motion together.
Once getting the hand shape, the system begins motion
detection. The system will calculate the hand data in every
frame. For different hand shapes predicting result, system
detects hand center or different ﬁngertips to realize gestures.
Based on the hand shapes from label 1 to label 9, there are
nine situations when recognize motions.
•
Situation 1: for OK hand shape, OK gesture is the
level 1 gesture and the system does not need to
recognize the motion.
•
Situation 2: for pointing hand shape, if the system
ﬁnds that two hands are in pointing hand shape,
system needs to detect the positions of two index
ﬁngertips. If the two index ﬁngertips are close, it
shows that the user is performing NO gesture. If only
one hand is in pointing hand shape, the system needs
to detect the direction and motion of index ﬁnger,
because we need to use pointing gesture to choose
a target or click a button.
•
Situation 3: for extending hand shape, it is a little
complexed. (a) if system detects that the palm center
orients to face and moving toward to face, it is pull
gesture; (b) if system detects that the palm center
orients forward and moving forward, it is push gesture;
(c) if system detects that the palm center orients
left and moving to left, it is waving; (d) if system
detects that the palm center orients to sky, it is holding
gesture; (e) if system detects that the palm center
orients forward and moving on a vertical plane, it is
drag gesture; (f) if system detects that the palm center
orients to sky and rotating around the palm center, it
is rotation gesture.
•
Situation 4: for grab hand shape, the system detects
the motion of palm center and the target object follows
the motion of palm center.
•
Situation 5: for zoom hand shape, the system detects
the motion of index and thumb ﬁngertips. If the
ﬁngertips move away from each other, it is zoom in
gesture; if the ﬁngertips move closely to each other, it
is zoom out gesture. The movement distance will be
used to change the size of the target object.
•
Situation 6: for scroll/swipe hand shape, the system
detects the motion of index ﬁnger. The movement
distance will be used to control menu.
•
Situation 7: for opening/closing hand shape, the sys-
tem detects the motion of index, middle and thumb
ﬁngertips. If their motions are moving closely to each
other, it is closing gesture; if their motion is moving
away from each other, it is opening gesture.
•
Situation 8: for changing area hand shape, system
detects the direction of index ﬁnger and the motion of
thumb ﬁngertip. If index ﬁnger points to a position on
the ﬂoor and thumb ﬁngertip clicks the index ﬁnger,
it is changing area gesture and the user will move to
the target position.
•
Situation 9: for natural hand shape, system does not
need to detect any motion. Because in this situation,
the user moves his hands freely in 3D space and does
not want to interact with system.
V.
PRELIMINARY EVALUATION
We apply the gesture set in our VR environment to build
the interaction system. We design a typical shopping activity
as an example: viewing and buying a laptop.
Firstly, the user can move to the desk with the change area
gesture where the laptops are placed in the room-scale VR
shopping environment, as shown in Figure 13. In this situation,
the desk is a little far away from the user and is out of the
original walking area of the user. Then, the user walks near
to the desk. He selects one of the laptops with the pointing
gesture. Once being selected, the laptop will show a bounce
animation, as shown in Figure 14(a). The user can make the
laptop move to his hand with the hold gesture, as shown in
Figure 14(b). The user can also grab the model. After that, the
user can view the details of the laptop with the zoom in/out
gesture, as shown in Figure 14(c). Besides, he can also call
out the menu to check more information with the open/close
gesture and the scroll/swipe gesture, as shown in Figure 14(d)
and Figure 14(e). Finally, he can perform the OK gesture to tell
the system that he decides to buy it, as shown in Figure 14(f).
Figure 13. Using change area gesture
We invited 5 students to use our room-scale VR shopping
system. The range of their ages is from 19 to 27. They repeated
the shopping activity that we designed and performed the
nine hand shapes for 50 times respectively. We collected data
and found the errors of hand shape classiﬁcation with SVM
method.
Table VI shows the errors of classiﬁcation of every hand
shapes for every user. There are nine hand shapes. So, every
196
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

(a) Pointing a product
(b) Hold the product
(c) Zoom in/out the prod-
uct
(d) Use Open/close ges-
ture
(e) Use scroll/swipe ges-
ture
(f) Use OK gesture
Figure 14. The gestures used in shopping activities
user performs the hand shapes for 450 times in all in our
evaluation. Then, we can get the accuracy rate when a user is
performing hand shapes in our system, as shown in Table VII.
TABLE VI. ERRORS OF CLASSIFICATION OF NINE HAND SHAPES FOR
EVERY USER
Hand Shape
User 1
User 2
User 3
User 4
User 5
OK
1
0
0
2
1
Pointing
0
0
1
2
1
Extending
3
2
4
5
4
Grab
1
2
2
3
3
Zoom
0
1
1
3
1
Scroll/swipe
1
1
1
2
1
Opening/closing
0
0
1
1
0
Changing area
1
1
1
4
2
Natural
4
3
4
5
4
TABLE VII. THE ACCURACY RATE OF HAND SHAPE
RECOGNITION OF 5 USERS
User
Amount
Accuracy
Accuracy Rate
1
450
439
97.56%
2
450
440
97.78%
3
450
435
95.56%
4
450
423
94.00%
5
450
433
96.22%
In Table VI, we can see the relative high error rates of
the extending hand shape and the natural hand shape. That
is because these two hand shapes are similar when the user
performs them in our VR environment. For example, some
users often extend their ﬁngers when they perform nature hand
shape. Some users sometimes bend their ﬁngers a bit when
they use extending hand shape to hold the models. In this
situation, system cannot detect which hand shape that they
want to perform.
VI.
RELATED WORK
With the perfection of VR technology, many researchers
and companies try to apply VR technology in e-commerce
ﬁeld and want to ﬁnd a way to generate economic value.
Alibaba is a famous IT company and is known for its great
on-line shopping services. Alibaba presented a VR shopping
application, called Buy+, running on the smart phone [17]. The
Buy+ tried to combine the convenience of on-line shopping
and the facticity of physical store shopping. With the simple
and cheap VR devices and smart phone, people in China could
view an overseas virtual shopping mall and pay for orders on-
line.
Some companies use VR technology to create virtual store.
IKEA is a famous furniture company. It presented a room-
scale VR kitchen to show its beautiful design [7]. In the
room-scale VR kitchen, the user could use HTC Vive to
view the equal proportion VR kitchen, even could interact
with the VR environment like opening the virtual range hood.
Comparing with physical furniture stores, the VR environment
could provide more functions and interactions. The user could
view the kitchen freely in his own room without warring about
the crowd in the physical IKEA mall. In the VR kitchen, the
user also could change the color of furniture freely, which is
impossible in the physical IKEA mall.
A VR technology company, inVRsion, presented a VR
supermarket system based on room-scale VR [8]. Their retail
space, products and shopping experience VR solutions provide
an immersive shopping environment. In the VR shopping
environment, businessman could analyze shopper behavior
through eye-tracking for extremely powerful market research
insights. The system could help sellers to test their category
projects, new packaging and communication in the stores
before implementation. The users could search their target
products more easily than physical supermarket. This system
tries to provide a method for people to view a big virtual
supermarket in his own room.
In our previous work, we extended 2D multi-touch inter-
action to 3D space and introduced a universal multi-touch
gestures for 3D space [16]. We called these midair gestures
in 3D as 3D multi-touch-like gestures.
The previous related works prove the broad application
prospect of room-scale VR shopping and gestures. Our work
about designing gesture set for room-scale VR can be used in
these systems to provide better VR shopping experience.
VII.
CONCLUSION
In this research, we built a room-scale VR shopping system
and proposed a new hand gesture set for the room-scale
VR shopping system. We used the gesture set to replace
the controllers of VR device to improve the limitation of
controllers in VR shopping activities. Researching on the
gesture set, we introduced a new gesture classiﬁcation. We
designed three levels for classiﬁcation method. The gestures
in level 1 were static hand shapes, the gestures in level 2 were
movement of palm and the gestures in level 3 combined the
features of gestures in level 1 and level 2. The gestures in level
1 and level 2 were simple and were not enough for room-scale
VR shopping activities. Therefor, we focused our research on
the level 3 gestures.
For the gestures in level 3, we introduced four categories
to classify them: core gesture, gestures for interaction with
virtual object, gestures for interaction with menu and gesture
for interaction with space. The classiﬁcations helped us to
understand the gesture set in room-scale VR shopping system.
Also, the gesture set and 3-level classiﬁcation method could
be transplanted to other VR or AR systems conveniently.
In order to achieve the complex gestures recognition, we
applied SVM method in our VR shopping system. In the
197
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

end, the user could walk around in his room to view the VR
shopping store and interact with the system with his natural
hand gestures.
In the future work, we will research on two aspects:
realizing “natural” hand gesture set and improving the accuracy
of hand shape recognition. The gestures we have designed
in this paper do not fully reﬂect the natural hand shapes.
Also some hand shapes are very similar and they lowers the
accuracy of hand shape recognition. Therefore, we would like
to further reﬁne our current gesture set and make our hand
gesture more natural and more accurate.
REFERENCES
[1]
B. Serrano, R. M. Ba˜nos, and C. Botella, “Virtual reality and stimulation
of touch and smell for inducing relaxation: A randomized controlled
trial,” Computers in Human Behavior, vol. 55, 2016, pp. 1 – 8, ISSN:
0747-5632, doi: 10.1016/j.chb.2015.08.007.
[2]
G.
D.
Bhatt,
“Bringing
virtual
reality
for
commercial
web
sites,” International Journal of Human-Computer Studies, vol. 60,
no.
1,
2004,
pp.
1–15,
ISSN:
1071-5819,
[Online].
Available:
https://dblp.org/rec/bib/journals/ijmms/Bhatt04 [accessed: 2018-12-14].
[3]
T. Chen, Z.-g. Pan, and J.-m. Zheng, “Easymall - an interactive virtual
shopping system,” in 2008 Fifth International Conference on Fuzzy
Systems and Knowledge Discovery, vol. 4, Oct 2008, pp. 669–673,
doi: 10.1109/FSKD.2008.124.
[4]
K. C. Lee and N. Chung, “Empirical analysis of consumer reaction
to the virtual reality shopping mall,” Computers in Human Behav-
ior, vol. 24, no. 1, 2008, pp. 88 – 104, ISSN: 0747-5632, doi:
110.1016/j.chb.2007.01.018.
[5]
A. Verhulst, J. Normand, C. Lombart, and G. Moreau, “A study on
the use of an immersive virtual reality store to investigate consumer
perceptions and purchase behavior toward non-standard fruits and
vegetables,” in 2017 IEEE Virtual Reality (VR), March 2017, pp. 55–
63.
[6]
M. Speicher, S. Cucerca, and A. Kr¨uger, “Vrshop: A mobile interactive
virtual reality shopping environment combining the beneﬁts of on- and
ofﬂine shopping,” Proceedings of the ACM on Interactive, Mobile,
Wearable and Ubiquitous Technologies, vol. 1, no. 3, 2017, pp. 102:1–
102:31, ISSN: 2474-9567, doi: 10.1145/3130967.
[7]
“Ikea
vr
experience,”
2016,
[Online].
Available:
https://www.ikea.com/ms/en us/this-is-ikea/ikea-highlights/virtual-
reality/index.html [accessed: 2018-12-14].
[8]
“Shelfzone
vr,”
2016,
[Online].
Available:
https://invrsion.com/shelfzone [accessed: 2018-12-14].
[9]
J. J. Lin, H. B. L. Duh, D. E. Parker, H. Abi-Rached, and T. A.
Furness, “Effects of ﬁeld of view on presence, enjoyment, memory,
and simulator sickness in a virtual environment,” in Proceedings IEEE
Virtual Reality 2002, March 2002, pp. 164–171, ISSN: 1087-8270, doi:
10.1109/VR.2002.996519.
[10]
P. Garg, N. Aggarwal, and S. Sofat, “Vision based hand gesture
recognition,” International Journal of Computer, Electrical, Automation,
Control and Information Engineering, vol. 3, no. 1, 2009, pp. 186 –
191, [Online]. Available: http://waset.org/Publications?p=25 [accessed:
2018-12-14].
[11]
J. P. Wachs, M. K¨olsch, H. Stern, and Y. Edan, “Vision-based hand-
gesture applications,” Communications of the ACM, vol. 54, no. 2,
2011, pp. 60–71, ISSN: 0001-0782, doi: 10.1145/1897816.1897838.
[12]
Y. Boussemart, F. Rioux, F. Rudzicz, M. Wozniewski, and J. R.
Cooperstock, “A framework for 3d visualisation and manipulation in
an immersive space using an untethered bimanual gestural interface,”
in Proceedings of the ACM Symposium on Virtual Reality Software
and Technology, ser. VRST ’04, 2004, pp. 162–165.
[13]
H. Karam and J. Tanaka, “Two-handed interactive menu: An application
of asymmetric bimanual gestures and depth based selection techniques,”
in Human Interface and the Management of Information. Information
and Knowledge Design and Evaluation. Springer International Publish-
ing, 2014, pp. 187–198, ISBN: 978-3-319-07731-4, doi: 10.1007/978-
3-319-07731-4 19.
[14]
“Leap motion,” 2018, [Online]. Available: https://www.leapmotion.com
[accessed: 2018-12-14].
[15]
“libsvm-3.22,”
2016,
[Online].
Available:
https://www.csie.ntu.edu.tw/ cjlin/libsvm [accessed: 2018-12-14].
[16]
C. Lu, L. Zhou, and J. Tanaka, “Realizing multi-touch-like gestures in
3d space,” in Human Interface and the Management of Information. In-
teraction, Visualization, and Analytics, 2018, ISBN: 978-3-319-92043-
6, doi: 110.1007/978-3-319-92043-6 20.
[17]
“Alibaba
buy+,”
2016,
[Online].
Available:
https://twitter.com/AlibabaGroup/status/789086429330575360
[accessed: 2018-12-14].
198
Copyright (c) IARIA, 2019.     ISBN:  978-1-61208-686-6
ACHI 2019 : The Twelfth International Conference on Advances in Computer-Human Interactions

