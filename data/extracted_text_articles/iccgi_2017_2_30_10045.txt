Screencasts: Enhancing Coursework Feedback for Game Programming Students
Bobby Law
Dept of Computing, Communications and Interactive Systems
Glasgow Caledonian University
Glasgow, Scotland
Email: robert.law@gcu.ac.uk
Abstract—Feedback is an important part of learning and, as
such is vital for students to develop and progress throughout
their academic life. Programming can be an abstract concept that
students ﬁnd challenging to comprehend therefore good feedback
is important to their progress and their motivation to continue
programming. This paper will discuss the process of enhancing
coursework feedback for Game Programming students through
the use of screencasts. The hypothesis being that game program-
ming by its nature is audio-visual thus, providing feedback using
an audio-visual medium should increase the students perception
of their feedback such that it is perceived to be clearer, easier to
comprehend and personalised.
Keywords—Screencasts; Feedback; Software Development.
I.
INTRODUCTION
The United Kingdom’s (UK) National Student Survey
(NSS)[1] is a survey for ﬁnal year students at all of the
UK’s publicly funded Higher Education Institutions (HEIs)
and is administered by Ipsos MORI. The NSS comprises of
27 questions across eight categories attempting to capture the
students learning experience. The NSS acts as a barometer of
student satisfaction and thus, is an inﬂuential survey giving the
student body a collective voice. The data from the survey is
publicly available and is used by prospective students when
choosing their potential University.
This survey has a number of different sections, one of
which is Assessment and Feedback. The perennial view from
students suggests that there is scope for improvement with re-
gard to Feedback. Comparing all eight categories it can be seen
that Assessment and Feedback is continually at the bottom.
This would suggest that there is still room for improvement.
Table 1 shows all the sections of the questionnaire and their
corresponding percentage satisfaction rating. It is noticeable,
from Table 1, that satisfaction with Assessment and Feedback
is between 5 and 14 percentage points behind 7 of the 8
remaining categories suggesting that the students’impression
of feedback and the instrument of feedback delivery have not
met entirely with the students’expectations [2][3].
Viewing the statistics on a nation by nation basis against
the UK average creates an interesting picture of how students
in each of the four nations differ in their perceptions of the
level of feedback they receive. Figure 1 shows a comparison
of all four nations. Working in an academic institution in Scot-
land the picture painted is somewhat alarming with Scotland
six points below the UK average [4]. The Assessment and
Feedback section of the survey is comprised of ﬁve questions;
two relating to assessment and three relating to feedback. The
feedback questions are shown in Table 2. The questions in
Table 2 emphasize the students’desire for expeditious, clear
and detailed feedback [5].
The remainder of this paper is organized as follows: Section
Figure 1.
Assessment and Feedback results 2016 by nation
TABLE I.
PERCENTAGE SATISFACTION ACROSS CATEGORIES FROM
NSS QUESTIONNAIRE
Categories
2015
2016
The teaching on my course
87
87
Assessment and feedback
73
74
Academic support
82
82
Organisation and management
79
79
Learning resources
85
86
Personal development
83
82
Overall satisfaction
86
86
TABLE II.
EXTRACT OF FEEDBACK QUESTIONS FROM NSS
QUESTIONNAIRE
Feedback Questions asked as part of NSS
Feedback on my work has been prompt.
I have received detailed comments on my work.
Feedback on my work has helped me clarify things I did not understand.
II will provide an overview of the author’s rationale for the
use of screencasts within the feedback process; indicating the
nature of the cohort and the subject area studied. Section
III will provide information about pedagogical issues related
to screencasting, Section IV offers an introduction to the
technologies available for screencasting. Section V presents
an overview of the screencasting process, while Section VI
reﬂects on the informally gathered feedback from the student
cohort. Section VII discusses issues encountered by the author
during the creation of the screencasts. Section VIII attempts
to derive conclusions based on the synthesis of Sections III,
IV, VI and VII. Section IX offers ideas for future work.
17
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-571-5
ICCGI 2017 : The Twelfth International Multi-Conference on Computing in the Global Information Technology

II.
RATIONALE
Teaching programming, and in particular, game program-
ming it can be difﬁcult to offer students feedback on course-
work submissions that are not either too generic and brief
or ultimately too verbose and overcomplicated. Getting the
balance of written feedback correct can be a daunting task.
Thompson and Lee [6] suggest that feedback is “a pedagogical
tool to improve learning by motivating students to rethink and
rework their ideas rather than simply proofread and edit for
errors.” . Interestingly, Thompson and Lee [6] quote Notar,
Wilson and Ross that “feedback should focus on improving the
skills needed for the construction of end products more than
on the end products themselves”. This particular observation is
very apt for teaching programming concepts and programming
languages as the feedback given is in the context of the students
programming skills rather than their end product, in this case
their game. The feedback is intended to improve the students
ability to produce structured, economical code and illustrate
the necessary skills for debugging program code.
The author teaches game programming modules at various
levels within the undergraduate programme BSc (Honours)
Game Software Development. It would seem natural for game
programming students who primarily work with a very audio
visual medium to receive feedback for their programming
coursework as an audio-visual screencast. It was therefore
decided to implement a trial with a second year cohort un-
dertaking the module Game Programming 1. This module was
chosen as it was a core module for both the Game Software
Development students and the Game Design students. The
module introduces students to coding using C++ and OpenGL
with the emphasise on the production of a 2D game prototype.
The module had approximately 70 students participating in it
with a near even split of Game Software Development and
Game Design students.
The coursework required the students to create a game
of their choosing. The coursework speciﬁcation provided the
students with a number of requirements that had to be met and
a marking scheme was provided as a guide to the aesthetic
appearance of their game and the functional aspects of the
underlying code.
III.
PEDAGOGICAL ISSUES
So what is a screencast? For the purposes of this paper a
screencast will be deﬁned as a recording of the current content
of the computer screen with an audio narration providing
relevant commentary, i.e., feedback [7]–[9]. As part of their
learning it is important for students to receive feedback on
any of the work that they produce.
Race [10] identiﬁes a number of common formats used
to disseminate feedback to students: handwritten, word pro-
cessed, model answers/solutions, rubric proformas, oral feed-
back, email and computer marked assessment. These methods
can be issued individually or as general feedback based on the
performance of a cohort or group.
Race [10] suggests ﬁve attributes of feedback: Timely,
intimate and individual, empowering, open doors not close
them and manageable. Timely feedback is a goal that is highly
desired and greatly prized, but, can be dictated by class size or
other commitments. Intimate and individual feedback should
reﬂect the student’s own submission. Empowering feedback is
harder to achieve, as it is a balancing act between positive feed-
back and a critic, warts and all, of the student’s submission.
Open doors, not close them refers to the use of language within
feedback and the expectation this can set for the student and the
feedback they receive for their next submission. Manageable,
is viewed from the perspective of both the student and the
lecturer, i.e., the effort expended by the lecturer to produce the
feedback and the volume of feedback received by the student
could cause them to miss something important [10].
Using the written word to provide annotated feedback to
students can be taken out of context [9] and therefore the
beneﬁt of the feedback can be lost. Worse still, the feedback
taken out of context can be misconstrued as a criticism of
their work [8] rather than a pointer to improvement. The
loss of visual and aural cues, which aid understanding [11],
from the written feedback process is therefore something that
screencasting can help combat.
As part of Evans [12] “12 pragmatic actions” for effective
feedback, one suggestion is for students to be presented with an
early assessment opportunity such that they can receive early
feedback, which, can be built upon prior to ﬁnal submission.
It has been mooted that audio-visual screencasts can create
for the Lecturer the concept of “social presence” and “an
opportunity for conveying positive encouragement through
intonation.” [8]. This ability to use intonation to emphasize
important [2] aspects of feedback make the use of screencasts
a beneﬁt for the student. Couple this with the ability to hear
the feedback in the manner the Lecturer intended it and the
loss of the visual and aural cue associated with face to face
feedback are somewhat restored. The volume of information
that can be presented to the student via the audio aspect of
screencasts is far larger than written feedback alone and in a
shorter time period [2][13][14].
Galanos et al. cite the use of screencasts as a method of
giving a student personalised feedback by recording the lec-
turer debugging the students program code while commenting
on it [15]. Also suggested is the use of an attached webcam to
offer “picture in picture” of the lecturer while debugging the
program code, helping to offer that personal touch [15].
It has been suggested that screencasts can aid the student’s
understanding of their feedback by negating the need for
continual cross-referencing between feedback and assessment
and secondly the use of conversation style feedback rather
than a more formal written academic feedback [8]. It has also
been suggested that students ﬁnd it clearer to “understand the
marker’s reasoning” [7] and comments [16] when presented in
a screencast.
Clarity of feedback is important to students [17]; they do
not want to receive feedback that could be deemed “vague,
unclear and confusing” [18]. Thus, the audio-visual nature
of screencasts can help enrich the feedback pinpointing un-
ambiguously exactly what is being commented on[18]. The
promptness or timeliness of feedback is another concern for
students as evidenced by the low scores in the National Student
Survey[4]. Hope suggests that educators are under an “obli-
gation to provide meaningful feedback within a reasonable
timeframe”[2]. Mathisen proffers anecdotal evidence from the
ﬁeld that screencasts can provide more feedback and can be
produced in less time[18].
It has been mooted by O’Malley that one of a quartet of
criteria needed for feedback to be effective is for it to be
personal [19]. Screencasting offers the student personalised
feedback that is tailored to their submission [8]. Chewar and
18
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-571-5
ICCGI 2017 : The Twelfth International Multi-Conference on Computing in the Global Information Technology

Matthews state that the use of screencasts to provide feedback
allows for more detailed, accurate and robust feedback [20].
Thomson and Lee also suggest that feedback given through
the use of screencasts has the capacity to motivate and boost
the students engagement with their learning [6].
IV.
TECHNOLOGY
There are a number of different combinations of hardware
and software that can be used to create a screencast. The
following sections will describe the hardware and software
used by the author to create feedback screencasts.
A. Hardware
To capture good quality audio it is advisable to refrain
from using the built-in device microphone but instead opt for
a headset or external microphone [2][21][22]. The beneﬁt of
using a headset is the consistent distance from the mouth
[23] and the ability to position it slightly below the mouth
to minimize the noise of breathing [21].
B. Software
A number of software packages are available and these
range from desktop applications to web based applications
which, in turn, vary in price from free to hundreds of pounds
[23]. Software used for this paper was Screencast-O-Matic a
web based application offering a limited version free. The free
version allows up to 15 minutes of recording, recording from
screen and webcam, the ability to publish to YouTube and the
ability to save in popular formats such as .MP4, .AVI and .FLV.
It is relatively easy to use [9] and has a very handy countdown
before recording begins.
V.
RECORDING SCREENCAST FEEDBACK
Although the screencast in this instance is being created in
response to an unknown entity it is still important to apply
the rules of creating instructional screencasts by planning
[23]. Planning is very important as there will be a number
of areas that will require feedback. For the game produced
by the students the coursework feedback was broken into the
following areas: aesthetics, game play, code structure and com-
pilation. Each of these areas was broken down further with key
points: aesthetics covered the games look and feel and interface
design; game play covered the ease and enjoyableness of the
game, responsiveness of game objects to keyboard/gamepad
interaction; code structure covered neatness, use of the funda-
mental programming building blocks, use of language features,
data structures, and the object oriented paradigm; compilation
covered the programming compiling and the appropriate use
of compilation switches.
Unlike recording a conventional educational screencast
there is no need to produce a script [24] as the coursework
submissions will not be predictable and a script can deper-
sonalize the feedback and make it feel unnatural [3]. Armed
with the marking scheme and the aforementioned plan the
process of creating the screencast could be started. A number
of considerations were taken into account before commencing
the screencast process:
•
Determining a location which has a low level of
background noise [21] and little chance of being
interrupted.
•
Use a good quality headset, positioning the micro-
phone slightly below the mouth [21].
•
Switch off any software that activates pop ups such as
email, Facebook or instant messenger as these could
end up being recorded [3].
•
Use and stick to the devised plan for consistency.
•
Speak naturally and positively [24] making good use
of intonation [2].
•
Use of the pause button [23] at the end of each section
to allow time to gather one’s thoughts prior to the start
of the next section.
During the recording process all mouse movements and clicks
are visible to the viewer as a large coloured circle that will
change colour when the mouse button is clicked. This is excep-
tionally useful for giving the student unambiguous and precise
feedback on their user interface design and layout pointing out
what is considered good and what needs improving.
The neatness and compactness of the actual code itself is
an important aspect of any programming thus, the screencast
gave the author the ability to highlight selected code within
the Integrated Development Environment (IDE), in this case
Microsoft Visual Studio, offering an audio narrative explaining
clearly any deﬁcient code and a visualisation of how the code
could be reworked in order to make it neater and more efﬁcient.
Good examples of student could also be highlighted and the
student commended for its use.
For student submissions that did not execute a debug
process could be illustrated that would hopefully allow the
student to solve a similar problem if encountered again. This
ability to show a debug process in operation is a valuable
process that merits a role out to all students as the ability to
debug code is a valuable skill.
Most of the screencasts were between 5 and 10 minutes
in length depending on the game produced and the exhibited
programming ability of the student, which is in keeping with
the surveyed literature. The feedback screencasts were then
subsequently compressed into a .zip ﬁle and returned to the
student via e-mail.
VI.
STUDENT FEEDBACK
Initial feedback from the students was, on the whole, posi-
tive and helpful with regard to reﬁning the screencast feedback
process. Comments were elicited from students in an informal
manner. Students were asked to write a short paragraph giving
their initial impression of receiving feedback in this manner. As
all students received both written feedback and feedback in the
form of screencasts this allowed the students to compare and
contrast the two forms of feedback proffering their thoughts.
From the respondents, the overwhelming feeling was the sense
of personalization and tailoring of feedback to their needs.
Students were also receptive to the visual code analysis they
received indicating that they understood more readily the need
for well written, neat and compact code. Although, anecdotal,
the quotes from students help to articulate their view of
screencasts for feedback.
“... felt like the feedback was personal to my work.”
“I could see what Bobby was talking about and this
helped me better understand how my code could be
improved.”
“Helped me with debugging especially break points.”
Based on this positive feedback an depth and more rigorous
case study will be undertaken in the next academic year to
provide a quantitative measure of the worth of screencasting
19
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-571-5
ICCGI 2017 : The Twelfth International Multi-Conference on Computing in the Global Information Technology

as a delivery mechanism for feedback.
VII.
ISSUES
From the perspective of the lecturer there are some issues
that need to be addressed. Firstly, the time taken to prepare the
screencast feedback does not necessarily equate to the actual
time of the screencast that the student will observe. This is
not, necessarily, due to the screencast being edited but the time
taken to record the screencast itself. Although, in Section V, a
key piece of advise is to plan and prepare for the screencast
by using some form of rubric, the application of this rubric
can leave the recording having a staccato and unnatural feel.
A solution to this is to pause the recording after each section
and compose oneself before recording the next section. This
will add time to the process but will prove worthwhile in the
long term.
Secondly, choosing a suitable location to record the screen-
casts is imperative as interruptions not only break the lecturers
concentration but also can be inadvertently recorded thus,
requiring the recording to be edited or, worse still, to be
scrapped. A quiet location devoid of interruptions is not always
possible in a busy University. It is not an insurmountable
challenge but deﬁnitely something to be aware of prior to
starting any recordings.
A third issue is the size of the recorded screencasts with
regard to the required disk storage. The size is dependant on
a number of factors including: video codec used, screen size
being recorded, and resolution of recording. For example a
screencast recorded using the H.264 video codec for YouTube
with a deﬁnition of 720p, a resolution of 1280x720, 25 frames
per second and lasting 5 minutes will require approximately
1.73 gigabytes of disk space. Thus, for a cohort of 70 stu-
dents, approximately 121 gigabytes of disk storage would be
required. This leads to a secondary issue with the delivery
mechanism used for distributing the recordings to the students.
Distribution by email can be a problem as there may be a
restriction on the maximum ﬁle size that can be attached to an
outgoing email. If this is the case then an alternative method
will be required; this could be by uploading the ﬁle to a
Managed Learning Environment (MLE).
All of the aforementioned issues are solvable with a bit
of careful planning and preparation prior to embarking on the
recording process.
VIII.
CONCLUSION
Results from this pilot project suggest that screencasts
could be potentially of beneﬁt to both students and staff.
If so, this would go along way to addressing the students
perception of feedback as highlighted by the UK’s National
Student Survey.
Reﬂecting on the creation of the feedback screencasts, it
is an interesting exercise to return to the ﬁve attributes of
feedback, as deﬁned by Race [10], and attempt to analyse,
albeit subjectively, if screencast feedback can be thought of as
improving the attribute.
Timely feedback can be considered as a property of the
turnaround time from student submission of coursework to
the lecturer returning feedback to the student; to this end
screencasting has no inﬂuence on this attribute. Intimate and
individual feedback is an interesting attribute; screencasts can
help to achieve this attribute, especially for programming, as
the student will receive feedback on their programming code,
hearing and seeing the lecturer discuss various aspects of their
game’s code. Empowering feedback is a balance between pro-
viding positive feedback and being able to critic the student’s
work in such a manner that they feel engaged and enthused
to progress and push forward. Screencasting feedback can
provide the student with the necessary aural and visual cues to
afford them the understanding of what is good with their work
but also, in a positive manner, how their work can be improved.
This is especially good for programming as it is important
for students to understand that code that works can still be
improved to make it more efﬁcient and that this is a learning
process and not a criticism. Open doors, not close them is a
delicate area but with a judicious use of appropriate language
and the correct vocal intonation the student can be presented
with aural cues and, to a certain extent, visual cues that will
allow them to synthesise the intended tone of the feedback.
Finally, Manageable, as noted by Race[10] has two aspects:
the level of work involved for the lecturer and the volume
of feedback given to the student. With regard to the level of
work involved for the lecturer this may ﬂuctuate depending on
the cohort and the quality of their submissions, therefore, it is
possible that it could add somewhat to the lecturers overhead
for producing feedback. However, for students, they should
have a targeted and enhanced quality of feedback which should
not overburden them but provide the important aspects of the
desired feedback they need to progress and improve.
The increased feedback that can be crammed into a 5
minute screencast is more personal, clearer and less ambiguous
than traditional written feedback. The student can play and
replay the video as many times as they like and the feedback
will always be viewed as it was intended. The time to produce
the screencasts varies by student submission but on the whole
it was surprisingly quick in comparison to written feedback of
the same depth.
IX.
FUTURE WORK
The intention is to repeat the screencast feedback in the
next academic year. The number of students undertaking the
module will, again, be in the region of 60 students and should
offer a suitable number for judging the timeliness of producing
feedback screencasts. The hypothesis is that the experience
from this ﬁrst large scale implementation will lead to a more
effective and quicker production process for each screencast
and the students will beneﬁt from clear, concise and helpful
feedback. The module is 12 weeks in duration and students
will be asked to submit work at the end of week 8 and also
at the end of week 12. Screencast feedback on their week
8 submission will be returned by week 10, which, should
allow for the students to beneﬁt from the feedback prior
to their ﬁnal submission in week 12 [12]. After receiving
the feedback screencasts the students will be surveyed to
ascertain a better representation of their feeling towards this
feedback mechanism. Screencast feedback will be returned
approximately 10 working days after week 12 submission
and should serve to inform the students of their programming
progress. The intention is to survey the students again at the
end of the module in an attempt to better understand their
opinion of screencasts as a means of delivering feedback. The
survey will attempt to elicit the students perceptions of the
screencast feedback based on the categories of engagement,
20
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-571-5
ICCGI 2017 : The Twelfth International Multi-Conference on Computing in the Global Information Technology

quality and quantity of feedback, helpfulness and comparison
to written feedback.
REFERENCES
[1]
N. U. of Students (NUS), “The nation student survey,” 2015, [retrieved:
July, 2017]. [Online]. Available: http://www.thestudentsurvey.com/
[2]
S. A. Hope, “Making movies: The next big thing in feedback?”
Bioscience Education, vol. 18, no. 1, 2011, pp. 1–14.
[3]
K. Haxton and D. McGarvey, “Screencasting as a means of providing
timely, general feedback on assessment,” New Directions, vol. 7, 2011,
pp. 18–21.
[4]
N.
U.
of
Students
(NUS),
“Nss
2015
national
head-
lines,”
2015,
[retrieved:
July,
2017].
[Online].
Available:
http://www.thestudentsurvey.com/
[5]
R. Law, “Using screencasts to enhance coursework feedback for game
programming students,” in Proceedings of the 18th ACM conference
on Innovation and technology in computer science education.
ACM,
2013, pp. 329–329.
[6]
R. Thompson and M. J. Lee, “Talking with students through screencast-
ing: Experimentations with video feedback to improve student learning,”
The Journal of Interactive Technology and Pedagogy, vol. 1, no. 1, 2012.
[7]
M. Robinson, B. Loch, and T. Croft, “Student perceptions of screencast
feedback on mathematics assessment,” International Journal of Research
in Undergraduate Mathematics Education, vol. 1, no. 3, 2015, pp. 363–
385.
[8]
K. Edwards, A.-F. Dujardin, and N. Williams, “Screencast feedback
for essays on a distance learning ma in professional communication,”
Journal of Academic Writing, vol. 2, no. 1, 2012, pp. 95–126.
[9]
G. Stieglitz, “Screencasting: Informing students, shaping instruction,”
UAE Journal of Educational Technology and eLearning, vol. 4, no. 1,
2013, pp. 58–62.
[10]
P. Race, “Using feedback to help students to learn,” HEA, York, 2001.
[11]
K. Mathieson, “Exploring student perceptions of audiovisual feedback
via screencasting in online courses,” American Journal of Distance
Education, vol. 26, no. 3, 2012, pp. 143–156.
[12]
C. Evans, “Making sense of assessment feedback in higher education,”
Review of Educational Research, vol. 83, no. 1, 2013, pp. 70–120.
[Online]. Available: http://dx.doi.org/10.3102/0034654312474350
[13]
M. Henderson and M. Phillips, “Video-based feedback on student
assessment: scarily personal.” Australasian Journal of Educational Tech-
nology, vol. 31, no. 1, 2015, pp. 51–66.
[14]
F. Harper, H. Green, and M. Fernandez-Toro, “Using screencasts in
the teaching of modern languages: investigating the use of jing R⃝ in
feedback on written assignments,” The Language Learning Journal,
2015, pp. 1–18.
[15]
R. Galanos, W. Brand, S. Sridhara, M. Zamansky, and E. Zayas, “Tech-
nology we can’t live without!: revisited,” in Proceedings of the 2017
ACM SIGCSE Technical Symposium on Computer Science Education.
ACM, 2017, pp. 659–660.
[16]
J. West and W. Turner, “Enhancing the assessment experience: improv-
ing student perceptions, engagement and understanding using online
video feedback,” Innovations in Education and Teaching International,
2015, pp. 1–11.
[17]
P. Marriott and L. K. Teoh, “Using screencasts to enhance assessment
feedback: Students’ perceptions and preferences,” Accounting Educa-
tion, vol. 21, 2012, pp. 583–598.
[18]
P. Mathisen, “Video feedback in higher education–a contribution to
improving the quality of written feedback,” Nordic Journal of Digital
Literacy, vol. 7, no. 02, 2012, pp. 97–113.
[19]
P. OMalley, “Screencasting and a tablet pc–an indispensable technology
combination for physical science teaching and feedback in higher and
further education,” in Aiming for excellence in STEM learning and
teaching: Proceedings of the Higher Education Academy’s First Annual
Learning and Teaching STEM Conference, 2012.
[20]
C. Chewar and S. J. Matthews, “Lights, camera, action!: video deliv-
erables for programming projects,” Journal of Computing Sciences in
Colleges, vol. 31, no. 3, 2016, pp. 8–17.
[21]
P. Smith, “Screencasting as a means of enhancing the student learning
experience,” Learning and Teaching in Action, 2014, p. 59.
[22]
D. Wolff-Hilliard and B. Baethe, “Using digital and audio annotations
to reinvent critical feedback with online adult students,” International
Journal for Professional Educators, 2014, p. 40.
[23]
S. Mohorovicic, “Creation and use of screencasts in higher education,”
in MIPRO, 2012 Proceedings of the 35th International Convention.
IEEE, 2012, pp. 1293–1298.
[24]
L. A. Jones, “Losing the red pen: Video grading feedback in distance
and blended learning writing courses,” Association Supporting Com-
puter Users in Education Our Second Quarter Century of Resource
Sharing, 2014, p. 54.
21
Copyright (c) IARIA, 2017.     ISBN:  978-1-61208-571-5
ICCGI 2017 : The Twelfth International Multi-Conference on Computing in the Global Information Technology

