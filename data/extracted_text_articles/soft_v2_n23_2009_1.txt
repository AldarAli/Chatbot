Dynamical Parallel Applications on
Distributed and High Performance Computing Systems
(Invited Paper)
Claus-Peter R¨uckemann
Leibniz Universit¨at Hannover (LUH), Hannover, Germany
Westf¨alische Wilhelms-Universit¨at M¨unster (WWU), M¨unster, Germany
North-German Supercomputing Alliance (HLRN), Germany
Email: rueckemann@rrzn.uni-hannover.de
Abstract
This paper provides an extended overview of recent de-
velopments for exploiting distributed, Grid and High Per-
formance Computing (HPC) resources with applications like
Geoscientiﬁc Information Systems (GIS) based on the GISIG
actmap-project. Focus is on frameworks for optimising the
dynamical parallel use of computing resources for future
cooperation and development concepts, integrating software
and hardware architecture aspects. Using parallel process-
ing and the method of event triggering from within Active
Source can be used to exploit the vast computing power
of distributed MultiCore parallel systems for a multitude of
purposes like geoinformation processing, geophysical data
analysis, information systems, and e-Science. An extended
case study for an application InfoPoint demonstrates the
algorithm here. As various obstacles showed up with imple-
menting and minimising the complexity of the application-
resource workﬂow, the creation of future Web and HPC
services on top of HPC and Distributed Systems will be a
solution for dedicated issues. The new extended framework
of the Grid-GIS house is presented here, showing the case
study for using these concepts for exploration purposes.
For implementation testing distributed resources and the
new multi-site supercomputer resources of HLRN-II (North-
German Supercomputing Alliance) have been used.
Keywords
High Performance Computing; Distributed Systems; Grid-
Computing; e-Science; Geoscientiﬁc Information Systems.
1. Introduction
The future of creating effective and efﬁcient applica-
tions for dynamical visualisation and information systems
is tightly linked with taking the advantage of parallel pro-
cessing on MultiCore systems. Dynamical visualisation and
advanced geoscientiﬁc information systems are prominent
examples [1] at state of the art of development. Using Dis-
tributed Systems and High Performance Computing (HPC)
resources therefore requires new concepts as integrating
these resources, that in nearly all case do have an unique
architecture and basic system conﬁguration is a challenge
for development and portability.
Extending the application spectrum, a new success story
using InfoPoints (groups of active information objects)
is presented implementing concepts of the Active Source
framework for using distributed components and resources,
suitable for Grid, Cloud, and HPC.
An extended implementation of the “Grid-GIS house”
framework for building services on top of Distributed and
HPC Systems for this purpose is presented here for the
ﬁrst time. Within the “Grid-GIS house” the state of the
art in accounting and billing for has been considered for
creating an integrated solution embracing all High End Com-
puting (HEC) namely HPC and Cluster Computing as well
as distributed and service oriented architectures with Grid
Computing and Cloud Computing [2], [3]. At the state of
the art of computing, hardware development today is getting
near the physical limits and software development faces new
challenges. This extended implementation is currently used
for building interdisciplinary cooperations for the purpose
of implementing geo-exploration systems based on parallel
computing components. As the next generation of dynamical
applications in the disciplines involved is as well strongly
depending on backend software as on hardware components
and high end networks this integrated modular framework
has proven suitable.
In the last years strong interests emerged, regarding High
End Computing like HPC and Distributed Computing, span-
ning industry as well as natural sciences [4], [5], [6]. HPC
resources available with the North-German Supercomputing
Alliance (HLRN) have been used for testing these devel-
opments. Software and hardware architecture are discussed
as resources used in the future will have to be efﬁciently
conﬁgured for the purpose of dynamic and interactive use.
Dynamical applications are characterised by the ability to
present various information and context based on interaction
in very ﬂexible ways. The concept of Active Source and the
172
International Journal on Advances in Software, vol 2 no 2&3, year 2009, http://www.iariajournals.org/software/

Object Graphics data type [7] based on source code, can be
used to create and integrate such applications. An example
is dynamical visualisation, being able to create complex
dynamical charts and diagrams enriched with accompanying
visual multimedia information, where the context can change
the state of the view.
Dynamical applications are in many cases limited by
the local resources as there can be large computational
requirements for generating new dynamical views in a short
time as for example with vertices calculation in Geoscientiﬁc
Information Systems (GIS) or multimedia production on de-
mand for Points of Interest (POI) data. Parallel programming
can make use of distributed resources enabling thousands of
parallel processes.
The combination of dynamical applications and parallel
programming components lead to “Dynamical Parallel Ap-
plications” being able to use loosely coupled as well as
embarrassingly parallel methods depending on the tasks.
Numerous applications and algorithms for handling dy-
namical visualisation and processing of scientiﬁc informa-
tion could evolve even more ﬂexibility and facilities if they
could use existing computing power more directly, namely
MPP (Massively Parallel Processing) and SMP (Symmetric
Multi-Processing) resources. Large beneﬁts can result from
using many cores of large computing resources in parallel,
within a shorter time interval, for quasi interactive use.
This paper presents the origins, problems and challenges
(Section 2-4) as well as the status of the implementation
(Section 5). A case study and a detailed InfoPoint example
will illustrate this (Section 6-7). Issues on software and
hardware resources used will be discussed, being essential
for effective distributed applications in the future (Sec-
tion 8). This leads to an evaluation and future work already
begun with the extended service-oriented framework for
Distributed and High Performance Computing (Section 9).
2. Origin and prior art
The idea of dynamical, distributed resource usage for
geoscientiﬁc information was introduced with the concept
of Active Source [7]. Over the years a Grid-GIS framework
with many features had to be implemented within the GISIG
actmap-project [8] including several programming libraries
providing a suitable Application Programming Interface
(API).
With computing resources evolving towards many cores
[9], [10], [11], [12] the idea of using these systems more
widely had been internationally presented and some major
obstacles have been identiﬁed [13], [14]. For integration of
HPC, Grid, and cluster resources these are:
• framework for the use of high end computing resources
for dynamical visualisation and information systems,
• integrability of concepts (e.g. batch and scheduling),
• frameworks for the application of algorithms needed,
• interfaces for ﬂexible and secure data and application
transfer, interchange, and distribution,
• portability of implementations, extendability of existing
methods, reusability of existing solutions.
Due to the limitations of “delivering” computing power from
High Performance Computing, Grid Computing, and cluster
computing resources interactively to a local application
on some workstation, a framework is needed to integrate
these resources. In absence of support for coupling these
resources, in the past some features had to be last on the list
to be addressed.
3. Problems addressed
As described in previous publications [13] GIS, Grid,
and HPC are working on the GISIG implementation in
order to overcome current obstacles, developing frameworks
for the use of HPC and MultiCore computing resources,
interfaces for data and application interchange, integrability,
and portability.
This paper does proceed to implement and disseminate
the proposed frameworks and interfaces for the purpose
of demonstrating implementing ways for opening power-
ful High Performance Computing resources to specialised
scientiﬁc applications and e-Science. It shows the ﬁrst
implementation results of case studies on a new HPC re-
source, using Massively Parallel Processing and Symmetric
Multi-Processing components of the HLRN architecture with
distributed resource locations.
Primary target disciplines are geoinformation processing,
seismic processing for oil and gas, geophysical data analysis,
computing expensive natural resource information systems,
computational geology, hurricane tracking, dynamical car-
tography, and geostatistics.
4. Challenges identiﬁed
The most important challenges identiﬁed with these im-
plementations on HPC resources have been grouped within
this context in order to be brieﬂy discussed.
• HPC resources and conﬁguration,
• batch system and scheduling,
• accessing computing resources / Actmap Computing
Resources Interface / Message Passing,
• distributing data,
• authorisation and system security,
• accounting jobs and processes.
The following sections brieﬂy describe the basic approaches
for the implemented solution before showing an overall case
study of an information system using distributed resources.
173
International Journal on Advances in Software, vol 2 no 2&3, year 2009, http://www.iariajournals.org/software/

5. Mastering complexity
As it has been shown, the problems we encounter with ex-
ploiting top-level backend resources are manyfold. Currently
the systems available only provide access to local resources
or there is only a batch implementation. In order to solve
the problems with implementing a complex system, affords
the integration of different technologies and methods:
• availability of Geoscientiﬁc Information System com-
ponents at source level (actmap),
• support for event-driven dynamical applications (Active
Source),
• access to High End Computing resources (SMP, MPP,
MultiCore, Grid) and conﬁguration,
• creation of loosely parallel coupling interfaces for in-
teractive batch jobs,
• parallelisation of functional components and algorithms
on High End Computing resources.
Emphasis has been put onto the integration of these topics.
In practice experts from different disciplines, information
sciences, geosciences, computer science, engineering, are
engaged. The integration has been done by deﬁning an open
framework for this purpose, based und the Grid-GIS house.
6. Status of the implementation
For the work described here, various distributed and HPC
resources of HLRN have been used. This is the derivative
based on new complementary methods to the predecessor
work handling Grid and Cluster Computing resources [15].
6.1. HPC resources and conﬁguration
HLRN is the North-German Supercomputing Alliance.
HLRN provides high-end High Performance Computing
(HPC) resources jointly used and co-funded by the northern
German states of Niedersachsen, Berlin, Bremen, Ham-
burg, Mecklenburg-Vorpommern, Schleswig-Holstein, and
the Federal Government of Germany / German Research
Society (DFG).
Those resources include HLRN-II [9], a system comprised
of two identical computing and storage complexes, one
located at the Leibniz Universit¨at Hannover, Regionales
Rechenzentrum f¨ur Niedersachsen (RRZN) and the other
at the Konrad-Zuse-Zentrum f¨ur Informationstechnik Berlin
(ZIB). By connecting the two systems via the HLRN-Link
dedicated ﬁbre optic network (Cisco Catalyst switches),
HLRN can operate and administer them as one system.
Each complex consists of MPP and SMP cluster compo-
nents (SGI Altix ICE and XE) [16] installed in two phases.
The ﬁrst phase has been installed by Silicon Graphics Inc.
in the year 2008.
The HLRN-II system (at 312 TFlop/s peak) operated with
SLES is used by scientists for HPC applications from a wide
range of disciplines, including Geosciences, Environmental
Sciences, Climatology, Physics, CFD, Modeling and Simu-
lation, Chemistry, Biology, and Engineering. All projects are
supported by the HLRN service and competence network.
So while still in an early phase this resource installa-
tion, incorporating different computing components, gave
the suitable context for individually conﬁguring an imple-
mentation as described in the following sections. With the
available HPC resources a number of software, application,
and network components have been conﬁgured (Table 1) for
integrating the framework and preparing a suitable software
and hardware environment for the case study scenarios.
Component
Software / Conﬁguration
Frameworks
GISIG, Actmap CRI, Grid-GIS
Operating System
S.u.S.E Linux / SLES
Batch system
Moab, Torque
Networks
MPI (InﬁniBand), I/O (InﬁniBand),
service and administrative networks
Parallelisation
MPI, OpenMP, MPT, MPICH
Transfer / interchange Secure Shell / keys, pdsh
Security
Sandboxing, Tcl, Tcl Plugin
Policies
home, javascript, trusted
Compilers
Intel Fortran, C, C++ suite, PGI, GNU
Libraries & Appl.
BLAS, LAPACK, NAG, ATLAS,
CPMD, MOLPRO, FEOM, NAMD,
Gaussian, FFT, TAU, NWChem,
VMD, EnSight, ABAQUS, ANSYS,
FLUENT, STAR-CD . . .
Parallelisation
SGI MPI / MPT, Intel MPI, OpenMP,
MPICH, MVAPICH, SHMEM . . .
Table 1. HPC software components conﬁgured.
This is an excerpt of basic software components like ap-
plications, libraries and compilers, available for applications
discussed in the context of this paper.
For security reasons a trusted computing interface using
sandboxing has been conﬁgured as various security policies
for integrating data and applications have been introduced
and successfully tested.
This conﬁguration allows very ﬂexible transfer of data,
secured execution of foreign Active Sources on demand,
accounting as well as batch and interactive use of resources.
The basic trusted environment is independent from the
computing architecture and can be used out of the box. The
speciﬁc architecture dependent conﬁguration part must be
done accordingly to the purposes where it is neccessary for
the service.
Primary targets might be key management, LDAP or ﬁre-
wall conﬁguration. The components used for management
of the components are shown in Table 2. Information on the
current state of these resources can be found online [9].
174
International Journal on Advances in Software, vol 2 no 2&3, year 2009, http://www.iariajournals.org/software/

Component
Management / Conﬁguration
Additional Server
HOME, Data, Login, Batch, LDAP
OSS, MDS, QFS, Repository
Mgmt. Altix ICE / XE
SGI Tempo / Scali Manage
Storage / Global FS
RAID 6, Storage Manager / Lustre
File Replication
NetVault Replicator (HLRN-Link)
Software Access
“Modules” (Compilers, Libs, Apps)
Monitoring
Nagios, Ganglia
Grid, Access, HA
Grid tools, Middleware, . . .
Proﬁling / Debugging
Intel Threading & Tracing Tools,
PCP, PerfSuite, TotalView, ddt, gdb
Conﬁguration Mgmt.
Cfengine, CVS
Table 2. HPC management components conﬁgured.
6.2. Batch system and scheduling
The batch system, scheduling and resource management
implemented on HLRN-II is based on Moab and Torque.
With this system the PBS (Portable Batch System) resource
speciﬁcation language [17] [18] is used. Interactive use and
calculation is highly dependent on features of the batch
system used. Currently the end user application will have
to do the job synchronisation. With a conventional system
conﬁguration the management of multi user operation is
difﬁcult. Both synchronising and multi user operation tend
to work against interactive use.
6.3. Accessing computing resources
The Actmap Computing Resources Interface (CRI) is an
actmap library containing procedures for handling com-
puting resources. Examples for using High Performance
Computing and Grid Computing resources include batch
system interfaces and job handling.
Listing 1 shows a simpliﬁed source code part of the
actmap call for loading the Actmap Computing Resources
Interface (Tcl or TBC) into the application stack. This library
can be extended and modiﬁed interactively on the ﬂy or via
scripting [19].
1
#BCMT------------------------------------------------
2
###EN \gisigsnip{Load actlcri}
3
#ECMT------------------------------------------------
4
if {"$behaviour_loadlib_actlcri" == "yes"} {
5
catch {
6
if {[info exists tcl_platform(isWrapped)]} {
7
puts "actlcri.tbc library initialized ..."
8
source actlcri.tbc
9
set status_in_actmap yes
10
} else {
11
puts "actlcri.tcl library initialized ..."
12
source [file join $ACTMAPHOME "actlcri.tcl"]
13
set status_in_actmap yes
14
} } }
Listing 1. Calling CRI.
This library (actlcri) can hold functions and proce-
dures and even platform speciﬁc parts in a portable way. It
can be used by calling the source code library as well as
the byte code library generated with a compiler like TclPro.
From an application, calling Actmap CRI can be done as
follows. For various applications, byte code (TBC) [7] has
been considered for any part of applications and data.
With CRI being part of Active Source, parallel processing
interfaces for Message Passing e.g. using InﬁniBand, can
be used, for example MPI (Message Passing Interface) and
OpenMP. Listing 2 and Listing 3 show an MPI and an
OpenMP job script used with Actmap CRI.
1
#!/bin/bash
2
#PBS -N myjob
3
#PBS -j oe
4
#PBS -l walltime=00:10:00
5
#PBS -l nodes=8:ppn=4
6
#PBS -l feature=ice
7
#PBS -l partition=hannover
8
#PBS -l naccesspolicy=singlejob
9
module load mpt
10
cd $PBS_O_WORKDIR
11
np=$(cat $PBS_NODEFILE | wc -l)
12
13
mpiexec_mpt -np $np ./dyna.out 2>&1
Listing 2. Active Source MPI (SGI MPT) script.
1
#!/bin/bash
2
#PBS -N myjob
3
#PBS -j oe
4
#PBS -A myproject
5
#PBS -l walltime=00:10:00
6
#PBS -l nodes=1:ppn=4
7
#PBS -l feature=xe
8
#PBS -l naccesspolicy=singlejob
9
cd $PBS_O_WORKDIR
10
export OMP_NUM_THREADS=4
11
12
./dyna.out 2>&1
Listing 3. Active Source OpenMP script.
Scripts of this type will on demand — this means using
event binding — be sent to the batch system for processing.
The sources can be semi-automatically generated, can be
called from a set of ﬁles or can be embedded into an actmap
component, depending on the ﬁeld of application.
6.4. Distributing data
Within event triggered jobs, MPI and batch means can
be used for distributing and collecting data and job output.
For distributing ﬁles automatically within the system e.g.
dsh, pdsh, C3 tools, Secure Shell (ssh and scp) are used.
Interactive communication is supported by the appropriate
Secure Shell key conﬁguration. It must be part of the system
conﬁguration to correctly employ authorisation keys and
crontab or at features.
175
International Journal on Advances in Software, vol 2 no 2&3, year 2009, http://www.iariajournals.org/software/

6.5. Authorisation and system security
Authorisation for accessing data and information asso-
ciated with the calculation currently affords to have one
instance of the application present on one of the servers
of the HPC resource, e.g. login or batch. A dedicated
network using secure keys can be conﬁgured for the pur-
pose of interactive application access in order to simplify
communication and data transfer between the nodes. As for
system security reasons large installations will tend to be
restricted to dedicated users with this scenario. For execution
of dynamic sources the trusted computing interface has been
conﬁgured as policy trusted.
6.6. Accounting jobs and processes
The implemented framework is incorporated in an in-
tegrated solution for monitoring, accounting, billing sup-
porting the geoinformation market. An outlook has been
given for Geographic Grid Computing at the International
Conference on Grid Services Engineering and Management
(GSEM). Especially for the extended use of GIS and com-
puting resources, the Grid-GIS framework, the “Grid-GIS
house” has been created [13] and is used within the D-Grid
[20], [21] and with Condor. The Active Source components
used here, are part of this framework, on top of the Grid
services, Grid middleware, and the HPC and Grid resources.
7. Case study
The selected case study overview shows different high
level GIS views implemented with dynamical cartography
(Active Map) in order to enable geocognostic insights.
With this solution, processing, data storage, and information
retrieval is done by using distributed resources. Handling is
triggered from within the application by events via the Ac-
tive Source framework. In oder to concentrate on the views
we omit features previously demonstrated, such as active
elements handling and visualisation, multimedia objects and
raster and vector layering.
With a suitable interface, distributed computing resources
can be used for creating any part of the application or data.
So data collection and automation, data processing, and data
transfer can be handled via existing means.
For example parallel processing of satellite data or satel-
lite photos can be triggered from within the Active Map.
The precalculation of views (Listing 4) can be automated
from the application, processing several hundred views at a
time using dedicated compute nodes for each calculation.
1
convert -scale 2400x1200 inview01.jpg outview01.jpg
2
convert -scale 2400x1200 inview02.jpg outview02.jpg
3
convert -scale 2400x1200 inview03.jpg outview03.jpg
4
...
Listing 4. Precalculation of satellite data.
An event binding command is shown in Listing 5. These
bindings can bind events to selective objects of a category.
The number of objects handled in object source is only
limited by the system and hardware used. This way it is
possible to provide any part of the application with support
of distributed computing and storage resources, e.g. for
simple cases via HTTP or HTTPS. The functional part can
be a procedure, another component or an executable.
1
$w bind precalc_bio <Button-1> {exec precalc_bio.sh}
Listing 5. Binding of precalculation script.
For the following examples all the components are linked
by the GISIG Active Source framework using event pro-
gramming and the most computing intensive operations are
done in the background on HPC compute resources.
Figure 1 shows part of an active satellite worldmap calcu-
lated on a HPC compute node as described. The respective
action for calculating the view is linked into the Active
Source data via an event bind call (Tcl) to the batch script.
The batch script using scripting and MPI is executed by the
batch system (Moab / Torque) to run on the compute nodes
of the speciﬁed MPP component.
The result is transferred back to an application working
directory from where the results calculated on the compute
nodes are loaded into the active map (Tcl canvas) in order
to build the desired view. Any objects of these views do
get unique identiﬁcation keys and may be automatically
equipped with logical identiﬁcation strings.
From within this interactive view one might want to
switch to an active ocean / depth or plate tectonics view
in a next step as in Figure 2 and end up in showing a
vegetation / biology view as in Figure 3.
Once calculated all the maps exist at the same time, they
can be regarded “precalculated”. Active Source uses a layer
concept meaning any number of objects can be grouped in
separate layers with all layers representing a stack of layers.
It can be deﬁned for the speciﬁc Active Source application
if all of the calculated views do reside in memory, stacked
in layers inside of the application as described or if they
shall be removed in favour of releasing memory.
In the ﬁrst case no data has to be recalculated, any views
precalculated this way can be accessed interactively. It can
be easily switched between the views by predeﬁned events.
The standard ways for doing so are key bindings to rotate
views and mouse events to bring the next or a deﬁned view
to the front.
176
International Journal on Advances in Software, vol 2 no 2&3, year 2009, http://www.iariajournals.org/software/

Figure 1. Precalculated topography view.
Any operation on the data suitable for interactive and
batch mode can be done from within the Active Source
framework. With this capability batch jobs can be created,
e.g. for dynamically adding synthetic data and raytraced
elements in interactive mode. Using parallelised applications
like parallel POV-Ray from within these jobs, distributed
computing resources can be used most effectively.
Figure 2. Precalculated plate tectonics view.
Figure 3. Precalculated biology view.
This can e.g. be encouraged in order to enhance geocog-
nostic views by generating hundreds of data sets for points
of interest. Ongoing from Figure 3, decision may fall to
viewing the pollution distribution within a city in a distinct
area as in Figure 4. One will select an “active spot” on the
map that is linked with an appropriate detailed active city
map. Most ﬂexible geocognostic views can be created this
way using the local and background computing resources at
any time in the process of user interaction. GISIG Active
Maps can consist of vector and raster layers as well as of
multimedia components and events. Problems of dynamical
cartography and geocognostic views with millions of data
points having to be connected with live, interactive data
being very computing intensive can be solved.
The example (Figure 4) shows a dynamical event-driven
city map containing environmental and infrastructure data
that is delivered from distributed sources. Now if one wants
to take a look at pollution values of the largest lake within
this city, as in Figure 5, a right click onto that object will
display the results. Any interactive and batch events may be
deﬁned. A deﬁned key bound will toggle a legend. Further
zooming can be done to any extent, e.g. to resolve elemen-
tary objects within views. This demonstrates cartography
combined with aerial data (vegetation and topography), and
vector data (infrastructure and surfaces of water) all linked
by events, and extensible by event triggered computing.
The selected part shown, is a highly zoomed area of
the previously presented map, here in different thematical
geocognostic context. Arbitrary detailed satellite maps and
supporting data may be calculated on the HPC resources
using the described algorithm.
177
International Journal on Advances in Software, vol 2 no 2&3, year 2009, http://www.iariajournals.org/software/

Figure 4. City, vector / raster layers, events.
Figure 5.
Detail, combined geocognostic view: map
data, aerial data, and vector data.
Used from via a login node the solution with HPC
compute nodes does show less latency than for previous
solutions with distributed Grid resources. The login nodes
used, are conﬁgured for interactive use of the batch system
so there is no queue wait-time and much less time necessary
for scheduling and re-scheduling. That way, avoiding a
standard batch system conﬁguration and a high job load,
interactive applications are possible, reducing the wait times
from about 12 up to 24 hours down to 1 to 10 minutes for
medium expensive computation events. For less consumptive
computation events the overall wait times are in the range
of seconds.
e-Science applications like dynamical cartography and
visualisation can use distributed resources in combination
with Inter-Process Communication (IPC) and remote con-
trol within the application in a standard way as the parts
calculated externally have been delivered back and loaded
into the application. Any part can be reloaded or removed
from memory separately so that memory usage is minimised.
8. InfoPoints using distributed resources
Using auto-events, dynamical cartography, and geocog-
nostic aspects, views and applications using distributed
compute and storage resources can be created very ﬂexibly.
As with the concept presented resources available from
Distributed Systems, High Performance Computing, Grid
and Cloud services, and available networks can be used.
The main components are:
• interactive dynamical applications (frontend),
• distributed resources, compute and storage, conﬁgured
for interactive and batch use,
• parallel applications and components (backend), as
available on the resources,
• a framework with interfaces for using parallel applica-
tions interactively.
Besides the traditional visualisation a lot of disciplines
like exploration, archaeology, medicine, epidemology and
for example various applications within the tourism industry
can proﬁt from the e-Science components. These e-Science
components can be used for Geoscientiﬁc Information Sys-
tems for dynamical InfoPoints and multimedia, Points of
Interest based on Active Source (Active POI), dynamical
mapping, and dynamical applications.
8.1. InfoPoints and dynamical cartography
Figure 6 shows an interactive Map of M´exico. The yellow
circle is an event sensitive Active Source object containing
a collection of references for particular objects in the ap-
plication. This type of object has been named InfoPoint.
InfoPoints can use any type of start and stop routines
triggered by events. Figure 7 shows a deﬁned assortment of
information, a view set, fetched and presented by triggering
an event on the InfoPoint.
178
International Journal on Advances in Software, vol 2 no 2&3, year 2009, http://www.iariajournals.org/software/

Figure 6. Interactive M´exico with InfoPoint Yucat´an.
Figure 7. Sample view set of InfoPoint Yucat´an.
The information has been referenced from within the
World Wide Web in this case. InfoPoints can depend on the
cognitive context within the application as this is a basic
feature of Active Source: Creating an application data set it
is for example possible to deﬁne the Level of Detail (LoD)
for zoom levels and how the application handles different
kinds of objects like Points of Interest (PoI) or resolution of
photos in the focus area of the pointing device.
8.2. Inside InfoPoints
The following passages show all the minimal components
necessary for a fully functional InfoPoint. The example
for this case study is mainly based on the Active Source
framework. Triggered program execution (“Geoevents”) of
applications is shown with event bindings, start and stop
routines for the data.
8.3. InfoPoints bindings and creation
Listing 6 shows the creation of the canvas for the Info-
Point and loading of the Active Source via bindings.
1
#
2
# actmap example -- (c) Claus-Peter R\"uckemann, 2008,
2009
3
#
4
5
#
6
# Active map of Mexico
7
#
8
9
erasePict
10
$w configure -background turquoise
11
12
pack forget .scale .drawmode .tagborderwidth \
13
.poly .line .rect .oval .setcolor
14
pack forget .popupmode .optmen_zoom
15
16
openSource
mexico.gas
17
removeGrid
18
19
##EOF:
Listing 6. Example InfoPoint Binding Data.
This dynamical application can be created by loading the
Active Source data with the actmap framework (Listing 7).
1
/home/cpr/gisig/actmap_sb.sfc mexico.bnd
Listing 7. Example creating the dynamical application.
8.4. InfoPoints Active Source
The following Active Source code (Listing 8) shows a
tiny excerpt of the Active Source for the interactive Map
of M´exico containing some main functional parts for the
InfoPoint Yucat´an (as shown in Figure 6).
1
#BCMT-------------------------------------------------
2
###EN \gisigsnip{Object Data: Country Mexico}
3
###EN Minimal Active Source example with InfoPoint:
4
###EN Yucatan (Cancun, Chichen Itza, Tulum).
5
#ECMT-------------------------------------------------
6
proc create_country_mexico {} {
7
global w
8
#
Yucatan
9
$w create polygon 9.691339i 4.547244i 9.667717i \
10
4.541732i 9.644094i 4.535433i 9.620472i 4.523622i \
11
9.596850i 4.511811i 9.573228i 4.506299i 9.531496i \
12
4.500000i 9.507874i 4.518110i 9.484252i 4.529921i \
13
9.460630i 4.541732i 9.437008i 4.547244i 9.413386i \
14
4.553543i 9.384252i 4.559055i 9.354331i 4.565354i \
15
9.330709i 4.588976i 9.307087i 4.612598i 9.283465i \
16
4.624409i 9.259843i 4.636220i 9.236220i 4.641732i \
17
9.212598i 4.641732i 9.188976i 4.648031i 9.165354i \
18
4.653543i 9.141732i 4.659843i 9.118110i 4.665354i \
179
International Journal on Advances in Software, vol 2 no 2&3, year 2009, http://www.iariajournals.org/software/

19
9.094488i 4.671654i 9.070866i 4.677165i 9.047244i \
20
4.688976i 9.023622i 4.695276i 9.000000i 4.707087i \
21
8.976378i 4.712598i 8.952756i 4.724409i 8.929134i \
22
4.730709i 8.905512i 4.736220i 8.881890i 4.748031i \
23
8.858268i 4.766142i 8.834646i 4.783465i 8.811024i \
24
4.801575i 8.787402i 4.813386i 8.763780i 4.830709i \
25
8.751969i 4.854331i 8.740157i 4.877953i 8.734646i \
26
4.901575i 8.728346i 4.925197i 8.746457i 4.937008i \
27
8.751969i 4.966929i 8.751969i 4.978740i 8.763780i \
28
5.007874i 8.763780i 5.019685i 8.787402i 5.025984i \
29
8.805512i 5.031496i 8.817323i 5.049606i 8.846457i \
30
5.055118i 8.876378i 5.055118i 9.248031i 5.468504i \
31
9.673228i 4.896063i 9.744094i 4.748031i 9.720472i \
32
4.553543i \
33
-outline #000000 -width 2 -fill green -tags {itemshape
province_yucatan}
34
}
35
36
proc create_country_mexico_bind {} {
37
global w
38
$w bind province_yucatan <Button-1> {showName "Province
Yucatan"}
39
$w bind province_quintana_roo <Button-1> {showName "
Province Quintana Roo"}
40
}
41
42
proc create_country_mexico_sites {} {
43
global w
44
global text_site_name_cancun
45
global text_site_name_chichen_itza
46
global text_site_name_tulum
47
set text_site_name_cancun
"Canc´un"
48
set text_site_name_chichen_itza
"Chich´en Itz´a"
49
set text_site_name_tulum
"Tulum"
50
51
$w create oval 8.80i 4.00i 9.30i 4.50i \
52
-fill yellow -width 3 \
53
-tags {itemshape site legend_infopoint}
54
$w bind legend_infopoint <Button-1> \
55
{showName "Legend InfoPoint"}
56
$w bind legend_infopoint <Shift-Button-3> \
57
{exec browedit$t_suff}
58
59
$w create oval 9.93i 4.60i 9.98i 4.65i \
60
-fill white -width 1 \
61
-tags {itemshape site cancun}
62
$w bind cancun <Button-1> \
63
{showName "$text_site_name_cancun"}
64
$w bind cancun <Shift-Button-3> \
65
{exec browedit$t_suff}
66
67
$w create oval 9.30i 4.85i 9.36i 4.90i \
68
-fill white -width 1 \
69
-tags {itemshape site chichen_itza}
70
$w bind chichen_itza <Button-1> \
71
{showName "$text_site_name_chichen_itza"}
72
$w bind chichen_itza <Shift-Button-3> \
73
{exec browedit$t_suff}
74
75
$w create oval 9.76i 5.20i 9.82i 5.26i \
76
-fill white -width 1 \
77
-tags {itemshape site tulum}
78
$w bind tulum <Button-1> \
79
{showName "$text_site_name_tulum"}
80
$w bind tulum <Shift-Button-3> \
81
{exec browedit$t_suff}
82
}
83
84
proc create_country_mexico_autoevents {} {
85
global w
86
$w bind legend_infopoint <Any-Enter> {set killatleave [
exec ./mexico_legend_infopoint_viewall.sh $op_parallel
] }
87
$w bind legend_infopoint <Any-Leave> {exec ./
mexico_legend_infopoint_kaxv.sh }
88
89
$w bind cancun <Any-Enter> {set killatleave [exec
$appl_image_viewer -geometry +800+400 ./
mexico_site_name_cancun.jpg $op_parallel ] }
90
$w bind cancun <Any-Leave> {exec kill -9 $killatleave }
91
92
$w bind chichen_itza <Any-Enter> {set killatleave [exec
$appl_image_viewer -geometry +800+100 ./
mexico_site_name_chichen_itza.jpg $op_parallel ] }
93
$w bind chichen_itza <Any-Leave> {exec kill -9
$killatleave }
94
95
$w bind tulum <Any-Enter> {set killatleave [exec
$appl_image_viewer -geometry +800+400 ./
mexico_site_name_tulum.jpg $op_parallel ] }
96
$w bind tulum <Any-Leave> {exec kill -9 $killatleave }
97
}
98
99
proc create_country_mexico_application_ballons {} {
100
global w
101
global is1
102
gisig:set_balloon $is1.country "Notation of State and
Site"
103
gisig:set_balloon $is1.color "Symbolic Color od State
and Site"
104
}
105
106
create_country_mexico
107
create_country_mexico_bind
108
create_country_mexico_sites
109
create_country_mexico_autoevents
110
create_country_mexico_application_ballons
111
112
scaleAllCanvas 0.8
113
##EOF
Listing 8. Example InfoPoint Active Source data.
The source contains a minimal example with the active
objects for the province Yucat´an in M´exico. The full data set
contains all provinces as shown in Figure 6. The functional
parts depicted in the source are the procedures for:
• create_country_mexico:
The cartographic mapping data (polygon data in this
example only) including attribute and tag data.
• create_country_mexico_bind:
The event bindings for the provinces. Active Source
functions are called, displaying province names.
• create_country_mexico_sites:
Selected site names on the map and the active objects
for site objects including the InfoPoint object. The
classiﬁcation of the InfoPoint is done using the tag
legend_infopoint. Any internal or external ac-
tions like context dependent scripting can be triggered
by single objects or groups of objects.
• create_country_mexico_autoevents:
Some autoevents with the event deﬁnitions for the
objects (Enter and Leave events in this example).
• create_country_mexico_application_ballons:
Information for this data used within the Active Source
application.
• Call section: The call section contains function calls
for creating the components for the Active Source
application at the start of the application, in this case
the above procedures and scaling at startup.
Any number of groups of objects can be build. This excerpt
only contains Cancun, Chichen Itza and Tulum. A more
complex for this example data set will group data within
topics, any category can be distinguished into subcategories
180
International Journal on Advances in Software, vol 2 no 2&3, year 2009, http://www.iariajournals.org/software/

in order to calculate speciﬁc views and multimedia informa-
tion, for example for the category site used here:
• city (M´exico City, Valladolid, M´erida, Playa del
Carmen),
• island (Isla Mujeres, Isla Cozumel),
• archaeological (Cob´a, Mayapan, Ek Balam, Ak-
tumal, Templo Maya de Ixchel, Tumba de Caracol),
• geological (Chicxulub, Actun Chen, Sac Actun, Ik
Kil),
• marine (Xel H´a, Holbox, Palancar).
Objects can belong to more than one category or subcategory
as for example some categories or all of these as well as
single objects can be classiﬁed touristic.
The data, as contained in the procedures here (mapping
data, events, autoevents, objects, bindings and so on) can be
put into a database for handling huge data collections.
8.5. Start an InfoPoint
Listing 9 shows the start routine data (as shown in
Figure 7). For simplicity various images are loaded in several
application instances (xv) on the X Window System. Various
other API calls like Web-Get fetchWget for fetching
distributed objects via HTTP requests can be used and
deﬁned.
1
xv -geometry +1280+0
-expand 0.8
mexico_site_name_cancun_map.jpg &
2
xv -geometry +1280+263 -expand 0.97
mexico_site_name_cancun_map_hotels.jpg &
3
4
xv -geometry +980+0
-expand 0.5
mexico_site_name_cancun.jpg &
5
xv -geometry +980+228
-expand 0.61
mexico_site_name_cancun_hotel.jpg &
6
xv -geometry +980+450
-expand 0.60
mexico_site_name_cancun_mall.jpg &
7
xv -geometry +980+620
-expand 0.55
mexico_site_name_cancun_night.jpg &
8
9
xv -geometry +740+0
-expand 0.4
mexico_site_name_chichen_itza.jpg &
10
xv -geometry +740+220
-expand 0.8
mexico_site_name_cenote.jpg &
11
xv -geometry +740+420
-expand 0.6
mexico_site_name_tulum_temple.jpg &
12
#xv -geometry +740+500
-expand 0.3
mexico_site_name_tulum.jpg &
13
xv -geometry +740+629
-expand 0.6
mexico_site_name_palm.jpg &
Listing 9. Example InfoPoint event start routine data.
8.6. Stop an InfoPoint
Listing 10 shows the stop routine data. For simplicity all
instances of the applications started with the start routine
are removed via system calls.
1
killall -9 --user cpr --exact xv
Listing 10. Example InfoPoint event stop routine data.
Using Active Source applications any forget or delete modes
as well as using Inter Process Communication (IPC) are
possible.
9. Software and hardware resources used
For using High Performance Computing (HPC) and
Grid Computing resources (ZIVGrid, ZIVcluster, ZIVsmp,
HLRN) for Distributed Computing with Geoscientiﬁc Infor-
mation Systems (GIS) it is has been shown [13], [1], [2] to
be necessary carrying out an integration and conﬁguration
regarding software and hardware components.
For the HPC resources it is an ongoing research and de-
velopment goal to optimise the single-system-properties with
the software and hardware installation used for the case stud-
ies discussed within this paper. Several software / hardware
conﬁgurations have been tested with the complex multi-
cluster-multi-site installation of HLRN-II in order to ensure
that the resulting system will be seen as one single system
for system administration and various user applications.
9.1. Integrating SW and HW resource components
As the HLRN consists of two complexes located at two
sites one goal is, to enable operation and use of all resources
as one single system. The integration of the different SMP
and MPP systems into this concept is an essential part, so
accessing these resources via applications will be managed
with an uniform interface. On the other hand it shall be
possible to use the redundancies of the complexes to increase
availability and minimise overall maintenance downtimes as
with the system architecture it has been taken care that each
complex can be down for full maintenance separately. The
most important aspects of the single-system-properties in
this context regard:
• Joint user and job management for one uniform user
space, regarding an uniform addressing, use, and ad-
ministration.
• System-spanning home directories including mirroring
and replication, reducing the need for explicit data
transfer and data synchronisation.
• Joint job and data scheduling with automated data
transfer (data staging).
• Storage integration, integration of SAN capabilities,
Data-Grid.
• MPI communication for very large applications using
MPI-2 in user space in order to use resources of the
spatially distributed complexes.
For the complexity involved with this, the following sections
focus on the architecture and the hardware and software
components and applications that had to be conﬁgured with
the installation.
Currently application use cases have been internationally
presented for this installation from application view only.
The example use cases and most important results on
hardware and software conﬁguration are referenced in a
separate section. This paper concentrates on the hardware
181
International Journal on Advances in Software, vol 2 no 2&3, year 2009, http://www.iariajournals.org/software/

and software components that had to be conﬁgured for
creating a suitable integrated HPC resource base.
9.2. Architecture and phases
Table 3 gives a compact overview of the most important
hardware resources of the HLRN-II system available for
applications within the main installation phases. For the
speciﬁc areas of application from the usage spectrum, the
complexes consist of suitable SMP and MPP components.
Separate InﬁniBand networks for fast MPI and IO are avail-
able. The system complexes at the sites Hannover and Berlin
are extended in two phases, with identical components and
conﬁguration. The hardware conﬁguration details left out
here due to legal issues will be provided in phase 2 of the
installation process.
As with computing at the top edge of maximum perfor-
mance and minimum of obstacles the story is not all about
software only. There is a number of limits reducing efﬁ-
ciency that are immanently appearent with the architecture,
in theses cases ordered by priority for use with the examples
presented here:
• latencies of network and batch system limiting the
response times for interactive use,
• throughput / IO limiting the streaming facilities with
model calculation on the compute nodes and data
servers,
• scalability of existing algorithms for computational
problem solving,
• memory limiting efﬁcient high-resolution simulation,
• storage capability limiting chain job restart and check-
pointing,
• number of CPUs (cores) limiting the number of loosely
coupled highly parallel compute events,
• availability of resources due to competitive jobs for
different user applications,
• non-certiﬁed components limiting ﬂexibility with appli-
cation porting and conﬁguration.
As a detailed description for a hardware solution is out
of scope of this paper, the most important aspects for
the applications handled are latencies and throughput. Fast
dedicated networks for example using InﬁniBand fabrics
can help to reduce the bottlenecks and latencies for highly
parallel as well as for dynamical and interactive applications.
For example with event triggered “dynamically” changing
visualisation controlled from within an interactive infor-
mation system, large computation tasks as well as large
visualisation IO (several hundred megabytes per second per
task) can result. This will even increase in the near future.
As far as separate physical networks dedicated for MPI and
IO are available, applications will proﬁt.
10. Evaluation and lessons learned
The current work of implementing and conﬁguring soft-
ware components and the case study shows use of computing
resources with the Active Source framework, spatial event
handling, and cognitive dynamical application.
With this solution it is possible to build sets of inter-
active, extensible, portable, and reusable applications with
interdisciplinary background based on the computing power
of MultiCore and HPC Systems.
In the last years many ”ﬂavours” of High End Computing
have been evaluated. Summing up the experiences of the
longterm project regarding this aspect, applications on the
following architectures and paradigms have been success-
fully implemented and tested:
• Distributed and High Performance Computing (DHPC)
on MPP, SMP, and vector computers,
• Grid Computing and Distributed Computing,
• Cluster Computing,
• Mobile, Utility, Tool, and Ubiquitous Computing.
With the current plans, the next topic on the agenda will be
the Cloud Computing top service level – XaaS (Application
as a Service, AaaS; Software as a Service / Security as a
Service, SaaS) based on the base levels (Infrastructure as
a Service, IaaS; Platform as a Service, PaaS; Desktop as a
Service, DaaS).
The InfoPoint concept has been demonstrated, working
for various disciplines, visualising and extending various
features of cartography and e-Science under cognostic as-
pects. These applications may also use resources interac-
tively but any short latencies are difﬁcult to achieve with
most current computing installations.
For optimising the use of resources the software conﬁg-
uration will have to be coordinated with the hardware con-
ﬁguration in order to build an efﬁcient system architecture.
Although the Active Source framework can integrate various
concepts, it is highly dependent on the system conﬁguration.
The most obvious obstacles limiting efﬁciency and ease
of use are the current state of HPC environments and
the missing standardisation and modularisation of system
components like for the batch system and scheduling. As
in the HPC world every installation comes with an unique
conﬁguration, this is a crucial point. So always not only take
a look on the software side but on the hardware, too.
11. Future work
The topics in focus for the next years can be grouped
in three sections: technical aspect, collaboration work, and
work within the participating disciplines.
11.1. Technical aspects
The basic algorithms have been implemented and tested
for enabling distributed and HPC systems for dynamical use.
182
International Journal on Advances in Software, vol 2 no 2&3, year 2009, http://www.iariajournals.org/software/

HLRN-II Overview
Phase 1 (2nd Quarter 2008)
Phase 2 (from 2009 on)
Total
Complex H/B each, MPP
MPP 1: SGI Altix ICE 8200EX (ICE+)
MPP 2: SGI Carlsbad 2
Number of nodes (blades)
320 (Colfax-S w/ Seaburg)
960
1280
Number of sockets / cores
640 (Quad-Core) / 2560
[details provided in phase 2]
Processor
Intel Xeon Harpertown, 3 GHz / 80 W
[Intel Next Generation Xeon]
Memory & network
5.1 TByte (2 GByte/core, IB 4×DDR)
29.3 TByte (IB 2×Dual DDR)
34.4 TByte
System peak performance
30.7 TFlop/s
≈100 TFlop/s
≈130 TFlop/s
Complex H/B each, SMP
SMP 1: SGI Altix XE 1300
SMP 2: SGI UltraViolet
Number of nodes
47 CN (+2 HN, XE250)
136
183
Number of sockets / cores
94 (Quad-Core) / 376
[details provided in phase 2]
Processor
Intel Xeon Harpertown, 3 GHz / 80 W
[Intel Next Generation Xeon]
Memory & network
2.8 TByte (8 GByte/core, IB 4×DDR)
8.7 TByte (NumaLink 5)
11.5 TByte
System peak performance
4.2 TFlop/s
≈22 TFlop/s
≈26 TFlop/s
Complex H+B overall
Phase 1
Phase 2
Total
Storage capacity (gross)
1.15 PByte (RAID-Array)
1.15 PByte (RAID-Array)
2.3 PByte
IO bandwidth
14 GByte/s
14 GByte/s
28 GByte/s
Number of cores (CN)
5824
19360
25184
Memory
16 TByte
76 TByte
92 TByte
System peak performance
70 TFlop/s
≈242 TFlop/s
≈312 TFlop/s
Table 3. HPC hardware resources in test situation, HLRN-II complexes Hannover (H) and Berlin (B).
The necessary conﬁguration of systems and resources has to
be standardised for practicing a uniform setup and in order
to minimise invasive overhead. In the future it cannot be the
user having the need to trigger most of the conﬁguration of
complex system components on every system an application
should be run, there will have to be suitable interfaces.
There will have to be standard interfaces for parallelisa-
tion in the future. For both distributed and High Performance
Computing, monitoring and accounting is necessary in order
to handle interactive use.
The application of the frameworks presented for high
level research and development consortium has already
begun and will accelerate to develop standardised means
of communication, like Web Services for HPC services for
dedicated issues.
Currently the collaboration partners prepare to integrate
the methods presented here for using distributed resources
developed into components of open and commercial geosci-
entiﬁc information systems for productive use.
11.2. Collaboration work
Based the current organisational structure for combining
work of the different interest groups, the block diagram
in Figure 8 illustrates the future directions of integrating
and co-developing large collaborative target frameworks
and applications for service-oriented Distributed and High
Performance Computing on management level. It shows the
dependencies of
• market and services (green colour, shingle and cross
pattern),
• computing services (red colour, brick pattern),
• HPC and distributed resources (blue colour, gravelly
pattern),
• and resources to be provisioned or developed (yellow
colour).
The collaboration partners in the ﬁelds of HPC, services,
geosciences and exploration, do regard the modular three
level framework structure essential for future development
of an integrated solution.
As presented during the DigitalWorld conference 2009
in Canc´un, M´exico and with the Leadership in Research
consortium, the proposed Computing Industry Alliance has
been regarded to be a suitable umbrella organisation for
Distributed and High Performance Computing and geo-
exploration sciences. The framework described is an exam-
ple currently building the base for creating efﬁcient inter-
disciplinary industry research cooperations for implementing
the next generation of dynamical applications on Distributed
and High Performance Computing resources based on the
“Grid-GIS house” [13]. Interests to force this development
exist not only in the Gulf of M´exico region but as well in
Russia and Saudi Arabia.
183
International Journal on Advances in Software, vol 2 no 2&3, year 2009, http://www.iariajournals.org/software/

Accounting
Grid middleware
Security
computing
Trusted
&
Geo−information market
Grid services
HPC
Geo−
Sciences
Market
Geoscientific
MPI
Interactive
Customers
Provider
Service
Legal
Point/Line
Parallel.
NG−Arch.
Design
Interface
Vector data
2D/2.5D
Raster data
Algorithms
Framework
Metadata
3D/4D
MMedia/POI
Batch
Data Service
Computing
Services
Distrib.
(c) Rückemann 2009
resources
Distributed
data storage
computing res.
Distributed
Workflows
Data management
Generalisation
Integration/fusion
Multiscale geo−data
GIS
components
Data Collection/Automation
Data Processing
Data Transfer
companies, universities ...
Scientific institutions,
Geo−scientific processing
Simulation
GIS
Resource requirements
Visualisation
Virtualisation
Navigation
Integration
Geo−data
Services
InfiniBand
Networks
High Performance Computing and Grid resources
Geo services: Web Services / Grid−GIS services
Visualisation
Service chains
Quality management
Distributed/mobile
Geoinformatics, Geophysics, Geology, Geography, ...
Exploration
Ecology
Geo
monitoring
Figure 8. Future directions for service-oriented Distributed and High Performance Computing (“Grid-GIS house”).
11.3. Disciplines
Three key player collaboration sections from High Per-
formance Computing and Distributed and Grid Comput-
ing, from services and technical development, and from
Geosciences are currently building the next generation of
information and computation system as shown in Figure 8.
• For the HPC and distributed resources section top level
(blue) HPC computing companies are engaged. Next
generation architectures and standards, for example
hardware and network conﬁguration, batch, and MPI,
for using, accessing, and managing backend resources
are the most prominent goals. Cooperations like DEISA
[22] and PRACE [23] expedite the evolution and vis-
ibility of the core factors for the overall European
resources.
• For Distributed Computing services, Grid and Cloud
(red) various organisations and activities regarding ser-
vices and technology will be important [24], [25], [26],
[27], [28], [29], [30], [31]. A number of requirements
regarding Security are exposed to be handled in in-
terdisciplinary context [32], [33], [34], [35], [36]. For
building a market ready network of partners a ﬂexible
accounting is most important. Regarding accounting, an
integrated solution with complex accounting units suit-
able for this scenario has been proposed [2] considering
suitable components [37], [38], [39], [40].
• On the level of market and services (green) various key
players cover science and research, as for geosciences
and exploration. A lot of work has been done in
the previous years in the disciplines of geophysics,
seismics, seismology als well as regarding oil and gas
in order to exploit High End Computing resources [41],
[42], [43], [44], [45], [46], [47], [48], [49], [50], [51],
[52]. The work has already begun on parallelisation
of geoscientiﬁc algorithms for parallel processing. The
future work will bring the essentials of these disciplines
together in order build an information and computing
system for the exploration sciences.
Currently there are no comprehensive frameworks available,
directly comparable to the Grid-GIS house. On this top
level for the next years, legal as well as technical aspects
are most important for integration of national an interna-
tional geospatial data integration (GDI / SDI) frameworks
like GSDI, INSPIRE, GDI-DE, GMES, GEOSS and Public
Sector Information (PSI) into these concepts.
184
International Journal on Advances in Software, vol 2 no 2&3, year 2009, http://www.iariajournals.org/software/

12. Summary and concluding remarks
In this article the implementation and employment of
dynamical applications for use with Distributed and High
Performance Computing resources has been presented. The
concept relies on source code based scripting applications
for utilising computing resources for specialised information
systems and e-Science. Event-driven object graphics are
based on Active Source, which has been developed within
the GISIG actmap-project.
Based on the current framework, efﬁcient access to dis-
tributed computing resources from HPC to Grid Computing
can be achieved. Design and conﬁguration in most cases of
HEC has to consider the hardware and network components,
too. Standardising interfaces helps to simplify the problems
of resource usage and encourage developers and users to
build new parallel networking applications. Overcoming
these obstacles using Distributed and HPC resources for
dynamical application, the step currently done is to im-
plement platforms with commercial support for integrating
these features into future applications.
The higher-level result is, that it will only be possible
to accomplish the goal of a ﬂexible integrated information
system for geosciences and exploration using distributed
High End Computing resources if partners from computing,
services, and various geoscience disciplines will collaborate.
With this goal and based on the extended Grid-GIS house,
building an high end international information computing
system for the exploration sciences is currently under way.
Acknowledgements
We are grateful to all the colleagues and staff at the North-
German Supercomputing Alliance (HLRN, Norddeutscher
Verbund f¨ur Hoch- und H¨ochstleistungsrechnen), at the Re-
gionales Rechenzentrum f¨ur Niedersachsen (RRZN), Leib-
niz Universit¨at Hannover (LUH), at the Konrad-Zuse-
Zentrum f¨ur Informationstechnik Berlin (ZIB), and at Silicon
Graphics Inc. (SGI) for their great work, support, and coop-
eration in the ﬁelds of operating, managing, and consulting
of High Performance Computing resources within the North-
German Supercomputing Alliance.
We like to thank all the colleagues at the Institut
f¨ur Rechtsinformatik (IRI) and the European Legal Infor-
matics Study Programme (EULISP) at the LUH, at the
Westf¨alische Wilhelms-Universit¨at M¨unster (WWU), the
Leibniz-Rechenzentrum der Bayerischen Akademie der Wis-
senschaften, M¨unchen (LRZ), at the Zentrum f¨ur Informa-
tionsdienste und Hochleistungsrechnen (ZIH) of the Technis-
che Universit¨at Dresden (TUD), at the Forschungszentrum
J¨ulich (FZJ), at the Forschungszentrum Karlsruhe (FZK),
at the Hochleistungsrechenzentrum Stuttgart (HLRS), the
Zentrum f¨ur Informationsverarbeitung M¨unster (ZIV), at the
Research Center L3S, Hannover, the German Grid Initiative
D-Grid, and at numerous other sites and HPC alliances, the
Gauss Centre for Supercomputing (GCS), DEISA, PRACE,
NorduGrid, and the AK Supercomputing / Zentren f¨ur
Kommunikation und Informationsverarbeitung in Lehre und
Forschung e.V. (ZKI) for the productive cooperation and
information exchange over the last years.
Thanks to IBM, Sun, and SGI and all their staff members
involved for supporting this work by managing and provid-
ing high end High Performance Computing, Cluster, Cloud,
and Grid Computing resources and services for generations.
References
[1] C.-P. R¨uckemann, “Using Parallel MultiCore and HPC
Systems for Dynamical Visualisation,” in Proceedings of
the
International
Conference
on
Advanced
Geographic
Information
Systems
&
Web
Services
(GEOWS
2009),
February
1–7,
2009,
Cancun,
Mexico
/
ICDS
2009,
ACHI 2009, ICQNM 2009, GEOWS 2009, eTELEMED
2009, eL&mL 2009, eKNOW 2009 / DigitalWorld 2009,
International Academy, Research, and Industry Association
(IARIA).
IEEE Computer Society Press, IEEE Xplore
Digital Library, 2009, pp. 13–18, Dragicevic, S., Roman,
D., Tanasescu, V. (eds.), 6 pages, BMS Part Number:
CFP0973F-CDR, ISBN: 978-0-7695-3527-2, URL: http:
//ieeexplore.ieee.org/stamp/stamp.jsp?
arnumber=4782685&isnumber=4782675
(PDF),
URL:
http://www.user.uni-hannover.de/cpr/
x/bib/Rueckemann_2009_MultiCore_HPC.bib
(BIBTEX
entry),
URL:
http://www.iaria.org/
conferences2009/AwardsGEOWS09.html
(Best
Paper Award) (HTML).
[2] C.-P. R¨uckemann, Accounting and Billing in Computing
Environments, M. Pankowska, Ed.
Business Science
Reference, IGI Global, Hershey, Pennsylvania, USA, Oct.
2009, chapter X, in: Pankowska, M. (ed.), Infonomics for
Distributed Business and Decision-Making Environments:
Creating Information System Ecology, 421 pages, ISBN: 978-
1-60566-890-1, URL: http://www.igi-global.com/
reference/details.asp?ID=34799
(Information)
(HTML),
URL:
http://www.igi-global.com/
bsr/details.asp?ID=34799&v=preface
(Preface)
(HTML), URL: http://www.user.uni-hannover.
de/cpr/x/bib/Rueckemann_2009_Computing_
Environments.bib (BIBTEX entry).
[3] M. Pankowska, Ed., Infonomics for Distributed Business and
Decision-Making
Environments:
Creating
Information
System
Ecology
(in
press).
IGI
Global,
Hershey,
Pennsylvania,
USA,
Oct.
2009,
ISBN:
978-1-60566-
890-1,
URL:
http://www.igi-global.com/
reference/details.asp?ID=34799
(Information)
(HTML),
URL:
http://www.igi-global.com/
bsr/details.asp?ID=34799&v=preface
(Preface)
(HTML), URL: http://www.user.uni-hannover.
de/cpr/x/bib/Rueckemann_2009_Computing_
Environments.bib (BIBTEX entry).
[4] “HPCwire,” 2009, URL: http://www.hpcwire.com.
[5] “insideHPC,” 2009, URL: http://www.insidehpc.
com.
185
International Journal on Advances in Software, vol 2 no 2&3, year 2009, http://www.iariajournals.org/software/

[6] “LX
Project,”
2009,
URL:
http://www.user.
uni-hannover.de/cpr/x/rprojs/de/index.
html.
[7] C.-P.
R¨uckemann,
“Beitrag
zur
Realisierung
portabler
Komponenten f¨ur Geoinformationssysteme. Ein Konzept
zur
ereignisgesteuerten
und
dynamischen
Visualisierung
und Aufbereitung geowissenschaftlicher Daten,” Disserta-
tion, Westf¨alische Wilhelms-Universit¨at, M¨unster, Deutsch-
land, 2001, 161 (xxii + 139) Seiten, Ill., graph. Darst.,
Kt., URL: http://wwwmath.uni-muenster.de/cs/
u/ruckema/x/dis/download/dis3acro.pdf.
[8] C.-P. R¨uckemann, “Active Map Software,” 2001, 2005,
URL:
http://wwwmath.uni-muenster.de/cs/u/
ruckema (information, data, abstract).
[9] “HLRN, North-German Supercomputing Alliance (Nord-
deutscher Verbund f¨ur Hoch- und H¨ochstleistungsrechnen),”
2009, URL: http://www.hlrn.de.
[10] “D-Grid, The German Grid Initiative,” 2009, URL: http:
//www.d-grid.de.
[11] “D-Grid-Integrationsprojekt (DGI),” 2009, URL: http://
dgi.d-grid.de.
[12] “NorduGrid,” 2009, URL: http://www.nordugrid.
org.
[13] C.-P. R¨uckemann, “Geographic Grid-Computing and HPC
empowering Dynamical Visualisation for Geoscientiﬁc In-
formation Systems,” in Proceedings of the 4th International
Conference on Grid Service Engineering and Management
(GSEM), September 25–26, 2007, Leipzig, Deutschland, co-
located with Software, Agents and services for Business,
Research, and E-sciences (SABRE2007), R. Kowalczyk, Ed.,
vol. 117.
GI-Edition, Lecture Notes in Informatics (LNI),
Gesellschaft f¨ur Informatik e.V. (GI), 2007, 66–80 pages,
ISBN: 78-3-8579-211-6, ISSN: 1617-5468.
[14] “Applications with Active Map Software, Screenshots,” 2005,
URL:
http://wwwmath.uni-muenster.de/cs/u/
ruckema/x/sciframe/en/screenshots.html.
[15] “GSEM, International Conference on Grid Services Engineer-
ing and Management,” 2007, URL: http://www.gsem.
de.
[16] “HLRN-II Photo Gallery,” 2008, RRZN Top-News, May 9,
2008, URL: http://www.rrzn.uni-hannover.de/
hlrn_galerie.html.
[17] “Torque
Administrator
Manual,”
2009,
URL:
http://www.clusterresources.com/wiki/
doku.php?id=torque:torque_wiki.
[18] “Moab
Admin
Manual,
Moab
Users
Guide,”
2009,
URL:
http://www.clusterresources.com/
products/mwm/moabdocs/index.shtml,
URL:
http://www.clusterresources.com/products/
mwm/docs/moabusers.shtml.
[19] “Tcl
Developer
Site,”
2009,
URL:
http://dev.
scriptics.com/.
[20] C.-P.
R¨uckemann,
Ed.,
Ergebnisse
der
Studie
und
Anforderungsanalyse
in
den
Fachgebieten
Monitoring,
Accounting,
Billing
bei
den
Communities
und
Ressourcenanbietern
im
D-Grid.
Koordination
der
Fachgebiete Monitoring, Accounting, Billing im D-Grid-
Integrationsprojekt, 1. Juni 2006, D-Grid, Deutschland,
2006,
141
Pages,
URL:
http://www.d-grid.de/
fileadmin/dgi_document/FG2/koordination_
mab/mab_studie_ergebnisse.pdf.
[21] C.-P. R¨uckemann, W. M¨uller, and G. von Voigt, “Comparison
of Grid Accounting Concepts for D-Grid,” in Proceedings
of the Cracow Grid Workshop, CGW’06, Cracow, Poland,
October 15–18, 2006, M. Bubak, M. Turała, and K. Wiatr,
Eds., Jul. 2007, pp. 459–466, ISBN: 83-915141-7-X.
[22] “Distributed European Infrastructure for Supercomputing Ap-
plications (DEISA),” 2009, URL: http://www.deisa.
org.
[23] “Partnership for Advanced Computing in Europe (PRACE),”
2009, URL: http://www.prace-project.eu.
[24] “European Grid Initiative (EGI),” 2009, URL: http://
www.egi.org.
[25] “Open Grid Forum (OGF),” 2009, URL: http://www.
ofg.org.
[26] “Global Grid Forum (GGF),” 2009, URL: http://www.
gridforum.org.
[27] “Globus Alliance,” 2009, URL: http://www.globus.
org.
[28] “UNICORE,” 2009, URL: http://www.unicore.eu.
[29] “Tcl
Developer
Site,”
2009,
URL:
http://dev.
scriptics.com.
[30] “Building scalable, high performance cluster/grid networks:
The role of ethernet,” Force10 Networks: Cluster/Grid Com-
puting, 2005, URL: http://www.force10networks.
com/applications/roe.asp.
[31] “International Conference on Grid Services Engineering and
Management (GSEM),” 2007, URL: http://www.gsem.
de.
[32] A. Chakrabarti, Grid Computing Security, 1st ed.
Springer
Berlin Heidelberg New York, 2007.
[33] “Computer Emergency Response Team (CERT),” 2007, URL:
http://www.cert.org.
[34] “KES – Die Zeitschrift f¨ur Informationssicherheit,” 2007,
URL: http://kes.info.
[35] “BSI-Forum,” 2007, URL: http://www.bsi.bund.de/
literat/forumkes.htm.
[36] C. Eckert, IT-Sicherheit, Konzepte – Verfahren – Protokolle,
4th ed.
Oldenbourg Wissenschaftsverlag GmbH, M¨unchen,
Wien, 2006.
[37] E. Elmroth, P. Gardfj¨all, O. Mulmo,
˚A. Sandgren, and
T. Sandholm, A Coordinated Accounting Solution for Swe-
Grid,
Oct.
2003,
URL:
http://www.pdc.kth.se/
grid/sgas/docs/SGAS-0.1.3.pdf.
[38] P. Gardfj¨all, Design Document: SweGrid Accounting Sys-
tem Bank, Dec. 2003, URL: http://www.pdc.kth.se/
grid/sgas/docs/SGAS-BANK-DD-0.1.pdf.
[39] P.
Gardfj¨all,
E.
Elmroth,
L.
Johnsson,
O.
Mulmo,
and
T.
Sandholm,
“Scalable
Grid-wide
capacity
allo-
cation with the SweGrid Accounting System (SGAS),”
Concurrency and Computation Practice and Experience,
2006, (Submitted for Journal Publication, October 2006),
URL: http://www.cs.umu.se/˜elmroth/papers/
sgas_submitted_oct_2006.pdf.
[40] J. Illik, Electronic commerce: Grundlagen und Technik f¨ur
die Erschließung elektronischer M¨arkte, 2nd ed. Oldenbourg
Wissenschaftsverlag GmbH, M¨unchen, Wien, 2002.
[41] D. Bevc, O. Feodorov, and A. Popovici, “Internet-Based Pro-
cessing: A Paradigm Shift for Exploration,” in Offshore Tech-
186
International Journal on Advances in Software, vol 2 no 2&3, year 2009, http://www.iariajournals.org/software/

nology Conference, Houston, TX, U. S. A., May 2001, URL:
http://www.3dgeo.com/docs/OTC13276.pdf.
[42] D. Bevc, O. Feodorov, A. Popovici, and B. Biondi, “Internet-
based seismic processing: The future of geophysical com-
puting,” in 70th Ann. Internat. Mtg., Soc., Expl. Geophys.,
Calgary, Canada, Aug. 2000, URL: http://www.3dgeo.
com/docs/insp.pdf.
[43] “Internet enabling remote processing,” The American Oil and
Gas Reporter, 2001, URL: http://www.3dgeo.com/
docs/0701_3dgeodevelopment_72dpi.pdf.
[44] F. Karbarz, “Grid computing for seismic processing,” The
Leading Edge, vol. 22, no. 1, pp. 58–60, 2003.
[45] DataSynapse, “Self-managed, guaranteed, distributed com-
puting platform,” Industry White Paper: Enabling Govern-
ment/Public Sector Applications for On-Demand Computing,
p. 28, 2003, URL: http://www.datasynapse.com/
pdf/DataSynapse_Enabling_Government.pdf.
[46] D. Bevc, O. Feodorov, I. Musat, and S. Zarantonello,
“Grid computing for energy exploration and development,”
in Minisymposium on Grid Computing for the Oil and
Gas Industry, SIAM Conference on Parallel Processing for
Scientiﬁc Computing, San Francisco, U. S. A., Feb. 2004,
URL: http://www.3dgeo.com/docs/SIAM_3DGeo_
abstract.pdf.
[47] D. Bevc, S. Zarantonello, N. Kaushik, and I. Musat,
“Grid computing helps allocate scientiﬁc data,” p. 14,
URL:
http://search390.techtarget.com/tip/
1,289483,sid10_gci882336,00.html.
[48] “Sun Infrastructure Solution for Grid Computing: Oil and
Gas,” Sun, May 2004, URL: http://www.sun.com/
solutions/documents/solution-sheets/EN_
grid-oil+gas-ds_FF.xml.
[49] D. Thomas and M. Petitdidier, “EGEODE: a Grid In-
frastructure for Research in Geosciences,” EAGE 67th
Conference
&
Exhibition,
Madrid,
Spain,
13–16
June
2005, 2005, URL: http://www.cgg.com/corporate/
research/articles/eage05/Thomas.pdf.
[50] D. Bevc, S. Zarantonello, N. Kaushik, and I. Musat, “Grid
computing for energy exploration,” in GGF14 - The Four-
teenth Global Grid Forum, Chicago, IL, U. S. A., 2005, URL:
http://www.3dgeo.com/docs/GGF14_3DGeo.pdf.
[51] M. K¨aser, H. Igel, J. de la Puente, B. Schuberth, G. Jahnke,
and P. Bunge, “Geowissenschaften: Erdbebenforschung durch
H¨ochstleistungsrechnen, Moderne Supercomputer erm¨ogli-
chen die Simulation realistischer Erdbeben-Szenarien unter
Ber¨ucksichtigung komplizierter, geophysikalischer Erdmo-
delle,” Akademie Aktuell, Zeitschrift der Bayerischen Akade-
mie der Wissenschaften, vol. 02, pp. 47–50, 2006.
[52] A. Bachem, H.-G. Hegering, T. Lippert, and M. Resch, “The
Gauss Centre for Supercomputing,” inSiDE, innovatives Su-
percomputing in Deutschland, vol. 4, no. 2, pp. 4–5, Autumn
2006.
187
International Journal on Advances in Software, vol 2 no 2&3, year 2009, http://www.iariajournals.org/software/

