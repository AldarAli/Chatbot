News Video Semantic Topic Mining Based on Multi-wing Harmoniums Model 
 
Xin Wen Xu and Yu Bo Shen 
Department of traffic and transportation engineering 
National University of Defense Technology 
Changsha, China 
{xinwen_xu, shenyubo}@126.com 
Guo Hui Li 
Department of System Engineering 
National University of Defense Technology 
Changsha, China 
guohli@nudt.edu.cn
 
 
Abstract—Two-layer 
undirected 
graphical 
model, 
Harmoniums, is a new approach to mine latent semantic topics 
from observed data. For the multi-modal heterogeneous 
features of news video, this paper proposes multi-wing 
Harmoniums (MWH) model that represents news video stories 
as latent semantic topics derived by jointly modeling the 
transcript text, color histogram and edge histogram of the 
video. This model includes a multivariate Poisson distribution 
and two multivariate Gaussian distributions. It extends and 
improves earlier models based on two-layer random fields, 
which capture bidirectional dependencies between hidden 
topic aspects and observed inputs. The model especially 
facilitates efficient inference and robust topic mixing, and 
provides high flexibilities in modeling the latent topic spaces. 
The variational algorithm efficiently reduces the difficulty of 
model learning. The experiments results on the CCTV news 
video collections and an extensive comparison with various 
extant models show the efficiency of MWH on news video 
semantic mining. 
Keywords-news video multi-wing Harmoniums; video mining; 
semantic mining; news video 
I. 
 INTRODUCTION 
Along with the rapid development of processor speed and 
the Internet as well as the availability of inexpensive massive 
digital storages, there have been strong demands for the 
modeling and mining of the multiple media sources, like text, 
image, audio and video. Numerous researchers have shown 
great interests to the news video as a mass medium for its 
easiness to acquire and richness in information. To fill the 
semantic gap between the low-level features and the high-
level semantic topic of news video, it’s necessary to make 
full use of the rich information provided by the multi-modal 
heterogeneous data so that we can effectively achieve the 
data mining tasks on news video like classification, cluster, 
retrieval 
and 
image 
annotation. 
The 
multi-modal 
heterogeneous data refers to the data of the objects to be 
described collected from different approaches or perspectives. 
And we refer to each of the approaches or perspectives as a 
modality. For example, in the multi-modal face detection, the 
multi-modal data can consist of the 2d face images and 3d 
face shape model; in the mining of multi-modal videos, 
videos can be decomposed into subtitles, audios, images, etc. 
Therefore, the key issues of video semantic mining 
researches are to model the associated data from multiple 
sources jointly and explore appropriate lower dimensional 
latent representations of the originally high-dimensional 
features. The fusion of multi-modal data like key frame 
image, audio and transcript text, has been a widely used 
technique in video processing. The fusion strategy includes 
the feature-level fusion in earlier stage and the later decision-
level fusion. It is an open question as to which fusion 
strategy is more proper for a task. Snoek et al. [1] compares 
the two strategies in the classification of videos. 
There are many approaches to obtain low-dimensional 
intermediate representations of video data. Principal 
component analysis (PCA) [2] has been the most popular 
method, which projects the raw features into a lower-
dimensional feature space where the data variances are well 
preserved. Independent Component Analysis (ICA) [3] and 
Fisher Linear Discriminant (FLD) [4] are also widely used in 
dimensionality reduction. Recently, there are also many 
proposals on modeling the latent semantic topics of the text 
and multimedia data. For example, Latent Semantic Indexing 
(LSI) [5] finds a linear transform of word counts into a latent 
eigenspace of document semantics. Though LSI can roughly 
obtain the latent semantic and work well in automatic index 
application, it generates overfitting for failing to meet the 
statistics principles. Later, LSI is extended to probabilistic 
Latent Semantic Indexing (pLSI) [6], which models the 
latent topic into the probability distribution of words and the 
documents into the probability distribution of the latent topic. 
The pLSI is based on the principle of probability and defines 
proper generative model, so it can be applied to model 
composition and can control the complexity. The Latent 
Dirichlet Allocation (LDA) [7] is a directed graphical model 
that provides generative semantics of text documents, where 
each document is associated with a topic-mixing vector and 
each word is independently sampled according to a topic 
drawn from this topic-mixing. LDA is later extended to 
Gaussian-Mixture LDA and Correspondence LDA [8], both 
of which are used to model annotated data such as captioned 
images or video with transcript text.  
In fact, the methods mentioned above are mainly used to 
transform high-dimension of raw features to low-
dimensional presentation and presumably gain the latent 
semantics of data. However, these methods are mainly 
applied to single modal data and can’t or can hardly be 
applied to the multi-modal heterogeneous data. Two-layer 
undirected graphical model, Harmoniums [9], is a new 
approach to mine latent semantic topics from observed data 
[16]. Based on Harmoniums models, we present news video 
74
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-265-3
MMEDIA 2013 : The Fifth International Conferences on Advances in Multimedia

 
multi-wing harmoniums (NVMWH) model that represents 
story unit as latent semantic topics derived by jointly 
modeling the transcript keywords, color histogram and edge 
histogram features of news video data for news video 
semantic topic mining. 
The rest of the paper is structured as follows. In Section 2, 
we present the latent semantic topic model of news video-
based on multi-wing harmoniums model and the Learning 
and Inference of its parameters. Section 3 presents the 
experiments and discussions. The paper concludes in Section 
4. 
II. 
THE LATENT SEMANTIC TOPIC MODEL OF NEWS VIDEO 
BASED ON MULTI-WING HARMONIUMS MODEL 
A. The basic Harmoniums model 
The basic Harmoniums model, which was originally 
studied by Smolensky (1986) [9] in his Harmony theory, 
defines a complete bipartite undirected graphical model 
containing two layers of nodes (Fig. 1). Let 
j
{ }
H
 h
 denotes 
the set of hidden units in such a graph, and let 
i
X
 
denotes the set of input units. The Harmoniums model 
creates a random field for the undirected graph model. 
{ }
x

i
j
ij
1
( , | )
exp{
( )
( )
( ,
)}
( )
i
i
j
j
ij
i
j
p x h
x
h
x h
Z



 







(1)
where 
e( )
   denotes the potential function defined on either a 
singleton or a connected pair of units (indexed by e) in the 
model, 
e  denotes the weight of the corresponding potential, 
and
( )
Z  stands for the log-partition function. 
Figure 1.  The basic Harmoniums Model 
The bipartite topology of the harmoniums graph suggests 
that if the nodes within the same layer are given then the 
nodes of the other opposite layer are conditionally 
independent. This makes possible a feasible definition of the 
Harmoniums 
distribution 
based 
on 
the 
conditional 
distribution function 
 and 
 between the two 
layers: 
i
i
( | )
p x h
| )
( | )
p h x
( | )
(
p x h
  p x
h , 
j
( | )
)
(
|
j
p x h
x
  p h
. Hence, it is 
semantically simple and easy to design. For simplicity, all 
conditional probabilities considered here adopt exponential 
forms. 
ˆ
ˆ
( | )
exp{
( )- ({ })}
i
ia ia
i
i
a
p x h
f
x A




ia
jb
 
(2)
ˆ
ˆ
( | )
exp{
( )- ({
})}
i
jb
jb
j
j
a
p h x
g
h B




 
(3)
where 
ia
 and
jb
 respectively denote the sufficient 
statistics of variable 
{
( )}
f 
{
g ( )}

ix  and 
jh . 
( )
iA 
and
j ( )
B 
denote 
respective log-partition functions. And the shifted parameters 
ˆ
ia
 and 
ˆ
jb
 are defined as 
jb
ia
ia
ia
jb
j
 and 
jb
jb
ia
ia
i . It’s produced by the input and all the 
matching pairs in hidden layers. Welling et al. [10] showed 
that these easily comprehensible and manipulable local 
conditionals precisely map to the Harmoniums random fields: 
ˆ
(
)
jb
W g
h


 
ˆ
( )
x

 
 
( |
xp{
jb
iaW
f
)
e
ia
ia
( )
(
)
( )
(
)}
jb
ia
i
jb
jb
j
jb
ijab
ia
ia
i
jb
j
p x h
f
x
g
h
W
f
x g
h







　　　　　　　
 
(4)
ia
 , 
jb
  and 
ia
W  are the set of parameters associated with 
their corresponding potential functions. It’s very difficult to 
make parameter estimation for the appearance of the log-
partition function with joint probability, so ratio symbol 
rather than precise equal symbol is used in the formula. Such 
a model was referred to as the Exponential Family 
Harmoniums (EFH) [10]. In the sequel, we will take 
advantage of this bottom-up strategy to construct specific 
Harmoniums 
from 
the 
easily 
comprehensible 
local 
conditionals. It can be shown that there is no marginal 
independence for either input or hidden variables in a 
Harmoniums model. However, an EFH enjoys the 
advantages of conditional independence between hidden 
variables, which is generally violated in the directed models. 
This property greatly reduces reasoning difficulty. But 
typically, learning harmonium is more difficult due to the 
presence of a global partition function. 
jb
B. The multi-wing Harmoniums model 
The hidden and input units in a Harmoniums model are 
symmetrical, which cannot contribute to explain their causal 
relationship in semanteme. However, the definition 
mentioned above of the local condition independence based 
on two layers can provide explanations for the bidirectional 
causality of Harmoniums structure. Essentially, the hidden 
unit H can be considered as latent topic which defines the 
production of input. Conversely, H can also be seen as the 
predictors produced by a discriminative model of the input 
unit. 
Figure 2.  Multi-wing Harmoniums Model 
In many applications, the input to the model does not 
have to be from a single source and/or of a homogeneous 
data type. For example, in the analysis of typical multi-media 
application video streaming, the input from video clips 
contains much relevant information, such as transcript texts, 
pictures, sounds and motion vectors. Assuming that all these 
inputs are combined together to present one topic center, it 
75
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-265-3
MMEDIA 2013 : The Fifth International Conferences on Advances in Multimedia

will be natural to model the shared topic center using a set of 
hidden units, and to group observations from all sources into 
multiple homogeneous arrays of input units, each 
corresponding to a single source. Thus a multi-wing 
Harmoniums model is constructed, as shown in Fig. 2. This 
model consists of three canonical Harmoniums joint by a 
shared array of hidden units. It’s straightforward to construct 
a multi-wing Harmoniums model from a canonical 
Harmoniums model. For example, a three-wing Harmoniums 
model added by two input sets Y={yn} and Z={zm} can be 
related to H via 
 and 
( | )
(
| h)
n
n
p y h
  p y
( | )
(
| )
m
m
p z h
p z
h
 
, 
where 


c
ˆ
ˆ
(
| )
exp
(
)
(
)
n
nc
nc
n
n
p y
h
e
y
C





nc
ˆmd

 
(5)


d
ˆ
(
| )
exp
(
)
(
)
m
md
kd
m
m
p z
h
s
z
D




 
(6)
ˆ
( )
jb
nc
nc
nc
jb
j
jbU g
h



 
(7)
ˆ
( )
jb
md
md
md
jb
j
jbV g
h



 
(8)
Together with (2), and slightly modified (3) that takes 
into account the influences from X, Y, and Z by loading the 
parameter ˆ with additional shift 
 




ˆ
jb
jb
jb
jb
jb
ia
ia
i
nc
nc
n
md
md
m
ia
nc
md
W f
x
U e
y
V s
z








jb
jb
， 
where 
nc
 and 
md
stand for the matching parameters 
between the hidden unit and the input sets Y and Z. Thus, we 
can get the random field of three-wing exponential family 
Harmoniums. 
{
U }
{
}
V


 






 










, , ,
      exp{
}
ia
ia
i
nc
nc
n
md
md
m
ia
nc
md
jb
jb
jb
k
ia
ia
i
jb
k
jb
ijab
jb
jb
nc
nc
n
jb
k
md
md
m
jb
k
njcb
mjdb
p x y z h
f
x
e
y
s
z
g
h
W
f
x
g
h
U e
y
g
h
V
s
z
g
h


















　　
　　
(9)
where 
, 
, 
, 
 stands for the fully 
statistical characteristics of xi, yn, zm, hk. This construction 
maintains the conditional independence between hidden 
variables given inputs and hence ensures the efficiency of 
inference (once the model is parameterized). Note that x, y, z 
are marginally dependent to each other, as can be quickly 
verified from the bipartite graph structure. This enables the 
application of inferring the features of one source from 
anther source, for example, automatic image annotation 
which attempts to infer related words from a given image. 
{
iaf ( )}

{
enc ( )}

{
smd ( )}

{
gjb ( )}

C. The multi-wing Harmoniums model of news video 
To model video streams, which contain both text and 
image information, in the following, we outline a news video 
multi-wing harmonium (MWH) model based on a text 
submodel and two image submodels using the modular 
constructive technique described above. 
The parameters of this model are defined as follows: 
 
The story unit of news video s denotes a four-
dimensional tuple of ( , , , )
x y z h , which respectively 
denotes the keywords, color features and texture 
features of key-frame, and latent semantic topics. 
 
The vector 
1
( , 2
,
,
I )
x
x x
x


 denotes the keyword 
feature extracted from the transcript associated with 
the shot. Here I is the size of the key words 
dictionary, and 
ix {0,1}
 indicates the absence or 
presence of the keyword i in the story of news video. 
 
The vector 
1
2
N
y
y y
y
 denotes the boundary 
histogram feature of the key frame in the story of 
news video. Similar to the color feature, each key 
frame is divided into rectangular regions of size N in 
fixed size.
C
ny
 R
 presents the color histogram 
feature of the region of size N denoted by C-
dimensional vector. So y is a stack vector whose 
length is CN.  
(
,
,
,
)


 
The vector 
1
2
 denotes the color 
histogram feature of the key frame in the story of 
news video. Each key frame is evenly divided into 
rectangular regions of size N in fixed size. 
 
presents the color histogram feature of the region of 
size N denoted by D-dimensional vector. So z is also 
a stack vector whose length is DN.  
( ,
,
,
N )
z
z z
z


C
nz
R

 
The vector 
1
( , 2
,
,
)
J  denotes the latent semantic 
topics of the story of the news video. J is the number 
of latent topic, 
jh
 R
 denotes the degree of 
correlation between the news story and the latent 
topic of the size j. 
h
h h
h


1) Text feature model 
Following the traditional bag-of-word model [11] for 
texts, we model the video transcript texts by adopting the 
word-count Poisson distribution model of the document. 
Instead of using a continuous surrogate of the discrete counts 
(as done in a mixture of Gaussian setting), or assuming that 
the counts of words are accumulated from independent 
draws from multinomial distributions (as done in LDA), the 
text Poisson model is based on the hypothesis that the rate of 
the word in a document can be described as the word’s 
accumulation of the Poisson distribution in the dictionary, 
namely the latent topic features associated with each 
document directly determine the expected rate of each word 
in a document. In this way, a Poisson distribution is assigned 
to the observation counts of each word in a document. This 
text model has key differences from a multinomial model, 
which is achieved directly by topic mixing in the distribution 
of word rates in the documents combining specific topic 
feature, rather than via an additive effect of multiple single 
topic draws of the same word or via marginalization of the 
latent topic of each word. In the conditional Poisson model 
for word counts, topic mixing is still stable and robust even 
when a word appears only once or a few times in a document, 
which is typical in video captions though. Whereas in the 
multinomial word model, single word i can only come from 
a single topic and is thus unable to satisfy the topic mixing 
directly. The text Poisson submodel is defined as follows: 
For each word 
{1,
, }
i
I


, its rate 
ix  is distributed as: 
76
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-265-3
MMEDIA 2013 : The Fifth International Conferences on Advances in Multimedia

i
(
| )
Poisson( |exp(
))
i
i
j
ij
j
p x h
x
h W



 
(10)
This model shows that, each key word of the transcript 
text in the story of news video relies on a Poisson 
distribution of the latent semantic topic h. In other words, the 
probability of a key word’s appearance is determined by the 
weighted array of latent topic feature h. The parameters 
i
 and 
 are scalar variables.
1
Wij
(
,
,
Ia )

 

( |
 is a I-dimension 
vector.
ij
W
 is a matrix of I×J. Because of the conditional 
independence between xi and h, there is 
[
W ]

)
(
| )
i
i
p x h
p x
h
 
. 
2) Image feature model 
As to the image feature of the story in news video, we 
denote the feature by adopting the 72-dimension feature of 
color histogram and the 80-dimension of edge histogram in 
the partition HSV of key frame image in video stories. The 
color histogram feature yn of the zone N in key frame image 
and the edge histogram feature zn separately satisfies the 
distribution of conditional multivariate Gaussian distribution. 
2
(
| )
(
|
(
),
)
n
n
n
n
j
nj
j
p y
h
N y
h U
2
n




 
(11)
2
(
| )
(
|
(
),
)
n
n
n
n
j
nj
j
p z
h
N z
h V
2
n




 
(12)
where 
n
  and 
nj
U are 72-dimension vectors. 
( 1
,
,
N )





 is 
a stack vector of 72×N. 
nj  is a matrix of 72×N×J, 
and 
n
[
]
U
U

2
  is a 72×72-dimension covariance matrix. 
n
  and 
nj  are 80-dimension vectors，
1
V
( ,
,
N )


 

 is a stack vector 
of 80×N. 
nj
V
 is a matrix of 80×N×J, and 
n
[
 V ]
2
 is a 80×
80-dimension covariance matrix. We adopt unit matrix to 
simplify operation. For the conditional independence 
between yn, zn and h, there are 
( | )
(
| )
n
n
p y h
p y
h
 
 and 
( | )
(
| )
n
n
p z h
p z
h
 
. 
3) Hidden semantic topic model 
As to the hidden unit h of latent topic feature in the news 
video story unit, we assume that each feature is a conditional 
unit-variance 
Gaussian 
distribution, 
whose 
mean 
is 
determined by a weighted combination of the key word 
feature in the video news story unit’s transcript text, the 
color histogram and the edge histogram. 



| , ,
           
|
,1
j
j
ij
i
nj
n
mj
m
i
n
m
p h
x y z
N h
W x
U
y
V z







(13)
where 
ij ,
nj and
mj
V share the same parameters with (10), 
(11) and (12). Similarly there is 
i
i
W
U
( | )
(
| )
p x h
p x
h
 
 from the 
conditional independence. This model denotes the topic 
vector as a random point in Euclidean space, while other 
feature models based on polynomial denote their topic joint 
vector as a point in the space of single feature. The condition 
distribution of all vectors in the model is shown above. 
These local conditional distributions can map into the 
random filed of Harmoniums shown as follows. 


2
2
2
, , ,
exp{
          
2
2
2
          
}
i
n
i
i
n
n
j
n
n
n
n
n
j
n
n
ij
nj
nj
ij
i
j
nj
n
j
nj
n
j
p x y z h
x
y
h
y
z
z
W x h
U y h
V z h





















(14)
By integrating out the hidden variables h in (14), we 
obtain the marginal distribution over the observed keyword 
and color features in the stories of news video. 


2
2
2
, ,
exp{
           
2
2
1
          
(
) }
2
i
n
i
i
n
n
n
n
n
n
n
n
n
j
i
n
n
i
i
nj
n
nj
n
p x y z
x
y
y
z
z
W x
U y
V z




















(15)
which also contains a hidden partition function in this 
distribution. 
The parameter of the NVMWH model, 
( ,
, ,
,
, )
s
    W U V
, 
is learnt by the maximum likelihood of a news video story 
set, where the likelihood function is defined by (15). Due to 
the presence of the global partition function, the learning 
process requires approximate inference methods, which will 
be discussed in the next section. We define the variance of 
the latent variables given the input variables to one in order 
to simplify the parameter estimation. Introducing a 
covariance matrix   can offer additional freedom for joint 
distribution


|
,
,
j
i
n
n
p h
x
y z
, but it would not lead to more 
general representations in terms of probability 

, , 
p x y z
 [10]. 
From the analyses above, we can find that the NVMWH 
model can denote the latent semantic topic of the story units 
in news video. In other words, the NVMWH model can be 
used to infer the latent semantic topic h if given the text 
feature x, the visual feature y and z of key frame image of 
the story unit in the news video. 
D. The learning of the model’s parameter 
By the analysis in the section above, we can find that the 
NVMWH model can be used to gain the hidden semantic 
topic of the story unit in news video but the parameters of the 
model have to be determined before using this model. There 
are many methods to estimate the model parameters. We 
adopt the maximum likelihood method according to the 
NVMWH model defined in the section above. Assuming that 
the training set contain N independent identically distributed 
(IID) story units, namely
n
n
{ , , }
{
,
,
,
1,
,
}
n
x y z
x
y
z n
N



( ,
, ,
,
,
, the 
parameter of the NVMWH model 
)
s
    W U V
 is 
estimated by maximizing the log-likelihood of the data 
defined by (15). Due to the complexity of this model, there is 
no closed-form solution to the maximization problem and we 
have to resort to an iterative method like gradient ascent. The 
learning rules (i.e., the gradients) can be obtained by setting 
the derivatives of (15) with respect to model parameters: 
,
,
,
,
,
i
i
i
n
n
n
p
p
p
p
n
n
n
ij
i
j
i
j
p
p
p
p
nj
n
j
n
j
nj
n
j
n
j
p
p
p
x
x
y
y
z
z
z
W
xh
xh
U
y h
y h
V
z h
z h
















p














 
(16)
where 
'
i
n
n
j
i
i
nj
n
nj
n
h
W x
U y
V z






, 
and 
p
p
  denote 
expectation under empirical distribution (i.e., data average) 
or model distribution of the harmonium, respectively. Like 
other undirected graph models, there is global normalizer 
(a.k.a partition function) in the likelihood function of the 
77
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-265-3
MMEDIA 2013 : The Fifth International Conferences on Advances in Multimedia

NVMWH model, so it’s very difficult to directly 
calculate
p
. Instead, we need approximate inference 
methods to estimate these model expectations 
p
 . We 
explored three approximate inference methods in our work, 
ion q as a product 
of singleton marginals over the variables: 
which are briefly discussed below. 
1) The mean field approximation 
Mean field (MF) is a variational method that 
approximates the model distribution p through a factorized 
form as a product of marginals over clusters of variables [12]. 
We use the naive version of MF, where the joint probability 
p is approximated by a surrogate distribut
( , , , )
(
)
(
,
)
                    
(
,
)
(
)
i
i
n
n
n
i
n
n
n
n
j
j
n
j
q x y z h
q x
q y
q z
q h











 
(17)
where 
the 
singleton 
margina
are 
defined 
ls 
as 
( ) ~ Poisson( )
q xi
i
, 
,
)
n
n
(
) ~
(
q y
N
n
 
, 
(
) ~
(
,
)
n
n
n
q z
N 

 and 
(
) ~
(
,1)
j
j
q h
N 
, and { ,
,
,
}
i
n
n
j
  

is variational parameter.  
The variation parameters can be computed by minimizing
the KL-divergence between p and q, which resu
 
lts in the 
following fixed-point updating equatio
: 
j
ns
exp(
)
i
i
j Wij






 
(18)
2(
)
n
n
n
nj
j
j U







 
(19)
2(
)
n
n
n
nj
j
j V

 


 
 
(20)
j
ij i
nj
n
nj
n
i
n
n
W
U
V









 

 s
. 
(21)
We iteratively update the variational parameters using the 
above fixed-point equations until they converge, and then the 
surrogate distribution q is fully
pecified We replace the 
intractable model expectations 
p
  with 
q
  in (16), which 
are easy to compute from the fully factorized surrogate 
distribution q. Then, we can update the model parameters 
using the learning rules defined in (16). Besides, when using 
the gradient ascent method in the mean field, the learning 
process includes two nested loops: the outer loop iteratively 
updates the model parameters using the learning rules (16), 
while the inner loop iteratively updates the variational 
parameters in order to approximate the model expectations in 
the learning rules. Whenever the model parameters are 
updated (and so are the model distribution p), the whole 
inner loop needs to be executed to recompute the surrogate 
distribution q to approximate the updated model distribution 
mentioned in 
p. 
2) Gibbs sampling 
Gibbs sampling, as a special form of the Markov chain 
Monte Carlo (MCMC) method, has been used widely for 
approximate inference in complex graphical models [13] 
[14]. This method repeatedly samples variables in a 
particular order, with one variable at a time and conditioned 
on the current values of the other variables. If the iteration 
number is big enough, the sampling of joint distribution and 
the boundary distribution is gained successively. For 
example, in the Poisson text submodel 
this 
paper, the sampling order is defined as
1
1
,
,
,
,
,
I
J
x
x h
h


and 
other variables are defined as inputs. First, {
jh }
is set as the 
current value and each
ix is sampled from the condition 
distribution defi ed in (10). Then, 
i
n
x  is set as the current 
value and each 
jh is sampled from the condition distribution 
defined in (13) , and repeat this process iteratively. After a 
large number of iterations (“burn-in” period), this procedure 
guarantees to reach an equilibrium distribution that in theory 
is equal to the model distribution p. Therefore, we use the 
empirical expectation computed using the samples collected 
r the burn-in period to approximate the true expectation 
afte
p
 . The number of “burn-in” iterations and samples is at 
least thousands and typically around tens of thousands. 
3) Contrastive divergence 
An alterative to exact gradient ascent search based on the 
learning rules in (16) is the contrastive divergence (CD) 
algorithm [15] that approximates the gradient learning rules. 
In each step of the gradient update, instead of computing the 
model expectation 
p
 , CD starts from the empirical values 
as the initial samples, runs the Gibbs sampling for up to only 
a few iterations and uses the resulting distribution q to 
approximate the model distribution p. It has been proved that 
the final values of the parameters by this kind of updating 
will converge to the maximum 
elihood estimation. In our 
implementation, we compute 
q
  from a large number of 
samples obtained by running only one step of Gibbs 
sampling with different initializations. Straightforwardly, CD 
is significantly more efficient than the Gib
lik
bs sampling 
m
n comparison with some of the 
A. 
ethod since the “burn-in” process is skipped. 
III. 
EXPERIMENTS AND DISCUSSIONS 
To verify the effectiveness of the NVMWH model, our 
experiments mainly include two parts. First, we show some 
illustrative examples of the latent semantic topics derived by 
the proposed models and discuss the insights they provide 
about the structure and relationships of video categories. In 
the second part, we evaluate the performance of our models 
in video classification i
existing approaches. 
Experimental data and features selection 
The experimental data adopted in our experiment come 
from the news video stories from CCTV news. We collect 
news programs of two years and four months, from May of 
2006 to July of 2008, which contain 25776 news story units. 
Each story unit of the news video is considered as a 
document or a training test example. Our experiment adopts 
4214 news story units which belong to 18 categories, namely 
fire disaster, flood, earthquake, storm (typhoon and 
hurricane), Olympic games, bird flu, Taiwan, the Korean 
nuclear issue, the United Nations, the United States, Russian, 
Japan, Iran, Iraq, terrorist attack, country, oil price and 
football. Each story unit is related to a category. Because the 
CCTV news shows only includes various important news, 
the distribution is uneven of the story unit in each category. 
The number of every kind of story unit in this experiment 
ranges from 26 to 828. 30% of the samples of each category 
78
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-265-3
MMEDIA 2013 : The Fifth International Conferences on Advances in Multimedia

are randomly selected as training samples and the rest are 
considered as the test samples. Table 1 describes the training 
and the test samples of the experiment in detail. 
TABLE I.  
THE SAMPLE DISTRIB
T 
nu
er 
C
 
n
sa
N
 
sa
s 
sa
s
UTION OF TR
Total 
umber of 
AINING SET A
umber of
training 
ND TEST SE
Number 
of test 
Serial 
mb
ategory
name 
mples 
mple
mple
1 
fir
e disaster 
80 
24 
56 
2 
flood 
61 
18 
43 
3 
ea
rthquake 
627 
188 
439 
4 
storm 
106 
32 
74 
5 
O
2
 
lympic 
games 
828 
48
580 
6 
bird flu 
26 
8 
18 
7 
Taiwan 
113 
34 
79 
8 
nuclear issue 
38 
11 
27 
the Korean 
9 
180 
54 
126 
the United 
Nations 
10 
the United 
States 
363 
109 
254 
11 
R
ussian 
259 
78 
181 
12 
Japan 
276 
83 
193 
13 
Iran 
244 
73 
171 
14 
Iraq 
142 
43 
99 
15 
ter
rorist attack 
85 
26 
60 
16 
country 
643 
193 
450 
17 
 oil price 
114 
34 
80 
18 
football 
29 
9 
20 
Total 
4214 
1264 
2950 
As to the text feature, we download all the transcript texts 
of all the news story units from the CCTV website. We adopt 
the word segmentation software of Beijing Language and 
Culture University to conduct word segmentation, and leave 
out all stopwords, and merger synonyms and near-synonyms. 
About nine thousand key words are gained from the 18 
topics above. To further reduce the complexity of the model 
operation, we ignore the low-frequency words whose 
frequencies are less than 6 and extract 3182 keywords as the 
text feature. Hence, the text feature of each story unit in the 
news video is denoted as a 3182-dimension binary feature 
vector where 1 stands for the appearance of a certain 
keyword in the story unit and 0 stands for no appearance. As 
to the visual feature, we adopt the 72-dimension HSV color 
histogram feature and the 80-dimention edge histogram 
fea
tion and the Gibbs sampling 
to c
pro
using E
 hidden variables 
onal 
B. The results and analysis of the latent topic mining 
experiment 
ture of the key frame image in MPEG-7. 
By default, NVMWH is trained via contrastive 
divergence with up to 1000 steps of gradient ascent. We 
adopt the mean field approxima
onduct the model training.  
To mitigate the issue of “identifiability” [10] that allows 
multiple parameters to share the same marginal likelihood, 
the initial estimations of parameters W, U and V in the 
NVMWH were determined by a SVD on the design matrix 
of text/images features over shots. We do not strongly 
emphasize this issue because in our analysis NVMWH is not 
mainly used to directly capture the exact semantics of the 
latent factors underlying the data space. In order to achieve 
semantically more accurate and informative latent factor 
representations, we can apply a subsequent clustering 
cedure on the lower-dimensional representations provided 
by NVMWH. 
The parameters of GM-Mix and GM-LDA were obtained 
M. We infer the latent topic captured by GM-Mix 
using the conditional probabilities of
p(h|x,z)
 and those by GM-LDA based on variati
Dirichlet posteriors of the topic weights. 
Figure 3.  Examples of latent topics 
The NVMWH model can automatically discover low-
dimension meaningful latent topics from the high-dimension 
multi-modal features of the texts and images in the news 
videos. We illustrate 8 latent topics out of the 18 topics 
learned by NVMWH in Fig 3. Each topic is described by the 
first 8 keywords and the first 5 key frame images related to 
the video shots. These keywords and key frame images 
possess highest condition probability in the latent topics. It is 
shown that the first 6 topics correspond to the scenes of oil 
price, forest fire, flood, bird flu, the Korean nuclear and the 
earthquake. They are a cluster based on the transcript texts 
and images. The last two topics illustrate some interesting 
patterns discovered by NVMWH. At the first sight, these key 
frames of the shots show great differences in visual features. 
79
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-265-3
MMEDIA 2013 : The Fifth International Conferences on Advances in Multimedia

It seems to present several totally different topics for the 
different scenes like helicopter, stadium, meeting, shore and 
meteorological chart. However, by examining the transcript 
texts of the news story units, we find that their semantic 
topics share some common aspects. Several news stories in 
topic 7 all mention the similar or same words, such as 
typhoon, attack and weather which are words related to the 
topic typhoon. Similarly, several news stories in topic 8 also 
mention some related words like Olympic Games, stadium 
and security work. Obviously, NVMWH model discovers 
the last two topics mainly based on the similarity between 
while the 
opics. 
C. 
stogram feature of the key frame image is set as 
72-
use the SVM
 package to learn a support vector machine 
(SVM) on the training data, and predict on the testing data. 
Figur
expressiveness of their representation schemes for the latent 
aspects (i.e., as Gaussian variables rather Dirichlet variables). 
Figur
the key words of the transcript texts in the videos, 
visual features functions a lot in discovering other t
The results and analysis of the Classification 
performance experiment of NVMWH model 
In order to show the predictive power of the low-
dimension latent semantic topic produced by NVMWH, we 
adopt classification, which is the most important task in 
multimedia analysis and application, to evaluate the 
performance of the NVMWH. In the experiment, the 
dimension of the latent semantic topic is set as less than 50 
and its original feature dimension is set as 3182-dimension. 
The color hi
dimension and the edge histogram feature is set as 80-
dimension. 
First, we evaluate the performance of NVMWH on 
classifying testing examples into one of the predefined 
categories, and compare this method with LSI, GM-Mix and 
GM-LDA. For each algorithm, the parameters are estimated 
using all data, ignoring their true class labels. Once the 
models are learned, we use them to project every example 
into a lower-dimensional latent topic space. Then we split the 
data into a training set and a testing set as shown in Table 1, 
Light
e 4.  The classification accuracy of every model at different topic 
dimension 
Fig. 4 shows the classification accuracy of five different 
models at different latent topic feature dimension ranging 
from 5 to 50, and the dimension is the number of the latent 
semantic topics. The Baseline method retains the available 
feature classification results of all the variables in training 
and test. We find that the NVMWH model can always 
achieve higher classification accuracy than the Baseline even 
with a large dimensionality reduction. Under the same topic 
dimension, it also outperforms LSI with a good margin. We 
believe that this may be partially explained by the arguably 
better assumptions adopted by NVMWH on modeling 
text/image features. Surprisingly, GM-Mix produces a 
considerably worse performance than the baseline because 
the modeling power of GM-Mix is too limited to capture 
multiple latent topics for each text/image pair. Too much 
information is eliminated in GM-Mix’s representations 
because the posterior distribution is usually peaked at one 
latent topic. Compared to GM-Mix, GM-LDA offers more 
flexibilities in modeling associated text/images and indeed it 
is (slightly) superior to all other models when latent aspect 
dimension is set to be 10. But it appears that GM-LDA may 
have suffered from overfitting or a low-dimensionality bias 
as its error curve rises significantly in higher-dimensional 
latent space. In contrast, we observe that the performances of 
LSI and NVMWH are relatively stable over a wide range of 
dimensions, which may reflect the robustness and 
e 5.  The classification accuracy of different leaning algorithms in 
the NVMWH model 
Fig. 5 presents the comparison of the performance 
between mean field, Gibbs sampling and contrastive 
divergence. We discover that mean field and the Gibbs 
sampling are similar in performance, while the accuracy of 
contrastive divergence is slightly better than the two. It’s 
mainly because the latter approach uses a fully factorized 
distribution to approximate the true distribution whereas the 
former uses a Monte Carlo approximation. Meanwhile, we 
make a study of the efficiency of the three methods by 
examining the time taken to reach the convergence of the 
learning methods during the training. The results show that 
80
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-265-3
MMEDIA 2013 : The Fifth International Conferences on Advances in Multimedia

the mean filed possess the highest efficiency at 2 minutes, 
the contrastive divergence method ranks second at 10 
minutes and the Gibbs sampling has the lowest efficiency for 
about 50 minutes. Therefore, it is better to adopt the 
contrastive divergence method to conduct the learning an
infe
with
ect. Thus, the classification performance 
woul
opi
he st
pre
deo semantic topics and
app  semantic features to the mining and analysis of 
intelligence in public news videos. 
 
d 
[1
rence of the model parameters for the comprehensive 
consideration of accuracy and efficiency. 
On the same news subject, why does not the 
classification performance increase steadily corresponding to 
the number of the latent topics? That is because the news 
subject is determined by the latent topics with high 
probability distribution, and the latent topics after certain 
dimension cannot help express the content of the subject of 
news. That is to say, the latent topics with low probability 
distribution have few associations, or even no associations, 
[
 the news subj
d not increase with increasing number of latent t
cs. 
[7] D. M. Blei, A. Y. Ng, and M. I. Jordan. “Latent dirichlet 
allocation,” Journal of Machine Learning Research, MIT 
Press, Mar. 2003, pp. 993-1022. 
IV. 
CONCLUSION AND FUTURE WORK 
We make a thorough study of the latent semantic topic 
mining of news video by adopting the multi-wing two-layer 
undirected graphical model. First, we construct a news video 
multi-wing 
Harmoniums 
model 
of 
multi-modal 
heterogeneous features based on the basic Harmoniums 
model, the transcript texts and the key frame images in the 
news video. In this model, the multivariate Gaussian 
variables denoted the latent topics. The condition distribution 
of specific features models on the inputs of various kinds of 
data resources, namely a multiple Poisson distribution is 
used to model the text features and two multivariate 
Gaussian models respectively denote features of color 
histogram and of the edge histogram in the key frame image 
of news video story. The probability distributions are 
determined by all the topics so that a better topic mixing is 
achieved. It expands and improves the previous random field 
model, which is based on two layers by the bidirectional 
dependence relationship between the latent topics and the 
observed input data, whose performance is especially shown 
in the aspects of promoting effective reasoning, robust topic 
mixing and flexible latent topic modeling. The experiments 
of the latent semantic topic extraction and the prediction 
performance based on the NVMWH model prove t
rong 
[15] M. Welling and G. E. Hinton. “A new learning algorithm for 
mean field Boltzmann machines,” Proceedings of the 
International Conference on Artificial Neural Networks, 
London, UK, Springer-Verlag, Aug. 2002, pp. 351-357. 
sentation ability and robustness of NVMWH model on 
latent semantic topic mining in news video stories. 
In the future work, we will adopt more audio-visual 
features like facial feature, voiceprint feature, etc. in 
NVMWH model, and study a more effective learning 
algorithm for model’s parameters, so as to improve the 
mining precision and efficiency of vi
 
[16]
ly
REFERENCES 
] C. Snoek, M. Worring, and A. Smeulders. “Early versus late 
fusion in semantic video analysis,” Proceedings of 13th ACM 
International Conference on Multimedia (MM 2005) in 
Singapore, ACM Press, Nov. 2005, pp. 399-402. 
[2] I. T.Jolliffe. “Principal component analysis,” 2nd ed., 
Springer Press, 2002. 
[3] A. Hyvarinen, J. Karhunen, and E. Oja. “Independent 
component analysis,” New York, USA: wiley, 2001. 
[4] L. Devroye, L. Györfi, and G. Lugosi, “A probabilistic theory 
of pattern recognition,” Springer Press, 1996, pp. 46-47. 
[5] S. C. Deerwester, S. T. Dumais, and T. K. Landauer. 
“Indexing by latent semantic analysis,” Journal of the 
American Society of Information Science, Sep. 1990, 41(6) 
pp. 391-407. 
6] T. Hofmann. “Probabilistic latent semantic analysis,” 
Proceedings of the 15th Conference on Uncertainty in 
Artificial Intelligence, Jul. 1999, pp. 289-296. 
[8] D. M. Blei and M. I. Jordan. “Modeling annotated data,” 
Proceedings of the 26th annual international ACM SIGIR 
conference on Research and decvelopment in information 
retrieval, ACM, Jan. 2003, pp. 127-134. 
[9] P. Smolensky. “Information processing in dynamical system: 
foundations 
of 
harmony 
theory. 
Parallel 
distributed 
processing: explorations in the microstructure of cognition 
foundations”, Cambridge: MIT Press. 1986, pp. 194-281. 
[10] M. Welling, M. Rosen-Zvi, and G. Hinton. “Exponenetial 
family Harmoniums with an application to information 
retrieval,” In Advance Neural Information Processing 
Systems, Dec. 2005, pp. 1481-1488. 
[11] D. Metzler. “Beyond bags of words: effectively modeling 
dependence and features in information retrieval,” SIGIR 
Forum, Dec. 2008, 42(1), pp. 77-77. 
[12] E. Xing, M. Jordan, and S. Russell. “A generalized mean field 
algorithm for variational inference in exponential families,” In 
uncertainty in artificial intelligence (UAI2003). Morgan 
Kaufmann Publishers, Aug. 2003, pp. 583-591. 
[13] W. R. Gilks, S. Ripley, and D. J.Spiegelhalter. “Markov chain 
Monte Carlo in practice,” Cambridge, UK: Chapman & 
Hall/CRC Press, 1996. 
[14] J. R. Smith and S. F. Chang. “Visually searching the web for 
content,” IEEE Multimedia Magazine, Jul.-Sep. 1997, 4(3), 
pp. 12-20. 
 E. Xing, R. Yan, and A. Hauptmann. “Mining associated text 
and images with dual-wing harmoniums,” Proceedings of the 
21st Conference in Uncertainty in Artificial Intelligence, Jul.  
2005, pp. 633-641. 
81
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-265-3
MMEDIA 2013 : The Fifth International Conferences on Advances in Multimedia

