Detecting Users from Website Sessions: A Simulation Study
Corn´e de Ruijt
Faculty of Science
Vrije Universiteit Amsterdam
Amsterdam, the Netherlands
Email: c.a.m.de.ruijt@vu.nl
Sandjai Bhulai
Faculty of Science
Vrije Universiteit Amsterdam
Amsterdam, the Netherlands
Email: s.bhulai@vu.nl
Abstract—In real click data sets, the user initiating a web session
may be censored, as unique users are commonly determined by
cookies. One way to study the effect of this censoring on various
website metrics, and to study the effectiveness of algorithms
trying to undo this censoring, is by simulation. We therefore
propose a click simulation model, which is capable of simulating
user censoring due to cookie churn or the usage of multiple
devices, but for which we still keep the uncensored ground
truth. To recover unique users from session data, we compare
several (H)DBSCAN*-type (Hierarchical Density-based Spatial
Clustering of Applications with Noise) algorithms, where we
assume that all sessions in a cluster likely originate from the
same user. From this comparison, we ﬁnd that even though the
best (H)DBSCAN*-type algorithm does signiﬁcantly outperform
other benchmark clustering methods, it performs considerably
worse than when using the observed cookie clusters. I.e., websites
for which the assumptions of our simulation model hold, our
results suggest that uncovering users from their session data using
clustering algorithms may lead to considerably larger errors in
terms of user related websites metrics, compared to using cookies
to uncover users.
Keywords–Click models; Session clustering; HDBSCAN*
I.
INTRODUCTION
The current Internet environment heavily relies on cookies
for the enhancement of our Internet browsing experience.
These cookies play a crucial role in session management,
the personalization of websites and ads, and user tracking.
However, the usage of multiple devices, multiple browsers,
and the focus on cookie management, have made the problem
of identifying single users over multiple sessions more com-
plex. One study reports that as much as 20% of all Internet
users delete their cookies at least once a week, whereas this
percentage increases to approximately 30% when considering
cookie churn on a monthly basis [1].
Not being able to track Internet users may lead to sub-
optimal behavior of search engines and online ads, as these
have less information about the previous search and click
behavior to infer the user’s preference for certain items. As
cookie churn and the usage of multiple devices cause the user
to be censored, we relate to this by the term user censoring.
The problem of unrevealing which session(s) originate(s)
from which user has been considered in previous literature,
which to our knowledge all have been using real-world data
sets to test their algorithms on. Although these real-world
data sets have the advantage of capturing much of the com-
plexity of users’ click decisions, they also have two clear
disadvantages. First, as users are only identiﬁed by cookies,
we can conclude that two sessions having the same cookie
originate from the same user. However, the opposite is not
always true: two sessions having different cookies are not
per deﬁnition different users. The user’s cookie might have
churned, or the user might have initiated a session from a
different device, creating another cookie. Hence, the ground
truth is only partially observed in these data sets. Second,
using real-world data sets limits the possibility of studying
the sensitivity of an algorithm on the underlying click data
set: most real-world data sets come from large search engines,
which may not always be representative for all websites.
Therefore, we propose a click simulation model, and use
realizations of this model to study the effectiveness of several
(H)DBSCAN*-type clustering algorithms on uncovering users
from their sessions. To measure the effectiveness of these
algorithms, we not only consider the error in terms of typical
supervised clustering error measures such the adjusted Rand
index, but also in terms of the error in estimating overall web
statistics, such as the number of unique users, distribution
of the number of sessions per user, and the user conversion
distribution.
To avoid making the simulation model overly complex, we
decided to model user interactions with a search engine. This
has two main advantages: ﬁrst, there exists quite extensive
literature on what type of parametric models are accurate
for modeling user behavior on search engines [2]. Second,
apart from dedicated search engines, a search tool is also
a common feature on websites serving other purposes [3].
Another measure against complexity is that we assume all
users push homogeneous queries, that is, although we allow
users to have different preferences for items returned by the
search engine, we do assume all users push the same query.
This paper has the following structure. Section II discusses
relevant literature related to session clustering. Section III
discusses the simulation model, adaptions of (H)DBSCAN*,
and the experimental setup. Section IV discusses the results,
whereas Section V discusses the implications of these results
and ideas for further research.
II.
RELATED WORK
Simulating click behavior is not a new concept: Chucklin et
al. [2][pp. 75-77] suggest using pre-ﬁtted click models for this
purpose, where the model is pre-ﬁtted to public click data sets.
One risk of using pre-ﬁtted models is an availability bias: can
the characteristics of public click data sets, commonly provided
by large search engines, easily be generalized over all search
35
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-816-7
DATA ANALYTICS 2020 : The Ninth International Conference on Data Analytics

engines? Also, these data sets do not always provide the type
of information one is interested in, such as the device used to
initiate a session.
Fleder and Hosanagar [4] provide a generative approach
for modeling user preferences, which we will discuss in more
depth in Section III-A. This model can be used as an alternative
to model user preferences. Pre-ﬁtted and generative models
do have a trade-off in terms of accuracy vs interpretability.
I.e., pre-ﬁtted models may have an accurate estimate of user
item preferences, but provide little understanding in why this
preference over different items has a certain shape, whereas
generative models do explain why a user has a certain item
preference, but these models might be less accurate.
Several authors have studied how cookie censoring occurs.
For example, [1][5][6] consider cookie censoring due to cookie
churn, whereas [7] considers cookie censoring due to using
multiple devices. Results from these studies can be used to
model cookie churn dynamics in a simulation model.
Identifying unique users from sessions can be seen as
a speciﬁc case of the entity/identity resolution problem [8].
Though what makes this problem special is the nature of the
data set. This typically consists of a large number of sessions,
for which clicks and web page meta-data (such as the URL)
are logged. Because of these characteristics, entity resolution
algorithms that do not account for these characteristics are
likely to fail in their objective. Karakaya et al. [9] give a
survey of the literature on cross-device matching, i.e., where
it is assumed that user censoring occurs as users use multiple
devices. However, many of the approaches listed can also be
applied to more general settings.
The problem of uncovering users from their sessions
obtained considerable scientiﬁc attention following the 2015
ICDM and 2016 CIKM machine learning challenges [9]–[11].
Interestingly, although at a ﬁrst glance much of the literature
seems to relate to the same problem, the context of the data,
and how the problem is interpreted seems to vary greatly. The
2015 ICDM and 2016 CIKM challenges consider the problem
from the perspective of an online advertiser, where data is
gathered from multiple websites. Others consider the problem
from a single website perspective [1][8][12]. Although the
underlying problem may be the same from both perspectives,
the solution may not. E.g., since in the advertisement case the
data set contains a variety of URLs from different websites,
these solutions rely more on natural language processing
techniques than in the single website case.
There also seems to be ambiguity in whether the solution
should allow for overlapping session clusters. Most commonly
(like in the 2015 ICDM and 2016 CIKM challenges), the
problem is modeled as a binary classiﬁcation problem, pre-
dicting whether pairs of sessions originate from the same user
[9]. As a result, this interpretation of the problem does allow
for overlapping session clusters. Other approaches restrict
themselves to non-overlapping clusters, but do however have
other disadvantages, like computational feasibility for large
data sets [13], or additional assumptions about user behavior
[1].
III.
METHODS
A. Simulating click data with cookie-churn
We will describe the simulation model in three parts. The
ﬁrst part models how users navigate through a single Search
Engine Result Page (SERP), for which we use a click model.
The second part models how user preferences over different
items are determined, while the third part models how a
session’s underlying user is censored due to cookie churn or
the usage of multiple devices.
To describe the simulation model, the following notation
will be used. Let i ∈ {1, . . . , n} be a query-session, which
produces a SERP of unique items Li
⊆ V, with V
=
{1, . . . , V } the set of all items, indexed by v. A (query-)session
consists of a sequence of interactions with a single SERP, and
these interactions are completely deﬁned by the click model.
Let ui ∈ U denote the user initiating query-session i, with
U = {1, . . . , U} the set of all users. The user index u is used
instead of ui in case the precise query-session i is irrelevant.
1) Simulating SERP interactions: To simulate clicks on a
search engine, we employ the Simpliﬁed Dynamic Bayesian
Network model (SDBN) [14]. The main reason for choosing
SDBN is that, though the model is simple, it seems to perform
reasonably well in comparison to other parametric click models
when predicting clicks [2]. The two main variables in this
model are, for all u ∈ U, v ∈ V, the probability of attraction
φ(A)
u,v (probability of user u clicking item v, given that v is
evaluated), and the probability of satisfaction φ(S)
u,v (probability
of user u evaluating an item at position l+1 in the SERP, given
that item v at position l was just clicked). SDBN assumes that
the ﬁrst item in a SERP is always evaluated.
2) User item preferences: To come up with reasonable
values for φ(A)
u,v and φ(S)
u,v, we use the same approach as in
[4], which we will refer to as Fleder-Hosanagar’s model. That
is, each user u ∈ U and each item v ∈ V is represented
by the vectors ηu =

η(u)
1 , η(u)
2

, and ψv =

ψ(v)
1 , ψ(v)
2

respectively, where both are drawn i.i.d. from a standard
bivariate normal distribution. The probabilities φ(A)
u,v , and φ(S)
u,v
are then determined by the multinomial logit:
φ(X)
u,v =
eωu,v+ν(X)
P
v′∈V\{v} eωu,v′ + eωu,v+ν(X) ,
(1)
with ωu,v = −q log δ(ηu, ψv), and X ∈ {A, S}. Here δ is
some distance function, in our case Euclidean distance. q ∈ R+
is some constant value that models the user preference towards
nearby products, and ν(A), ν(S) are salience parameters for
attraction and satisfaction, respectively.
3) User censoring: User censoring is incorporated in the
simulation model in two ways: by letting cookies churn after
some random time T, and by switching from device d to some
other device d
′. First, we consider the cookie lifetime T cookie
u,o,d
for the o-th cookie of user u on device d, and the user lifetime
T user
u
. Whenever the cookie lifetime of cookie o ends, but the
current user lifetime is strictly smaller than T user
u
, a new cookie
o
′ is created, for which the lifetime is drawn from the cookie
lifetime distribution F cookie. For a period of T cookie
u,o′,d, all click
behavior of user u on device d will now be registered under
cookie o
′.
Second, after each query-session, a user may switch from
device d to d
′, which happens according to transition matrix
P. Whenever a user switches devices, we consider whether
the user has used this device before. If not, a new cookie o
′
is created, and we draw a new cookie lifetime from F cookie.
36
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-816-7
DATA ANALYTICS 2020 : The Ninth International Conference on Data Analytics

However, the cookie lifetime T cookie
u,o,d does not end prematurely
when the user switches from device d to d
′. If later on the user
switches back to device d while the cookie lifetime T cookie
u,o,d has
not ended, the behavior of user u is again tracked via cookie
o until another device switch occurs or cookie o churns.
Putting this censoring into practice requires us to provide
ﬁve distributions: 1) a distribution F abs for the time between
query-sessions, which following [6] we will refer to as the
absence time, 2) a distribution for the cookie lifetime (F cookie),
3) a distribution for the user lifetime (F user), 4) the device
transition matrix P, and 5) the initial device probability π.
All distributions were adopted from previous literature, which
is summarized in Table I.
TABLE I. DISTRIBUTIONS REGULATING COOKIE CHURN.
Variable
Description
Distribution
T cookie
u,o,d
Cookie lifetime o-th
Hyper-exponential [1]
cookie of user u on device d
T abs
u,i
Time between sessions i and
Pareto-I, ﬁtted using data from [6]
i + 1 of user u
T user
u
Lifetime of user u
Sum of Nu hyper-exponentials [1],
Nu ∼ geom(ρ)
P , π
Device transition matrix and
From [7], removing the game console
initial transition probabilities
device from the state space
4) Summary of the simulation procedure: By combining
the three simulation modules, we obtain the full simulation
procedure. In short, we ﬁrst simulate attraction and satisfac-
tion parameters φ(A)
u,v , φ(S)
u,v for all (u, v) pairs using Fleder-
Hosanagar’s model. Second, we simulate clicks for a set of
Uwarm-up users (where Uwarm-up ∩ U = ∅) using SDBN, where
the item order for each query-session is determined uniformly
at random. Third, we again run SDBN, now incorporating user
censoring, over the set U. The item order in each query-session
is now draw i.i.d. from a multinomial distribution (without
replacement), where the probabilities are proportional to the
overall item popularity found during the warm-up phase. The
simulation’s source code is available via Github [15].
Although so far we assumed all users arrive at t = 0, we
shift all times after the simulation to obtain click behavior
spread out over time. Here, we assume a Poisson arrival
process with rate γ. I.e., the ﬁrst query-session of user u starts
some exponentially distributed time after the initial query-
session of user u − 1. Note that these inter-ﬁrst session times
only depend on the time of the ﬁrst session of the previous
user, not on any other subsequent behavior of that user.
B. Session clustering
1) Introducing maximum cluster sizes to HDBSCAN* and
DBSCAN*: Due to space limitations, we will refer to [16][17]
and [18] for details on how the DBSCAN* and HDBSCAN*
algorithms work. What we will use, is that both algorithms
initially represent the data in a dendogram. The dendogram is
obtained by computing a Minimum Spanning Tree (MST) over
the complete weighted graph of pairwise distances between
data points (in our case sessions), which causes a consider-
able speed improvement compared to many other hierarchical
clustering methods.
As the data is represented in a dendogram, all data points
are at the leaves of this binary tree. This tree has the property
that if the shortest path between two leave nodes has to use
an edge closer to the root node, then these two data points
are further apart. This implies that, if we perform a horizontal
cut on the tree, this cut relates to some maximum distance
ϵ, and all branches below this cut share the property that all
data points connected in this branch are at most ϵ apart. Leaf
nodes above the ϵ-cut are labeled as noise, and obtain their
own cluster.
To impose a maximum cluster size, we employ three strate-
gies, which we name MS-DBSCAN*, MS-HDBSCAN*−
and MS-HDBSCAN*+ (the abbreviation ‘MS’ stands for
‘Maximum Size’). In MS-DBSCAN*, we ﬁrst make the ϵ-cut
to obtain branches T = {τ1, . . . , τm}. Next, if some branch
τj contains more than β data points, we split the branch again
at the root node: considering the branches at the left and right
child of the root of τj as potential clusters. This splitting is
continued until all branches have fewer than β data points,
after which we assign all data points in one branch to the
same cluster.
Both MS-HDBSCAN*− and MS-HDBSCAN*+ ﬁrst
perform HDBSCAN* using the relative excess of mass to
split the dendogram into different branches. Next, we apply
the same strategy as in MS-DBSCAN*, where we continue
splitting the branches until all branches have fewer than β
data points. As a last step, we rerun HDBSCAN* separately
on the individual branches, which may again split a branch
into smaller branches if that improves the relative excess of
mass over all resulting clusters. The only difference between
MS-HDBSCAN*− and MS-HDBSCAN*+ occurs when
some found branch (in other words, cluster) does not make
any splits for which the left and right child have at least M
data points, for some given M ∈ N. In MS-HDBSCAN*+,
we consider all points in this branch to be contained in the
same cluster, whereas MS-HDBSCAN*− assumes all data
points in this branch are noise points.
2) Session cluster re-evaluation: As one might have no-
ticed, insofar we have not used any information from the cook-
ies. I.e., knowing which sessions have the same cookie could
provide valuable information about the underlying user. In
particular, we wish to train a model that can function as an al-
ternative to the standard distance measure δ in (H)DBSCAN*,
such as Euclidean or Manhattan distance, which we then again
can plug into the adapted (H)DBSCAN* algorithms.
Obtaining session clusters with re-evaluation is done as
follows. Assume we have a trained classiﬁer ˆf(Xi, Xi′ ), which
returns the probability of sessions Xi and Xi′ originating from
the same user. First, like in [19], we ﬁnd for each point Xi
the K nearest neighbors, which gives us a set X of all nearest
neighbor session pairs. Second, we compute − log( ˆf(Xi, Xi′ ))
for all (Xi, Xi′ ) ∈ X, and ﬁll this into a (sparse) n × n
distance matrix W. For all pairs (Xi, Xi′ ) /∈ X, we assume
the distance is some large value δmax, which allows us to store
W efﬁciently, and greatly speeds up computations compared to
evaluating all pairwise same user probabilities. Distance matrix
W can subsequently be used as distance measure δ in the
algorithms discussed in Section III-B to obtain new session
clusters.
We use a logistic regression model for ˆf, which we train by
undersampling from a set Xclust∪Xcookie, where Xcookie contains
all pairwise sessions sharing the same cookie cluster, and Xclust
is a set of all pairwise sessions sharing the same computed
37
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-816-7
DATA ANALYTICS 2020 : The Ninth International Conference on Data Analytics

cluster. These computed clusters are obtained by running a
(H)DBSCAN*-type algorithm using Euclidean distances.
3) DBSCAN* with random clusters: To benchmark the
clustering approaches just discussed, we consider the following
benchmark. We ﬁrst cluster the sessions using the ordinary
DBSCAN* algorithm, in which way we obtain initial clusters
B0
1, . . . , B0
m. Next, for each cluster Bh
j (h ∈ N0, with initially
h = 0), if |Bh
j | > β, we iteratively select min{sj,h, |Bh
j |, β}
points uniformly at random from Bj to form a new cluster
eB, and update Bh+1
j
← Bh
j \ eB. Here, sj,h, j = 1, . . . , m;
h = 0, 1, . . .; are drawn from a geometric distribution with
p = 0.5. This process continues until for all j ∈ {1, . . . , m}:
|Bh
j | ≤ β for some h, at which the remaining points in Bh
j are
labeled as one cluster.
Intuitively, we select this benchmark as it captures the
higher-level hierarchy clustering of DBSCAN*, but not the
low-level clusters (as these clusters are picked at random).
Therefore, comparing the previous methods with this random
clustering approach allows us to assess whether the smaller
size clusters reveal more information than the larger ones.
C. Experimental setup
1) Simulation parameters: Our experimental design con-
sists of two steps. First, we consider a simulation base case
on which we evaluate the clustering approaches discussed in
Section III-B. In this base case, the users’ ﬁrst query arrival
follows a Poisson process with rate γ = 0.2 (minutes), after
which subsequent behavior over time of a particular user is
modeled according to F abs, F cookie, F user, F device, P, and π,
of which the parameters were already given in Section III-A3.
We use U = 20, 000 users and Uwarm-up = 2, 000 warm-up
users.
Furthermore, we remove the ﬁrst 250 sessions (not part
of the ﬁrst Uwarm-up users, who are only used to estimate the
overall item popularity), as these are likely to all be ﬁrst
sessions from newly arriving users, and therefore including
them may lead to a bias in the data. Likewise, we remove
all observations after 43, 200 minutes (30 days) to avoid the
opposite bias: not having any new users. Users could pick from
V = 100 items, and we choose as maximum list size L = 10.
For parameters that could not be adopted from the litera-
ture, we tried several parameter values. We ﬁnd that q = 1
(user preference for nearby products), ρ = 0.5 (geometric
parameter for the number of user lifetime phases Nu), and
salience parameters ν(A) = ν(S) = 5 are reasonable for our
base case. In the second step of the experimental design, we
make adjustments to the latter parameters, that is, those not
adapted from the literature.
2) Features and MS-(H)DBSCAN* hyper-parameter set-
tings: The simulated data set is split into a training and
test set according to a 70/30 split over the users. For each
session, we use the session’s start time, observed session
count (as observed by the cookie), number of clicks, and
whether the session’s SERP has at least one click as features.
Furthermore, to obtain a vector representation of the items
and interactions with the SERP, we ﬁrst compute a bin-count
table. This table contains per item the total number of clicks,
skips (no click), and the log-odds ratio between clicks and
skips over 30 percent of all training sessions, which combined
are used as item vector representations. For each SERP, we
subsequently concatenate the item vector representations based
on their position in the SERP, and multiply this vector with the
vector of clicks, vector of skips (=no click), and a hot vector of
the last clicked item. The resulting three vectors are, together
with the features mentioned earlier, concatenated to obtain the
ﬁnal session vector.
For each method, we experiment with (H)DBSCAN’s
k-NN parameter, for which we tried k
∈ {1, 3, 5}. For
DBSCAN*-like algorithms, we try
ϵ ∈
n
qmax (qmin/qmax)ℓ/N ℓ ∈ {1, . . . , N}
o
,
(2)
with N = 9 and qmin, qmax the minimum and maximum
Euclidean distance between all session pairs of 1,000 sam-
pled sessions. For HDBSCAN*-type algorithms, we set the
minimum cluster size to M = 2. To train classiﬁer ˆf, we ﬁrst
run MS-DBSCAN* with the best found values for k and ϵ
from earlier validation of MS-DBSCAN* on the training set
to, together with the cookie clusters, obtain Xtrain. Next, we
compute the Manhattan and Euclidean distances, and inﬁnity
norm between the session vectors of each pair (i, i
′) ∈ Xtrain,
which are used as feature vector to train a logistic regression
model. We select for each point the K = 1, 000 nearest
neighbors to evaluate the classiﬁer ˆf on. All non-evaluated
pairs receive distance δmax = − log

the different hyper-parameters tried for that method under that
data set. I.e., in theory the hyper-parameters might be slightly
different between training and test, though in practice we found
this was rarely the case.
TABLE II. RESULTS ON THE BASE CASE.
Data set
ARI
KL-div.
KL-div.
APE unique
New user
Model
session count
conversion
user
accuracy
MS-DBSCAN*
train
0.0012
0.55
0.13
15
0.56
MS-DBSCAN*p
train
0.14
0.74
0.092
77
0.5
DBSCAN*-RAND
train
0.0002
1
0.096
0.011
0.42
MS-HDBSCAN*+
train
0.0007
0.75
0.15
10
0.52
MS-HDBSCAN*−
train
0.0007
0.75
0.15
10
0.52
MS-HDBSCAN*+
p
train
0.092
0.9
0.11
0.011
0.46
MS-HDBSCAN*−
p
train
0.1
0.9
0.11
0.011
0.46
OBS
train
0.91
0.017
0.0032
15
0.95
MS-DBSCAN*
test
0.0022
0.11
0.0026
60
0.56
MS-DBSCAN*p
test
0.0015
1.4
0.13
6.8
0.4
DBSCAN*-RAND
test
0.0004
0.32
0.015
40
0.5
MS-HDBSCAN*+
test
0.002
0.16
0.0042
53
0.55
MS-HDBSCAN*−
test
0.002
0.16
0.0042
53
0.55
MS-HDBSCAN*+
p
test
0.0015
1.4
0.13
7.2
0.4
MS-HDBSCAN*−
p
test
0.0015
1.4
0.13
7.2
0.4
OBS
test
0.91
0.1
0.0076
51
0.95
The OBS model in the table are the scores one would obtain
if the observed cookies would be used as clusters. Models
using the classiﬁer as distance measure are indicated using
subscript p. What immediately becomes apparent is that, com-
pared to these observed cookie clusters, all methods perform
considerably worse. Hence, in the scenario we consider: a
single query where the true location ηu is only revealed by
clicked and skipped item locations, our approaches do not
come near what one would obtain if one would simply take
the observed cookies.
However, the scores do reveal some interesting patterns.
First, approaches using a probabilistic distance measure seem
to overﬁt the data: they perform relatively well (compared to
the other approaches) on various measures on the training
set, but on the test set these results are mitigated: here
MS-DBSCAN* seems to work best when considering mul-
tiple error measures. Looking at the results from different
hyper-parameter settings for MS-DBSCAN*, we observe that
selecting k = 1 performed best. Furthermore, due to our
maximum size constraint, the clusters did not alter for ℓ ≥ 4
(corresponding to ϵ ≥ 6.33).
Furthermore, methods without a probabilistic distance mea-
sure do outperform the DBSCAN*-RAND method on most
measures. I.e., they perform better at picking sessions origi-
nating from the same user given high-level clusters, than if
we would pick session pairs at random from these high-level
clusters. Although it is difﬁcult to draw a ﬁrm conclusion, these
ﬁndings might be an indication that the same user signal we try
to infer from the click data is somewhat weak: if our methods
would not pick up a signal at all, we would expect them to
have the same result as the DBSCAN*-RAND method.
B. Results on multiple simulation scenarios
In order to judge the sensitivity of our ﬁndings on the
parameter settings of the simulation model, we permute the
simulation settings to see if this alters our results. As re-
running all models on all simulation settings would be com-
putationally rather expensive, we only re-evaluate the best
performing models on the simulation cases. Since in our
base case we found that the parameters k = 1, and ϵ =

qmax (qmin/qmax)2/3
work reasonably well, these param-
eters are used for MS-DBSCAN* and DBSCAN*-RAND.
The maximum cluster size remains the same as in the base
case.
Fig. 1 shows how the models perform over the different
simulation settings in terms of ARI, which is our main
response variable. The ﬁgure suggests that all cluster models
do stochastically dominate DBSCAN*-RAND. Furthermore,
MS-DBSCAN* seems to outperform the other clustering
methods in terms of ARI. As assumptions like homogene-
ity of variance or normality do not hold in this case, we
used a Kruskall-Wallis test, which rejects in this case that
all median ARI scores over the different methods are the
same for any reasonable signiﬁcance level (e.g., α = .01,
p < 10−4). Pairwise one-sided Wilcoxon signed rank tests
between MS-DBSCAN* and all other methods also indicate
that MS-DBSCAN* performs signiﬁcantly better than the
other methods (all p-values are smaller than 10−4).
DBSCAN-RAND
MS-DBSCAN*
MS-HDBSCAN*−
MS-HDBSCAN*+
0.000
0.001
0.002
0.003
ARI
Model
Figure 1. Scores over all simulations.
Considering the variance in the ARI scores, we noticed that
strengthening the signal, that is, increasing click probabilities,
leads to some improvement in ARI. The most obvious way to
do so is by decreasing the number of items (which, as we use
bin-counting, ensures each item has sufﬁcient data for bin-
counting). However, these improvements remain small. Also
interesting is that, when correlating the different error scores
over all simulation cases, ARI seems to be weakly correlated
with most other error measures, with the sign being in the
desired direction (i.e., decrease in KL-divergence for both
session count and conversion, but an increase in the new user
accuracy). However, improved APE for the number of unique
users seems to lead to worse performance in terms of ARI and
new user accuracy (Pearson correlations 0.38, and 0.95 resp.).
V.
CONCLUSION AND DISCUSSION
In this paper, we presented a homogeneous query click
simulation model, and illustrated its usage to the problem
of uncovering users from their web sessions. The simulation
model is composed of several parametric models, of which
previous literature suggests that these models work well in
explaining typical patterns observed in click data, while re-
maining relatively simple. Such patterns include the position
bias, cookie censoring, and user preference over multiple
products.
39
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-816-7
DATA ANALYTICS 2020 : The Ninth International Conference on Data Analytics

Furthermore, we illustrated the simulation model on the
problem of (partially observed) session clustering, that is,
identifying unique users from their query-sessions. To solve
this problem, we tested several mutations of (H)DBSCAN*,
where these mutations differ from HDBSCAN* or DBSCAN*
as they allow for incorporating a maximum cluster size. From
comparing these (H)DBSCAN*-type algorithms, we found that
the accuracy of using cookies largely outperform that of not or
partially using cookie data. This considerable difference seems
to be due to two reasons. 1) The simulated censored cookies
turned out to be rather accurate, implying that, assuming the
parameters used for cookie censoring adapted from previous
literature are accurate, censoring in cookie data does not
impose that much of a problem in accurately measuring the
metrics studied in this paper. 2) As we only considered a ho-
mogeneous query, the user preferences are only revealed from
the items users clicked, a signal the various (H)DBSCAN*-
type algorithms found difﬁcult to detect. Strengthening this
signal, e.g., by increasing the number of clicks, led to a small
improvement in ARI.
Other interesting observations include the difference be-
tween using Euclidean distance and a probability distance
measure in the (H)DBSCAN*-type algorithms, the latter being
obtained from training a classiﬁer on detecting whether session
pairs originate from the same user. The results suggest that
the probabilistic classiﬁer tends to overﬁt. Also interesting is
that, when considering the correlations between the various
error metrics considered in this paper, we observed that some
error measures show contradictory correlations. In particular,
the positive correlation between cluster ARI and average per-
centage error in the number of unique users (.38), and between
the accuracy in estimating whether the next session originates
from a new user and the new user average percentage error
(.95), indicate that optimizing for one of these error measures
may lead to decreased performance in the other.
Although our ﬁndings suggest that the practicality of
session clustering from single query click data is limited,
the usage of the simulation model did allow for studying
the sensitivity of the clustering algorithms on different click
behavior, something that would not easily have been possible
with real click data. It also allowed us to study the effects of
user censoring caused by cookie churn or the usage of multiple
devices, which showed that if we adopt models of cookie churn
behavior found in the literature, this censoring only has a small
negative effect on the accuracy of the website metrics discussed
in this paper, with an exception for estimating the number of
unique users.
Given our ﬁndings, a number of questions remain. First, it
would be interesting to extend the simulation model to allow
for multiple queries. As the solutions to the (multi-query)
CIKM 2016 and ICDM 2015 cross-device matching compe-
titions were quite successful, a logical hypothesis would be
that incorporating multiple queries into the simulation model
would improve the results obtained from (H)DBSCAN*-type
algorithms. Second, in this study, we only used a logistic re-
gression model to approximate the probability of two sessions
originating from the same user. Given the limited success of
this approach so far, it would be interesting to consider other
approaches. As the limited results seem to be due to overﬁt-
ting, including regularization or using bootstrap aggregation
approaches could lead to better results. Third, there is still
limited knowledge on how cookie censoring occurs. Currently,
multiple models exist in the literature, but most models only
consider a speciﬁc type of censoring, from which one cannot
infer how these different types of censoring interact.
REFERENCES
[1]
A. Dasgupta, M. Gurevich, L. Zhang, B. Tseng, and A. O. Thomas,
“Overcoming browser cookie churn with clustering,” in Proceedings of
the ﬁfth ACM international conference on Web search and data mining.
ACM, 2012, pp. 83–92.
[2]
A. Chuklin, I. Markov, and M. d. Rijke, Click models for web search.
Morgan & Claypool Publishers, 2015.
[3]
C. Luna-Nevarez and M. R. Hyman, “Common practices in destination
website design,” Journal of destination marketing & management,
vol. 1, no. 1-2, 2012, pp. 94–106.
[4]
D. Fleder and K. Hosanagar, “Blockbuster culture’s next rise or fall:
The impact of recommender systems on sales diversity,” Management
science, vol. 55, no. 5, 2009, pp. 697–712.
[5]
D. Coey and M. Bailey, “People and cookies: Imperfect treatment
assignment in online experiments,” in Proceedings of the 25th Interna-
tional Conference on World Wide Web.
ACM, 2016, pp. 1103–1111.
[6]
G. Dupret and M. Lalmas, “Absence time and user engagement: evalu-
ating ranking functions,” in Proceedings of the sixth ACM international
conference on Web search and data mining. ACM, 2013, pp. 173–182.
[7]
G. D. Montanez, R. W. White, and X. Huang, “Cross-device search,” in
Proceedings of the 23rd ACM International Conference on Information
and Knowledge Management.
ACM, 2014, pp. 1669–1678.
[8]
D. Jin, M. Heimann, R. Rossi, and D. Koutra, “node2bits: Compact
time-and attribute-aware node representations,” in ECML/PKDD Euro-
pean Conference on Principles and Practice of Knowledge Discovery
in Databases, Proceedings, Part 1, 2019, pp. 483–506.
[9]
C. Karakaya, H. To˘guc¸, R. S. Kuzu, and A. H. B¨uy¨ukl¨u, “Survey
of cross device matching approaches with a case study on a novel
database,” in 2018 3rd International Conference on Computer Science
and Engineering (UBMK), 2018, pp. 139–144.
[10]
ICDM,
ICDM
2015:
Drawbridge
Cross-Device
Connections,
2015,
https://www.kaggle.com/c/icdm-2015-drawbridge-cross-device-
connections, retrieved: August, 2020.
[11]
CIKM, CIKM Cup 2016 Track 1: Cross-Device Entity Linking Chal-
lenge, 2016, https://competitions.codalab.org/competitions/11171, re-
trieved: August, 2020.
[12]
S. Kim, N. Kini, J. Pujara, E. Koh, and L. Getoor, “Probabilistic
visitor stitching on cross-device web logs,” in Proceedings of the 26th
International Conference on World Wide Web.
ACM, 2017, pp. 1581–
1589.
[13]
F. M. Naini, J. Unnikrishnan, P. Thiran, and M. Vetterli, “Where you
are is who you are: User identiﬁcation by matching statistics,” IEEE
Transactions on Information Forensics and Security, vol. 11, no. 2, 2016,
pp. 358–372.
[14]
O. Chapelle and Y. Zhang, “A dynamic bayesian network click model
for web search ranking,” in Proceedings of the 18th international
conference on World wide web.
ACM, 2009, pp. 1–10.
[15]
C.
de
Ruijt
and
S.
Bhulai,
SDBNSimulator,
2020,
https://github.com/cornederuijtnw/SDBNSimulator, retrieved: August,
2020.
[16]
R. J. Campello, D. Moulavi, and J. Sander, “Density-based clustering
based on hierarchical density estimates,” in Paciﬁc-Asia Conference on
Knowledge Discovery and Data Mining.
Springer, 2013, pp. 160–172.
[17]
R. J. Campello, D. Moulavi, A. Zimek, and J. Sander, “Hierarchical
density estimates for data clustering, visualization, and outlier detec-
tion,” ACM Transactions on Knowledge Discovery from Data (TKDD),
vol. 10, no. 1, 2015, pp. 1–51.
[18]
L. McInnes and J. Healy, “Accelerated hierarchical density clustering,”
arXiv preprint arXiv:1705.07321, 2017.
[19]
M. C. Phan, Y. Tay, and T.-A. N. Pham, “Cross device matching for
online advertising with neural feature ensembles: First place solution at
CIKM cup 2016,” arXiv preprint arXiv:1610.07119, 2016.
[20]
L. Hubert and P. Arabie, “Comparing partitions,” Journal of classiﬁca-
tion, vol. 2, no. 1, 1985, pp. 193–218.
40
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-816-7
DATA ANALYTICS 2020 : The Ninth International Conference on Data Analytics

