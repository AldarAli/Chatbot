166
International Journal on Advances in Software, vol 12 no 1 & 2, year 2019, http://www.iariajournals.org/software/
2019, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Facilitating a Statewide GIS Metadata Standard through Training, Outreach  and Programmatic 
Metadata Evaluation
Timothy Mulrooney 
Dept. of Environmental, Earth and Geospatial Sciences 
North Carolina Central University 
Durham, NC, USA 
e-mail: tmulroon@nccu.edu 
 
Abstract— Under the supervision of the North Carolina Geo-
graphic Information Coordinating Council and Statewide 
Mapping Advisory Committee, a committee defined and devel-
oped a State and Local Government Metadata profile intended 
for use in North Carolina.   This profile is based on the Inter-
national Organization for Standardization 191** standards.  
In addition to dictating best practices and conventions for ex-
isting metadata entries such as the Title, Publication Date and 
Use Constraints, this standard accounts for evolving technolo-
gies that did not exist when original metadata standards were 
first developed.  While the rate at which geoinformation is 
created has exponentially increased, the time dedicated to cata-
loging and subsequently assessing and evaluating this metadata 
information remains nearly the same.  In addition to educating 
the North Carolina Geographic Information Systems commu-
nity on the use and application of metadata, as well as this new 
standard, the research team is developing tools so GIS manag-
ers can gauge standard compliance more efficiently and proac-
tively than in the past.   In this paper, the research team has 
been using programming methods in which metadata entries 
from multiple layers in large geospatial databases can be as-
sessed and evaluated.  These methods were tested using various 
quantitative methods, including the Technology Acceptance 
Model.  This can provide insight into the various accuracies 
(horizontal, vertical, temporal, etc.) of layers which in turn can 
dictate future efforts.  It can also be used to identify inconsist-
encies in metadata entries with an end goal of understanding 
misinterpretation and misunderstanding of the profile so it can 
be improved in future incarnations.              
 
Keywords-GIS Metadata; Metadata; Metadata Profile; North 
Carolina State and Local Government Profile. 
I. INTRODUCTION 
A Geographic Information System (GIS) serves as the 
tangible and intangible means by which information about 
spatially related phenomena can be created, stored, analyzed 
and rendered in the digital environment [1].  Experts in 
many dissimilar fields have seen the utility of GIS as a 
means of quantifying and expanding their research. GIS is 
used in disciplines such as business, sociology, justice stud-
ies, surveying and the environmental sciences.  In the North 
Carolina GIS community, GIS is used to represent transpor-
tation routes, elevation, delineate land ownership parcels, 
school attendance, highlight patterns of crime and help 
make zoning decisions.  The manner in which geospatial 
data is captured varies.  Some methods include using a 
Global Positioning System (GPS) unit, extracting or im-
proving existing GIS data, downloading data from a web 
site, connecting to a service, the use of an Unmanned Aerial 
Vehicle (UAV) or some other remote sensing platform, or 
creating data from an analog format via digitization or 
georectification.   Regardless of the method, the resources 
(e.g., the computers, time and people dedicated to the pro-
cess of collecting and creating geospatial data) are the most 
time-consuming portion of a GIS-related project [2].  As a 
result, the GIS community needs to ensure the quality of 
geospatial data created from these methods is captured, 
stored and assessed in a systematic way.   
Geospatial metadata serves as the formal framework to 
catalog descriptive, administrative and structural infor-
mation about geospatial data.  Geospatial metadata is inher-
ently different from other forms of electronic metadata be-
cause each metadata file can be applied a spatial component 
that is not implicit with other forms of metadata.  These 
spatial components encompass a wide array of information 
to include the date, methods and sources by which geospa-
tial information was captured, means to ensure that the geo-
spatial information adheres to acceptable standards and/or 
aligns with other geospatial datasets to ensure seamless 
analysis, projection information of the dataset and bounding 
coordinates of the dataset.  All of these entries, in addition 
to the data’s non-spatial components can be queried within 
the confines of geospatial data portal such as one found at 
North Carolina OneMap, the geospatial data portal for the 
state of North Carolina [3].       
  Given the capricious rate at which all forms of geo-
information can be created, formal metadata (i.e. metadata 
stored using a widely-recognized and agreed-upon format) 
serves as a lifeline between the tacit knowledge of the data 
creator and current and future generations of geospatial data 
consumers.  In the United States, the Federal Geographic 
Data Committee (FGDC) metadata standard, commonly 
referred to as the Content Standard for Digital Geospatial 
Metadata (CSDGM) allows for more than 400 individual 
metadata elements.  The North Carolina GIS community has 
been proactive about understanding the importance of 
metadata.    
Maintaining a complete and comprehensive metadata 
record is a continual and interactive process.  GIS metadata 
is one of the most overlooked and underappreciated aspects 
of any GIS enterprise or project.  If time or resources need 
to be sacrificed in the course of a project, it is usually at the 

167
International Journal on Advances in Software, vol 12 no 1 & 2, year 2019, http://www.iariajournals.org/software/
2019, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
expense of metadata.  Information is key to an organiza-
tion’s vitality, sustainability and success.  Metadata should 
be treated as an investment.  Maps and analysis are only as 
good as the data on which they are based.  Metadata is a 
direct reflection of this investment and the organization 
which makes this investment.  Metadata captures important 
information related to data creators, data quality and the 
various accuracies (horizontal, vertical, temporal, attribute, 
semantic, etc.) with which we can quantitatively measure 
GIS data.  These measurements help guide the decision- 
making process, especially in larger (hundreds of layers) 
spatial databases.  Not only is good metadata a wise busi-
ness practice, but saves time, money and resources in the 
long run.  Unfortunately, metadata’s true value is not real-
ized until it is absent, and few studies have been done to 
place a direct monetary value on metadata.    
Under the supervision of the North Carolina Geographic 
Information 
Coordinating 
Council 
(NCGICC) 
and 
Statewide Mapping Advisory Committee (SMAC), a com-
mittee was tasked to develop a State and Local Government 
Metadata profile for geospatial data intended for use in 
North Carolina as well as educating the North Carolina GIS 
user community about this standard.  This standard is based 
on the International Standards Organization (ISO) 191** 
format and is an improvement over prior metadata standards 
to account for evolving technologies such as remotely 
sensed imagery, online services and ontologies.  These were 
not considered when original metadata standards such as the 
CSDGM (formally known as FGDC-STD-001-1998) were 
first published.   
At this time, assessing and evaluating adherence to this 
standard for large spatial databases is an exhaustive process, 
as users must toggle through multiple levels of metadata 
records among multiple features a using a metadata editor.  
In this day and age, it is ‘‘unrealistic to depend on tradition-
al humanly generated metadata approaches’’ when attempt-
ing to assess metadata integrity [4].  However, a happy me-
dium must be found between quality assurance, quality con-
trol and the necessary human component involved in this 
process that cannot be replicated in the digital environment.  
While some research [5] subscribes to the mechanization of 
metadata assessment processes as the most effective and 
efficient, other research [6] [7] concedes that metadata is 
best managed through the integration of the human and digi-
tal components.  While the level of human interaction in this 
process should be minimal, it should not be eliminated alto-
gether.  On that end, the goal of this paper is to propose a 
programmatic and faster assessment and evaluation alterna-
tive within the context of statewide metadata training that 
can be used by GIS management to facilitate decision-
making.  In doing so, it addresses and reinforces how pro-
grammatic metadata assessment and evaluation has begun to 
be implemented by the professional community.    
The rest of this paper is organized as follows. Section II 
describes the evolution of metadata. Section III describes the 
specific use and application of the North Carolina State 
metadata profile. Section IV addresses the how standard 
compliance is addressed. Section V discussed preliminary 
results.  The acknowledgement and conclusions close the 
article. 
II. 
THE EVOLUTION OF METADATA SCIENCE 
AND ASSESSMENT 
Metadata serves as an organized means to describe a da-
taset, and it provides the formal framework for providing 
information about a dataset’s lineage, age and creators.  
Metadata is composed of both qualitative and quantitative 
information and while metadata’s original use was simply as 
a means to catalog data, its storage and assessment has be-
come a science in itself.    
The FGDC regularly meets to determine all possible val-
ues, parameters and domains that can be captured and ex-
pressed within the confines of GIS metadata.  First formed 
in the early 1990s, the FGDC serves as a governing body for 
geospatial data and metadata in the United States.   The 
FGDC defines metadata as the following: 
 
A metadata record is a file of information, usually pre-
sented as an XML document, which captures the basic 
characteristics of a data or information resource. It rep-
resents the who, what, when, where, why and how of 
the resource. Geospatial metadata are used to document 
geographic digital resources such as Geographic Infor-
mation System (GIS) files, geospatial databases, and 
earth imagery. A geospatial metadata record includes 
core library catalog elements such as Title, Abstract, 
and Publication Data; geographic elements such as Ge-
ographic Extent and Projection Information; and data-
base elements such as Attribute Label Definitions and 
Attribute Domain Values [8]. 
 
FGDC metadata standards dictate that a plethora of indi-
vidual entries (now more than 400 and counting) are popu-
lated for compliant GIS metadata [8].  Thus, ensuring 
metadata integrity for large spatial data sets is a time-
consuming process if done by hand.  It is not uncommon for 
organizations to employ thousands of individual data layers 
within their digital warehouses.  Since traditional GIS data 
are ever-evolving, metadata standards must be flexible 
enough to account for new technologies.  Policy should dic-
tate that these standards be revisited periodically to ensure 
adaptability that can be implemented through large-scale 
changes or the publishing of new metadata standards.  The 
GIS community has employed a set of content standards to 
ensure compatibility across the entire GIS community.  The 
updated State and Local Government Metadata profile de-
veloped by the NCGICC based on the ISO 191** discussed 
in this paper highlights this adaptability and is an example 
of one of these standards.   
While regarded as a relatively new concept, both formal 
spatial and non-spatial metadata has existed in one form or 
another for the past 50 years.  MARC (Machine Readable 
Cataloging) and its successor MARC 21 are used by the Li-
brary of Congress to catalog bibliographic resources.  This 

168
International Journal on Advances in Software, vol 12 no 1 & 2, year 2019, http://www.iariajournals.org/software/
2019, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
system has been in place since the 1960s, but it was not 
originally designed for computer interfacing, and the format 
is not very intuitive [9]. A more popular format called the 
Dublin Core was created in 1995 for electronic recourses 
such as web pages and software applications.  While the 
FGDC and GIS metadata standards described here actually 
predate this more generalized format, GIS metadata data 
contains a variety of geographically-explicit descriptors that 
may not be fully understood by the non-GIS community 
[10].  
 Dublin Core and FGDC generally share a base level of 
descriptive metadata elements.  While Dublin Core is used 
to describe electronic resources and digital representations 
of physical resources such as artwork, GIS metadata adheres 
to FGDC and more recently ISO standards.  These require-
ments are always changing as dictated by technology.  Be-
cause of the spatial nature of GIS data, FGDC requirements 
dictate that information pertaining to absolute location be 
retained.  These fields include datum, coordinate system, 
false easting, false northing and bounding coordinates.  
While Dublin Core does make accommodations for place 
keywords and spatial descriptors, it does not contain place-
holders for elements that help describe geodetic elements 
associated with the quantitative representation of location 
with as much detail as GIS metadata. 
  Because of the different goals of each standard, a pre-
cipitous balance between MARC, Dublin Core and FGDC 
Metadata must be found.  Cross-walking, a tedious and 
sometimes imprecise process where either people or algo-
rithms find matching elements between the different stand-
ards may be necessary because various organizations use 
these popular formats interchangeably on a routine basis.  
Cross-walking methods have been used to match geospatial 
data to standards outside of FGDC, such as examining the 
feasibility of compatibility with the Dublin Core metadata 
standard [11][12].   
 
Current research in the field of metadata can be closely 
associated with statistics and high-speed processing.  Given 
the exponential increase in electronic resources and media, 
technologies must be able to accommodate the automation 
of resources that are viewed, accessed, and assessed.  Re-
search examined the role of metadata and its ability to be 
assessed, arguing that metadata for metadata’s sake does no 
good [13].  Metadata must have some utility as it needs to 
be assessed and have a role within the decision-making pro-
cess.  Metadata must ultimately serve a purpose and specifi-
cally the greater good of the user community.  While other 
research proposes a quality assessment for metadata, it fails 
to do so with regards to changes in metadata quality, their 
accompanying values and the holistic structure used to store 
them [14].  With the standardization of XML-based FGDC 
and ISO metadata standards, metadata can be compared 
from one time period to the next.  One of these structures is 
through ontology, a semantic representation of a concept 
through various domains and properties.  Most recently, e-
learning technologies were applied to these ontological 
metadata structures [15].  However, the lack of human cog-
nition within these ontologies cannot eliminate unnecessary 
or ambiguous terms using results from previous analysis, 
sometimes referred to as semantic accuracy within the con-
fines of GIS Quality Assessment/Quality Control (QA/QC) 
circles.     
 
The role of metadata assessment can be seen in a varie-
ty of different fields.  An Electronic Metadata Record 
(EMR), for example, is an emerging technology that is pro-
duced and edited when an electronic document is edited or 
created, such as a patient record or digital x-ray. A number 
of other related technologies for the medical industry have 
been developed to serve as a quality assurance and adminis-
trative tools.  The process of accessing, viewing, and com-
menting on patient files or x-rays by physicians in electronic 
form can be documented and stored in a metadata file. 
Hardcopy records are often times time-consuming to com-
plete, and they can be easily lost or destroyed. Thus, the 
ease of storing, accessing, and retrieving electronic metadata 
and files for medical data can help prevent litigation against 
malpractice lawsuits [16].  For example, a complex statisti-
cal analysis was developed to retrieve biomedical articles 
from more than 4,800 journals to help support the decision-
making process [17].  It is impossible to scrutinize each of 
14 million individual manuscripts.  Clustering and classifi-
cation methods performed on metadata derived from tradi-
tional statistical techniques are used to explore and retrieve 
related information within biomedical literature.  If properly 
maintained, metadata serves as a capable surrogate when 
querying scanned imagery or hard copy information is not 
feasible and further validates in-situ decisions as they are 
reinforced by easily accessible support literature.           
Metadata has the flexibility to capture many forms of 
qualitative and quantitative information stored as numbers, 
text strings, domain values and dates.  However, it does 
have its drawbacks.  In addition to the time, resources and 
expertise required to populate the information, ancillary 
concerns exist.  Metadata can be applied to any electronic 
resource, but there are data privacy concerns, especially 
within the medical community.  For example, metadata can 
be updated and collected to determine the number of times a 
medical professional has viewed patients’ information with-
in the EMR [16].  Not only does this address privacy con-
cerns by documenting access to particular records, but 
serves to report when, by whom and how long a digital rec-
ord was viewed.  In addition, EMR should not serve as an 
end-all diagnostic tool, especially when clinical data do ex-
ist.  Metadata should aid in the evaluation and decision-
making process.   Other research used image sharing com-
munity to further reinforce this point and brought up more 
excellent points [18].  Metadata for an image (date of image, 
place, context, etc.) is collected and stored with the image.  
Furthermore, social metadata not only explores information 
about the image itself within its place in the social media 
environment, but also tangential information related to an 
image such as comments about the image, information about 

169
International Journal on Advances in Software, vol 12 no 1 & 2, year 2019, http://www.iariajournals.org/software/
2019, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
people who have posted comments about the image and the 
user groups to which these commenters belong [19]. Limit-
ing this information greatly reduces the amount of analysis 
that can be performed on the accompanying image, decreas-
ing the availability to knowledge in order to make sound 
business decisions.  As this applies to GIS metadata, a hap-
py medium must be found so privacy concerns can be satis-
fied while dutiful analysis can be performed.  Given the 
relative infancy of these subjects and lack of established 
doctrine, the body of knowledge is still growing in this sub-
ject.     
The very nature of spatial data dictates that a different 
approach must be taken for assessment and reporting within 
the digital environment.  The proliferation of spatial tech-
nologies underscores the widely accepted and legitimate 
role of metadata within the GIS user community [20].  All 
elements intrinsic to spatial data, such as those associated 
with position (e.g. latitude, longitude) as well as its repre-
sentation (e.g., accuracy) must be carefully documented and 
recorded in GIS metadata.  It is important that information 
about the data format, a description of the data, the process-
es by which the data were created, the areal extent of the 
data and the people who aided in data creation be retained.  
Formal controls may dictate specific tolerances for horizon-
tal and temporal accuracy. This information is not only im-
portant from a legal standpoint, but it also validates GIS 
analysis by speaking directly to such necessary components 
as its horizontal and temporal accuracy.  Since GIS analysis 
is only as good as the data on which it is based, metadata 
reinforces the data and ultimately the analysis and organiza-
tions which develop the GIS data.    
As mentioned previously, metadata is important in help-
ing to document dimensions of quantifiable GIS data quality 
such as attribute accuracy, horizontal accuracy and attribute 
completeness.  Other forms of GIS data accuracy do in fact 
exist.  FGDC and spatial data transfer standards (SDTS) 
also consider vertical accuracy (error in measured vs. repre-
sented elevation), data lineage (source materials of data) and 
logical consistency (compliance of qualitative relationships 
inherent in the data structure) as part of data quality 
[21][22].  In some GIS circles, temporal accuracy (age of 
the data compared to usage date) and semantic accuracy or 
“the quality with which geographical objects are described 
in accordance with the selected model” are also considered 
elements of data quality [23].  Placeholders within FGDC 
metadata exist to capture all of this information either quan-
titatively or qualitatively.    
Early pioneers of GIS recognized the importance of data 
quality, not only from a cost efficiency standpoint, but be-
cause of the legal ramifications in publishing incorrect spa-
tial information which may lead to accidents or the misuse 
of data [24].  Even then, they understood the reconciliation 
between accuracy, the cost of creating accurate data and the 
eventuality that some error will occur.  It is unreasonable to 
expect an organization such as the North Carolina Depart-
ment of Transportation (NCDOT) to photo-revise and field 
check every single road in their GIS database, re-attribute it 
correctly and then verify them using another party in a time-
ly manner given current personnel and financial constraints.  
This compromise is referred to as uncertainty absorption 
[25].  Regardless of resource allocation, verification of data 
quality should be done by discipline experts with a long-
term goal of developing data quality standards.  This helps 
to protect the GIS data producer from the potential misuse 
of GIS data and metadata serves as the means to formally 
inform the data user of data quality measures applied to da-
ta, as well as protect GIS data stewards from its misman-
agement [26]. 
 
In and of itself, data quality has no inherent value or 
worth, but is ultimately realized when action is taken on 
information pertaining to data quality [27].  Along those 
same lines, the end goal of information quality is to satisfy 
customer needs, in this case being the many users who uti-
lize GIS data with the understanding that the data have un-
dergone some form of validation [28].  Quantitative 
measures related to this validation with qualitative processes 
needs to be highlighted in metadata.   
 
Early research and commentary on the concept of geo-
spatial metadata has touted its value as an effective deci-
sion-support tool, regardless of its native format [29].   
These formats include Hyper Text Markup Language 
(HTML), Extensible Markup Language (XML) along with 
its various ISO standards (19115, 19115-1, 19139), TXT 
(Text File), Geography Markup Language (GML) and 
Standard Generalized Markup Language (SGML), as well 
as proprietary formats.  Methodology has explored the abil-
ity to integrate spatial metadata to a stand-alone database 
long before GIS metadata was stored in a standardized for-
mat, as well as compiling statistics about metadata elements 
within the confines of specific software [30] [31].   
 
To that end, the population of geospatial metadata is a 
monotonous process and subject to error, although research 
has explored the large-scale production of standards-based 
metadata in order to alleviate these issues [32] [33].  Be-
cause of this, research maintains that human nature alone 
undermines the immediate and long-term goals of metadata 
for an organization and the GIS user community [34].  
While the omission of one minor element would not degrade 
a layer’s metadata or invalidate the geospatial data on which 
it is based, it may compromise quantitative data quality 
measures captured from which decisions can be made.  
More recently, feature level metadata has been able to cap-
ture data quality information, but is typically limited to 
quantitative measures of positional accuracy and qualitative 
information related to data lineage within eight of the more 
than 400 entries that comprise a complete FGDC-compliant 
metadata file [35] [36].  Even now, the population of these 
metadata elements is not fully automated and some entries 
must be done by a GIS data steward.  However, methodolo-
gies to explore its assessment and evaluation are evolving.  
Efforts have been made to quantitively assess metadata 
quality using both a human and statistically-automated ele-
ment [37] and this paper explores this notion within the con-
fines of and applied to a particular standard.   

170
International Journal on Advances in Software, vol 12 no 1 & 2, year 2019, http://www.iariajournals.org/software/
2019, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
III. 
THE NORTH CAROLINA STATE AND LOCAL 
GOVERNMENT PROFILE 
Geospatial metadata standards serve as a cohesive and 
standardized means by which organizations can define, store 
and more importantly share information about geospatial 
data.  It defines the categories of information that needs to 
be stored, individual entries, or tags, of individual elements 
within these categories and the types of data (text, date, 
number) and their lengths that can be stored while represent-
ing these tags.  FGDC metadata is divided into 7 sections or 
divisions that transcend descriptive, administrative and 
structural components.  They are: Identification Infor-
mation, Data Quality Information, Spatial Data Organiza-
tion Information, Spatial Reference Information, Entity and 
Attribute 
Information, 
Distribution 
Information, 
and 
Metadata Reference Information [38]. 
 
Within these high-level divisions, subdivisions and even-
tually individual metadata tags can be populated to catalog 
various forms of information about the GIS data layer.  The 
hierarchy of these divisions and subdivisions are consistent 
with a standard.  In addition to providing this structure, the 
FGDC also creates guidelines by dictating which metadata 
elements are to be populated.  The FGDC requires seven 
metadata elements be populated for all GIS data.  The 
FGDC also suggests that fifteen metadata elements be popu-
lated.  These suggested and required elements are included 
in Table I below. 
 
TABLE I:  REQUIRED AND SUGGESTED FGDC ELEMENTS. 
FGDC -Required 
Elements 
FGDC- Suggested Elements 
Title 
Reference Date 
Language 
Topic Category 
Abstract 
Point of Contact 
Metadata Date 
 
• Dataset Responsible 
Party 
• Geography Locations 
by Coordinates (X) 
• Geography Locations 
by Coordinates (Y) 
• Data Character Set 
• Spatial Resolution 
• Distribution Format 
• Spatial Representation 
Type 
• Reference System  
• Metadata Character 
Set  
• Lineage Statement 
• Online Resource 
• Metadata File Iden-
tifier 
• Metadata Standard 
Name 
• Metadata Standard 
Version 
• Metadata Language 
        
Organizations actively create content standards for new 
technologies and manners in which geospatial data are col-
lected and stored.  One such example is the FGDC content 
standard for Remotely Sensed Data.  This includes two divi-
sions germane to the equipment and methods such as plat-
form name, sensor information and algorithm information 
used to capture the imagery, in addition to the seven existing 
aforementioned divisions [39]. In order to further elucidate 
descriptive, administration and structural information, addi-
tional addendums to existing metadata standards are also 
attached to specific geospatial-specific data such as address-
es, biological data, shoreline data, and vegetation data.  
Standards such as these must be increasingly flexible and 
updatable to account for the evolving technologies in which 
geospatial data can now be captured (crowdsourcing, Un-
manned Aerial Vehicle, large scale geocoding), processed 
(new geostatistical and interpolation algorithms) and ulti-
mately delivered (web map service, web feature service) to 
the GIS user community.       
In recent years, the North Carolina SMAC has recog-
nized most GIS data managers lack the time and resources 
necessary to learn and apply a metadata standard that main-
tains dataset integrity and retains pertinent information 
while not being too demanding on existing resources, most 
notably time and people. To address the problem of missing 
or incomplete metadata records among state and local data 
publishers, the SMAC chartered an ad-hoc Metadata Com-
mittee in October 2012 to “recommend ways to expand and 
improve geospatial metadata in North Carolina that are effi-
cient for the data producer and benefit data users in the dis-
covery and application of geospatial data.” The Metadata 
Committee submitted a draft of this profile, based on the 
ISO 19115 (for Geographic Information – Metadata: 2003), 
ISO 19115-1 (for Geographic Information – Metadata – Part 
1:  Fundamentals: 2014) and ISO 19119 (Geographic In-
formation – Services: 2016) standards. After review and 
modification by SMAC and its standing committees, the 
most current version of this standard has been in effect since 
December 30, 2016 and is available through the NCOneMap 
portal [40].  While not entirely ground-breaking, North Car-
olina has been a forerunner in developing sub-country 
metadata standards.  The SMAC worked with the Canadian 
Province of Alberta, who has already developed a standard 
germane to their province while states such as Missouri and 
Virginia have developed some level of uniform metadata 
available with their products.  Internationally, GeoDCAT-
AP is a metadata profile designed to facilitate interchange 
for data portals operated by EU Member States.  It uses the 
aforementioned ISO 19115 format and the INSPIRE 
metadata standard, which is primarily used in Europe [41].    
Given seven required and fifteen recommended metadata 
elements are fairly ambiguous and less than ideal for many 
organizations whose data is integrated into the NCOneMap, 
the North Carolina state geospatial data portal, this profile 
provides explicit guidance on required/suggested metadata 
elements, wording for these elements, standardization of 
naming/date conventions and domain fields for topic catego-
ries for more than 75 metadata tags [42].  A few examples 
of the rules for geospatial metadata include: 
1. Title is required as a free-text entry. 
2. Publication Date is required and the format for Publica-
tion Date is YYYY-MM-DD or YYYYMMDD.  If day 
is not known, use YYYY-MM and use YYYY if month 
is not known.    
3. Abstract is required as a free text entry.   Do not use 
YYYYMM since it can’t be distinguished from the in-
correct, but still used YYMMDD.   
4. Status is required and only possible values are ‘histori-
calArchive’, ‘required’, ‘planned’, ‘onGoing’ ‘complet-
ed’, ‘underDevelopment’ and ‘obsolete’.  
5. Topic Category is required and can be one of 23 possible 
values from domain table.  
6. Use Constraints required as a free-text entry to describe 
any restrictions with using the data.   

171
International Journal on Advances in Software, vol 12 no 1 & 2, year 2019, http://www.iariajournals.org/software/
2019, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
7. Online linkage is required to an URL address that pro-
vides access, preferably direct access, to the data. 
 
In addition, given their nature and distinct differences be-
tween their geospatial data counterparts, the SMAC has 
defined rules for geospatial services to include the follow-
ing: 
1. Metadata Scope code must be ‘service’. 
2. Online Function code is required from domain of one 
of five possible values.   
3. Title is required as a free-text entry. 
4. Metadata Contact is required as a free-text entry, rep-
resenting Organization Name of the agency that 
serves as a point of contact for the metadata record. 
 
This richer metadata enables content consistency and 
improves the search and discovery of data through 
NCOneMap. 
As part of a needs assessment for this project, a survey 
was developed to help dictate and direct metadata needs 
within the state of North Carolina.  This survey was devel-
oped in 2017 and distributed to the North Carolina GIS user 
community.   Forty (40) respondents answered the survey, 
who ranged from GIS Technicians and Property Mappers to 
GIS Coordinators and Managers throughout the state of 
North Carolina.  Questions were asked about respondents’ 
experience with data development, metadata, as well as or-
ganizational requirements as it pertains to metadata.   
Most prominent was the schism between respondents’ 
experience with data development and experience with 
metadata, as shown in Figures 1 and 2.   Respondents gen-
erally had an ‘excellent working knowledge’ of data devel-
opment, 
but 
only 
‘some 
experience’ 
or 
‘working 
knowledge’ on the metadata created as a result of these data 
development techniques.  These underscore technical expe-
rience in creating new data in support of analysis and pro-
jects, but less experience in cataloging these same data used 
for analysis and maps.   
 
 
 
 
Figure 1:  A survey of GIS professionals and experience with data devel-
opment. 
 
      
 
Figure 2:  A survey of GIS professionals and experience with metadata. 
     
Not only is this schism evidenced at the individual level, 
but also at the organizational level.  In a same survey of 
these 40 GIS professionals, they describe their organiza-
tion’s approach to metadata as shown in Figure 3.  More 
than half of all respondents’ organizations have no metadata 
requirement whatsoever and only five respondents work in 
an organization that has a firm metadata requirement.  The 
rest do have a metadata requirement, but it is not upheld.     
 
 
 
 
Figure 3:  A survey of metadata requirements at the organization level. 
 
In response to this need and the motivation for this paper, 
the research team earned a research and education grant 
through the NCDOT and NCGICC to provide metadata 
training, education and support to the state of North Caroli-
na.  The goal of the “Facilitating the New Statewide GIS 
Metadata Standards Through Training and Outreach” grant 
ran from August 2016 through March, 2019.  The goals of 
this project include: 
1. Assessing the existing knowledge base of GIS users on 
the subject of GIS metadata through surveys given to 
training attendees. 
2. Technical and material support to the implementation 
and education of the new metadata profile to the North 
Carolina GIS user community. 
3. Understanding needs of GIS data managers when it 
comes to metadata population and requirements for data 
under their purview through surveys given to GIS data 
managers. 
4. Determining best methods to deliver metadata training to 
the North Carolina GIS user community that close gaps 

172
International Journal on Advances in Software, vol 12 no 1 & 2, year 2019, http://www.iariajournals.org/software/
2019, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
between existing knowledge base and needs of GIS data 
managers and the larger NC GIS community. 
5. Assessment and evaluation of training activities to de-
termine best practices for future training and future sup-
port through post-training quantitative and qualitative 
surveys. 
 
This grant has provided support to the North Carolina 
GIS user community through an assessment and evaluation 
of current metadata activities by the North Carolina GIS 
user community, work on the North Carolina State and Lo-
cal Government Metadata Profile, development of online 
and face-to-face training materials and data in response to 
this evaluation, and the delivery of face-to-face and virtual 
training to the North Carolina GIS user community.  This 
grant has provided the opportunity for North Carolina Cen-
tral University (NCCU) students to develop metadata skills 
and interface with members of the North Carolina GIS user 
community in this niche and unique training opportunity 
relatively unique to this state.  As such, specific measurable 
research tasks over the life of the project include: 
1. Curriculum Review and QA/QC  
2. Material and Training Support 
3. Data Development 
4. Facilitation of Online Resources 
5. Development and Assessment of Metadata Templates 
6. Development of Metadata Scripts 
7. Report of Training Activities 
8. Outreach to Educational Institutions and Professional 
Organizations 
 
In support of many of these tasks, face-to-face training 
was provided on the campus of North Carolina Central Uni-
versity (Figure 4), as well as online.   
 
 
Figure 4:  Face-to-face metadata training given at North Carolina Central 
University in 2018. 
 
The Department of Environmental, Earth and Geospatial 
Sciences (DEEGS) at NCCU hosts YouTube tutorials  rang-
ing from metadata basics and the use and application of the 
North Carolina State and Local Government Profile to more 
advanced topics such as XML translators and Python pro-
gramming solutions in metadata assessment and evaluation 
[43].  These tutorials have been utilized more than 3,500 
times.   
IV. 
ASSESSING STANDARD COMPLIANCE 
Given the ever-increasing size of GIS data sets and the 
metadata requirements for each data layer, there needs to be 
a mechanism to assess the quality of these metadata not seen 
in previous generations or documented in existing literature.  
There also needs to be a means by which individual metada-
ta entries adhere to predefined profiles and standards.   This 
is in support of Task 6 of the research tasks.  Computer pro-
gramming languages and templates have helped to stream-
line this process.  Templates populate redundant features 
that are common throughout an entire GIS database such as 
the purpose, supplementary information, distribution liabil-
ity statements and ordering instructions that can be specific 
to an agency or department.  These templates can be import-
ed one at a time, but programming techniques and software 
packages have allowed users to assess information that 
would take a human days or perhaps weeks to do.  The 
NCGICC provides a number of templates through their web 
portal, NCOneMap [44].  The themes for these templates are 
at the request of North Carolina GIS users, and include 
buildings, cadastre, municipal boundaries, school attendance 
districts, street centerlines, address points and orthoimagery.   
These templates contain much of the verbiage about a lay-
er’s description and creation processes, and can be easily 
imported and edited specific to the user’s contact infor-
mation.  A sample of the identification information for the 
cadastral data template can be seen in Figure 5. 
 
 
 
Figure 5.  Identification information metadata entries for cadastral template 
provided through NCOneMap web portal.   
  
 
Open source programming solutions using Perl and R 
have been used to assess and evaluate metadata by travers-
ing geospatial metadata stored in XML format as per FGDC 
requirements, resulting in quantitative metrics, graphs and 
reports regarding metadata compliance, as shown in Figure 
6 [45].      

173
International Journal on Advances in Software, vol 12 no 1 & 2, year 2019, http://www.iariajournals.org/software/
2019, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
 
 
Figure 6:  Sample of Metadata Compliance Report Generated  
Using Open Source Assessment Tool. 
 
 
 
As applied to the NC State and Local Government Pro-
file, one major challenge exists.  Primarily, geospatial data 
and metadata is typically software specific.  While optimal 
open source solutions could be used to gleam information 
from metadata stored in XML using an appropriate xPath, 
these software-agnostic solutions are typically loosely-
coupled and not intuitive to the average user.  As a result of 
reliance on Esri products throughout the state, the Python 
programming language is being used to run this iteration of 
an assessment and evaluation tool before open source solu-
tions are explored.   
Using the NC State and Local Government Profile as a 
guideline, the research team has been developing tools for 
data managers to access and evaluate metadata entries.  At 
the current time, metadata entries are written to CSV 
(Comma Separated Values) files.  While doing this, string 
operations are run to ensure that required entries are popu-
lated, date entries comply with required conventions and 
domain entries match those in the domain table, all while 
agglomerating results and statistics at the database, layer 
(record) and tag (attribute) level.  This can provide GIS 
managers with insight on non-compliant metadata entries to 
determine relationships between non-compliant entries and 
the responsible data steward or particular attributes that are 
continually non-compliant.  
While QC procedures need to be performed to determine 
if a metadata entry is accurate, below are a few examples of 
the many programming rules employed to determine if en-
tries are populated properly.   
1. Title, Responsible Party Organization Name, Online 
Linkage, Abstract, Use Constraints, Feature Catalogues, 
Process Description, Spatial Reference Information and 
Metadata Contact Name cannot be Null 
2. Data Type can only have values of ‘creation’, ‘publica-
tion’, or ‘revision’. 
3. Publication Date, Temporal Extent of Data and Metada-
ta Creation Date must follow appropriate format.  This 
entails: 
a. The date cannot be Null and must be populated. 
b. The date can only have a length of 10 (YYYY-MM-
DD) 8 (YYYYMMDD), 7 (YYYY-MM) or 4 charac-
ters (YYYY). 
c. Besides the hyphens (‘-‘), the date can only contain 
numbers whose value range from 0 through 9.  Let-
ters and other characters are not allowed.   
d. If a date contains hyphens (‘-‘), there will be 2 hy-
phens in a string that has a length of 10 (YYYY-
MM-YY) and there will be 1 hyphen in a string that 
has a length of 7 (YYYY-MM). 
e. Regardless of the format used, the first character of a 
string will either be ‘1’ or ‘2’ since the year of publi-
cation or creation will begin in only those 2 numbers.   
4. Metadata Contact Role Code can only have values of 
‘custodian’ or ‘pointOfContact’. 
5. Progress Code can only have values of ‘completed’. 
‘historicalArchive’, ‘obslete’, ‘onGoing’, ‘planned’, ‘re-
quired’ or ‘underDevelopment’. 
6. Maintenance and Update Frequency can only have val-
ues of ‘continual’, ‘daily’, ‘weekly’, ‘fortnightly’, 
‘monthly’, 
‘quarterly’, 
‘ 
biannually’, 
‘annually’, 
‘asNeeded’, ‘irregular’, ‘notPlanned’ or ‘unknown’. 
 
The current application has a basic GUI (Figure 7) that 
allows for 4 input parameters: 1) an input database that  
contains the features classes for which metadata will be 
checked 2) an output folder to which XML metadata is writ-
ten.  Python cannot directly access metadata in geodatabase 
format, so this proprietary metadata is converted to XML 
format and traversed.  Options exist so these XML files are 
immediately deleted.  3)  The name and location of the out-
put entries that store all metadata entries that are checked 
from the North Carolina State and Local Government Pro-
file, as well as a summary of the percentage of individual 
metadata elements that are correct and a summary of per-
centage of correct elements on a feature class by feature 
class basis and 4) the location of an error file that highlights 
errors within the metadata (Figure 8).   
 

174
International Journal on Advances in Software, vol 12 no 1 & 2, year 2019, http://www.iariajournals.org/software/
2019, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
 
 
 
 
Figure 8:  Sample Error File Output from Metadata Assessment and Evalu-
ation.  
V. 
RESULTS 
The Technology Acceptance Model (TAM) was used to 
assess and quantify the effectiveness of the metadata as-
sessment tool.  The TAM that we know of today was origi-
nally created as a means to universally quantify the effec-
tiveness of technology by exploring relationships between 
the technology’s Perceived Ease of Use, Perceived Useful-
ness, Attitude Towards Using and the Intention to Further 
Use the technology [46].  Using Chronbach’s Alpha, Princi-
pal Components Analysis and Simple Linear Regression, 
associations can be found between these various compo-
nents, as shown in Figure 9.   
 
Given that the intended usership of this research is 
geared towards GIS professionals as opposed to developers 
or programmers, a testing mechanism geared toward this 
group would be more appropriate than testing code efficien-
cy or complexity.    The TAM has served as a means to as-
sess and quantify the effectiveness of a technology for more 
than 35 years and will do so once again for this research.  
The TAM was originally created as a means to universally 
quantify the effectiveness of technology [46].  It was born 
from the fact that the adoption of new technologies is de-
pendent upon ambiguous and sometimes qualitative notions 
such as psychological disposition, attitudes, intentions and 
our own personal biases related to this new technology that 
make it difficult to test and validate [47].  TAM is actually 
the technical manifestation of the Theory of Reasoned Ac-
tion (TRA) [48]. TRA is the theory in which beliefs, com-
posed of attitudes, values and opinions at the individual lev-
el, eventually result in enacted behavior.  Within the TAM, 
this enacted behavior is the decision to adopt technology.      
 Empirical studies on the use and application of TAM 
find that a technology’s acceptance is most related to its 1) 
Perceived Usefulness and 2) Perceived Ease of Use.  Per-
ceived Usefulness refers to the quality in which a technolo-
gy would help one’s job performance.  While research [49] 
[50] [51] has explored this usefulness dimension, TAM also 
looks at this in concert with this technology’s “freedom 
from difficulty or great effort” [45].   This ease of use factor 
helps support the self-efficacy theory which focuses on 
one’s innate ability to accomplish goals [52].  While there 
are differences between the roots of this effectiveness and 
seminal outcomes which at times can be paradoxical, TAM 
encapsulates this within one encompassing desired end-state 
of ultimately accomplishing one’s tasks with as little effort 
as possible.   Studies have actually shown the relationship 
between this Perceived Usefulness and Ease of Use with the 
adoption of technology is regardless of variables such as 
gender and computer experience [53].   
Using these two indicators as a guideline, Davis creates 
questions that try to explain the usage and acceptance pat-
terns of a technology as per TRA.  Users of the technology 
are asked to scale responses to these questions similar on a 
7-point Likert-type scale, representing “Strongly Agree” 
through “Strongly Disagree”.   Regression analysis between 
this effectiveness and ease of use variables is determined at 
various confidence intervals.  In addition, principal compo-
nents analysis is used to explain the variance of usage inten-
tions as a function of Perceived Usefulness and this attitude 
towards the technology.  This and other hypotheses related 
to Perceived Usefulness, Perceived Ease of Use, Attitude 
Towards Using the technology and behavioral intention of 
use are tested among each other [54].    
TAM represents a milestone towards understanding hu-
man behavior as applied to the technology realm.  Germane 
to this research, TAM has been applied to e-learning [54] as 
well as its place in the e-commerce environment as TAM 
integrates technology and human behavior when applied to 
online shopping [55].  Visiting an online store for the first 
time has overwhelming consequences on whether the visitor 
will visit again or make unplanned purchases.  Making this 
online shopping experience an easy, enjoyable and memo-
rable one is of utmost concern to these businesses.  The or-
der in which material is presented, the amount of material 
presented and the user’s cognitive impression of this materi-
al, its volume and its underlying messages play into this 
perceived enjoyment factor.  Finally, all of these facets need 
to be assessed in a manner free of bias, confusion, miscon-
ception and misunderstanding.      
 
Figure 7:  Metadata Assessment and Evaluation Tool.  

175
International Journal on Advances in Software, vol 12 no 1 & 2, year 2019, http://www.iariajournals.org/software/
2019, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
 
 
Figure 9:  Regression Used to Test Research Hypotheses where 
TAM was Tested Against Null Hypotheses to Test Correlations.  
      
The validity of these assessment tools was tested using a 
TAM.  Responses from GIS professionals regarding the 
results of this methodology were captured to find a relation-
ship between this technology’s Perceived Ease of Use, Per-
ceived Usefulness, Attitude Towards Using and the Inten-
tion to Further use this technology.     
 
The results from TAM analysis (Figure 9) show 3 out of 
the 5 research hypotheses (H1, H2 and H4) relating the 
tool’s Perceived Ease of Use, Perceived Usefulness, Atti-
tude Towards Using and Intention to Use were accepted at a 
95% confidence interval.  Another (H5) could be accepted 
at about a 70% confidence interval.  In one non-supported 
hypothesis (H3), it seemed that the strong responses from 
the Perceived Ease of Use component obfuscated the Per-
ceived Usefulness component.  As a result, the Perceived 
Usefulness did not have much of an effect on the linear re-
gression model used to support the hypothesis.  In the other 
model (H5), the dependent variable is the Intention to Use 
component using the Attitude Towards Using component as 
the independent variable.  The two components delve into 
the question of implementing these assessment tools onto 
their individual system and have elicited a wide array of 
responses from respondents due to technical experience and 
familiarity or understanding with the programming envi-
ronment, which is essentially an extension to the typical GIS 
system and typically requires training above and beyond 
that of a GIS Technician or Junior Analyst.  While the effi-
cacy for these programming solutions exists, the efficiency 
may not reciprocate for this reason. Nonetheless, these are 
difficult to model. Even modeled by themselves without the 
Perceived Usefulness component, a linear regression model 
between these two factors (Attitude Towards Using MART 
vs.  Intention to Use) only produces an R2 value of 
.118.  Other factors – either those not captured within the 
assessment or just unquantifiable by their very nature factor 
into the non-support of this hypothesis.    For example, GIS 
Technicians working on few GIS data layers have little to 
no need for metadata assessment and therefore no intention 
to further use it.  When enough GIS Managers have com-
pleted the assessment on which TAM is based, it will be run 
once again on this new tool to assess its effectiveness for a 
more germane usership.   Regardless, these results help sat-
isfy the theoretical impetus of this research and are both 
intriguing and promising for the future of GIS metadata and 
widening role as an effective tool to elicit action.    
VI. 
DISCUSSION 
While a powerful and efficient tool, the programmatic 
assessment and evaluation of geospatial metadata still can-
not altogether replace the human component.  While these 
technologies can traverse metadata schema and extract tags 
to deem if they are complete, compliant or belong to a par-
ticular domain, it does not necessarily mean they are correct.  
For example, while the Publication Date tag may be proper-
ly populated (2016-02-29 for example) as per the rules dic-
tated in the North Carolina State and Local Government 
Profile, it may not necessarily mean the data were published 
on that date.  QA/QC techniques should be used to deter-
mine metadata quality across the entire dataset via Ameri-
can National Standards Institute (ANSI), American Society 
of Quality Control (ANSQ) or other institution-wide 
QA/QC procedures that best fit needs, resources and limita-
tions. 
While the level of attribution within metadata has im-
proved with each new standard and this particular profile, it 
is in no way complete.  As technologies improve and there 
include more diverse ways to collect, manipulate and create 
GIS data, metadata must be flexible enough to accommo-
date all of these techniques.  For example, the standard 
CSDGM does not contain placeholders germane to the col-
lection of data created via a GPS unit like the various Dilu-
tion of Position (DOP) measures such as vertical, horizontal 
and 3D.  In addition, detailed information directly associat-
ed with the quality of data specific to GPS-collected data 
such as ephemeris can be entered via a free text field, but 
lacks the placeholders within the CSDGM as well as this 
standard.   In addition, GIS data now extend well beyond the 
typical raster and data models that a GIS professional may 
have solely encountered only a decade earlier.  GIS data 
may now include stand-along tables, Triangulated Irregular 
Networks (TINs), relationship classes and even topologies.  
They each have their own intrinsic qualities that make their 
creation and update difficult to encapsulate within a single 
catch-all metadata format.         
In addition, TAM does have its limitations.  TAM may 
not adequately explain for social influences [56].  More spe-
cifically, it is difficult to discern whether intent, attitude or 
some other referent characteristic sufficiently explain usage 
behavior.  Principal Component Analysis can only do so 
much within the paradigm of the testing environment. In 
addition, it is difficult to explain how this physiological at-
tachment related to attitude and behavioral intention can be 
assessed within TAM.  Research has attempted to explain 
this by expanding the dimensionality of testing elements 
within a rotated component matrix, but this begins to fall 
outside of the scope of this research [57].  Nonetheless, any 
technology is an investment.   Even using this model, it is 
difficult to realize the value of the large-scale investment for 

176
International Journal on Advances in Software, vol 12 no 1 & 2, year 2019, http://www.iariajournals.org/software/
2019, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
an organization at an individual level given the multiple 
intrinsic behaviors and intentions independent of this organ-
izational goal.   
While the intersection of these various subjects serves as 
the theoretical impetus of this research, assessing these 
techniques can take on a variety of different forms.  How 
will technology be further disseminated in the working 
world?  How can this technology be assessed?  While fac-
tors such as lost income and usership are quantitative in 
nature, they are interwoven with determinants such as mar-
keting, depth and level of human-computer interaction, or-
ganizational structure and management of the technology, 
which are tangential at best to this technology.  Some of 
these factors do not speak to the effectiveness of the tech-
nology, but the diffusion of this technology which helps to 
proliferate its use.   With this ‘chicken or the egg’ scenario, 
it is sometimes difficult to compartmentalize a valid meas-
urement scale to assess technology acceptance alone for a 
single piece of technology within the user community.    
VII. CONCLUSION  
The increasing schism between the rate at which data are 
created and the efficiency at which the metadata are as-
sessed serves as the impetus of this research.  GIS metadata 
serves as the means by which spatially-related phenomena 
can be catalogued within a formal framework.  It is here 
where implicit information can be codified for use by the 
larger GIS community.  Given the ever-increasing size of 
GIS data sets and the proficiency with which GIS data are 
created, there needs to be a mechanism to educate, assess 
and evaluate the human element to keep up with this profi-
ciency.  A means has been created to educate and inform a 
statewide GIS data community about a new standard, creat-
ed by North Carolinians for North Carolinians based on 
input from North Carolinians.  Programming techniques and 
software packages have allowed users to assess descriptive 
information about this standard that would take a human 
days or perhaps weeks to do.   
In addition to addressing the quantitative need for 
metadata as well as a means to educate the GIS community 
about the salient procedures and technologies necessary to 
be conversant in the science of metadata, this paper ad-
dressed solutions to educate a statewide community about 
metadata as well as measure adherence to a state-level pro-
file.  As per one of the goals of this paper, a programmatic 
solution using the Python programming language has been 
implemented.  However, it is too early to tell how well these 
can be integrated into business processes at organizations 
such as the NCGICC.  Revisiting this at a later time will 
provide time for acceptance of these techniques into main-
stream GIS with little to no prior programming knowledge.  
This research highlights the importance and need of pro-
grammatic approaches to the assessment and evaluation of 
metadata for large spatial datasets.  This information can 
provide GIS Managers with already limited resources with 
the tools to make informed decisions that are not feasible 
with visual inspection or a qualitative understanding of 
these increasingly large datasets.   
ACKNOWLEDGMENT 
The author wishes to thank the North Carolina Depart-
ment of Transportation (NCDOT) for their generous support 
of this research as well as NCCU Undergraduate Student 
Richard Foster for his work on programming solutions relat-
ed to this paper.   
 
REFERENCES 
[1] T. Mulrooney.  “Assessing and Evaluating Standard Compliance with 
a State and Local Government GIS Metadata Profile in Large 
Geospaital Databases,”  Proceedings of the Tenth International 
Conference on Advanced Geographic Information Systems, pp. 36 – 
39. Rome, Italy, 2018. 
[2] K. Leiden, K. Laughery, J. Keller, J. French, J., W. Warwick and S. 
Wood, “A Review of Human Performance Models for the Prediction 
of Human Error,”  Moffett Field, CA :  National Aeronautics and 
Space Administration, 2001.   
[3] North Carolina One Map, Geographic Data Serving a Statewide 
Community  [online].  Available from http://www.nconemap.gov/ 
[retrieved October 2018] 
[4] J. Greenberg, K. Spurgin and A. Crystal, “Functionalities for 
Automatic Metadata Generation Applications: A Survey of Metadata 
Experts’ Opinions,” International Journal of Metadata, Semantics and 
Ontologies  vol. 1(1), pp  3–20, 2006. 
[5] J. Anderson and J. Perez-Carballo, “The Nature of Indexing: How 
Humans and Machines Analyze Messages and Texts for Retrieval: 
Part I: Research, and the Nature of Human Indexing,” Information 
Processing and Management: An International Journal, vol. 37(2), pp.   
231–254, 2001. 
[6] C. Schwartz, “Sorting Out the Web: Approaches to Subject Access,” 
Westport, Connecticut: Ablex Publishing, 2002.  
[7] T. Craven, “DESCRIPTION Meta Tags in Public Home and Linked 
pages,” LIBRES: Library and Information Science Research 
Electronic Journal, vol. 11(2), 2001. 
[8] Federal Geographic Data Committee, “Content Standard for Digital 
Geosp atial Metadata Workbook.”  Washington D.C.:  Federal 
Geographic Data Committee, 2000.    
[9] Library of Congress.  MARC Standards.  [online], available from 
http://www.loc.gov/marc/ [retrieved August 2018]. 
[10] Dublin Core Metadata Initiative.  Dublin Core Metadata Element Set, 
Version 1.1: Reference Description.  [online], available from  
http://dublincore.org/documents/dces/ [retrieved September 2018]. 
[11] T. Reese, “Bibliographic Freedom and the Future Direction of Map 
Cataloging,”  Journal of Map and Geography Librariess, vol. 2(1), pp.   
67-90, 2005. 
[12] J. Batcheller, “Automating Geospatial Metadata Generation—An 
Integrated Data Management and Documentation Approach,”   
Computers & Geosciences, vol.  34, pp. 387 – 398, 2008. 
[13] B. Stvilia and L. Gasser, “Value-Based Metadata Quality 
Assessment.”  Library & Information Science Research, vol. 30(1),  
pp. 67 – 74, 2008.   
[14] T. Bruce and D. Hillman, “The Continuum of Metadata Quality: 
Defining, Expressing, Exploiting,”  In: D. Hillman and E. 
Westbrooks, Editors, Metadata in Practice, ALA Editions, Chicago, 
pp. 238–256, 2004. 
[15] M. Lee, K. Hua Tsai and T. Wang, “A Practical Ontology Query 
Expansion Algorithm For Semantic-Aware Learning Objects 
Retrieval,”  Computers & Education, vol.  50(4), pp. 1240-1257, 
2008. 
[16] T. McLean, L. Burton, C. Haller and P. McLean, “Electronic Medical 
Record Metadata: Uses and Liability.”  Journal of the American 
College of Surgeons, vol.  206(3), pp.  405 – 411, 2008.   

177
International Journal on Advances in Software, vol 12 no 1 & 2, year 2019, http://www.iariajournals.org/software/
2019, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
[17] T. Theodosiou, L. Angelis and A. Vakali, “Non-Linear Correlation of 
Content and Metadata Information Extracted From Biomedical 
Article Datasets,”  Journal of Biomedical Informatics, vol. 41(1), pp.  
202 – 216, 2008.     
[18] J. Skågeby, “Semi-Public End-User Content Contributions—A Case-
Study of Concerns and Intentions in Online Photo-Sharing,”   
International Journal of Human-Computer Studies, vol. 66(4), pp.   
287-300, 2008 
[19] J. Skågeby, “Exploring Qualitative Sharing Practices of Social 
Metadata:  Expanding the Attention Economy,” The Information 
Society, vol. 25(1), pp. 60 – 72, 2009.   
[20] T. Limbach, A. Krawczyk, and G. Surowiec, “Metadata  Lifecycle 
Management with GIS Context,” Proceedings of the 10th EC GI & 
GIS Workshop, Warsaw, Poland, 2004. 
[21] Federal Geographic Data Committee, “Content Standard for Digital 
Geosp atial Metadata Workbook.”  Washington D.C.:  Federal 
Geographic Data Committee, 2000.    
[22] United States Geological Survey, “Spatial Data Transfer Standard 
(SDTS): Logical specifications.”  Reston, Virginia:  United States 
Geologic Survey, 1997. 
[23] F. Salgé, “Semantic accuracy,”  In S.C. Guptill and J.L. Morrison 
(Eds.), Elements of Spatial Data Quality (pp. 139-152).  New York: 
Elsevier Science Ltd., 1995. 
[24] E. Epstein, “Litigation over information: The use and misuse of 
maps,”  Proceedings, IGIS: The Research Agenda 1 (pp. 177-184). 
Washington, D.C.: NASA, 1988. 
[25] Y. 
Bedard, 
“Uncertainties 
in 
land 
information 
systems 
databases,”  Proceedings of the 8th International Symposium on 
Computer Assisted Cartography (Auto Carto 8), (pp. 175-184),  
Baltimore, Maryland, 1987.   
[26] S. Aronoff, “Geographic Information Systems:  A Management 
Perspective,”  Ottawa:  WDL Publications,   1989.  
[27] E. Dalcin, “Data quality concepts and techniques applied to 
taxonomic databases,” Ph.D. Thesis, School of Biological Sciences, 
Faculty of Medicine, Health and Life Sciences, University of 
Southampton, 2004.  
[28] L. English, “Improving data warehouse and business information 
quality: Methods for reducing costs and increasing profits,” New 
York: John Wiley and Sons, 1999. 
[29] D. Wong and C. Wu, “Spatial Metadata and GIS for Decision 
Support,”  Proceedings of the Twenty-Ninth Hawaii International 
Conference, vol. 3 (3 – 6), pp. 557 – 566, 2006. 
[30] D. Lanter, “A Lineage Meta-Database Approach Towards Spatial 
Analytic Database Optimization,”  Cartography and Geographic 
Information Systems, vol. 20(2), pp.  112-121, 1993. 
[31] D. Lanter, “The Contribution of ARC/INFO's Log File to Metadata 
Analysis of GIS Data Processing,” Proceedings of the Fourteenth 
Annual ESRI User Conference, Palm Springs, California, 1994. 
[32] G. Giuliani, Y. Guigoz, P. Lacroix, N. Ray and A. Lehmann, 
“Facilitating the production of ISO-compliant metadata of geospaital 
datasets,” International Journal of Applied Earth Observation and 
Geoinformation, vol. 44, 23-243.   
[33] S. Trilles, L. Diaz and J. Huerta, “Approach to facilitating a 
geospatial data and metadata publication using a standard 
geoservice,” International Journal of Geo-Information, vol. 6(5), pp 
126. 
[34] C. Doctorow. Metacrap: Putting the Torch to Seven Straw-Men of the 
Meta-Utopia.  
[online]. 
 
Available 
from 
http://www.well.com/~doctorow/metacrap.htm. [retrieved February 
2018].     
[35] L. Qiu, G. Lingling, H. Feng and T. Yong,  “A unified metadata 
information management framework for the digital city,” Proceedings 
of IEEE’s Geoscience and Remote Sensing Symposium, pp. 4422–
4424, 2004 
[36] R. Devillers, Y. Bédard, and R. Jeansoulin, “Multidimensional 
management of geospatial data quality information for its dynamic 
use within Geographical Information Systems,” Photogrammetric 
Engineering and Remote Sensing, vol. 71(2), pp. 205–215, 2005. 
[37] R. Tolosana-Calsanz, J. Alvarez-Robles, J. Lacasta, J. Iso-Noguera, 
P. Muro-Medrano and F. Zarazaga-Soria.  “On the Problem of 
Identifying the Quality of Geographic Metadata” in J. Gonzalo, C. 
Thanos, M. Verdejo and R. Carrasco (Eds.), Research and Advanced 
Technology for Digital Libraries, ECDL, Lecture Notes in Computer 
Science, vol. 4172, Berlin:  Springer, 2006. 
[38] Federal Geographic Data Committee (FGDC), “Content Standard for 
Digital Geospatial Metadata Workbook,”  Washington D.C.:  Federal 
Geographic Data Committee, 2000.   
[39] Federal Geographic Data Committee (FGDC), “Content Standard for 
Digital Metadata:  Extensions for Remote Sensing Data,”  
Washington D.C.:  Federal Geographic Data Committee, 2002.      
[40] North Carolina Geographic Information Coordinating Council 
(NCGICC), North Carolina State and Local Government Metadata 
Profile for Geospatial Data and Services  [online].  Available from 
http://www.nconemap.gov/DiscoverGetData/Metadata.aspx#iso. 
[retrieved February 2018] 
[41] European Commission, Infrastructure for Spatial Information in 
Europe 
[online]. 
 
Available 
from 
https://inspire.ec.europa.eu/documents/geodcat-ap.  [Retrieved May 
2019]. 
[42] North Carolina Geographic Information Coordinating Council 
(NCGICC).  North Carolina OneMap [online].  Available from 
http://www.nconemap.gov. [retrieved February 2018].   
[43] Department of Environmental, Earth and Geospaital Sciences 
YouTube Portal.  Metadata Training [online].  Available from 
https://www.youtube.com/playlist?list=PL0JtGFJGnWTONvMk6dB
OXqIN6pTlmoj5o. [retrieved November 2018].   
[44] North Carolina Geographic Information Coordinating Council 
(NCGICC).  North Carolina OneMap [online].  Available from 
http://www.nconemap.gov. [retrieved February 2018].   
[45] T. Mulrooney, “Turning Data into Information:  Assessing and 
Reporting GIS Metadata Integrity Using Integrated Computing 
Technologies,” Greensboro, North Carolina:  University of North 
Carolina, Greensboro, 2009.   
[46] F. Davis, “Perceived Usefulness, Perceived Ease of Use and User 
Acceptance of Information Technology,” MIS Quarterly, vol. 13(3), 
pp.  319-340, 1989. 
[47] R. Bagozzi, F. Davis and P. Warshaw, “Development and Test of a 
Theory of Technological Learning and Usage,” Human Relations, 
vol. 45(7), pp. 660-686, 1992.  
[48] M. Fishbein and I. Ajzen,   1975.  Belief, Attitude Intention and 
Behavior: An Introduction to Theory and Research. Reading, MA: 
Addison-Wesley. 
[49] T. Stewart, “Task Fit, Ease-of-Use and Computer facilities,” In N. 
Bjørn-Andersen, K. Eason, and D. Robey (Eds.), Managing Computer 
Impact: An international Study of Management and Organizations 
(pp. 63-76), Norwood, NJ: Ablex, 1986. 
[50] R. Schultz and D. Slevin, 1975.  Implementation and Organizational 
Validity: An Empirical Investigation, New York:   American 
Elsevier, NY, 1975. 
[51] D. Robey, “User Attitudes and Management In- formation System 
Use,”  Academy of Management Journal, vol. 22(3), pp.  527- 538, 
1979.  
[52] A. Bandura,  “Self-Efficacy Mechanism in Human Agency,”   
American Psychologist, vol. 37(2), pp. 122-147, 1982. 
[53] D. Dimitrova and Y. Chen, “Profiling the Adopters of E-government 
Information 
and 
Services: 
The 
Influence 
of 
Psychological 
Characteristics, Civic Mindedness, and Information Channels,” Social 
Science Computer Review, vol. 24(2), pp. 172-188, 2006. 
[54] M. Masrom, “Technology Acceptance Model and E-learning,” In: 
12th International Conference on Education, May 21 - 24, Sultan 
Hassanal Bolkiah Institute of Education, Universiti Brunei 
Darussalam, 2007 

178
International Journal on Advances in Software, vol 12 no 1 & 2, year 2019, http://www.iariajournals.org/software/
2019, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
[55] M. Koufaris,  “Applying the Technology Acceptance Model and 
Flow Theory to Online Consumer Behavior,” Information Systems 
Research, vol. 13(2), pp. 205-223, 2002. 
[56] E. Hufnagel and C. Conca, “Use Response Data: The Potential for 
Errors and Biases,” Information Systems Research, vol. 5, pp. 48-73, 
1994. 
[57] Y. Malhotra and F. Galletta, “Extending the Technology Acceptance 
Model to Account for Social Influence: Theoretical Bases and 
Empirical Validation,” Proceeding of the 32nd Hawaii International 
Conference on System Sciences, 1999. 

