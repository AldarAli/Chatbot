194
International Journal on Advances in Networks and Services, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/networks_and_services/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Application-Aware Bandwidth Scheduling for Data Center Networks
Andrew Lester1
Yongning Tang2
Tibor Gyires2
1Cloud Networking Group. Cisco Systems, Inc. - San Jose, CA, USA
2School of Information Technology. Illinois State University. Normal, IL. USA
aeleste@cisco.com, ytang@ilstu.edu, tbgyires@ilstu.edu
Abstract—Recent study showed that many network
applications require multiple different network ﬂows
to complete their tasks. Provisioning bandwidth to
network applications other than individual ﬂows in
data center networks is becoming increasingly important
to achieve user satisfaction on their received network
services. Modern data center networks commonly adopt
multi-rooted tree
topologies. Equal-Cost
Multi-Path
(ECMP) forwarding is often used to achieve high link
utilization and improve network throughput. Meanwhile,
max-min fairness is widely used to allocate network
bandwidth
fairly
among
individual
network
ﬂows.
Today’s data centers usually host diverse applications,
which have various priorities (e.g., mission critical
applications) and service level agreements (e.g., high
throughput).
It
is
unclear
how
to
adopt
ECMP
forwarding and max-min fairness in the presence of
such requirements. In this paper, we ﬁrst propose
a ﬂow-based scheduling mechanism (called FlowSch)
to provide a prioritized Max-Min fair multiple path
forwarding to improve link utilization and improve
application performance. Then, we demonstrate and
discuss that FlowSch may not perform effectively when
network applications commonly use multiple network
ﬂows to accomplish their tasks. Accordingly, we design
an
application-aware
scheduling
mechanism
(called
AppSch) to tackle this challenge. AppSch can optimally
allocate
available
bandwidth
to
satisfy
application
requirements. Our performance evaluation results show
that FlowSch can improve ﬂow throughput 10-12% on
average and increase overall link utilization especially
when the total demanded bandwidth is close or even
exceeds the bisectional bandwidth of a data center
network. However, when most applications rely on
multiple
network
ﬂows,
AppSch
can
improve
link
utilization more effectively and reduce the application
completion time 36-58%.
Keywords-
application-aware; SDN;
max-min
fair;
scheduling.
I. INTRODUCTION
Elastic cloud computing is becoming pervasive
for many emerging applications, such as big data
online analysis, virtual computing infrastructure, and
various web applications. Various cloud applications
commonly share the same network infrastructure [2]
[4] [29] in a data center, and compete for the shared
resource (e.g., bandwidth). Many of these emerging
cloud
applications
are
complex
combinations
of
multiple services, and require predictable performance,
high availability, and high intra-data center bandwidth.
For example, Facebook “experiences 1000
times
more trafﬁc inside its data centers than it sends
to
and
receives
from
outside
users”,
and
the
internal
trafﬁc
has
increased
much
faster
than
Internet-facing bandwidth [40]. Meanwhile, many data
center networks are oversubscribed, as high as 40 :
1 in some Facebook data centers [41], causing the
intra-data center trafﬁc to contend for core bandwidth.
Hence, providing bandwidth guarantees to speciﬁc
applications is highly desirable, in order to preserve
their response-time predictability when they compete
for bandwidth with other applications.
The challenge of achieving high resource utilization
makes cloud service providers under constant pressure
to guarantee quality of service and increase customer
satisfaction.
A Data Center (DC) refers to any large, dedicated
cluster of computers that is owned and operated
by a single authority, built and employed for a
diverse
set
of
purposes.
Large
universities
and
private enterprises are increasingly consolidating their
Information Technology (IT) services within on-site
data centers containing a few hundred to a few
thousand servers. On the other hand, large online
service providers, such as Google, Microsoft, and
Amazon, are rapidly building geographically diverse

195
International Journal on Advances in Networks and Services, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/networks_and_services/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
cloud data centers, often containing more than 10,000
servers, to offer a variety of cloud-based services
such as web servers, storage, search, on-line gaming.
These service providers also employ some of their data
centers to run large-scale data-intensive tasks, such as
indexing Web pages or analyzing large data-sets, often
using variations of the MapReduce paradigm.
Many
data
center
applications
(e.g.,
scientiﬁc
computing,
web
search,
MapReduce)
require
substantial bandwidth. With the growth of bandwidth
demands for running various user applications, data
centers also continuously scale the capacity of the
network
fabric
for
new
all-to-all
communication
patterns, which presents a particular challenge for
traditional data forwarding (switching and routing)
mechanisms.
For
example,
MapReduce
based
applications, as a currently adopted default computing
paradigm for big data, need to perform signiﬁcant
data shufﬂing to transport the output of its map phase
before proceeding with its reduce phase. Recent
study shows the principle bottleneck in large-scale
clusters is often inter-node communication bandwidth.
Trafﬁc pattern study [27] showed that only a subset
(25% or less) of the core links often experience high
utilization.
The disruptive Software-Deﬁned Networking (SDN)
technology shifts today’s networks that controlled
by
a
set
of
vendor speciﬁc network
primitives
to a new network paradigm empowered by new
programmatic
abstraction.
OpenFlow
provides
a
protocol such that the logical centralized controller
can exploit forwarding tables on SDN switches for
programmatic multi-layer forwarding ﬂexibility. One
of the fundamental transformations that ﬂow based
forwarding presents is the inclusion of multi-layer
header information to make forwarding match and
action logic programmatically. Programmatic policy
is vital to manage the enormous combinations of
user requirements. For example, an SDN controller
can ﬂexibly deﬁne a network ﬂow using a tuple as
(incoming port, MAC Src, MAC Dst, Eth Type, VLAN
ID, IP Src, IP Dst, Port Src, Port Dst, Action), or
schedule speciﬁc ﬂows onto desired network paths.
With the new ﬂexibility and capability on network
trafﬁc manipulation empowered by SDN, various new
network architecture and control mechanisms have
been proposed for data center networks to optimize
their resource allocation.
Modern data center networks commonly adopt
multi-rooted tree topologies [2] [4] [29]. ECMP is
often used to achieve high link utilization and improve
network throughput. Meanwhile, max-min fairness is
widely used to allocate network bandwidth fairly
among multiple applications. Many current data center
schedulers, including Hadoops Fair Scheduler [37] and
Capacity Scheduler [35], Seawall [34], and DRF [38],
provide
max-min
fairness.
The
attractiveness
of
max-min fairness stems from its generality. However,
today’s data centers usually host diverse applications,
which have various priorities (e.g., mission critical
applications) and service level agreements (e.g., high
throughput). It is unclear how to
adopt ECMP
forwarding and max-min fairness in the presence of
such requirements.
In this paper, we ﬁrst propose a Flow-based
Scheduling mechanism (called FlowSch) to provide
a prioritized Max-Min fair multiple path forwarding
to improve link utilization and ﬂow-based throughput.
FlowSch can optimally allocate current available
bandwidth to
satisfy
user demands speciﬁed by
per ﬂow. When predeﬁned user requirements are
available, FlowSch can prioritize current demands and
allocate available bandwidth accordingly. Then, we
demonstrate and discuss that FlowSch may not perform
effectively
when
network
applications
commonly
use
multiple
network
ﬂows
to
accomplish
their
tasks. Accordingly, we design an Application-Aware
Scheduling mechanism (called AppSch) to tackle
this challenge. Our evaluation shows that AppSch
can optimally allocate available bandwidth to satisfy
application requirements.
The rest of the paper is organized as the following.
Section
II
discusses
the
related
research
work.
Section III describes FlowSch. Section IV formalizes
the application-aware scheduling problem and presents
our solution AppSch. Section V presents our simulation
design and results, respectively. Finally, Section VI
concludes the paper with future directions.
II. RELATED WORK
Current large data center networks connect multiple
Ethernet LANs using IP routers and run scalable
routing algorithms over a number of IP routers. These
layer 3 routing algorithms allow for shortest path
and ECMP routing, which provide much more usable
bandwidth than Ethernets spanning tree. However, the

196
International Journal on Advances in Networks and Services, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/networks_and_services/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Core
Aggregation
Edge
Pod1
Pod2
Pod3
Pod4
S11
S22
P1
P2
P3
P4
L1
L2
L3
L4
L5
L6
App 
Servers
Figure 1. Fat tree topology.
mixed layer 2 and layer 3 solutions require signiﬁcant
manual conﬁguration.
The trend in recent works to address these problems
is to introduce special hardware and topologies.
For example, PortLand [4] is implementable on Fat
Tree topologies and requires ECMP hardware that is
not available on every Ethernet switch. TRILL [5]
introduces a new packet header format and thus
requires new hardware and/or ﬁrmware features.
There have been many recent proposals for scale-out
multi-path
data
center
topologies,
such
as
Clos
networks [6] [8], direct networks like HyperX [9],
Flattened Butterﬂy [11], DragonFly [12], etc., and even
randomly connected topologies have been proposed in
Jellyﬁsh [16].
Many
current
proposals
use
ECMP-based
techniques,
which
are
inadequate
to
utilize
all
paths, or to dynamically load balance trafﬁc. Routing
proposals for these networks are limited to shortest
path routing (or K-shortest path routing with Jellyﬁsh)
and end up under utilizing the network, more so
in the presence of failures. While DAL routing [9]
allows deroutes, it is limited to HyperX topologies. In
contrast, Dahu [29] proposes a topology-independent,
deployable solution for non-minimal routing that
eliminates routing loops, routes around failures, and
achieves high network utilization.
Hedera [17] and MicroTE [22] propose a centralized
controller to schedule long lived ﬂows on globally
optimal paths. However, they operate on longer time
scales and scaling them to large networks with many
ﬂows is challenging. Techniques like Hedera, which
select a path for a ﬂow based on current network
conditions, suffer from a common problem: when
network conditions change over time the selected
path may no longer be the optimal one. While
DevoFlow [24] improves the scalability through switch
hardware changes, it does not support non-minimal
routing or dynamic hashing. Dahu can co-exist with
such techniques to better handle congestion at ﬁner
time scales.
MPTCP [19] proposes a host based approach for
multi-path load balancing by splitting a ﬂow into
multiple sub ﬂows and modulating how much data
is sent over different subﬂows based on congestion.
However, as a transport protocol, it does not have
control over the network paths taken by subﬂows.
Dahu [29] exposes the path diversity to MPTCP and
enables MPTCP to efﬁciently utilize the non-shortest
paths in a direct connect network. There have also
been proposals that employ variants of switch-local
per-packet trafﬁc splitting [30].
Trafﬁc engineering has been well studied in the
context of wide area networks. TeXCP [31] and
REPLEX [32] split ﬂows on different paths based on
load. However, their long control loops make them
inapplicable in the data center context that requires
faster response times to deal with short ﬂows and
dynamic trafﬁc changes. PDQ [10] and pFabric [36]
can support a scheduling policy like shortest ﬂow ﬁrst

197
International Journal on Advances in Networks and Services, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/networks_and_services/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
(SFF), which minimizes ﬂow completion times by
assigning resources based on ﬂow sizes. FLARE [11]
exploits the inherent burstiness in TCP ﬂows to
schedule “ﬂowlets” (bursts of packets) on different
paths to reduce extensive packet reordering.
In our previous work [1], a ﬂow-based bandwidth
scheduling approach has been introduced. Several
recent work [3] [7] [14] contributed to task-Aware
Schedulers and network Abstractions. Orchestra [13]
and CoFlow [7] argued for bringing task awareness in
data centers. Orchestra focuses on how task awareness
could provide beneﬁts for MapReduce style workloads,
and focuses on improvement in the average task
completion time for batched workload. Baraat [3]
makes the scheduling decisions in a decentralized
fashion based on a revised FIFO mechanism. Baraat
can also improve the tail completion time, for dynamic
scenarios and multi-stage workloads. CoFlow [7]
focuses on a new abstraction that can capture rich
task semantics, which is orthogonal to Baraats focus
on scheduling policy and the underlying mechanism.
However, beyond the abstraction, CoFlow does not
propose any new scheduling policy or mechanism to
achieve task-awareness.
III. FLOW-BASED PRIORITIZED MAX-MIN FAIR
BANDWIDTH SCHEDULING
While
ECMP
is
often
used
to
achieve
high
link utilization, max-min fairness is widely used to
allocate network bandwidth fairly among multiple
applications. However, today’s data centers usually
host diverse applications, which have various priorities
(e.g., mission critical applications) and service level
agreements (e.g., high throughput). It is unclear how
to adopt ECMP forwarding and max-min fairness in the
presence of such requirements. We propose Prioritized
Max-Min Fair Multiple Path forwarding (FlowSch)
to tackle this challenge. In the following, we ﬁrst
formalize the problem, and then present how FlowSch
works.
A. Problem Formalization
Consider a data center network with K-ary fat-tree
topology as shown in Fig.1, composed of a set of core
switches Sc, a set of aggregation switches Sa, a set of
edge switches Se, and a set of hosts H. Each switch
has k-port. There are k pods. Each pod contains k/2
aggregation switches and k/2 edge switches. In each
Input: A list of tasks {Ti}; current link utilization
U(Lj)
Output: Path assignment PA with PAi for each task Ti
1: Sort {Ti} based on their priority levels Ki
2: Start from the highest priority W = m /*m is the
highest priority level*/
3: for all Ti! = ∅ (PL(Ti) = W) do
4:
/*The function PL() returns the priority level
of a given task*/
5:
Find all paths for each task Ti
6:
Assign a unit bandwidth (UB) to the least
utilized path for each task /*we choose UB =
100Kbps*/
7:
PAi ← {Ti, {Pi}}
8:
PA ← PA ∪ {PAi}
9:
if A path P is saturated and P ∈ APL(Ti) then
10:
APL(Ti) ← APL(Ti) − P
11:
end if
12:
if APL(Ti) == ∅ then
13:
Remove Ti
14:
end if
15:
if ({Ti} == ∅) and (W > 1) then
16:
W = m − 1
17:
end if
18: end for
19: return PA
Figure 2. Multi-Level progressive ﬁlling algorithm
pod, each k-port edge switch is directly connected to
k/2 hosts and k/2 aggregation switches. The ith port of
each core switch si ∈ Sc(i ∈ [1, (k/2)2]) is connected
to pod i [4]. We assume all links (e.g., L1 in Fig.1)
have the same bandwidth for both uplink (e.g., Lu
1) and
downlink (e.g., Ld
1) connections.
Recent study [27] showed that less than 25% of
the core links have been highly utilized while packet
losses and congestions may still often occur. In this
paper, we only focus on inter-pod network trafﬁc that
requires bandwidth from core links. We denote all links
between aggregation and core layers as a set Lac, all
links between edge and aggregation layers as a set Lea,
and all links between application server and edge layers
as a set Lse. Generally, in a network with K-ary fat-tree
topology , there are k paths between any two hosts
from different pods.

198
International Journal on Advances in Networks and Services, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/networks_and_services/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
 0
 20
 40
 60
 80
 100
1
10
20
40
80
Performance Improvement to FS (%)
Number of Flows per Application
Flow-based vs. Application-based
SFF
FlowSch
Greedy
Figure 3. Completion time comparison
A network task Ti is speciﬁed by a source and
destination hosts (e.g., S11 and S22) and the expected
trafﬁc volume. We also consider each task with
different priority level wi. Here, wi ∈ [1, m] with
the lowest and highest priority levels as 1 and
m, respectively. A network scheduler modular (also
simply referred to as scheduler) on an SDN controller
needs to decide how to allocate available bandwidth
to maximally satisfy the application requirements. We
deﬁne a valid Network Path Assignment PAi for a
given task Ti is a set of paths and their corresponding
allocated bandwidths connecting the source to the
destination (e.g., a subset of {P1, P2, P3, P4}), in
which each path consists of a list of directional links
(e.g., P1 = {Lu
1, Lu
2, Lu
3, Ld
4, Ld
5, Ld
6}) connecting the
source to the destination hosts. Here, Lu
1, Ld
6 ∈ Lse;
Lu
2, Ld
5 ∈ Lea; Lu
3, Ld
4 ∈ Lac.
There is a variety of applications on a data center
network, which have different service requirements
regarding throughput, packet loss, and delay. For
our
analysis,
we
characterize
the
applications’
requirements through their priority levels, which can
be the output of some utility function. Priorities can
offer a basis for providing application and business
oriented service to users with diverse requirements. We
consider a model where the weight associated with the
different priority classes is user-deﬁnable and static.
Users can freely deﬁne the priority of their trafﬁc, but
are charged accordingly by the network. We aim to
study the bandwidth-sharing properties of this priority
scheme. Given a set of network tasks T = {Ti} (i ≥ 1)
and their corresponding priority levels K = {Ki}, we
consider a Network Path Assignment problem is to ﬁnd
a set of path assignment PA = {PAi} to satisfy the
condition of Prioritized Max-Min Fairness.
Deﬁnition
1. Prioritized
Max-Min
Fairness A
feasible path assignment PAx is “prioritized max-min
fair” if and only if an increase of any path bandwidth
within the domain of feasible bandwidth allocations
must be at the cost of a decrease of some already
less allocated bandwidth from the tasks with the
same or higher priority level. Formally, for any
other feasible bandwidth allocation scheme PAy, if
BW(PAy
Ti) > BW(PAx
Ti), then it decreases the
allocated bandwidth of some other path with the same
or higher priority level. Here, BW(PAy
Ti) is the total
allocated bandwidth for the task Ti in the bandwidth
allocation scheme PAy.
Deﬁnition 2. Saturated Path A path Pi is saturated
if at least one bottleneck link Lj exists on the path Pi.
A link is bottlenecked if the total assigned bandwidth
on this link from the given tasks is more than or
equal to the maximum bandwidth of the link. Formally,
a bottleneck link is the one that P
i BWTi(Lj) ≥
BWmax(Lj).
B. The Algorithm of Multi-Level Progressive Filling
The
network
tasks
can
be
dynamically
and
continuously generated, and submitted to the scheduler.
In FlowSch, the scheduler can periodically query all
network switches to collect current link utilizations.
Once a new task list received, the scheduler will use
a practical approach called “progressive ﬁlling” [33]
provisioning available bandwidth that results in a
prioritized
max-min
fair
allocation following the
priority order from the highest to the lowest priority
level. The idea is shown in Fig. 2: The scheduler
starts with all provisioned bandwidth equal to 0 and
increases all bandwidths together at the same pace
for the tasks with the same priority level until one
or several saturated paths are found. The bandwidth
for the corresponding tasks that use these paths are
not increased any more and the scheduler continue
increasing the bandwidth for other tasks on the same
priority level. All the tasks that are stopped have a
saturated path. The algorithm continues until it is not
possible to increase the bandwidth for the tasks at
certain priority level. Then, the algorithm moves to
the next priority level and repeats the same bandwidth
provisioning operations until all tasks are assigned

199
International Journal on Advances in Networks and Services, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/networks_and_services/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
 0
 0.2
 0.4
 0.6
 0.8
 1
 5
 10
 15
 20
 25
 30
 35
 40
Link Utilization
Sequence Number of all Links
Higher Priority for Long Flows
Max-Min
Random
(a)
 0
 0.2
 0.4
 0.6
 0.8
 1
 5
 10
 15
 20
 25
 30
 35
 40
Link Utilization
Sequence Number of all Links
Higher Priority for Small Flows
Max-Min
Random
(b)
 0
 0.2
 0.4
 0.6
 0.8
 1
 5
 10
 15
 20
 25
 30
 35
 40
Link Utilization
Sequence Number of all Links
Higher Priority for Mixed Flows
Max-Min
Random
(c)
Figure 4. Path utilization with higher priority for (a) long ﬂows (b)short ﬂows (c)mixed ﬂows
 0
 0.2
 0.4
 0.6
 0.8
 1
 5
 10
 15
 20
 25
 30
 35
 40
Link Utilization
Sequence Number of Aggregation Links
Higher Priority for Long Flows
Max-Min
Random
(a)
 0
 0.2
 0.4
 0.6
 0.8
 1
 5
 10
 15
 20
 25
 30
 35
 40
Link Utilization
Sequence Number of Aggregation Links
Higher Priority for Short Flows
Max-Min
Random
(b)
 0
 0.2
 0.4
 0.6
 0.8
 1
 5
 10
 15
 20
 25
 30
 35
 40
Link Utilization
Sequence Number of Aggregation Links
Higher Priority for Mixed Flows
Max-Min
Random
(c)
Figure 5. Aggregation link utilization with higher priority for (a) long ﬂows (b)short ﬂows (c)mixed ﬂows
to some paths. The algorithm terminates because the
total paths and tasks are ﬁnite. When the algorithm
terminates all tasks have been served at some time
and thus have a saturated path. By Deﬁnition 1 the
allocation is max-min fair for the tasks at the same
priority level.
min
|A|
X
k=1
Tk
(1)
such that:
|P |
X
j=1
xk
ij = 1
(2)
xk
ij ∈ {0, 1}
(3)
Bj ∈ {0, 1}
(4)
IV. APPLICATION-BASED SCHEDULING
Recent study [38] showed that 70% of applications
involve 30-100 ﬂows, 2% involve more than 150 ﬂows.
In a multi-ﬂow based application, the application ﬂows
may traverse different parts of the network and not
all of them may be active at the same time. Only
after all these related ﬂows ﬁnish, the corresponding
application ﬁnishes and the user gets a response.
The distributed nature and scale of data center
applications results in rich and complex work ﬂows.
Typically, these applications run on many servers
that, in order to respond to a user request, process
data and communicate across the internal network.
Traditionally, allocation of network bandwidth has
targeted per-ﬂow fairness. Because latency is the
primary goal for many data center applications, recent
proposals [21] [36] indicate that per-ﬂow fairness
scheduling that optimizes ﬂow-level metrics (e.g.,
minimizing ﬂow completion time) may not necessarily
improve user perceivable application performance.
Typical data center application applications can have
many ﬂows, potentially of different sizes. Flow-based
scheduling presents some inefﬁciency when applied to
applications relying on multiple ﬂows.
In
the
following,
we
ﬁrst
present
the
result
of
a
simple
simulation
that
demonstrates
the
different
effect
on
application
performance
(i.e.,
completion time) improvement with three different
scheduling algorithms, namely Shortest Flow First

200
International Journal on Advances in Networks and Services, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/networks_and_services/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
(SFF), FlowSch, and an off-line greedy scheduling
algorithm (Greedy). Then, we tackle the inefﬁciency
of
ﬂow-based
scheduling
by
introducing
a
new
application-aware scheduling approach called AppSch.
A. Flow-based vs. Application-based Scheduling
FlowSch is designed as an application agnostic
scheduler
that
targets
per-ﬂow
fairness
among
applications
with
the
same
priority,
which
may
not improve the performance of multi-ﬂow based
applications.
We
validate
this
through
a
simple
simulation
that compares performance improvement in terms
of application completion times with three different
approaches,
namely
Shortest
Flow
First
(SFF),
FlowSch, and an off-line greedy scheduling algorithm
(Greedy). SFF schedules the shorter ﬂows of every task
ﬁrst, leaving longer ﬂows to the end. This can hurt
application performance by delaying completion of
tasks. FlowSch considers max-min fair sharing among
all ﬂows that allocates resources with a lower bound.
Greedy is an off-line greedy algorithm searching for
the “best” assignments for all ﬂows, which also shows
the room for improvement.
In this simulation, we use a simple single-stage
partition-aggregate work ﬂow scenario [7] with 60
applications comprising ﬂows uniformly chosen from
the range [5, 40] KB. Fig. 5 shows SFFs improvement
over fair-sharing as a function of the number of
ﬂows in a application. If an application has just a
single ﬂow, SFF reduces the application completion
time by almost 40%. However, as we increase the
number of ﬂows per application, the beneﬁts reduce.
The same observation also occurred with FlowSch,
which however, outperforms SFF due to its max-min
sharing. Comparing to the “best” scheduling offered by
the off-line greedy scheduling algorithm, application
agnostic ﬂow-based scheduling does not perform well
in terms of the improvement on application completion
time, comparing to the performance of an off-line
application-aware scheduler referred to as Greedy in
Fig. 5.
B. Application Requirements Abstraction
Although
many
data-intensive
applications
are
network-bound [13] [17], network scheduling remains
agnostic to application speciﬁc network requirements.
In recent work, Coﬂow [7] argues for tasks (or
Coﬂows) as a ﬁrst-order abstraction for the network
data plane to compensate the mismatch that often
affects
application-level
performance,
even
when
network-oriented metrics like ﬂow completion time
(FCT) or fairness improve. The recently proposed
coﬂow abstraction [7] represents such collections of
parallel ﬂows to convey application-speciﬁc network
requirements, for example, minimizing completion
time or meeting a deadline to the network and enables
application-aware network scheduling.
Allowing applications to expose their semantics
to the network could signiﬁcantly help the network
optimize its resource allocation for application-level
metrics. For example, allocating network bandwidth
to applications in a FIFO fashion, such that they are
scheduled over the network one at a time, can improve
the average application completion time as compared
to per-ﬂow fair sharing (e.g., TCP).
In this paper, we characterize two features of
application tasks in todays data centers: 1) the
task size, and 2) the number of ﬂows per task.
Both
information
are
critical
when
considering
application-aware scheduling for the network; the
ﬁrst
inﬂuences the
scheduling
policy,
while
the
latter
governs
when
application-aware
scheduling
outperforms ﬂow-based scheduling.
In the following, we formalize the application-aware
scheduling as a variant bin packing problem, and
presents a heuristic algorithm to tackle this NP-hard
problem.
C. Bin Packing with Varying Capacities
We assume that the amount of data each ﬂow in
an application needs to transfer is known before it
starts [13] [23] [36]. Analysis of production application
traces [14] shows wide variations in application ﬂow
characteristics in terms of total size, the number of
parallel ﬂows, and the size of individual ﬂows. But
commonly these applications can be modeled as an
ordered ﬂow request list.
Let A be a list of applications Ai to be scheduled.
Each application Ai has a list of ﬂow volumes Vi =
{vi1, · · · , vik}. Let P
= {P1, · · · , Pm} be the set
of available network paths and let Bi be the current
available bandwidth for path Pi. Without any loss of
generality, we assume that the bandwidths associated
with the network paths are integers.

201
International Journal on Advances in Networks and Services, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/networks_and_services/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
 0
 0.2
 0.4
 0.6
 0.8
 1
 5
 10
 15
 20
 25
 30
 35
 40
Link Utilization
Sequence Number of Core Links
Higher Priority for Long Flows
Max-Min
Random
(a)
 0
 0.2
 0.4
 0.6
 0.8
 1
 5
 10
 15
 20
 25
 30
 35
 40
Link Utilization
Sequence Number of Core Links
Higher Priority for Short Flows
Max-Min
Random
(b)
 0
 0.2
 0.4
 0.6
 0.8
 1
 5
 10
 15
 20
 25
 30
 35
 40
Link Utilization
Sequence Number of Core Links
Higher Priority for Mixed Flows
Max-Min
Random
(c)
Figure 6. Core link utilization with higher priority for (a) long ﬂows (b)short ﬂows (c)mixed ﬂows
We
deﬁne
the
path-selection
variables
S
=
(s1, · · · , sm), where sj = 1 if path pj is selected and
sj = 0, otherwise; and the ﬂow-to-path assignment
variables xk
ij, where xk
ij
=
1 if ﬂow fik from
application Ak is scheduled on path Pj and xk
ij = 0,
otherwise. We want to schedule all application related
ﬂows to optimally utilize all available bandwidth, so
as to minimize the total application completion time
(T). For application Ak, the corresponding application
completion time Tk = Pm
j=1 vij/(xk
ij ∗ Bj).
The objective function (1) minimizes the total
amount of application completion time. Constraint (2)
ensure that each ﬂow request is assigned exactly to one
network path; and constraints (3) and (4) enforce the
integrality requirements for all decision variables.
The scheduling policy determines the order in which
applications are scheduled across the network paths.
Determining an ordering that minimizes application
completion time can be easily reduced to a bin packing
problem with varying bin sizes, which is NP-hard.
Some previous similar work like ﬂow-shop scheduling
[14] [25], is considered as one of the hardest NP-hard
problems, with exact solutions not known for even
small instances of the problem [15]. Thus, we need
to consider heuristic scheduling policies. The heuristic
policy should help reduce both the average as well as
tail application completion time. Guided by ﬂow-based
policies that schedule ﬂows one at a time [17], we
consider serving applications one at a time. This can
help ﬁnish applications faster by reducing the amount
of contention in the network. Consequently, we deﬁne
application packing as the set of policies where an
entire application is scheduled before moving to the
next.
Speciﬁcally,
for
such
a
bin
packing
NP-hard
problem [18], we design a heuristic algorithm called
AppSch
that
adapts
the
well-known
Best
First
Decreasing loading heuristic [15] and extends a
number of fundamental concepts [15] [18] in the
bin covering and knapsack methodology. AppSch
ﬁrst sorts all application ﬂow requests according to
the non-increasing order of their data sizes, and
then sequentially assigns them into the path with
the maximum available bandwidth. For each ﬂow
request, AppSch ﬁrst attempts to assign it into the
“best” already-selected path to increase the path
utilization. If the ﬂow request cannot be assigned to an
already-selected path, a new path is selected and the
ﬂow request is assigned to it. One challenge in this
problem different from classic bin packing problem
where all bins are homogeneous, is how to choose a
new path when required. Inspired by the item-selection
rule for knapsack problems, we select paths according
to the non-increasing order of the ratios of their
data sizes and available path bandwidths, and in the
non-decreasing order of their data sizes when the data
sizes are equal.
V. EVALUATION
In this section, we present our evaluation metrics,
methodology and evaluation results.
A. Data Center Network Trafﬁc Pattern
Several recent studies [26] [27] [28] have been
conducted
in
various
data
center
networks
to
understand network trafﬁc patterns. The studied data
center networks include university campus, private
enterprise
data
centers,
and
cloud
data
centers
running Web services, customer-facing applications,
and intensive Map-Reduce jobs. The studies have

202
International Journal on Advances in Networks and Services, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/networks_and_services/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
 0
 2
 4
 6
 8
 10
 5
 10
 15
 20
 25
 30
Throughput (Mbps)
Flow Sequence Number
Higher Priority for Long Flows
Max-Min
Random
(a)
 0
 2
 4
 6
 8
 10
 5
 10
 15
 20
 25
 30
Throughput (Mbps)
Flow Sequence Number
Higher Priority for Short Flows
Max-Min
Random
(b)
 0
 2
 4
 6
 8
 10
 5
 10
 15
 20
 25
 30
Throughput (Mbps)
Flow Sequence Number
Higher Priority for Mixed Flows
Max-Min
Random
(c)
Figure 7. Throughput with higher priority for (a) long ﬂows (b)short ﬂows (c)mixed ﬂows
 0
 20
 40
 60
 80
 100
1
20
40
80
120
160
200
Performance Improvement to FS (%)
Number of Flows per Application
Web-Service Applications
AppSch
FIFS
CoFlow
(a)
 0
 20
 40
 60
 80
 100
1
20
40
80
120
160
200
Performance Improvement to FS (%)
Number of Flows per Application
Batch-request Applications
AppSch
FIFS
CoFlow
(b)
 0
 20
 40
 60
 80
 100
1
20
40
80
120
160
200
Performance Improvement to FS (%)
Number of Flows per Application
Cluster-computing Applications
AppSch
FIFS
CoFlow
(c)
Figure 8. Application completion time for (a) Web Services (b) Batch Requests (c) Cluster Computing
shown some interesting facts: (1) The majority of
the trafﬁc in data center networks is TCP ﬂows. (2)
Most of the server generated trafﬁc in the cloud data
centers stays within a rack, while the opposite is
true for campus data centers. (3) At the edge and
aggregation layers, link utilizations are fairly low and
show little variation. In contrast, link utilizations at
the core network are high with signiﬁcant variations
over the course of a day. (4) In some data centers, a
small but signiﬁcant fraction of core links appear to
be persistently congested, but there is enough spare
capacity in the core to alleviate congestion. (5) Losses
on the links that are lightly utilized on the average
can be attributed to the bursty nature of the underlying
applications run within the data centers.
B. Methodology and Metrics
In our experiments, we simulate a data center with a
fat-tree topology. We implemented FlowSch based on
RipL [39], a Python library that simpliﬁes the creation
of data center code, such as OpenFlow network
controllers, simulations, or Mininet topologies. We
compared FlowSch scheduler with a commonly used
randomization based scheduling method.
In our evaluation, we use three different priority
policies for a mixture of trafﬁc patterns: (1) high
priority for long TCP ﬂows with the total data size
between 1MB and 100MB; (2) high priority for short
TCP ﬂows with the total data size between 10KB
and 1MB; (3) high priority for random selected ﬂows
including both short and long ones referred to as mixed
TCP ﬂows.
We focus on two performance metrics: (1) Link
Utilization
that demonstrates how effectively the
scheduler utilizes the network bandwidth. Intuitively,
when there are high bandwidth demands from user
applications, the overall link and path utilizations
should be kept in high. (2) Network throughput that
shows how efﬁciently the network serves different
applications.
For evaluating the performance of application-aware
scheduler AppSch, we setup different application
scenarios
to
mimics
(1)
web-service:
a
typical
web-service scenario with one pod dedicated to the
front-end nodes, while the other pods are used as
caching back-end. For the experiment, we consider an

203
International Journal on Advances in Networks and Services, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/networks_and_services/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
online scenario where each user independently receives
requests based on a Poisson arrival process. Each
request (or application) corresponds to a multi-get that
involves fetching data from randomly chosen back-end
servers; (2) batched requests: to evaluate the impact of
varying the number of concurrent applications in the
system. For this experiment, one pod acts as a client
while the other pods in the network act as storage
servers. For the request, the client retrieves 100 − 800
KB chunks from each of the servers. The request
ﬁnishes when data is received from all servers; and
(3) cluster computing: our workload is based on a
Hive/MapReduce trace collected from a tier-1 ISP IDS
system. We consider jobs with non-zero shufﬂe and
divide them into bins based on the fraction of their
durations spent in shufﬂe.
C. Link Utilization
We created 16 test scenarios to evaluate FlowSch
with different inter-pod trafﬁc patterns. We ran 5
tests for each scenarios. In all test scenarios, the
test trafﬁc traversed all edge, aggregation, and core
links. The results of multiple test runs from the same
test scenario present similar results. In the following,
we only report the result of one test run for each
test scenario that created trafﬁc between two pods
in both directions. Under the same three different
priority policies, Fig.4(a)∼(c) shows the overall path
utilization; Fig.5(a)∼(c) shows the aggregation link
utilizations; and Fig.6(a)∼(c) shows the core link
utilizations. Comparing to the randomization based
scheduler, our algorithm 2 achieves high utilization
on path level, aggregation and core link
levels
by: (1) dynamically observing all link utilization
status, and (2) progressively ﬁlling the jobs of the
same priority with the available bandwidth with the
max-min fairness. The average gain on utilization is
approximately improved from 59% to 66%. Note that
with the increase of link utilization, idle bandwidth
can be effectively utilized by demanding network
applications, which can correspondingly improve their
performance by reducing their network latencies.
D. Network Throughput
Once the overall utilization can be increased, we
expect that the overall application throughput should
also be improved. The experiment results presented
some interesting results as shown in Fig.7(a)∼(c).
When we emulate more realistic application scenarios,
where short and long TCP ﬂows are randomly mixed
together, our FlowSch scheduler obviously outperforms
the performance of the random scheduler with about
10-12% improvement. In the scenario of the different
policies favoring either short or long ﬂows, our
scheduler adopts max-min fairness, and thus, the
average throughput has been improved from 2.52Mbps
in the random scheduler and to 3.46Mbps in the
max-min scheduler.
E. Application Completion Time
The minimum completion time of an application
Ai can be attained as long as all ﬂows in the
application
ﬁnish
at
time
Ti.
We
choose
three
applications (1) Web service, (2) Batch request, and
(3) Cluster computing application that represent typical
application communication patterns in today’s data
center networks to validate AppSch and compare with
FIFS and CoFlow [7]. From the experiment results
as shown in Fig. 8, AppSch performs steadily with
evident application performance (completion time)
improvement (36%-58%) for all different types of
applications.
VI. CONCLUSION
The role of the data center network is becoming
ever more crucial today, which is evolving into
the
integrated
platform
for
next-generation
data
centers. Because it is pervasive and scalable, the
data center network is developing into a foundation
across which information, application services and
all data center resources, including servers, storage
are shared, provisioned, and accessed. Modern data
center networks commonly adopt multi-rooted tree
topologies. ECMP is often used to achieve high
link utilization and improve network throughput.
Meanwhile, max-min fairness is widely used to
allocate network bandwidth fairly among multiple
applications. However, today’s data centers usually
host diverse applications, which have various priorities
(e.g., mission critical applications) and service level
agreements (e.g., high throughput). It is unclear how
to adopt ECMP forwarding and max-min fairness in the
presence of such requirements. We propose Prioritized
Max-Min Fair Multiple Path forwarding (FlowSch) to
tackle this challenge. FlowSch can prioritize current
demands and allocate available bandwidth accordingly.

204
International Journal on Advances in Networks and Services, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/networks_and_services/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Our performance evaluation results show that FlowSch
can improve application throughput 10-12% on average
and increase overall link utilization especially when the
total demanded bandwidth close or even exceed the
bisectional bandwidth of a data center network.
REFERENCES
[1] A. Lester, Y. Tang, T. Gyires. “Prioritized Adaptive
Max-Min Fair Residual Bandwidth Allocation for
Software-Deﬁned
Data
Center
Networks,”
In
the
Thirteenth International Conference on Networks (ICN),
2014.
[2] M. Al-Fares, A. Loukissas, and A. Vahdat, A Scalable,
“Commodity Data Center Network Architecture,” In
ACM conference of the Special Interest Group on Data
Communication (SIGCOMM), 2008
[3] F. R. Dogar, T. Karagiannis, H. Ballani, and A.
Rowstron, “Decentralized Task-Aware Scheduling for
Data
Center
Networks,”
In
ACM
conference
of
the Special Interest Group on Data Communication
(SIGCOMM), 2014
[4] R. N. Mysore, A. Pamboris, N. Farrington, N. Huang, P.
Miri, S. Radhakrishnan, V. Subramanya, and A. Vahdat,
“PortLand: A scalable fault-tolerant layer 2 data center
network fabric,” In ACM conference of the Special
Interest Group on Data Communication (SIGCOMM),
2009
[5] R.
Perlman,
“Rbridges:
Transparent
routing,”
In
IEEE
Conference
on
Computer
Communications
(INFOCOM), 2004.
[6] A. Greenberg, J. R. Hamilton, N. Jain, S. Kandula, C.
Kim, P. Lahiri, D. A. Maltz, P. Patel, and S. Sengupta,
“VL2: A Scalable And Flexible Data Center Network,”
In ACM conference of the Special Interest Group on
Data Communication (SIGCOMM), 2009.
[7] M. Chowdhury and I. Stoica, “Coﬂow: A networking
abstraction for cluster applications,” In ACM Hot Topics
in Networks (HotNets) workshops, 2012.
[8] V.
Liu,
D.
Halperin,
A.
Krishnamurthy,
and
T.
Anderson, “F10: A Fault-Tolerant Engineered Network,”
In USENIX Symposium on Networked Systems Design
and Implementation (NSDI), 2013
[9] J. H. Ahn, N. Binkert, A. Davis, M. McLaren, and R. S.
Schreiber, “HyperX: Topology, Routing, and Packaging
of Efﬁcient Large-Scale Networks,” In ACM Conference
on High Performance Computing Networking, Storage
and Analysis, 2009.
[10] C. Hong, M. Caesar, and P. Godfrey, “Finishing ﬂows
quickly with preemptive scheduling,” ACM SIGCOMM
Computer Communication Review (CCR), 2012.
[11] J.
Kim,
W.
J.
Dally,
and
D.
Abts,
“Flattened
butterﬂy: A Cost-efﬁcient Topology for High-radix
networks,”
In
ACM
International
Symposium
on
Computer Architecture (ISCA), 2007.
[12] J.
Kim,
W.
J.
Dally,
S.
Scott,
and
D.
Abts,
“Technology-Driven,
Highly-Scalable
Dragonﬂy
Topology,”
In
ACM
International
Symposium
on
Computer Architecture (ISCA), 2008
[13] M. Chowdhury, M. Zaharia, J. Ma, M. I. Jordan, and
I. Stoica, “Managing data transfers in computer clusters
with Orchestra,” In ACM conference of the Special
Interest Group on Data Communication (SIGCOMM),
2011.
[14] M. Chowdhury, Y Zhong, and I. Stoica, “Efﬁcient
Coﬂow Scheduling with Varys,” In ACM conference
of the Special Interest Group on Data Communication
(SIGCOMM), 2014.
[15] W T Rhee and M Talagrand, “Optimal bin covering
with
items
of
random
size,”
SIAM
Journal
on
Computing. 1989.
[16] A. Singla, C.-Y. Hong, L. Popa, and P. B. Godfrey,
“Jellyﬁsh: Networking Data Centers Randomly,” In
USENIX Symposium on Networked Systems Design
and Implementation (NSDI), 2012.
[17] M. Al-Fares, S. Radhakrishnan, B. Raghavan, N.
Huang,
and
A.
Vahdat,
“Hedera:
Dynamic
Flow
Scheduling for Data Center Networks,” In USENIX
Symposium
on
Networked
Systems
Design
and
Implementation (NSDI), 2010.
[18] A. S. Fukunaga and R. E. Korf, “Bin Completion
Algorithms for Multicontainer Packing, Knapsack, and
Covering Problems,” Journal of Artiﬁcial Intelligence
Research 28 (2007) pp. 393 ∼ 429
[19] M. Alizadeh, A. Greenberg, D. A. Maltz, J. Padhye,
P. Patel, B. Prabhakar, S. Sengupta, and M. Sridharan,
“Data center TCP (DCTCP),” In ACM conference of
the Special Interest Group on Data Communication
(SIGCOMM), 2010.
[20] T. Benson, A. Akella, and D. A. Maltz, “Network
Trafﬁc Characteristics of Data Centers in the Wild,” In
ACM Internet Measurement Conference (IMC), 2010.
[21] C.
Wilson,
H.
Ballani,
T.
Karagiannis, and
A.
Rowstron, “Better never than late: Meeting deadlines in
datacenter networks,” In ACM conference of the Special
Interest Group on Data Communication (SIGCOMM),
2011.
[22] T. Benson, A. Anand, A. Akella, and M. Zhang,
“MicroTE: Fine Grained Trafﬁc Engineering for Data
Centers,” In ACM Conference on emerging Networking
EXperiments and Technologies (CoNEXT), 2011.
[23] C.-Y. Hong, M. Caesar, and P. B. Godfrey, “Finishing
ﬂows quickly with preemptive scheduling,” In ACM
conference of the Special Interest Group on Data
Communication (SIGCOMM), 2012.
[24] A. R. Curtis, J. C. Mogul, J. Tourrilhes, P. Yalagandula,

205
International Journal on Advances in Networks and Services, vol 7 no 3 & 4, year 2014, http://www.iariajournals.org/networks_and_services/
2014, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
P. Sharma, and S. Banerjee, “DevoFlow: Scaling Flow
Management for High-Performance Networks,” In ACM
conference of the Special Interest Group on Data
Communication (SIGCOMM), 2011.
[25] OpenFlow
Switch
Speciﬁcation
(Version
1.1),
www.openﬂow.org/documents/openﬂow-spec-v1.1.0.pdf,
(retrieved: Nov. 2014)
[26] S. Kandula, S. Sengupta, A. Greenberg, P. Patel,
and
R.
Chaiken,
“The
Nature
of
Data
Center
Trafﬁc: Measurements and Analysis,” In ACM Internet
Measurement Conference (IMC), 2009.
[27] T. Benson, A. Akella, and D. A. Maltz, “Network
Trafﬁc Characteristics of Data Centers in the Wild,” In
ACM Internet Measurement Conference (IMC), 2010.
[28] G. Wang, D. G. Andersen, M. Kaminsky, M. Kozuch,
T. S. E. Ng, K. Papagiannaki, M. Glick, and L.
Mummert, “Your data center is a router: The case for
reconﬁgurable optical circuit switched paths,” In ACM
Hot Topics in Networks (HotNets) workshops, 2009.
[29] S. Radhakrishnan, M. Tewari, R. Kapoor, G. Porter,
and A. Vahdat, “Dahu: Commodity Switches for Direct
Connect Data Center Networks,” In Proceedings of
the 9th ACM/IEEE Symposium on Architectures for
Networking and Communications Systems (ANCS13),
October 2013
[30] D. Zats, T. Das, P. Mohan, D. Borthakur, and R.
Katz, “DeTail: Reducing the Flow Completion Time
Tail in Datacenter Networks,” In ACM conference of
the Special Interest Group on Data Communication
(SIGCOMM), 2012.
[31] S. Kandula, D. Katabi, B. Davie, and A. Charny,
“Walking the Tightrope: Responsive Yet Stable Trafﬁc
Engineering,” In ACM conference of the Special Interest
Group on Data Communication (SIGCOMM), 2005.
[32] S. Fischer, N. Kammenhuber, and A. Feldmann,
“REPLEX: Dynamic Trafﬁc Engineering Based on
Wardrop Routing Policies,” In ACM Conference on
emerging Networking EXperiments and Technologies
(CoNEXT), 2006.
[33] A. Ghodsi, M. Zaharia, S. Shenker and I. Stoica,
“Choosy: Max-Min Fair Sharing for Datacenter Jobs
with Constraints,” In European Conference on Computer
Systems (EuroSys), 2013.
[34] A. Ghodsi, M. Zaharia, B. Hindman, A. Konwinski,
I.
Stoica,
and
S.
Shenker,
“Dominant
resource
fairness: Fair allocation of multiple resource types,” In
USENIX Symposium on Networked Systems Design
and Implementation (NSDI), 2011
[35] Hadoop Capacity Scheduler,
hadoop.apache.org/docs/r1.2.1/capacity scheduler.html,
(retrieved: Nov. 2014)
[36] M. Alizadeh, S. Yang, M. Sharif, S. Katti, N.
McKeown, B. Prabhakar, and S. Shenker, “pfabric:
Minimal near-optimal datacenter transport,” In ACM
conference of the Special Interest Group on Data
Communication (SIGCOMM), 2013.
[37] M. Zaharia, D. Borthakur, J. Sen Sarma, K. Elmeleegy,
S. Shenker, and I. Stoica, “Delay Scheduling: A
Simple Technique for Achieving Locality and Fairness
in Cluster Scheduling,” In European Conference on
Computer Systems (EuroSys), 2010.
[38] A.
Shieh,
S.
Kandula,
A.
Greenberg, C.
Kim,
and B. Saha, “Sharing the data center network,” In
USENIX Symposium on Networked Systems Design
and Implementation (NSDI), 2011.
[39] M. Casado, D. Erickson, I. A. Ganichev, R. Grifﬁth,
B. Heller, N. Mckeown, D. Moon, T. Koponen, S.
Shenker, and K. Zariﬁs, “Ripcord: A modular platform
for data center networking,” UC, Berkeley, Technical
Report UCB/EECS-2010-93
[40] “Facebook Future-Proofs Data Center With Revamped
Network,” http://tinyurl.com/6v5pswv, (retrieved: Nov.
2014)
[41] N.
Farrington
and
A.
Andreyev,
“Facebook’s
Data Center Network Architecture,” IEEE Optical
Interconnects, 2013.

