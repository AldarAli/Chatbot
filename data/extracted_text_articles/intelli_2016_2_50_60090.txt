Supporting Humanitarian Logistics with Intelligent Applications
for Disaster Management
Francesca Fallucchi
University Guglielmo Marconi
Rome, Italy
Email: f.fallucchi@unimarconi.it
Massimiliano Tarquini
Consorzio S3log
Rome, Italy
Email: massimiliano.tarquini@s3log.it
Ernesto William De Luca
Georg Eckert Institute
Brunswick, Germany
Email: deluca@gei.de
Abstract—The management of humanitarian logistical operations
was for many years the weak link in the relief chain. Within the
advent of big data, this challenge has been changing life and
the way of dealing in the humanitarian ﬁeld. Data is increasing
exponentially due to the growing digitization of modern life and
the further evolution of data collection in combination with the
digital technology. More and more humanitarian organizations
designed numerous system with speciﬁc objectives in order to
manage the massive amounts of data. Hereby, we propose an
intelligent application scenario for disaster management. Here,
we use the multi-source information correlation together with
the implementation of our own approach based on sudoku
principles for supporting humanitarian logistics. This paper
aims to show how our system can alert and provide decision
support for disaster response and recovery management through
the integration of heterogeneous data sources from different
organizations.
Keywords–Data linkage; Disaster Information; Knowledge Base
System.
I.
INTRODUCTION
The digital evolution of the humanitarian response in
disaster relief brings new challenges for the systematic strategy
to collect, store and retrieve digital information. These massive
data have to be provided in a structured way and should
be connected to the experience of humanitarians on the one
side and to the experience of logistics experts on the other
side. This process could help in distributing the knowledge
to those who need it in a timely manner and in a clear
structured way. In all stages of disaster relief, the decision
makers need a large variety of information to react, such as
disaster situation, availability and movement of relief supplies,
population displacement, disease surveillance, relief expertise,
and meteorological satellite images or maps, etc. In addition
to that, during Humanitarian Assistance and Disaster Relief
(HADR), it is also required the intervention and aid of various
agencies in a concerted and timely manner. As a result, HADR
operations involve dynamic information exchange, planning,
coordination and all negotiation. The HADR mission goal is
to work jointly with other national, UN and Non-governmental
organization entities, bringing military speed and scale to the
problem in a coordinated planned response as a force for good.
The mission is to support military, etc., for planning processes
to save lives, relieve suffering, limit damage and initially re-
store critical services where possible. This includes search and
rescue operations, providing supplies such as food, water and
shelter, bringing medical care and autonomous critical support
facilities, providing planning and tactical coordination services
with communications, building shelters, providing vehicles,
restoring infrastructure such as power, communications, and
transportation. The HADR mission also needs communication
support and liaison between military and international govern-
ment entities. The goal is to provide a rapid tailored response
with consideration of all actions including physical, political,
legal, and economic impacts on host populations and other
supplying nations.
In all humanitarian organizations, there are numerous sys-
tems designed with speciﬁc tasks. In this paper, we propose
objectives and with its own data sources. Each sources, and
in general any archive containing information, both structured
and unstructured responds and was produced in response to
speciﬁc needs. What is often missing is a vision that is able
to grasp phenomena that go beyond the vision that any single
archive can provide. Each source, in general, provides different
information of a same entity creating a partial point of view
of that entity. Each of these data sources represents a “point
of view” of reality, and the sum of these points of view
can provide a more complete representation. The combination
of multiple views of the same entity could bring out new
knowledge. The search for an overview and cross meanings
within this growing body of data in HADR, it is currently faced
with methods known by the term big data management, namely
through tools able to process large volumes of data with the
aim of improving research processes or through the application
of statistical methods of investigation and representation of
ﬁnancial highlights. The sources were not born either with
the purpose to provide a formal semantics of the data, thus
making the tools of automatic processing very difﬁcult to
apply to. Linked Data is about using the web to connect
related data that wasn’t previously linked, or using the web to
lower the barriers to linking data currently linked using other
methods. Structuring Linked Data is to enable a wide range of
applications to process the content contained in the datasets.
It is extremely difﬁcult to recognize, for example, that two
records belonging to different sources are referring to the same
logical object to improve the effectiveness and efﬁciency of
the prediction of response for future disasters. Record linkage
(RL) usually is used to ﬁnd records in a data set that refer to
the same entity across different data sources. RL is necessary
when joining data sets based on entities that may or may not
share a common identiﬁer. It should be noted that, being in a
multi source environment, the problem doesn’t deal only with
51
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-518-0
INTELLI 2016 : The Fifth International Conference on Intelligent Systems and Applications (includes InManEnt 2016)

the comparison of two tables, but the comparison of n tables
from m different sources. To reduce the complexity of the RL
approach that require to join all table of all sources, we propose
correlations between the data directly in the databases of
origin (sources), using record linking techniques and statistical
algorithms for the identiﬁcation of common elements between
heterogeneous data sources using a approach sudoku. The
idea of the proposed approach is to correlate only relevant
candidates records of each source that have been produced
by sudoku method heuristics. The proposed Records Linkage
Approach uses the sudoku principles to reconcile the sources
linear method instead of an exponential one because only
certain candidates records of each source are compared.
In this paper, we propose a new knowledge management
system, that allows to correlate heterogeneous data sources
using sudoku approach to reduce record linking computation
complexity, for support of the humanitarian logistic response
to a natural or man-made disaster. The rest of the paper is
organized as in the following: after the presentation of the
related work (Section II), we ﬁrstly focus on our approach
to combining logistics processes with knowledge management
(Section III) and then we describe our HL-KMS framework
(Section IV). Furthermore, we show a case of study (Section
V). Finally, we draw some conclusions and describe shortly
the future work (Section VI).
II.
RELATED WORK
In the following we describe the related work, explaining
in more details how humanitarian supply chains (see Sec-
tion II-A) and Knowledge Management System to support
HADR (see Section II-B).
A. Humanitarian supply chains
There are many organizational approaches looking for
the best way of coordinating humanitarian supply chains.
Humanitarian organizations have acquired an exemplary know-
how with their numerous past experiences, but a number of
stakeholders poses a problem of coordination, considering
that the different actors, often widely different in nature,
size and specialization, are also compartmentalized in their
operating modes [1]. This coordination is a direct condition
for successful aid. In order to improve the monitoring of
humanitarian aid, actors will have to learn how to co-elaborate
and co-manage relief chains. In other words, an efﬁcient
collective strategy will be able to improve the performance of
humanitarian supply chains, while a lack of it has dramatic
consequences for the stricken populations. Then, it is nec-
essary to better deﬁne the logistical coordination difﬁculties
throughout the complexity of humanitarian operations [2]. In
a highly uncertain world, where the shortest possible timing
will probably save thousands of victims, the issue is not only a
matter of money, but also and above all a matter of human life.
Saving lives will not be possible without developing a knowl-
edge management approach, in other words learning from
previous disasters by capturing, codifying and transferring
knowledge about logistics operations [3]. There are systems
like SUMA(Supply Management Project) [4], a management
tool for post-disaster relief supplies, that use simple software
on laptop computers to track and sort incoming donations
and their destinations, allowing disaster managers to see what
they have and send it where it is needed. Among the works
dedicated to humanitarian logistics, we should note the place
occupied by research focusing on transportation optimization
issues, perhaps to the detriment of a wider reﬂection on the
monitoring of all relief chains. These works tend to modelize
the use of transport resources in disaster relief, by referring, for
example, to models imported from the military context [5][6].
Although transport management remains a major concern in
the literature on humanitarian logistics, it must be admitted
that it is no longer the only one.
B. Knowledge Management Systems to support HADR
Today, more and more information technologies have
been adopted in support of knowledge management how-
ever, Knowledge Management in Humanitarian Assistance and
Disaster Relief (KM in HADR) is still in the early stage.
KM in HADR is referred to the entire process of acquisi-
tion, management, and utilization of disaster information and
knowledge for the support of HADR operations [7]. Managing
past knowledge for reuse can expedite the process of disaster
response and recovery management plays important role. Here
we need the most important KMS reference that should be
given. KMS is vital for disaster detection, response planning,
and efﬁcient and effective disaster response and management
[8]. KMS plays important role in gathering and disseminating
the natural disaster related information. Murphy and Jennex
[9] explore the use of KMS with emergency information
system concluded that KMS should be included in more crisis
response. Mistilis and Sheldon [10] describe that knowledge
is a powerful resource to help governments and organizations
in order to plan and to manage disasters and crises. Groups
have proposed and created KMS that allow for more efﬁcient
use of data and faster response. One example that has been
proposed is the Information Management System for Hurricane
disasters (IMASH) [11], an information management system
based on an object-oriented database design, able to provide
data for response to hurricanes. Wolz and Park [12] present
another example of knowledge-based system, which serves
as an electronic central repository to meet the information
needs of the humanitarian relief community. There are other
several KMS for the support of speciﬁc disaster such as in
India [13], in Hurricane Katrina [8], in Malaysia [14], These
systems have resulted in a step change in the efﬁciency and
effectiveness of HADR chains, such improvements have the
potential to achieve similar advances in humanitarian logistics.
Humanitarian logistics is the process of planning, implement-
ing and controlling the efﬁcient, cost-effective ﬂow and storage
of goods and materials as well as related information, from the
point of origin to the point of consumption for the purpose
of meeting the end beneﬁciarys requirements [15]. A ﬁrst
step towards the development of a broad HLKMS in [16],
a conceptual model and an associated taxonomy are given to
support the development of a body of knowledge in support
of the logistic response to a natural or man-made disaster.
In the next sections we discuss our system that implements
Knowledge Management System to support HADR in a logis-
tic scenario.
III.
COMBINING LOGISTICS PROCESSES WITH
KNOWLEDGE MANAGEMENT
The humanitarian ﬁeld has been forever changed by the
advent of big data and the attendant challenges of dealing
52
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-518-0
INTELLI 2016 : The Fifth International Conference on Intelligent Systems and Applications (includes InManEnt 2016)

Figure 1. Knowledge lifecycle to support logistic
with it the disaster response. In order to utilize big data in
humanitarian logistic organization response, there are multiple
steps in the process that must be undertaken before being
able to make decisions based on the information. This section
tries to identify how big data can be used to support logistics,
identify knowledge lifecycles, and assign responsibilities (see
Fig. 1). The 10 well-known steps of such a lifecycle have to
be developed for addressing 5 relief needs of decision makers:
1)
Discover which critical information/knowledge is re-
quired by different disaster relief tasks.
2)
Identify which organizations or agencies are the ma-
jor information sources for each particular relief task.
3)
Specify the standard structures for each kind of
information and knowledge.
4)
Determine how to acquire the relevant information
from those authorized sources. The organizations that
own information sources could submit the newly
updated information to the knowledge base as soon
as it is available.
5)
Examine the acquisition process to make sure that it is
manageable and can be aided by information systems
and technologies.
In the following section, we presents a linear method to
correlate data derived from the game of sudoku: at ﬁrst
certain numbers are entered. This process brings out less
certain number. In our method, at ﬁrst we aggregate certain
records and store aggregated data into the KMS, than we
use aggregated data into the KMS to aggregate less certain
records.
IV.
APPLYING KNOWLEDGE MANAGEMENT FOR BIG
DATA HADR
The humanitarian ﬁeld has been forever changed by the
advent of big data and the attendant challenges of dealing
with it the disaster response. In order to utilize big data in
humanitarian logistic organization response, there are multiple
steps in the process that must be undertaken before being able
to make decisions based on the information.
The purpose of our approach is to identify the correct
correlations between the data contained in the database. If you
make the assumption that sources are able to provide their
information in a table, you need an algorithm that is able to join
the lines of the various tables sources, taking count of errors
and inconsistencies that may occur. Moreover, sources may
also be relevant to different moments and then it’s necessary
managing the execution of the transaction between the current
state of knowledge base and the new source. To realize a
diachronic multi source RL we propose to correlate only more
certain candidates records of each source produced by sudoku
method heuristics. The innovation of this reconciliation is also
for the return entity and the associated knowledge. There
is better accuracy of research. Furthermore there are more
reliable results because the knowledge base is more rich and
multidimensional.
The proposed solution is the connective substrate to collect
and harmonize data coming from heterogeneous sources, thus
integrated with business intelligence solutions for graphics and
data analysis. In our method, at ﬁrst we aggregate certain
records and store aggregated data into the KMS, than we
use aggregated data into the KMS to aggregate less certain
records. We propose an Humanitarian Logistics Knowledge
Management System (HL-KMS) able to acquire information
sources where data coming from different sources are re-
covered. Our framework performs linkage operation between
heterogeneous data from different information sources using
a sudoku approach and performs data analysis generating
new knowledge. In this way we guarantee validity of the
information content by means of keeping a constant trade-off
between data quality and the need of human help.
The uniqueness of our approach is the use of a knowledge
management system explained by the combination of the
following elements:
•
“sudoku” approach, in which knowledge is consoli-
dated from simple correlations and increasingly arriv-
ing to complex interrelationships.
•
diachronicity: it follows the evolution of each source
of information over time, recording not only the status
of an entity but also all its subsequent changes.
•
“Secularism” of the system with respect to the sources:
no source is considered primary for data nor free
from errors. The information is obtained from the
correlation of data, not from a weighted importance
scale.
•
“Quality control” the module always uses a bench-
mark to verify the quality of the information produced.
•
“cost-quality trade-off” the module is able to predict
ex ante which will be the necessary cost to improve
the quality of produced information.
HL-KMS framework is structured in 5 modules, as depicted
in Fig.
2. Each of these modules populate the Knowledge
Base. It provides a layered architecture for data management.
The different modules can operate sequentially or indepen-
dently one from each other. Each module can have one or
more components. The ﬁrst phase of the process consists
in the acquisition of the data to solve, given heterogeneity
problems, misalignment problems and inconsistency problems
all due to the multiplicity of data sources (Data Acquisition
module). Once the sources have been suitably normalized,
it is possible to understand if two observations refer to the
same entity by means of proceeding with an operation of
linkage between certain candidate records with the sudoku
heuristic (RL-Sudoku module). To discovery new knowledge,
the reasoner module browses the relationship between the
cross-linked data (Reasoner Relationship module). Validity of
the information content (Quality control module) is guaranteed
either keeping a constant trade-off between data quality and
53
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-518-0
INTELLI 2016 : The Fifth International Conference on Intelligent Systems and Applications (includes InManEnt 2016)

Figure 2. HL-KMS framework Functional Architecture
Step Four 
Step One 
Step Two 
Step Three 
Figure 3. Data preparation module steps.
the need for human help (Cost quality trade-off module).
Quality control is a process governed by predictable costs,
in accordance to [17]. The HL-KMS framework has a series
of dashboards that provide analysis of the data for decision
support.
A. Data Acquisition
Before proceeding to entity correlation, Data Acquisition
component is responsible of providing connectors to data
sources. Each connector is itself responsible of normalizing
data and structuring it in a format used by entity correla-
tion component. Connectors do not change data: a connector
merely transform input format producing a structured dataset
that closely represent the input. We consider datasource to be
both structured or unstructured. Data acquisition component
and connectors are also responsible of the life cycle of in-
formation before data il correlated and imported within the
KMS. The proposed system consider a data source as a set of
observations relating to a speciﬁc phenomenon. An observation
is made of records and each record may contains ﬁelds of
data. These observations may refer to static phenomenon
(that do not vary over time) or dynamics phenomenon. A
dynamic phenomenon produces new observations or changes
in existing ones. How a phenomenon varies over time is also
an information. The Data Acquisition is also responsible of
representing how diachronically data sources varies over time.
Fig. 3 summarizes the steps of the Data Preparation module.
In the following we describe them in more details:
•
First step: recovery of the observations published by
sources;
•
Second step: sending observations to the connectors;
•
Third step: normalization of the observations;
•
Fourth step: normalized observations are submitted to
Knowledge Base.
B. RL-Sudoku
This is the component responsible of data reconciliation,
correlating data from connectors to the data stored into the
KMS. As we said, the process of aggregating data is based
upon a function that at ﬁrst aggregate and store into the kms
linked entities, extract the knowledge from newly imported
and correlated data, used generated knowledge to aggregate
less certain sources record. The RL-Sudoku function has the
following features:
1)
It is a linear function: each observation to be pro-
cessed is compared only with a small subset of
record from the KMS. This is possible thanks to a
component responsible of selecting a small subset of
data from the KMS eligible to possible candidate to
data aggregation.
2)
When comparing entities from the KMS with a new
observation, it proﬁt by using all the knowledge pre-
viously acquired. It is because entities stored within
the KMS are the sum of already linked observation.
Comparing to the Sudoku, we are using already
entered numbers to enter new ones.
3)
The RL-Sudoku method allow the operator (we use
to call operator as the Oracle) to train the function to
transmit speciﬁc knowledge about data sources to be
used during data aggregation. This is especially when
different data sources may offers different point of
view of the same observation: ﬁelds that are relevant
for a data source to describe an observation, should
be insigniﬁcant when found in another data source. If
it happens, such ﬁeld should be unveriﬁed, affected
from errors, aged, etc.
4)
For each step of sudoku, the Oracle conﬁgures the
function assigning weights and thresholds. Weight are
used to evaluate how similar are to entities, thresholds
are used to determine if similarity is certain. If not,
the Sudoku-RL asks to the Oracle to suggest him if
two entities can be aggregated. When it happens, the
Oracle transfer knowledge to the Sudoku-RL. such
knowledge will be used for future entity linking.
C. Relationship Reasoner
This section describes how we represent, extract, store
knowledge within the kms. Once sources are reconciled, we
can extract the information in the form of data using the the
Relationship Reasoner module. Information is used to generate
knowledge. We have classiﬁed knowledge in three categories:
1)
Explicit knowledge: the knowledge clearly written
within the source
2)
Implicit or tacit knowledge. Tacit knowledge [18] as
the kind of knowledge that is difﬁcult to transfer
to another person by means of writing it down or
verbalizing it. We consider tacit knowledge as the
knowledge that cannot be explicated and requires the
creation of dedicated structures to be represented.
3)
Inferred knowledge: the knowledge derived from the
aggregation of the two sources.
Explicit knowledge is represented by linking entities within
the KMS using one or more graph. A knowledge graph
54
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-518-0
INTELLI 2016 : The Fifth International Conference on Intelligent Systems and Applications (includes InManEnt 2016)

can be a weighted graph, weighted oriented graph, simple
graph depending on the type of relation between entities. It
is automatically handled by the Reasoner. Tacit knowledge re-
quires a strategy to be extracted and represented. Our platform
provides a programmable interface useful to add capabilities
to the reasoner. Inferred knowledge derives from the corre-
lation (Sudoku) and linkage (Reasoner) of entities. Inferred
knowledge do not need to be represented. For example, in
our experiment, the consequence of correlating data about
people per municipalities, health infrastructure, geographical
data, gives us a clear representation of the distribution of
people per area and per hospital.
D. Data Validation
As highlighted a reconciliation process is difﬁcult to
achieve fully automatic and especially not guarantee the re-
liability that this problem requires. It is therefore necessary to
develop a tool for the use of the algorithm by an operator to
allow corrections, validations or additions needed to strengthen
the process of reconciliation. The cost for manual control of the
results of the algorithm must be supported. It should therefore
be a tool that keeps a constant trade-off between data quality
and the need for human intervention in fact optimizing costs
and algorithms. If you accept a loss of performance it could
therefore not be necessary to consult the oracle. This process
has the following properties:
1)
predict and, consequently, to plan the cost of human
intervention needed to ensure a quality set;
2)
to control, at run time, the cost of human intervention
needed to maintain the agreed level of quality;
3)
provide the ability to predict the minimum cost nec-
essary to achieve the objectives of guaranteed quality.
According to our previous work done by M. Bianchi et al. [17],
we extend the approach involving the identiﬁcation of a range
of indecision determined ex post, in which the performance
of the automated systems are not considered appropriate, to
identify ex ante the minimum set that must be processed by
human intervention. The interval of indecision varies depend-
ing on the threshold values obtained ex post to the prediction,
control and minimization of the cost of human intervention
with a guaranteed quality of service. The proposed approach
allows to apply automatic systems in the production chain, for
example in industrial processes with constraints of guaranteed
quality. In fact, the measurement system reduces costs in a
real production chain, limiting the processing to manually
necessary to ensure certain performance values taking into
account a planned budget to correct errors of the automatic
systems.
V.
A CASE STUDY: MAPPING OF GEO-POLITICAL AND
INFRASTRUCTURAL SITUATION IN ITALY
In this section, we explain how we incorporated and
collected Big Data into a HADR framework basing upon a
real use case which has been started since 2007. The main goal
of the project was to create a Geo-Political and Economical
map of Italy, using Big Data as a knowledge base, for future
mapping and understanding of other HADR related knowledge
domains. Approaching to the problem with a software platform
would be restrictive because of multiple related issues (known
or unknown). TABLE I lists issues to be addressed:
TABLE I. CRITICAL ITEMS TO BE ADDRESSED GROUPED BY
DOMAIN
Data Source related issues
•
How to identify data sources? Where the information is?
•
How to normalize structured and un-structured data-sources?
•
How to address data linkage? Can I re-use acquired knowledge to
improve future data import and linkage?
•
How often does data need updated/refreshed?
•
How representative are datasets of a HADR speciﬁc domain?
Data Representation related issues
•
How will data change over time and how long are datasets valid?
•
How to represent diachronic variation of data-sources?
•
What means knowledge? How to represent it?
Data Acquisition related issues
•
What is the overall big data strategy?
•
Can big Data be used preventively?
To address all the items, we created a framework including
of software libraries, best practices and strategies. In details:
•
A strategy to address the problem of connecting and
extracting data from data sources;
•
A method together to a software library to approach
to data-sources record linkage and a strategy to decide
what data-source and when to import;
•
A method to address to the problem of extracting
explicit knowledge from data-sources and to extract
implicit knowledge when two or more data sources
are linked;
•
A method to design a database suitable to import data
and represent knowledge keeping in mind possible
future growth and implementations.
In the following, we present some examples of questions that
have impact on logistics.
TABLE II. PUBLIC ADMINISTRATION OPEN DATA
—Public Administration Open Data—
Name
Type
Contents
IPA
Structured
Index of public administration covering PA, Pub-
lic Security, Defense.
Ancitel/Ancitada
Structured
Containing data about municipalities in terms of
resident population, extension of the territory.
LineAmica
Structured
Index of public administration covering PA, Pub-
lic Security, Defense.
MinSanita
Structured
Covering health
MISE
Structured
Index of communication and internet service
providers.
MIUR
Structured
Covering education.
TABLE III. ITALIAN COMPANIES DATA
—Italian Companies Data—
Name
Type
Contents
http://www.guidamonaci.it
Unstructured
Italian Companies grouped
by industry sector.
EPO (European Patent Ofﬁce)
REST services
Information
about
ﬁled
patents
per
company
and
market product classiﬁcation.
55
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-518-0
INTELLI 2016 : The Fifth International Conference on Intelligent Systems and Applications (includes InManEnt 2016)

TABLE IV. OTHER RELEVANT BIG DATA DATA-SOURCES
—Other relevant big data data-sources—
Name
Type
Contents
Google
Unstructured
An entry point to navigate the
internet for speciﬁc contents.
World Wide Web
Unstructured
Information space where web re-
sources are identiﬁed by URLs.
Open Street Map
Structured
Open Geo Data.
ICANN
Text-Unstructured
Index of ISP, domains, ip owners.
For the experimental phase we decided to trace the follow-
ing scenarios:
•
Presence of public administration and coverage area;
•
Presence of civil protection and coverage area;
•
Presence of infrastructures: barracks, health, educa-
tion, warehouses, etc.;
•
Population distribution and infrastructures coverage;
•
Economical ecosystems and distribution of companies
per market per area (thin can be also used to deﬁne
strategies in the domain of cyber security);
•
Capacity and independence for the supply of essential
goods and technologies;
•
Communication Service Providers.
Such knowledge base can be used to extract HADR rele-
vant information and decision support. Knowledge generated
can also be used to implement strategies to approach to a
real scenario in terms of supporting decision and enhancement
of the knowledge base (knowledge can be used to generate
knowledge). The following table lists data sources used re-
spectively in Public Administration Open Data TABLE II, in
Italian Companies Data TABLE III, and in other relevant big
data data-sources TABLE IV.
VI.
CONCLUSION AND FUTURE WORK
Disaster managers have realized the true potential of KMS
to provide a more effective and rapid response in case of
disaster. Disaster response requires the intervention and the
coordination of a large number of organizations, people, and
resources. Accessing to real time information is the key success
for a real-time knowledge base decision making.
This paper proposed the implementation of a framework
used to generate a KMS to create a Geo-Political and Eco-
nomical map of Italy as a knowledge base for future mapping
and understanding of other HADR related domains. The aim
of the framework is to collect and to integrate information
resources from different public and private organizations and
from other and not institutional sources in order to create
situation awareness and support decision maker to make the
right decision within the timely manner. In this paper we
have also shown how knowledge can be used as the basis
for creating new knowledge and providing data analysis for a
wide range of HADR scenarios.
REFERENCES
[1]
J. Chandes and G. Pache, “La coordination des chaines logistiques
multi-acteurs dans un context humanitaire: quels cadres conceptuels
pour amliorer laction?” Logistique & Management, vol. 14, no. 1, 2006,
pp. 33–42.
[2]
J.Chandes and G.Pache, “Strategizing humanitarian logistics: the chal-
lenge of collective action,” pp. 104–112, 2010.
[3]
T. Rolando and van Wassenhove L N, Humanitarian logistics. Palgrave
Macmillan, 2009, vol. INSEAD business press.
[4]
C. V. de Goyet, E. Acosta, P. Sabbat, and E. Pluut, Supply Management
Project, a management tool for post-disaster relief supplies.
World
Health Stat Q., 1996, vol. 49.
[5]
S. J. Pettit and A. K. C. Beresford, “Emergency relief logistics: an
evaluation of military, non-military and composite response models,”
International Journal of Logistics Research and Applications, vol. 8,
no. 4, 2005, pp. 313–331.
[6]
M. R. Weeks, “Organizing for disaster: Lessons from the military,”
Business Horizons, vol. 50, no. 6, 2007, pp. 479 – 489.
[7]
D. Zhang, L. Zhou, and J. F. Nunamaker Jr, “A knowledge manage-
ment framework for the support of decision making in humanitarian
assistance/disaster relief,” Knowledge and Information Systems, vol. 4,
no. 3, 2002, pp. 370–385.
[8]
S. Otim, “A case-based knowledge management system for disaster
management: fundamental concepts,” in Proceedings of the 3rd Inter-
national ISCRAM Conference, Newark, NJ (USA), 2006, pp. 598–604.
[9]
T. Murphy and M. E. Jennex, “Knowledge management, emergency
response, and hurricane katrina,” International Journal of Intelligent
Control Systems, vol. 11, no. 4, 2006, pp. 199–208.
[10]
N. Mistilis and P. Sheldon, “Knowledge management for tourism crises
and disasters,” Tourism Review International, vol. 10, no. 1-2, 2006,
pp. 39–46.
[11]
E. Iakovou and C. Douligeris, “An information management system
for the emergency management of hurricane disasters,” International
Journal of Risk Assessment and Management, vol. 2, no. 3-4, 2001,
pp. 243–262.
[12]
C. Wolz and N.-h. Park, “Evaluation of reliefweb,” in Ofﬁce for the Co-
ordination of Humanitarian Affairs, UN, Forum One Communications,
2006.
[13]
M.Sujit, P.Biswajit, K.Hermang, and I.Rajeev, “Knowledge manage-
ment in disaster risk reduction. the indian approach,” Ministry of Home
Affairs, National Disaster Management Division, Government of India,
2005.
[14]
N. A. Hassan, N. Hatiyusuh, and K. Rasha, “The implementation of
knowledge management system (kms) for the support of humanitarian
assistance/disaster relief (ha/dr) in malaysia,” International Journal of
Humanities and Social Science, vol. 1, no. 4, 2011, pp. 89–112.
[15]
A. Thomas and M. Mizushima, “Logistics training: necessity or luxury,”
Forced Migration Review, vol. 22, no. 22, 2005, pp. 60–61.
[16]
P. Tatham and K. Spens, “Towards a humanitarian logistics knowledge
management system,” Disaster Prevention and Management: An Inter-
national Journal, vol. 20, no. 1, 2011, pp. 6–26.
[17]
M. Bianchi, M. Draoli, F. Fallucchi, and A. Ligi, “Service level
agreement constraints into processes for document classiﬁcation,” in
Proceedings of the 16th ICEIS 2014, 2014, pp. 545–550.
[18]
I. Nonaka, The knowledge-creating company.
Harvard Business
Review Press, 2008.
56
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-518-0
INTELLI 2016 : The Fifth International Conference on Intelligent Systems and Applications (includes InManEnt 2016)

