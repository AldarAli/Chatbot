339
International Journal on Advances in Intelligent Systems, vol 8 no 3 & 4, year 2015, http://www.iariajournals.org/intelligent_systems/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Predicative Recursion, Diagonalization,
and Slow-growing Hierarchies of Time-bounded Programs
Emanuele Covino and Giovanni Pani
Dipartimento di Informatica
Università di Bari, Italy
Email: emanuele.covino@uniba.it, giovanni.pani@uniba.it
Abstract—We deﬁne a version of predicative recursion, a
related programming language, and a hierarchy of classes of
programs that represents a resource-free characterization of
register machines computing their output within polynomial
time O(nk), for each ﬁnite k. Then, we introduce an operator of
diagonalization, that allows us to extend the previous hierarchy
and to capture the register machines with computing time
bounded by an exponential limit O(nnk). Finally, by means
of a restriction on composition of programs, we characterize
the register machines with a polynomial bound imposed over
time and space complexity, simultaneously.
Index Terms—Time/space complexity classes; Implicit compu-
tational complexity; Predicative recursion; Diagonalization.
I. INTRODUCTION
A complexity class is usually deﬁned by imposing an
explicit bound on time and/or space resources used by
a Turing Machine (or another equivalent model) during
its computation. On the other hand, different approaches
use logic and formal methods to provide languages for
complexity-bounded computation; they aim at studying com-
putational complexity without referring to external measuring
conditions or a to particular machine model, but only by
considering language restrictions or logical/computational
principles implying complexity properties. In particular, this
is achieved by characterizing complexity classes by means
of recursive operators with explicit syntactical restrictions on
the role of variables. In this paper, we extend the result intro-
duced in [1] by deﬁning a resource-free characterization of
register machines computing their output within polynomial
time O(nk), for each ﬁnite k, and exponential O(nnk); we
achieve this result by means of our version of predicative
recursion and of a new diagonalization operator, and a related
programming language.
One of the ﬁrst characterization of the polynomial-time
computable functions was given by Cobham [2], in which
these functions are exactly those generated by bounded recur-
sion on notation. The ﬁrst predicative deﬁnitions of recursion
can be found in the work of Simmons [3], Bellantoni and
Cook [4], and Leivant [5], [6]: they introduced a ramiﬁ-
cation principle that does not require that explicit bounds
are imposed on the deﬁnition of functions, proving that this
principle captures the class PTIMEF. Following this approach,
several other complexity classes have been characterized by
means of unbounded and predicative operators: see, for in-
stance, Leivant and Marion [7] and Oitavem [8] for PSPACEF
and the class of the elementary functions; Clote [9] for the
deﬁnition of a time/space hierarchy between PTIMEF and
PSPACEF; a theoretical insight has been provided by Leivant
[5], [10] and [6]. All these approaches have been dubbed
Implicit Computational Complexity: they share the idea that
no explicitly bounded schemes are needed to characterize a
great number of classes of functions and that, in order to
do this, it sufﬁces to distinguish between safe and normal
variables (or, following [3], between dormant and normal
ones) in the recursion schemes. Roughly speaking, the normal
positions are used only for recursion, while the safe positions
are used only for substitution. The two main objectives of this
area are to ﬁnd natural implicit characterizations of various
complexity classes of functions, thereby illuminating their
nature and importance, and to design methods suitable for
static veriﬁcation of program complexity.
Our version of the safe recursion scheme on a binary
word algebra is such that f(x, y, za) = h(f(x, y, z), y, za);
throughout this paper we will call x, y and z the auxiliary
variable, the parameter, and the principal variable of a pro-
gram deﬁned by recursion, respectively. We do not allow the
renaming of variable z as x, implying that the step program
h cannot assign the value f(x, y, z) of the being-deﬁned
program f to the principal variable z: in other words, we
always know in advance the number of recursive calls of the
step program in a recursive deﬁnition. We obtain that z is a
dormant variable, according to Simmons’ [3], or a safe one,
following Bellantoni and Cook [4]. Starting from a natural
deﬁnition of constructors and destructors over an algebra of
lists, we deﬁne the hierarchy of classes of programs Tk, with
k ∈ N, where programs in T1 can be computed by register
machines within linear time, and Tk+1 are programs obtained
by one application of safe recursion to elements in Tk; we
prove that programs deﬁned in Tk are exactly those programs
computable within time O(nk).
Using the deﬁnition of structured ordinals as given in
[11], we introduce an operator of constructive diagonal-
ization, and we extend the previous hierarchy to Tλ, with
ω ≤ λ ≤ ωω. Programs in Tα+1 are obtained by one
application of safe recursion to elements in Tα; if λ is a limit
ordinal, and λ1, . . . , λk, . . . is the associated fundamental

340
International Journal on Advances in Intelligent Systems, vol 8 no 3 & 4, year 2015, http://www.iariajournals.org/intelligent_systems/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
sequence, programs in Tλ are obtained by diagonalization on
the previously deﬁned sequence of classes Tλ1, . . . , Tλk, . . ..
This allows us to harmonize in a single hierarchy of classes
of programs all the register machines with computing time
bounded by polynomial time O(nk) and exponential time
O(nnk), for each ﬁnite k. Similar results, achieved with
different approaches, can be found in [12], [13], [14], and
[15].
Then, we restrict Tk to the hierarchy Sk, whose elements
are the programs computable by a register machine in linear
space. By means of a restricted form of composition between
programs, we deﬁne a polytime-space hierarchy T Sqp, such
that each program in T Sqp can be computed by a register ma-
chine within time O(np) and space O(nq), simultaneously.
Similar results are in [16] and [7], and are a preliminary step
for an implicit classiﬁcation of the hierarchy of time-space
classes between PTIME and PSPACE, as deﬁned in [9].
The paper is organized as follows: in Section II, we
introduce the basic instructions of our language, the notion
of composition of programs, and the classes of programs T0
and T1; in Section III, we recall the deﬁnition of register
machine, the model of computation underlying our charac-
terization, and we prove that programs in T1 capture exactly
the computations performed by a register machine within
linear time; in Section IV, we deﬁne the ﬁnite hierarchy Tk,
with k ≥ 1, and we prove that programs in this hierarchy
capture the computations performed within polynomial time;
in Section V, we introduce the diagonalization operator,
and we extend the ﬁnite hierarchy in order to capture the
computations with time-complexity up to exponential time;
in Section VI, we redeﬁne the notion of composition, and we
give a characterization of classes of programs with time and
space polynomial bound.
II. BASIC INSTRUCTIONS AND DEFINITION SCHEMES
In this section, we introduce the basic operators of our
programming language and the ﬁrst two classes on which
our hierarchy is based. The language is deﬁned over a binary
word algebra, with the restriction that words are packed into
lists, with the symbol ©acting as a separator between each
word. This allow us to handle a sequence of words as a single
object. The basic instructions allow us to manipulate lists of
words, with some restrictions on the renaming of variables;
the language is completely deﬁned adding our version of
recursion and composition of programs.
A. Recursion-free programs and class T0
B
is
the
binary
alphabet
{0, 1}.
a, b, a1, . . .
de-
note
elements
of
B,
and
U, V, . . . , Y
denote
words
over
B.
p, q, . . . , s, . . .
stand
for
lists
in
the
form
Y1©Y2© . . . ©Yn−1©Yn. ϵ is the empty word. The i-th
component (s)i of s = Y1©Y2© . . . ©Yn−1©Yn is Yi. |s| is
the length of the list s, that is the number of letters occurring
in s. We write x, y, z for the variables used in a program, and
we write u for one among x, y, z. Programs are denoted with
the letters f, g, h, and we write f(x, y, z) for the application
of the program f to variables x, y, z, where some among
them may be absent.
Deﬁnition 2.1: The basic instructions are:
1) the identity I(u) that returns the value s assigned to u;
2) the constructors Ca
i (s) that add the digit a at the right
of the last digit of (s)i, with a = 0, 1 and i ≥ 1;
3) the destructors Di(s) that erase the rightmost digit of
(s)i, with i ≥ 1.
Constructors Ca
i (s) and destructors Di(s) leave the input s
unchanged if it has less than i components.
Example 2.1: Given the word s = 01©11©©00, we have
that |s| = 9, and (s)2 = 11. We also have C1
1(01©11) =
011©11, D2(0©0©) = 0©©, D2(0©©) = 0©©.
Deﬁnition 2.2: Given the programs g and h, f is deﬁned
by simple schemes if it is obtained by:
1) renaming of x as y in g, that is, f is the result of the
substitution of the value of y to all occurrences of x
into g. Notation: f =RNMx/y(g);
2) renaming of z as y in g, that is, f is the result of the
substitution of the value of y to all occurrences of z
into g. Notation: f =RMNz/y(g);
3) selection in g and h, when for all s, t, r we have
f(s, t, r) =



g(s, t, r)
if the rightmost digit
of (s)i is b
h(s, t, r)
otherwise,
with i ≥ 1 and b = 0, 1. Notation: f =SELb
i(g, h).
Simple schemes are denoted with SIMPLE.
Example 2.2: if f is deﬁned by RNMx/y(g) we have that
f(t, r) = g(t, t, r). Similarly, f deﬁned by
RNMz/y(g)
implies that f(s, t) = g(s, t, t). Let s be the word 00©1010,
and f =SEL0
2(g, h); we have that f(s, t, r) = g(s, t, r), since
the rightmost digit of (s)2 is 0.
Deﬁnition 2.3: Given the programs g and h, f is deﬁned
by safe composition of h and g in the variable u if it is
obtained by the substitution of h to u in g, if u = x or
u = y; the variable x must be absent in h, if u = z.
Notation: f =SCMPu(h, g).
The reason of this particular form of composition of
programs will be clear in the following section, where we
will show to the reader how to combine composition and
recursion in order to obtain new time-bounded programs.
Deﬁnition 2.4: A modiﬁer is obtained by the safe com-
position of a sequence of constructors and a sequence of
destructors.
Deﬁnition 2.5: T0 is the class of programs deﬁned by
closure of modiﬁers under selection and safe composition.
Notation: T0=(modifier; SCMP, SEL).
All programs in T0 modify their inputs according to the
result of some test performed over a ﬁxed number of digits.

341
International Journal on Advances in Intelligent Systems, vol 8 no 3 & 4, year 2015, http://www.iariajournals.org/intelligent_systems/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
B. Safe recursion and class T1
In what follows we introduce the deﬁnition of our form of
recursion and iteration on notation (see [4] and [10]).
Deﬁnition 2.6: Given the programs g(x, y) and h(x, y, z),
f(x, y, z) is deﬁned by safe recursion in the basis g and in
the step h if for all s, t, r we have
{ f(s, t, a)
=
g(s, t)
f(s, t, ra)
=
h(f(s, t, r), t, ra),
with a ∈ B. Notation: f =SREC(g, h).
In particular, f(x, z) is deﬁned by iteration of h(x) if for all
s, r we have
{ f(s, a)
=
s
f(s, ra)
=
h(f(s, r)).
with a ∈ B. Notation: f =ITER(h). We write h|r|(s) for
ITER(h)(s, r) (i.e., the |r|-th iteration of h on s).
Deﬁnition 2.7: T1 is the class deﬁned by closure under
simple schemes and safe composition of programs in T0 and
programs obtained by one application of ITER to T0 (denoted
with ITER(T0)).
Notation: T1=(T0, ITER(T0); SCMP, SIMPLE).
As we have already stated in the Introduction, we call
x, y and z the auxiliary variable, the parameter, and the
principal variable of a program obtained by means of the
previous recursion scheme. Note that the renaming of z
as x is not allowed (see deﬁnition 2.2), and if the step
program of a recursion is deﬁned itself by safe composition
of programs p and q, no variable x (i.e., no potential recursive
calls) can occur in the function p, when p is substituted
into the principal variable z of q (see deﬁnition 2.3). These
two restrictions implies that the step program of a recursive
deﬁnition never assigns the recursive call to the principal
variable. This is the key of the polynomial-time complexity
bound intrinsic to our programs.
Deﬁnition 2.8:
1) Given f ∈ T1, the number of com-
ponents of f is max{i|Di or Ca
i or SELb
i occurs in f}.
Notation: #(f).
2) Given a program f, its length is the number of con-
structors, destructors and deﬁning schemes occurring
in its deﬁnition. Notation: lh(f).
III. COMPUTATION BY REGISTER MACHINES
In this section, we recall the deﬁnition of register machine
as presented in [6], and we give the deﬁnition of computation
within a given time and/or space bound. We prove that
programs in T1 are exactly those computable within linear
time.
Deﬁnition 3.1: Given a free algebra A generated from
constructors c1, . . . , cn (with arity(ci) = ri), a register
machine over A is a computational device M having the
following components:
1) a ﬁnite set of states S = {s0, . . . , sn};
2) a ﬁnite set of registers Φ = {π0, . . . , πm};
3) a collection of commands, where a command may be:
a branching siπjsi1 . . . sik, such that when M is in
the state si, switches to state si1, . . . , sik according to
whether the main constructor (i.e., the leftmost) of the
term stored in register πj is c1, . . . , ck;
a constructor siπj1 . . . πjri ciπlsr, such that when M
is in the state si, store in πl the result of the application
of the constructor ci to the values stored in πj1 . . . πjri ,
and switches to sr;
a p-destructor siπjπlsr (p ≤ max(ri)i=1...k), such
that when M is in the state si, store in πl the p-th
subterm of the term in πj, if it exists; otherwise, store
the term in πj. Then it switched to sr.
A conﬁguration of M is a pair (s, F), where s ∈ S and
F
: Φ → A. M induces a transition relation ⊢M on
conﬁgurations, where κ ⊢M κ′ holds if there is a command
of M whose execution converts the conﬁguration κ in κ′.
A computation of M on input ⃗X = X1, . . . , Xp with output
⃗Y = Y1, . . . , Yq is a sequence of conﬁgurations, starting with
(s0, F0), and ending with (s1, F1) such that:
1) F0(πj′(i)) = Xi, for 1 ≤ i ≤ p and j′ a permutation
of the p registers;
2) F1(πj′′(i)) = Yi, for 1 ≤ i ≤ q and j′′ a permutation
of the q registers;
3) each conﬁguration is related to its successor by ⊢M;
4) the last conﬁguration has no successor by ⊢M.
Deﬁnition 3.2: A register machine M computes the pro-
gram f if, for all s, t, r, we have that f(s, t, r) = q implies
that M computes (q)1, . . . , (q)#(f) on input (s)1, . . . ,
(s)#(f), (t)1, . . . , (t)#(f), (r)1, . . . , (r)#(f).
Deﬁnition 3.3: Given a register machine M and the poly-
nomials p(n) and q(n), for each input ⃗X (with | ⃗X| = n),
1) M computes its output within time O(p(n)) if its
computation runs through O(p(n)) conﬁgurations;
2) M computes its output in space O(q(n)) if, during the
computation, the global length of the contents of its
registers is O(q(n)).
3) M computes its output with time O(p(n)) and space
O(q(n)) if the two bounds occur simultaneously, dur-
ing the same computation.
Note that the number of registers needed by M to compute
a program f has to be ﬁxed a priori (otherwise, we should
have to deﬁne a family of register machines for each program
to be computed, with each element of the family associated
to an input of a given length). According to deﬁnitions 2.8
and 3.2, M uses a number of registers which linearly depends
on the highest component’s index that f can manipulate or
access with one of its constructors, destructors or selections;
and which depends on the number of times a variable is used
by f, that is, on the total number of different copies of the
registers that M needs during the computation. Both these
numbers are constant values, and can be detected before the

342
International Journal on Advances in Intelligent Systems, vol 8 no 3 & 4, year 2015, http://www.iariajournals.org/intelligent_systems/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
computation occurs.
Unlike the usual operators cons, head and tail over Lisp-
like lists, our constructors and destructors can have direct
access to any component of a list, according to deﬁnition
2.1. Hence, their computation by means of a register machine
requires constant time, but it requires an amount of time
which is linear in the length of the input, when performed
by a Turing machine.
Codes. We write si©Fj(π0)© . . . ©Fj(πk) for the word
that encodes a conﬁguration (si, Fj) of M, where each
component is a binary word over {0, 1}.
Lemma 3.1: f belongs to T1 if and only if f is computable
by a register machine within time O(n).
Proof: To prove the ﬁrst implication we show (by
induction on the structure of f) that each f ∈ T1 can be
computed by a register machine Mf in time cn, where c is
a constant which depends on the construction of f, and n is
the length of the input.
Base. f ∈ T0. This means that f is obtained by closure of
a number of modiﬁers under selection and safe composition;
each modiﬁer g can be computed within time bounded by
lh(g), the overall number of basic instructions and deﬁnition
schemes of g, i.e., by a machine running over a constant num-
ber of conﬁgurations; the result follows, since the selection
can be simulated by a branching, and the safe composition
can be simulated by a sequence of register machines, one for
each modiﬁer.
Step. Case 1. f =ITER(g), with g ∈ T0. We have that
f(s, r) = g|r|(s). A register machine Mf can be deﬁned as
follows: (s)i is stored in the register πi (i = 1 . . . #(f)) and
(r)j is stored in the register πj (j = #(f) + 1 . . . 2#(f));
Mf runs Mg (within time bounded by lh(g)) for |r| times.
Each time Mg is called, Mf deletes one digit from one of
the registers π#(f)+1 . . . π2#(f), starting from the ﬁrst; the
computation stops, returning the ﬁnal result, when they are
all empty. Thus, Mf computes f(s, r) within time |r|lh(g).
Case 2. Let f be deﬁned by simple schemes or safe composi-
tion. The result follows by direct simulation of the schemes.
In order to prove the second implication, we show that
the behaviour of a k-register machine M, which operates
in time cn can be simulated by a program in T1. Let
nxtM be a program in T0, such that nxtM operates on
input s = si©Fj(π0)© . . . ©Fj(πk) and it has the semantic
if state[i](s) then Ei, where state[i](s) is a test that is true
if the state of M is si, and Ei is a modiﬁer that updates the
code of the state and the code of one among the registers,
according to the deﬁnition of M. By means of c − 1 safe
compositions, we deﬁne nxtc
M in T0, which applies nxtM
to the word that encodes a conﬁguration of M for c times.
We deﬁne in T1
{ linsimM(x, a) =
x
linsimM(x, za) =
nxtc
M(linsimM(x, z))
linsimM(s, r) iterates nxtM(s) for c|r| times, returning the
code of the conﬁguration that contains the ﬁnal result of M.
IV. THE TIME HIERARCHY
In this section, we recall the deﬁnition of the class of pro-
grams T1; we deﬁne our hierarchy of classes of programs, and
we prove the relation with the classes of register machines,
which compute their output within a polynomially-bounded
amount of time.
Deﬁnition 4.1:
1) ITER(T0) denotes the class of pro-
grams obtained by one application of iteration to pro-
grams in T0;
2) T1 is the class of programs obtained by closure under
safe composition and simple schemes of programs in
T0 and programs in ITER(T0);
Notation: T1=(T0, ITER(T0); SCMP, SIMPLE);
3) Tk+1 is the class of programs obtained by closure under
safe composition and simple schemes of programs in
Tk and programs in SREC(Tk), with k ≥ 1;
Notation: Tk+1=(Tk, SREC(Tk); SCMP, SIMPLE).
Lemma 4.1: Each f(s, t, r) in Tk can be computed by a
register machine within time bounded by |s| + lh(f)(|t| +
|r|)k, with k ≥ 1.
Proof: Base. f ∈ T1. The relevant case is when f is
in the form ITER(h), with h ∈ T0. In lemma 3.1 (step, case
1) we have proved that f(s, r) can be computed within time
|r|lh(h); hence, we have the thesis.
Step. f
∈ Tp+1. The most signiﬁcant case is when
f =SREC(g, h). By the inductive hypothesis there exist two
register machines Mg and Mh which compute g and h within
the required time. Let r be the word a1 . . . a|r|; recalling
that f(s, t, ra) = h(f(s, t, r), t, ra), we deﬁne a register
machine Mf that calls Mg on input s, t, and calls Mh for |r|
times on input stored into the appropriate set of registers (in
particular, the result of the previous recursive step has to be
stored always in the same register). By inductive hypothesis,
Mg needs time |s| + lh(g)(|t|)p in order to compute g; for
the ﬁrst computation of the step program h, Mh needs time
|g(s, t)| + lh(h)(|t| + |a|r|−1a|r||)p.
After |r| calls of Mh, the ﬁnal conﬁguration is obtained
within overall time |s| + max(lh(g), lh(h))(|t| + |r|)p+1 ≤
|s| + lh(f)(|t| + |r|)p+1.
Lemma 4.2: The behaviour of a register machine which
computes its output within time O(nk) can be simulated by
a program f in Tk, with k ≥ 1.
Proof: Let M be a register machine respecting the
hypothesis. As we have already seen, there exists nxtM ∈ T0
such that, for input the code of a conﬁguration of M, it
returns the code of the conﬁguration induced by the relation
⊢M. Given a ﬁxed i, we write the program σi by means of i
safe recursions nested over nxtM, such that it iterates nxtM
on input s for ni times, with n the length of the input:

343
International Journal on Advances in Intelligent Systems, vol 8 no 3 & 4, year 2015, http://www.iariajournals.org/intelligent_systems/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
σ0 :=ITER(nxtM) and
σn+1 :=RNMz/y(γn+1), where γn+1 :=SREC(σn, σn).
We have that
σ0(s, t) = nxt|t|
M(s), σn+1(s, t) = γn+1(s, t, t), and



γn+1(s, t, a)
= σn(s, t)
γn+1(s, t, ra)
= σn(γn+1(s, t, r), t)
= γn(γn+1(s, t, r), t, t)
In particular, we have
σ1(s, t) =
γ1(s, t, t) =
σ0(σ0(. . . σ0(s, t) . . .))
|
{z
}
|t| times
= nxt|t|2
M
σ2(s, t) =
γ2(s, t, t) =
σ1(σ1(. . . σ1(s, t) . . .))
|
{z
}
|t| times
= nxt|t|3
M
By induction, we see that σk−1 iterates nxtM on input s
for |t|k times, and that it belongs to Tk. The result follows
deﬁning f(t) = σk−1(t, t), with t the code of an initial
conﬁguration of M.
Theorem 4.1: A program f belongs to Tk if and only if f
is computable by a register machine within time O(nk), with
k ≥ 1.
Proof: By lemma 4.2 and lemma 4.1.
We recall that register machines are polytime reducible to
Turing machines; thus, the sequence of classes Tk captures
PTIMEF (see [6] and [12] for similar characterization of this
complexity class).
V. EXTENDING THE POLYNOMIAL-TIME HIERARCHY
TO TRANSFINITE
In this section, we extend the deﬁnition of the classes of
programs Tk, with k ≥ 1, to a transﬁnite hierarchy of classes;
in order to do this, we recall the deﬁnition of structured
ordinals and of hierarchies of slow/fast growing functions, as
reported in [11]. Then, we introduce a natural slow growing
function B, and we give the deﬁnition of diagonalization at
a given limit ordinal λ, based on the sequence of classes
Tλ1, . . . , Tλn, . . . associated with the fundamental sequence
of λ. A similar constructive operator can be found in [15]
and [12]. We prove that this transﬁnite hierarchy of programs
characterize the classes of register machines computing their
output within time between O(nk) and O(nnk) (with k ≥ 1
and n the length of the input), that is, the computations with
time complexity between polynomial and exponential time.
A. Structured ordinals and hierarchies
Following [11], we denote limit ordinals with greek small
letters α, β, λ, . . ., and we denote with λi the i-th element
of the fundamental sequence assigned to λ. For example,
ω is the limit ordinal of the fundamental sequence 1, 2, . . .;
and ω2 is the limit ordinal of the fundamental sequence
ω, ω2, ω3, . . ., with (ω2)k = ωk.
The slow-growing functions Gα : N → N are deﬁned by
the recursion



G0(n)
=
0
Gα+1(n)
=
Gα(n)
Gλ(n)
=
Gλn(n).
The fast-growing functions Fα : N → N are deﬁned by
the recursion



F0(n)
=
n + 1
Fα+1(n)
=
F n+1
α
(n)
Fλ(n)
=
Fλn(n).
We deﬁne the slow-growing functions Bα : N → N by
means of the recursion



B0(n)
=
1
Bα+1(n)
=
nBα(n)
Bλ(n)
=
Bλn(n).
Note that Bk(n) = nk, Bω(n) = nn, Bω+k(n) = nn+k,
Bωk(n) = nn·k, Bωk(n) = nnk, and Bωω(n) = nnn;
moreover, we have that Bα+β(n) = Bα(n) · Bβ(n), and
that Gωα(n) = nGα(n) = Bα(n).
B. Diagonalization and transﬁnite hierarchy
The ﬁnite hierarchy T0, T1, T2, . . . , Tk, . . ., captures the
register machines that compute their output with time
in O(1), O(n), O(n2), . . . , O(nk), . . ., respectively. Jumping
out of the hierarchy requires something more than safe
recursion.
A possible approach consist in deﬁning a kind of ranking
function that counts the number of nested recursion violating
our "no-bad-renaming" rule or, in general, not respecting the
predicative deﬁnition of a program. A class of time-bounded
register machines is associated to each level of violation. This
idea was introduced in [17].
On the other hand, given a limit ordinal λ, we propose
a new operator that diagonalizes at level λ over the classes
Tλi, that is, that selects programs in a previously deﬁned class
according to the length of the input. There is no circularity
in a program deﬁned by diagonalization, and we believe that
this program isn’t less predicative than a program deﬁned by
safe recursion. For instance, at level ω, we are able to select
programs in the sequence Ti, where the value of i depends
on the length of the input; thus, this level of diagonalization
captures the class of all register machines whose computation
is bounded by a polynomial. Extending this approach to
next levels of structured ordinals, we are able to reach the
machines computing within exponential time.
Deﬁnition 5.1: Given a limit ordinal λ with the funda-
mental sequence λ0, . . . , λk, . . ., and given an enumerator
program q such that q(λi) = fλi, for each i, the program
f(x, y) is deﬁned by diagonalization at λ if for all s, t if
f(s, t) =ITER|t|(q(λ|t|))(s, t)

344
International Journal on Advances in Intelligent Systems, vol 8 no 3 & 4, year 2015, http://www.iariajournals.org/intelligent_systems/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
where{
ITER1(p)(s, t)
=
ITER(p)(s, t)
ITERk+1(p)(s, t)
=
ITER(ITERk(p))(s, t).
and fλi belongs to a previously deﬁned class Cλi, for each
i. Notation: f =DIAG(λ).
Note that the previous deﬁnition requires that fλi ∈ Cλi,
but no other requirements are made on how the C’s classes are
built. In what follows, we introduce the transﬁnite hierarchy
of programs, with an important restriction on the deﬁnition
of the C’s.
Deﬁnition 5.2: Given λ < ωω, Tλ is the class of programs
obtained by
1) closure under safe composition and simple schemes of
programs in Tα and programs in SREC(Tα), if λ =
α + 1;
Notation: Tα+1=(Tα, SREC(Tα); SCMP, SIMPLE).
2) closure under simple schemes of programs obtained by
one application of diagonalization at λ, if λ is a limit
ordinal, with fλi ∈ Tλi, for each λi in the fundamental
sequence of λ.
Notation: Tλ=(DIAG(λ); SIMPLE);
Lemma 5.1: Each f(s, t, r) in Tλ (λ < ωω) can be
computed by a register machine within time Bλ(n).
Proof: By induction on λ. We have three cases:
(1) λ is a ﬁnite number; we note that Bk(n) = nk, and the
proof follows from Lemma 4.1.
(2) λ = β + 1; this implies that f ∈ Tβ+1, and the relevant
subcase is when f =SREC(g, h), with both g and h belonging
to Tβ. By the inductive hypothesis, there exist the register
machines Mg and Mh computing g and h, respectively,
within time bounded by Bβ(n). A register machine Mf can
be deﬁned, such that it calls Mg on input s, t, and calls
Mh for |r| times on input stored into the appropriate set of
registers. Mf needs time Bβ(n) + |r|Bβ(n) to perform this
computation; thus, the overall time is bounded by Bβ+1(n),
by deﬁnition of B.
(3) λ is a limit ordinal; this means that f is deﬁned
by DIAG(λ), that is f(s, t)=ITER|t|(g(λ|t|))(s, t), with λi
the fundamental sequence of λ and g(λi)=fλi
∈
Tλi.
By induction on the length of the input, we have that
f(s, a)=ITER|a|(g(λ|a|))(s, a)=s; obviously, there exists a
register machine computing the result within time Bλ|a|(n).
As for the step case we have that
f(s, ta)
=ITER|ta|(g(λ|ta|))(s, ta)
=ITER(ITER|t|(g(λ|ta|)))(s, t);
by inductive hypothesis, there exist a sequence of register
machines Mλ|ta| computing the programs g(λ|ta|)’s within
time Bλ|ta|(n). We deﬁne a register machine Mf such that,
on input s, t iterates |t| times Mλ|ta|, within time Bλ|ta|(n) ≤
Bλ(n)
Lemma 5.2: The behaviour of a register machine which
computes its output within time O(Bλ(n)) can be simulated
by a program f in Tλ.
Proof: Given a register machine M respecting the hy-
pothesis, we have already seen that there exists a program
nxtM ∈ T0 such that, for input the code of a conﬁguration
of M, it returns the code of the conﬁguration induced by the
relation ⊢M. We have three cases:
(1) λ is a ﬁnite number; the proof follows from Lemma 4.2.
(2) λ is in the form β+1; in this case, we deﬁne the program
σλ as follows:
σβ+1 :=RNMz/y(γβ+1), where γβ+1 :=SREC(σβ, σβ).
We have that
σβ+1(s, t) = γβ+1(s, t, t), and



γβ+1(s, t, a)
= σβ(s, t)
γβ+1(s, t, ra)
= σβ(γβ+1(s, t, r), t)
= γβ(γβ+1(s, t, r), t, t)
In particular, we have
σβ+1(s, t) =
γβ+1(s, t, t) =
σβ(σβ(. . . σβ(s, t) . . . , t), t)
|
{z
}
|t|times
By induction we see that σβ iterates nxtM on its input s for
Bβ(|t|) times, and that it belongs to Tβ. The result follows
observing that σβ+1 iterates nxtM for |t|Bβ(|t|) = Bβ+1(|t|)
times.
(3) λ is a limit ordinal. Let λ1, . . . , λn, . . . the funda-
mental sequence associated to λ, and σλ1, . . . , σλn, . . .
the sequence of programs enumerated by g, such that
g(λi)
=
σλi
∈
Tλi. We deﬁne γλ
by diagonal-
ization at λ. With a ﬁxed input s, t, we have that
γλ(s, t) =ITER|t|(g(λ|t|))(s, t) =ITER|t|(σλ|t|)(s, t).
The programs g(λ|t|) are deﬁned in Tλ|t|, and they iterate
the program nxtM on its input for Bλ|t|(|t|) times; this
implies that γλ iterates nxtM for Bλ|t|(|t|) = Bλ(|t|), for
each t.
Theorem 5.1: A program f belongs to Tα if and only if f
is computable by a register machine within time O(Bα(n)),
with α < ωω.
Proof: By lemma 5.1 and lemma 5.2.
VI. THE TIME-SPACE HIERARCHY
In this section, we introduce a restricted version of the
previously deﬁned time-hierarchy of recursive programs,
and we prove the equivalence with the classes of register
machines, which compute their output with a simultaneous
bound on time and space, following the work of [2].
A. Recursion-free programs and class S0
The reader should refer to Section II for the deﬁnitions of
basic instruction (the identity I(u), the constructors Ca
i (s),
and the destructors Di(s)); simple schemes (the renaming
RNMx/y(g) and RMNz/y(g), and the selection SELb
i(g, h));
and safe composition SCMPu(h, g). In particular, a modiﬁer is

345
International Journal on Advances in Intelligent Systems, vol 8 no 3 & 4, year 2015, http://www.iariajournals.org/intelligent_systems/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
obtained by the safe composition of a sequence of construc-
tors and a sequence of destructors; according to deﬁnition
2.5, the class T0 is the class of programs deﬁned by closure
of modiﬁers under SEL and SCMP.
Deﬁnition 6.1: Given f ∈ T0, the rate of growth rog(f)
is such that
1) if f is a modiﬁer, rog(f) is the difference between the
number of constructors and the number of destructors
occurring in its deﬁnition;
2) if f=SELb
i(g, h), then rog(f) is max(rog(g), rog(h));
3) if f=SCMPu(h, g), then rog(f) is max(rog(g), rog(h)).
Deﬁnition 6.2: S0 is the class of programs in T0 with non-
positive rate of growth, that is S0 = {f ∈ T0|rog(f) ≤ 0}.
Note that all programs in S0 modify their inputs according
to the result of some test performed over a ﬁxed number of
digits and, moreover, they cannot return values longer than
their input.
B. Safe recursion and class S1
As written in section 2.1, a program f(x, y, z) is deﬁned by
safe recursion in the basis g(x, y) and in the step h(x, y, z)
if for all s, t, r we have
{ f(s, t, a)
=
g(s, t)
f(s, t, ra)
=
h(f(s, t, r), t, ra).
In this case, f is denoted with SREC(g, h). In particular,
f(x, z) is deﬁned by iteration of h(x) if for all s, r we have
{ f(s, a)
=
s
f(s, ra)
=
h(f(s, r)).
In this case, f is denoted with ITER(h), and we write h|r|(s)
for ITER(h)(s, r).
Deﬁnition 6.3:
1) ITER(S0) denotes the class of pro-
grams obtained by one application of iteration to pro-
grams in S0;
2) S1 is the class of programs obtained by closure under
safe composition and simple schemes of programs in
S0 and programs in ITER(S0);
Notation: S1=(S0, ITER(S0); SCMP, SIMPLE);
3) Sk+1 is the class of programs obtained by closure under
simple schemes of programs in Sk and programs in
SREC(Sk).
Notation: Sk+1=(Sk, SREC(Sk); SIMPLE).
Hence, hierarchy Sk, with k ∈ N, is a version of Tk in
which each program returns a result whose length is exactly
bounded by the length of the input; this does not happen if
we allow the closure of Sk under SCMP. We will use this
result to evaluate the space complexity of our programs.
Deﬁnition 6.4: Given the programs g and h, f is ob-
tained by weak composition of h in g if f(x, y, z) =
g(h(x, y, z), y, z). Notation: f =WCMP(h, g).
In the weak form of composition the program h can
be substituted only in the variable x, while in the safe
composition the substitution is possible in all variables.
Deﬁnition 6.5: For all p, q ≥ 1, T Sqp is the class of
programs obtained by weak composition of h in g, with
h ∈ Tq, g ∈ Sp and q ≤ p.
Lemma 6.1: For all f in Sp, we have |f(s, t, r)|
≤
max(|s|, |t|, |r|).
Proof: By induction on p. Base. The relevant case is
when f ∈ S1 and f is deﬁned by iteration of g in S0 (that is,
rog(g) ≤ 0). By induction on r, we have that |f(s, a)| = |s|,
and |f(s, ra)| = |g(f(s, r))| ≤ |f(s, r)| ≤ max(|s|, |r|).
Step. Given f ∈ Sp+1, deﬁned by SREC in g and h in Sp,
we have
|f(s, t, a)|
= |g(s, t)|
by deﬁnition of f
≤ | max(|s|, |t|)|
by inductive hypothesis.
and
|f(s, t, ra)|
= |h(f(s, t, r), t, ra)|
≤ | max(|f(s, t, r)|, |t|, |ra|)|
≤ | max(max(|s|, |t|, |r|), |t|, |ra|)|
≤ | max(|s|, |t|, |ra|)|.
by deﬁnition of f, inductive hypothesis on h and induction
on r.
Lemma 6.2: Each f in T Sqp (with p, q ≥ 1) can be
computed by a register machine within time O(np) and space
O(nq).
Proof: Let f be in T Sqp. By deﬁnition 6.5, f is deﬁned
by weak composition of h ∈ Tq into g ∈ Sp, that is,
f(s, t, r) = g(h(s, t, r), t, r). The theorem 5.1 states that
there exists a register machine Mh, which computes h within
time nq, and there exists another register machine Mg, which
computes g within time np. Since g belongs to Sp, lemma
6.1 holds for g; hence, the space needed by Mg is at most
n.
We deﬁne now a machine Mf that, by input s, t, r, performs
the following steps:
(1) it calls Mh on input s, t, r;
(2) it calls Mg on input h(s, t, r), t, r, stored in the appro-
priate registers.
According to lemma 4.2, Mh needs time equal to |s| +
lh(h)(|t| + |r|)q to compute h, and Mg needs |h(s, t, r)| +
lh(g)(|t| + |r|)p to compute g.
This happens because lemma 4.2 shows, in general, that the
time used by a register machine to compute a program is
bounded by a polynomial in the length of its inputs, but,
more precisely, it shows that the time complexity is linear
in |s|. Moreover, since in our language there is no kind
of identiﬁcation of x as z, Mf never moves the content
of a register associated to h(s, t, r) into another register
and, in particular, into a register whose value plays the
role of recursive variable. Thus, the overall time-bound is
|s|+lh(h)(|t|+|r|)q +lh(g)(|t|+|r|)p which can be reduced
to np, being q ≤ p.
Mh requires space nq to compute the value of h on input
s, t, r; as we noted above, the space needed by Mg for the

346
International Journal on Advances in Intelligent Systems, vol 8 no 3 & 4, year 2015, http://www.iariajournals.org/intelligent_systems/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
computation of g is linear in the length of the input, and thus
the overall space needed by Mf is still nq.
Lemma 6.3: A register machine which computes its output
within time O(np) and space O(nq) can be simulated by a
program f ∈ T Sqp.
Proof: Let M be a register machine, whose computation
is time-bounded by np and, simultaneously, is space-bounded
by nq. M can be simulated by the composition of two
machines, Mh (time-bounded by nq), and Mg (time-bounded
by np and, simultaneously, space-bounded by n): the former
delimits (within nq steps) the space that the latter will
successively use in order to simulate M.
By theorem 5.1 there exists h ∈ Tq that simulates the
behaviour of Mh, and there exists g ∈ Tp that simulates
the behaviour of Mg; this is done by means of nxtg, which
belongs to S0, since it never adds a digit to the description
of Mg without erasing another one.
According to the proof of lemma 4.1, we are able to deﬁne
σn−1 ∈ Sn, such that σn−1(s, t) = nxt|t|n
g
. The result
follows deﬁning sim(s) = σp−1(h(s), s) ∈ T Sqp.
Theorem 6.1: f belongs to T Sqp if and only if f is
computable by a register machine within time O(np) and
space O(nq).
Proof: By lemma 6.2 and lemma 6.3.
VII. CONCLUSIONS AND FURTHER WORK
In this paper, we have introduced a version of safe recur-
sion, together with constructive diagonalization; by means of
these two operators, we’ve been able to deﬁne a hierarchy of
classes of programs Tλ, with 0 ≤ λ < ωω. Each ﬁnite level of
the hierarchy characterizes the register machines computing
their output within time O(nk); using the natural deﬁnition of
structured ordinals, and combining it with the diagonalization
operator, we have that the transﬁnite levels of the hierarchy
characterize the classes of register machine computing their
output within time bounded by the slow-growing function
Bλ(n), up to the machines with exponential time complexity.
In the last section, we have deﬁned a hierarchy of programs
with simultaneous time and space bound.
While the safe recursion scheme has been studied thor-
oughly, we feel that the diagonalization operator as presented
in this work, or as in Marion’s approach (see [12]), deserves
a more accurate analysis. In particular, we believe that it can
be considered as predicative as the safe recursion, and that it
could be used to stretch the hierarchy of programs in order
to capture the low Grzegorczyk classes (see [17] for a non-
constructive approach).
REFERENCES
[1] E. Covino and G. Pani, “A Specialized Recursive Language for
Capturing Time-Space Complexity Classes,” in The Sixth International
Conference on Computational Logics, Algebras, Programming, Tools,
and Benchmarking, (COMPUTATION TOOLS 2015), Nice, France,
2015, pp. 8–13.
[2] A. Cobham, “The intrinsic computational difﬁculty of functions,” in Y.
Bar-Hillel (ed), Proceedings of the International Conference on Logic,
Methodology, and Philosophy of Science, Amsterdam. North-Holland,
1962, pp. 24–30.
[3] H. Simmons, “The realm of primitive recursion,” Arch.Math. Logic,
vol. 27, 1988, pp. 177–121 885.
[4] S. Bellantoni and S. Cook, “A New Recursion-Theoretic Characteriza-
tion Of The Polytime Functions,” Computational Complexity, vol. 2,
1992, pp. 97–110.
[5] D. Leivant, “A foundational delineation of computational feasibility,”
in Proceedings of the 6th Annual Symposium on Logic in Computer
Science, (LICS’91), Amsterdam. IEEE Computer Society Press, 1991,
pp. 2–18.
[6] ——, Predicative recurrence and computational complexity I: word
recurrence and polytime.
Birkauser, 1994, pp. 320–343.
[7] D. Leivan and J.-Y. Marion, “Ramiﬁed recurrence and computational
complexity II: substitution and polyspace,” in J.Tiuryn and L.Pocholsky
(eds), Computer Science Logic, LNCS 933, Amsterdam.
Springer
Berlin Heidelberg, 1995, pp. 486–500.
[8] I. Oitavem, “New recursive characterization of the elementary func-
tions and the functions computable in polynomial space,” Revista
Matematica de la Univaersidad Complutense de Madrid, vol. 10, 1997,
pp. 109–125.
[9] P. Clote, “A time-space hierarchy between polynomial time and
polynomial space,” Math. Sys. The., vol. 25, 1992, pp. 77–92.
[10] D. Leivant, “Stratiﬁed functional programs and computational com-
plexity,” in Proceedings of the 20th Annual ACM SIGPLAN-SIGACT
Symposium on Principles of Programming Languages, (POPL’93),
Charleston.
ACM, 1993, pp. 325–333.
[11] M. Fairtlough and S. Weiner, Hierarchies of provably recursive func-
tions.
Elsevier, Amsterdam, 1998, chapter 3, pp. 149–207.
[12] J. Marion, “On tiered small jump operators,” Logical Methods in
Computer Science, vol. 5, no. 1, 2009.
[13] T. Arai and N. Eguchi, “A new function algebra of EXPTIME functions
by safe nested recursion,” ACM Transactions on Computational Logic,
vol. 10, 2009, pp. 1–19.
[14] D. Leivant, “Ramiﬁed recurrence and computational complexity III:
higher type recurrence and elementary complexity,” Annals of pure
and applied logic, vol. 96, 1999, pp. 209–229.
[15] S. Caporaso, G. Pani, and E. Covino, “A predicative approach to the
classiﬁcation problem,” Journal of Functional Programming, vol. 11,
2001, pp. 95–116.
[16] M. Hofmann, “Linear types and non-size-increasing polynomial time
computation,” in Proceedings of the 14th Symposium on Logic in
Computer Science (LICS’99), Trento.
IEEE Computer Society Press,
1999, pp. 464–473.
[17] S. Bellantoni and K. Niggl, “Ranking primitive recursion: the low
Grzegorczyk classes revisited,” SIAM Journal on Computing, vol. 29,
1999, pp. 401–4015.

