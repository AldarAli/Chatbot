Point Cloud Mapping Using Only Onboard Lidar  
in GNSS Denied and Dynamic Environments 
Misato Yamaji, Seiya Tanaka                                     Masafumi Hashimoto, Kazuhiko Takahashi 
Graduate School of Science and Engineering                                          Faculty of Science and Engineering 
Doshisha University                                                                           Doshisha University 
Kyotanabe, Kyoto 610-0394 Japan                                                   Kyotanabe, Kyoto 610-0394 Japan 
e-mail: {mhashimo, katakaha}@mail.doshisha.ac.jp 
 
Abstract— This paper presents a 3D point cloud mapping in 
Global Navigation Satellite Systems (GNSS) denied and 
dynamic outdoor environments using only a scanning 
multilayer lidar mounted on a vehicle. Distortion in scan data 
from the lidar is corrected by estimating the vehicle’s pose (3D 
positions and attitude angles) in a period shorter than the lidar 
scan period based on Normal Distributions Transform (NDT) 
scan matching and Extended Kalman Filter (EKF). The 
corrected scan data are mapped onto an elevation map; static 
and moving scan data, which are originated from static and 
moving objects, respectively, in the environments, are classified 
using the occupancy grid method. Only the static scan data are 
applied to generate a point cloud map using NDT-based 
Simultaneous Localization And Mapping (SLAM) and graph-
based SLAM. Experimental results in a public road 
environment show the performance of the proposed method. 
Keywords-lidar; point cloud map; distortion correction; NDT 
SLAM; graph SLAM; GNSS denied environment; dynamic 
environment. 
I. 
 INTRODUCTION 
Recently, studies have been conducted on autonomous 
driving and active safety of vehicles, such as automobiles 
and personal mobility vehicles, and on autonomous robots 
for 
last-mile 
and 
first-mile 
automation. 
Important 
technologies in these studies include environmental map 
generation [1] and map-matching based self-pose estimation 
by vehicles using the generated maps [2]. A lot of their 
related studies using cameras, lidars, and radars have been 
actively conducting [3][4].   
In this paper, we focus on map generation with a lidar 
mounted on a vehicle. When compared with camera (vision) 
based map generation, lidar based map generation is robust 
to lighting conditions and require less computational time. 
Furthermore, lidar based map generation provides mapping 
accuracy better than radar based map generation due to 
higher spatial resolution of lidar. From these reasons, we 
focus on lidar based map generation. 
In Intelligent Transportation Systems (ITS) domains, 
mobile mapping systems are utilized to map generation in 
wide road environments, such as highways and motorways 
[5]. We have been studying a method for point cloud 
mapping (map generation) using only a lidar mounted on a 
vehicle in narrow road environments, such as at community 
and scenic roads in urban and mountainous areas [6]. The 
generated map could be applied to autonomous driving and 
navigation of various smart vehicles, such as intelligent 
wheelchairs, personal mobility vehicles, and delivery robots 
[7]. The generated maps may also be utilized in various 
social services, such as disaster prevention and mitigation.  
In urban and mountainous environments, the information 
of Global Navigation Satellite Systems (GNSS) is often 
disturbed and denied. For map generation in GNSS denied 
environments, Simultaneous Localization And Mapping 
(SLAM) using Normal Distributions Transform (NDT) [8] or 
Iterative Closest Points (ICP) [9] methods have been applied. 
In their scan matching based SLAM, the accuracy in 
mapping wide area degrades due to the accumulation error. 
To reduce the accumulation error, the graph-based SLAM 
[10] is usually applied. Recently, we presented a map 
generation method using NDT-based SLAM and graph-
based SLAM [6]. A vehicle equipped with a lidar was moved 
so that loops could be made in road network topology, and 
several submaps (maps of different small areas) were 
generated using recursive NDT-based SLAM and graph-
based SLAM. Several submaps were also merged using 
graph-based SLAM. Such approach in submap generation 
and merging makes it easy to update and maintain maps. 
In environments, moving objects, such as automobile, 
two-wheeled vehicles, and pedestrians, exist. In such 
dynamic environments, the lidar scan data are therefore 
classified into two types: scan data originated from moving 
objects (referred to as moving scan data), and those 
originated from static objects (static scan data), such as 
buildings, trees, and traffic poles. For accurate map 
generation, the moving scan data should be removed, and 
only the static scan data should be utilized. Several methods 
for mapping in dynamic environments have been presented 
[11][12][13]; 
however, 
map 
generation 
in 
dynamic 
environments still represent a significant challenge compared 
with map generation in static environments. Our map 
generation method [6] was also applicable in static 
environments. Apart from map generation, we have been 
studying moving object detection and tracking in crowded 
dynamic environments [14][15]. The detection and tracking 
methods can contribute in accurately generating maps by 
extracting the static scan data from the lidar data. 
Map generation using the onboard lidar is performed by 
mapping lidar scan data captured in a sensor coordinate 
frame onto a world coordinate frame using the vehicle’s 
self-pose (position and attitude angles) information. The 
43
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-771-9
ICONS 2020 : The Fifteenth International Conference on Systems

lidar obtains range measurements by scanning lidar beams. 
Thus, when the vehicle moves, the entire scan data within 
one scan (lidar beam rotation of 360° in a horizontal plane) 
cannot be obtained at the same pose of the vehicle. 
Therefore, if the entire scan data obtained within one scan is 
mapped onto the world coordinate frame using the vehicle’s 
pose information, distortion arises in environmental maps. 
To correct this distortion, the vehicle’s pose should be 
determined more frequently than the lidar scan period, i.e., 
for every lidar measurement in the scan. Many methods for 
distortion correction have been proposed [16][17][18]. We 
also presented a distortion correction method using only the 
lidar information; the NDT scan matching and Extended 
Kalman Filter (EKF) were applied to estimate the vehicle’s 
pose, and the distortion in the lidar scan data was corrected 
using the pose estimates [19]. 
In this paper, to generate a 3D point cloud map in GNSS 
denied and dynamic environments using only the onboard 
scanning lidar, we integrate three methods that we 
previously proposed: distortion correction in the lidar scan 
data, extraction of the static scan data from the entire lidar 
scan data, and point cloud mapping based on NDT and 
graph-based SLAM. The mapping performance is shown 
through experimental results in public road environments. 
The rest of this paper is organized as follows. In Section 
II, we give the experimental system and overview a map 
generation method based on NDT-based and graph-based 
SLAM. In Section III, we explain a correction method of 
distortion in the lidar scan data, and in Section IV, we 
describe an extraction method of the static scan data. In 
Section V, we conduct experiments to verify the proposed 
method, followed by conclusions in Section VI. 
II. 
EXPERIMENTAL SYSTEM AND SLAM OVERVIEW  
In this section, we show our experimental system and 
briefly describe the scan data mapping using NDT scan 
matching (NDT-based SLAM) and graph-based SALM. 
A. Experimental System 
As shown in Fig. 1, our experimental small vehicle is 
equipped with a scanning 32-layer lidar (Velodyne HDL-
32E). The maximum range of the lidar is 70 m, the 
horizontal viewing angle is 360° with a resolution of 0.16°, 
and the vertical viewing angle is 41.34° with a resolution of 
 
 
 
Figure 1. Overview of the experimental vehicle.  
 
 
Figure 2.  Overview of NDT-based SLAM. 
 
 
1.33°. The lidar provides 384 measurements (the object’s 
3D position and reflection intensity) every 0.55 ms (at 2° 
horizontal angle increments). The period for the lidar beam 
to complete one rotation (360°) in the horizontal direction is 
100 ms, and 70,000 measurements are then obtained in one 
rotation. 
In this paper, one rotation of the lidar beam in the 
horizontal direction (360°) is referred to as one scan, and the 
data obtained from this scan is referred to as scan data. 
Moreover, the lidar scan period (100 ms) is denoted as 
 
and scan data observation period (0.55 ms) as 
. 
B. NDT-based SLAM 
The process for NDT-based SLAM is shown in Fig. 2. 
To be clear, the NDT scan matching [8] is described in this 
subsection. Distortion correction method is detailed in the 
following section. 
First of all, the scan data related to road surfaces are 
removed, and the scan data related to objects are mapped 
onto a 3D grid map (a voxel map) represented in the 
vehicle’s coordinate frame, 
b . A voxel grid filter is applied 
to downsize the scan data. The voxel used for the voxel grid 
filter is a tetrahedron with a side length of 0.2 m. 
In the world coordinate frame, 
W , a voxel map with a 
voxel size of 1 m is used for NDT scan matching. For the i-
th (i = 1, 2, …n) measurement in the scan data, we define 
the position vector in 
b  as 
bi
p  and that in
W as 
ip . Thus, 
the following relation is given by the homogeneous form: 
1
)
(
1
bi
i
p
Τ X
p
                                    (1) 
where 
T
x y z
( , , , , , )
X
 is the vehicle’s pose. 
x y z T
( , , )
 
and 
T)
( , ,
 are the 3D position and attitude angle (roll, 
pitch, and yaw angles) of the vehicle, respectively, in 
W . 
T(X) is the following homogeneous transformation matrix: 
1
0
0
0
cos cos
sin cos
sin
cos
sin
sin
cos sin
cos cos
sin
sin sin
sin
cos
sin sin
cos
cos sin
cos sin
cos
sin sin
cos
cos
)
(
z
y
x
Τ X
 
44
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-771-9
ICONS 2020 : The Fifteenth International Conference on Systems

The scan data obtained at the current time t  (t = 0, 1, 2, 
…), 
( )
( )
( )
1
2
,
,
t
t
t
b
b
b
p
p
p
, are referred to as the new input 
scan, and the scan data obtained in the previous time  before 
)1
(t
, 
1)
(
(1)
(0)
,
,
,
P t
P
P
P
, are referred to as the 
reference scan (environmental map). 
NDT scan matching conducts a normal distribution 
transformation for the reference scan data in each grid on a 
voxel map. It calculates the average value and covariance of 
the lidar measurement positions. By matching the new input 
scan at t
 with the reference scan data obtained prior to 
)1
(t
, the vehicle’s pose, 
X(t)
, at t
 is determined. The 
vehicle’s pose is used for conducting a coordinate transform 
with (1), and the new input scan can then be mapped to 
W , 
and the reference scan is updated.  
C. Graph-based SLAM 
To reduce the accumulation error of the map generated 
by NDT-based SLAM, we apply the graph-based SLAM 
[20]. The vehicle’s poses obtained by NDT-based SLAM are 
mapped onto a factor graph. To detect the loop (revisit area), 
we first obtain the candidates of the revisit areas using the 
information on self poses of the vehicle. Thereafter, the loop 
probability indicator (LPI) [21] and matching distance 
indicator (MDI) are calculated using the lidar scan data 
captured during the initial visit and revisit of the vehicle. A 
higher degree of similarity between the lidar scan data of the 
initial visit and revisit of the vehicle will lead to a larger LPI 
and a smaller MDI. Thus, we detect the loop using the LPI 
and MDI values. 
When the loop is detected, the vehicle’s pose is 
calculated at the revisit area relative to that at the first-visit 
area based on NDT scan matching. The relative poses of the 
vehicle are inputted to the factor graph as loop constraints. 
We then minimize the objective function of (2) so that the 
accuracy in the map generated by the NDT-based SLAM 
can be improved [21]: 
,
ˆ
ˆ
( )
(
)
(
)
T
ij
ij
ij
ij
ij
i j
F χ
X
X
Ω
X
X
            (2) 
where 
1
2
(
,
,
,
)
T
T
T
T
i
χ
X
X
X
. 
i
X is the vehicle’s pose at 
the i . 
ij
X  is the pose of the vehicle at the j  relative to 
that at the i , which is calculated from NDT scan matching. 
ˆ
Xij
 is the estimate of the relative pose. 
Ωij
is the 
information matrix. 
III. 
DISTORTION CORRECTION OF LIDAR SCAN DATA 
In this section, we describe a motion model of the 
vehicle for EKF and EKF-based correction method of 
distortion in lidar scan data. 
A. Motion Model 
As shown in Fig. 3, the vehicle’s linear velocity in 
b  is 
defined as Vb (the velocity in the xb-axis direction), and the 
angular velocities about the xb-, yb-, and zb- axes are defined 
as 
b , 
b , and 
b , respectively. If the vehicle is assumed to 
move at nearly constant linear and angular velocities, the 
following motion model can be derived: 
( )
( )
( )
( )
1
( )
( )
( )
( )
1
(
1)
( )
( )
( )
1
(
1)
2
3
4
(
1)
(
1)
(
1)
(
1)
(
1)
(
1)
(
1)
(
1)
cos
cos
cos
sin
sin
( )
( )
( )sin ( )
( )cos (
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
b
t
b
t
b
t
b
x
a
y
a
x
z
a
y
t
a
t
a
t
t
a
t
z
V
　
3
4
3
4
( )
( )
( )
( )
)
tan ( )
( )
( )cos ( )
( )sin ( )
( )
( )sin ( )
( )cos ( )
1
cos ( )
b
b
b
b
b t
V
t
b
t
b
t
b
t
t
t
a
t
t
a
t
t
t
a
t
t
a
t
t
t
V
w
w
w
w
      (3) 
where t and t+1 are time steps. 
2
1
/ 2
b
b
V
a
V
w
, 
a 2
 
2
/ 2
b
b
w
, 
2
3
/ 2
b
b
a
w
and 
a 4
  
b
 
2
/ 2
w b
. 
wV b
, 
w b
, 
w b
, and 
w b
 are the acceleration 
disturbances.  
Equation (3) is expressed in the vector form as follows: 
,
,)
(
1)
(
w
f ξ
ξ
t
t
                                 (4) 
where 
T
b
b
b
Vb
x y z
)
,
,
,
,
( , , , , ,
ξ
 and 
w
 
T
V
b
b
b
b
w
w
w
w
)
,
,
,
(
. 
We define the vehicle’s pose obtained at t
 using NDT 
scan matching as 
( )
( )
ˆ t
NDT t
X
z
. The measurement equation 
is then 
( )
( )
( )
NDT t
t
NDT t
z
Hξ
z
                         (5) 
where 
zNDT
 is the measurement noise, and H is the 
measurement matrix. 
B. EKF-based Distorion Correction 
The flow of distortion correction of the lidar scan data is 
shown in Fig. 4. The lidar scan period ( ) is 100 ms, and 
the scan data observation period (
) is 0.55 ms. When the 
scan data are mapped onto 
W  using the vehicle’s pose  
 
 
 
Figure 3.  Notation related to vehicle motion. 
45
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-771-9
ICONS 2020 : The Fifteenth International Conference on Systems

 
 
Figure 4.  The flow of distortion correction. 
 
 
calculated every lidar scan period, the distortion arises in 
environmental maps. We therefore correct the distortion in 
the lidar scan data by estimating the vehicle’s pose using 
EKF every scan data observation period. 
The state estimate and its error covariance obtained at 
)1
(t
 using EKF are denoted as 
1)
1 /
(ˆ
t
ξ t
 and 
1)
1 /
(
t
Γ t
, 
respectively. From these estimates, EKF gives the state 
prediction, 
1)
/
1,1
(ˆ
t
ξ t
, and its error covariance, 
1)
/
1,1
(
t
Γ t
, 
at 
)1
(t
+
 as follows: 
T
t
t
t
t
T
t
t
t
t
t
t
t
t
t
t
t
t
1)
1 /
(
1)
1 /
(
1)
1 /
(
1)
1 /
(
1)
1 /
(
1)
/
1,1
(
1)
1 /
(
1)
/
1,1
(
]
,0,
[ ˆ
ˆ
QG
G
F
Γ
F
Γ
f ξ
ξ
       (6) 
where F = 
ξ
f
/ ˆ
, G = 
f / w
, and Q is the covariance 
matrix of the plant noise, w. 
By 
a 
similar 
calculation, 
the 
state 
prediction, 
1)
/
,1
(ˆ
t
j
ξ t
, and its error covariance, 
1)
/
,1
(
j t
Γ t
, at 
)1
(t
+ j
 (where j = 1, 2, …,180) can be obtained by 
(
1,
/
1)
(
1,
1 /
1)
(
1,
/
1)
(
1,
1 /
1)
(
1,
1 /
1)
(
1,
1 /
1)
(
1,
1 /
1)
(
1,
1 /
1)
ˆ
[ ˆ
,0,
]
t
j
t
t
j
t
T
t
j
t
t
j
t
t
j
t
t
j
t
T
t
j
t
t
j
t
ξ
f ξ
Γ
F
Γ
F
G
QG
   
(7) 
In the state prediction 
1)
/
,1
(ˆ
t
j
ξ t
, we denote the 
elements related to the vehicle’s pose, 
( , , , , , )
x y z
, as 
(
1,
/
1)
ˆ
t
j
t
X
. Using (1) and the pose prediction, the scan 
data,
)
,1
(
j
pbi t
, in 
b  obtained at 
j
t
)1
(
 can be 
transformed to 
1)
/
,1
(
t
j
ip t
 in 
W  as follows: 
1
)
( ˆ
1
)
,1
(
1)
/
,1
(
1)
/
,1
(
j
t
bi
t
j
t
t
j
i t
p
Τ X
p
      (8) 
Because the lidar scan period (
) is 100 ms, and the 
scan data observation period (
) is 0.55 ms, the time t  is 
almost equal to (t-1)
+180
. Using the pose prediction, 
1)
,1 180 /
(ˆ
t
X t
 at t
, the scan data, 
1)
/
,1
(
t
j
ip t
, at 
j
t
)1
(
 in 
W  is transformed into the scan data, 
* ( )t
pbi
, 
at t  in 
b  as follows: 
1
)
ˆ
(
1
1)
/
,1
(
1
1)
,1 180 /
(
* ( )
t
j
t
i
t
t
bi t
p
X
p
         (9) 
Using the corrected scan data, 
bp* ( )t
 
*
*
( )
( )
1
2
,
,
t
t
b
b
p
p
, 
within one scan (lidar beam rotation of 360° in a horizontal 
plane), as the new input scan, NDT scan matching can 
accurately calculate the vehicle’s pose, 
zNDT (t)
, at t . Based 
on (4) and (5), EKF then gives the state estimate, 
/ )
(ˆ
ξ t t
, and 
its error covariance, 
( / )
Γ t t
, at t  by 
1)
,1 180 /
(
( )
1)
,1 180 /
(
/ )
(
1)
,1 180 /
( )
( )
1)
,1 180 /
(
( / )
}
(ˆ
{
ˆ
ˆ
t
t
t
t
t
t
t
t
t
t
NDT
t
t
t
t
t
HΓ
K
Γ
Γ
Hξ
z
K
ξ
ξ
    (10) 
where 
(
1, 180 /
1)
ˆ t
t
ξ
 and 
(
1,180 /
1)
t
t
Γ
 are the state 
prediction 
and 
its 
error 
covariance 
at 
t
(t-
1)
+180
respectively. 
K ( )t
1
(
1, 180 /
1)
( )
T
t
t
t
Γ
H S
 
and 
S(t)
(
1,180 /
1)
T
t
t
HΓ
H
R . R is the covariance 
matrix of 
zNDT
. 
The corrected scan data 
bP* ( )t
 are mapped onto 
W  
using the pose estimate calculated by (10), and the distortion 
in environmental maps can then be removed. 
IV. 
EXTRACTION OF STATIC SCAN DATA 
In dynamic environments wherein moving objects, such 
as cars, two-wheeled vehicles, and pedestrians, exist, the 
lidar scan data related to moving objects (referred to as 
moving scan data) should be removed from the entire scan 
data, and only the scan data related to static objects (static 
scan data), such as buildings and trees, should be utilized in 
map generation. 
To extract the static scan data, first, we classify the lidar 
scan data into two types: scan data that are originated from 
road surfaces (referred to as road surface scan data) and 
scan data that are originated from objects (referred to as 
object scan data) based on a rule-based method. The object 
scan data are mapped onto an elevation map represented in 
W . In this study, the cell of the elevation map is a square 
with a side length of 0.3 m.  
A cell in which scan data exist is referred to as an 
occupied cell. For the moving scan data, the time to occupy 
the same cell is short, whereas for the static scan data, the 
time is long. Therefore, using the occupancy grid method 
based on the cell occupancy time [14][15], we identify two 
types of cells: moving and static cells, which are occupied 
by the moving and static scan data, respectively. Since the 
scan data related to an object usually occupy more than one 
cell, adjacent occupied cells are clustered. Then, the scan 
data in clustered static cells are applied to map generation. 
When moving objects pause, the occupancy grid-based 
method mentioned above often misidentifies their scan data 
as the static scan data. To address this problem, the road 
surface scan data are mapped onto the elevation map, and 
the cells in which the road surface scan data are occupied 
for a while are determined as the road surface cells. If the 
object scan data exist on the road surface cells, we always 
determine the object scan data as the moving scan data and 
remove the moving scan data from the entire scan data. 
46
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-771-9
ICONS 2020 : The Fifteenth International Conference on Systems

V.    EXPERIMENTAL RESULTS 
Although as mentioned in Section I, our study focuses on 
map generation in narrow road environments, such as at 
community roads and scenic roads in urban and mountainous 
areas, we conducted experiments of map generation in highly 
traffic road environments (Fig. 5(a)) to discuss the 
performance of our method in dynamic environments.  
The traveled distance of the vehicle was about 2903 m, 
and the maximum speed of the vehicle was 40 km/h. In the 
urban road environment, there were 114 cars，26 two-  
 
 
(a) Moved path (red line) of the vehicle (top view).  
 
 
(b) Photo of area #1 (bird-eye view). Red line indicates moved path of the 
vehicle 
 
 
(c) Photo of area #2 (bird-eye view). Red line indicates moved path of the 
vehicle 
 
Figure 5.  Experimental environment. 
 
 
Figure 6.  Point cloud map (top view) 
 
 
 
(a)  Case 1 
 
 
(b)  Case 2 
 
 
(c)  Case 3 
 
Figure 7.  Mapping result of area #1 (bird-eye view). Black and red dots 
indicate the static and moving scan data, respectively.  
47
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-771-9
ICONS 2020 : The Fifteenth International Conference on Systems

 
 
 (a)  Case 1 
 
 
(b)  Case 2 
 
 
(c)  Case 3 
 
Figure 8.  Mapping result of area #2 (bird-eye view). Black and red dots 
indicate the static and moving scan data, respectively.  
 
 
                     
 
                (a)  Photo of traffic sign                       (b)  Case 1 
 
Figure 9.  Mapping result of a tree in area #2. 
                         
 
            (c)  Case 2                                             (d)  Case 3 
 
Figure 9.  Continued.  
 
 
TABLE I. DEVIATION BETWEEN START AND GOAL POSITIONS OF THE 
VEHICLE  
True 
NDT-based SLAM 
NDT and 
graph-based 
SLAM 
Case 1 
Case 2 
Case 3 
2.88 [m] 
22.41 [m] 
18.48 [m] 
132.16 [m] 
4.98 [m] 
 
 
wheeled vehicles, and 37 pedestrians. 
Figure 6 shows the mapping result using the NDT-based 
SLAM in conjunction with the methods of distortion 
correction of the lidar scan data and extraction of the static 
scan data. To evaluate the mapping performance using the 
NDT-based SLAM in detail, Figs. 7 and 8 show the 
enlarged map of area #1 (Fig. 5 (b)) and #2 (Fig. 5 (c)), 
respectively. Figure 9 also shows the mapping result of a 
traffic sign in area #2. For comparison purpose, the maps 
were generated in the following three cases: 
Case 1: Mapping by the proposed method; NDT-based 
SLAM with the methods of correcting distortion in the lidar 
scan data and extracting the static scan data,  
Case 2: NDT-based SLAM with the distortion correction 
method and without the method of static scan data 
extraction, and   
Case 3: NDT-based SLAM without the distortion 
correction method and with the method of static scan data 
extraction. 
In Figs. 7 and 8, case 1 (proposed method) and 3 more 
significantly remove the track of moving objects than case 2. 
In Fig. 9, the mapping results by case 1 and 2 are more 
crispness than the result by case 3. It is concluded from these 
figures that the proposed method provides better mapping 
result than case 2 and 3. 
Table I shows the positioning performance of the vehicle 
by SLAM; deviation between the start and goal positions of 
the vehicle. The true deviation is calculated from the 
position information using the onboard Real Time 
Kinematic-Global Positioning Systems (RTK-GPS) unit. It 
is clear from the table that distortion correction of the lidar 
scan data provides better mapping accuracy in NDT-based 
SLAM. In addition, it is clear that graph-based SLAM 
further improves the mapping accuracy. 
48
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-771-9
ICONS 2020 : The Fifteenth International Conference on Systems

VI.    CONCLUSION 
This paper presented lidar based map generation in 
GNSS denied and dynamic outdoor environments using only 
the onboard scanning lidar. The 3D point cloud mapping was 
performed by integrating three algorithms that we previously 
proposed: distortion correction in the lidar scan data, 
extraction of the static scan data (removal of the moving scan 
data) from the entire lidar scan data, and NDT-based and 
graph-based SLAM. The performance of the map generation 
was shown through experimental results in urban road 
environments.  
We are currently improving performance of removal of 
moving scan data. We are also evaluating the proposed 
method in various environments, including large-scale 
residential environments.  
ACKNOWLEDGMENT 
This study was partially supported by the KAKENHI 
Grant #18K04062, the Japan Society for the Promotion of 
Science (JSPS). 
REFERENCES 
[1] 
C. Cadena, et al., “Past, Present, and Future of Simultaneous 
Localization and Mapping: Toward the Robust-Perception 
Age,” IEEE Trans. on Robotics, vol. 32, no. 6, pp. 1309–
1332, 2016. 
[2] 
L. Wang, Y. Zhang, and J. Wang, “Map-Based Localization 
Method for Autonomous Vehicles Using 3D-LIDAR,” IFAC-
Papers OnLine, vol. 50, issue 1, pp. 276-281, 2017. 
[3] 
B. Huang, J. Zhao, and J. Liu, “A Survey of Simultaneous 
Localization and Mapping,” eprint arXiv:1909.05214, 2019. 
[4] 
S. Kuutti, et al., “A Survey of the State-of-the-Art 
Localization 
Techniques 
and 
Their 
Potentials 
for 
Autonomous Vehicle Applications, ” IEEE Internet of Things 
Journal, vol.5, pp.829–846, 2018. 
[5] 
H. G. Seif and X. Hu, “Autonomous Driving in the iCity—
HD Maps as a Key Challenge of the Automotive Industry,” 
Engineering, vol. 2, pp.159–162, 2016. 
[6] 
K. Morita, M. Hashimoto, and K. Takahashi, “Point-Cloud 
Mapping and Merging using Mobile Laser Scanner,” Proc. of 
the third IEEE Int. Conf. on Robotic Computing (IRC 2019), 
pp.417–418, 2019. 
[7] 
D. Schwesinger, A. Shariati, C. Montella, and J. Spletzer, “A 
Smart Wheelchair Ecosystem for Autonomous Navigation in 
Urban Environments,” Autonomous Robot, vol. 41, pp. 519–
538, 2017. 
[8] 
P. Biber and W. Strasser, “The Normal Distributions 
Transform: A New Approach to Laser Scan Matching,” Proc. 
of IEEE/RSJ Int. Conf. on Intelligent Robots and Systems 
(IROS 2003), pp. 2743–2748, 2003. 
[9] 
P. J. Besl and N. D. McKay, “A Method of Registration of 3-
D Shapes,” IEEE Trans. on Pattern Analysis and Machine 
Intelligence, vol. 14, no. 2, pp. 239–256, 1992. 
[10] G. Grisetti, R. Kummerle, C. Stachniss, and W. Burgard, “A 
Tutorial 
on 
Graph-based 
SLAM,” 
IEEE 
Intelligent 
Transportation Systems Magazine, pp. 31–43, 2010. 
[11] G. Bresson, Z. Alsayed, L. Yu, and S. Glaser, “Simultaneous 
Localization and Mapping: A Survey of Current Trends in 
Autonomous Driving,” IEEE Trans. on Intelligent Vehicles, 
vol. 2, pp.194–220, 2017. 
[12] J. P. Saarinen, H. Andreasson, T. Stoyanov, and A. J. 
Lilienthal, “3D Normal Distributions Transform Occupancy 
Maps: An Efﬁcient Representation for Mapping in Dynamic 
Environments,” Int. J. of Robotics Research, vol.32, no.14, 
pp.1627–1644, 2013. 
[13] X. Ding, Y. Wang, H. Yin, L. Tang, and R. Xiong, “Multi-
session 
Map 
Construction 
in 
Outdoor 
Dynamic 
Environment,” Proc. of the 2018 IEEE Int. Conf. on Real-
time Computing and Robotics (IRC2018), pp. 384–389, 2018. 
[14] S. Sato, M. Hashimoto, M. Takita, K. Takagi, and T. Ogawa, 
“Multilayer Lidar-Based Pedestrian Tracking in Urban 
Environments,” Proc. of IEEE Intelligent Vehicles Symp. 
(IV2010), pp. 849–854, 2010. 
[15] S. Kanaki, et al., “Cooperative Moving-Object Tracking with 
Multiple Mobile Sensor Nodes -Size and Posture Estimation 
of Moving Objects using In-vehicle Multilayer Laser 
Scanner-,” Proc. of 2016 IEEE Int. Conf. on Industrial 
Technology (ICIT 2016), pp. 59–64, 2016. 
[16] S. Hong, H. Ko, and J. Kim, “VICP: Velocity Updating 
Iterative Closest Point Algorithm,” Proc. of 2010 IEEE Int. 
Conf. on Robotics and Automation (ICRA 2010), pp. 1893–
1898, 2010. 
[17] F. Moosmann and C. Stiller, “Velodyne SLAM,” Proc. of 
IEEE Intelligent Vehicles Symp. (IV2011), pp. 393–398, 
2011. 
[18] J. Zhang and A. Singh, “LOAM: Lidar Odometry and 
Mapping in Real-time,” Proc. of Robotics: Science and 
Systems, 2014. 
[19] K. Inui, M. Morikawa, M. Hashimoto, and K. Takahashi, 
“Distortion Correction of Laser Scan Data from In-vehicle 
Laser Scanner based on Kalman Filter and NDT Scan 
Matching,” Proc. of the 14th Int. Conf. on Informatics in 
Control, Automation and Robotics (ICINCO), pp. 329–334, 
2017. 
[20] R. Kümmerle, G. Grisetti, H. Strasdat, K. Konolige, and W. 
Burgard, 
“G2o: 
A 
General 
Framework 
for 
Graph 
Optimization,” Proc. of 2011 IEEE Int. Conf. on Robotics 
and Automation, pp. 3607–3613, 2011. 
[21] F. Martín, R. Triebel, L. Moreno, and R. Siegwart, “Two 
Different Tools for Three-dimensional Mapping: DE-based 
Scan Matching and Feature-based Loop Detection,” Robotica, 
vol. 32, pp. 19–41, 2017. 
 
 
49
Copyright (c) IARIA, 2020.     ISBN:  978-1-61208-771-9
ICONS 2020 : The Fifteenth International Conference on Systems

