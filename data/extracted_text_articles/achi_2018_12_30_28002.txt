Activity Recognition With Multiple Wearable Sensors for Industrial Applications
Adrien Malais´e, Pauline Maurice, Francis Colas, Franc¸ois Charpillet and Serena Ivaldi
Inria, Villers-l`es-Nancy, F-54600, France
Universit´e de Lorraine, Loria, UMR 7503, Vandoeuvre-l`es-Nancy, F-54506, France
CNRS, Loria, UMR 7503, Vandoeuvre-l`es-Nancy, F-54506, France
Email: firstname.surname@inria.fr
Abstract—In this paper, we address the problem of recognizing
the current activity performed by a human operator, providing
an information useful for automatic ergonomic evaluation for
industrial applications. While the majority of research in activity
recognition relies on cameras observing the human, here we
explore the use of wearable sensors, which are more suitable
in industrial environments. We use a wearable motion tracking
suit and a sensorized glove. We describe our approach for activity
recognition with a probabilistic model based on Hidden Markov
Models, applied to the problem of recognizing elementary acti-
vities during a pick-and-place task inspired by a manufacturing
scenario. We show that our model is able to correctly recognize
the activities with 96% of precision if both sensors are used.
Keywords–Activity recognition; Hidden Markov Model; Wear-
able sensors.
I.
INTRODUCTION
In developed countries, work-related musculoskeletal dis-
orders (MSDs) are a major health issue. MSDs affect almost
50 % of industrial workers and represent an important cost
for companies [1]. In order to reduce the prevalence of work-
related MSDs, the ergonomics of the workplace needs to
be evaluated and improved. Standards ergonomic assessment
methods rely on pen-and-paper worksheets ﬁlled by experts,
such as the commonly used European Assembly Worksheet
(EAWS) [2][3]. Some digital human modeling software pro-
vide automatic ﬁlling of these ergonomic worksheets [4], but
this cannot be done directly from raw data (video, motion cap-
ture, etc.) because the scoring system depends on the activity
that is being performed (e.g., walking, bending, carrying an
object). The software user has ﬁrst to manually identify the
different types of movements occurring in the task. Therefore,
there exists no tool to inform a worker in real-time whether
s/he is performing a task in an ergonomic way or not. Yet, such
an evaluation could help reducing the risk of MSDs. This is
one of the objectives of the European AnDy project [5].
The ﬁrst step towards a fully automatic ergonomic as-
sessment is to automatically identify the different activities
within an industrial task. To address this problem, this paper
proposes a method based on wearable sensors and Hidden
Markov Model (HMM). We focus our activity recognition on
a pick-and-place task inspired by a manufacturing scenario.
The whole-body motions of the operator are recorded with
inertial sensors embedded in a suit. Though motion-capture
based activity recognition using HMM already exists [6][7],
the recognition rate is not yet perfect and could be improved.
This can be an issue for industrial applications. Therefore,
a.
b.
Figure 1. Wearable sensors used in the experiment: (a) XSens MVN suit
[16]; (b) e-glove from Emphasis Telematics.
we propose to complement the motion capture system with
a glove embedding force sensors to detect hand contacts with
the manipulated objects. This paper focuses on evaluating the
beneﬁt of using the contact information for the recognition
performance with HMM-based models.
The paper is organized as follows: Section II presents
state-of-the-art activity recognition methods based either on
external sensors or on wearable sensors. Section III describes
the proposed HMM-based recognition method and presents the
experimental test-bed. The results of the comparison with vs.
without contact information are presented in Section IV and
discussed in Section V.
II.
RELATED WORK
A. Motion capture based activity recognition
Human activity recognition methods can exploit external
or exteroceptive sensors and wearable sensors [6][8][9].
Most external sensors approaches use vision-based sys-
tems, such as RGB-D cameras or optical motion capture
systems. RGB-D cameras, such as Microsoft Kinect, require
229
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-616-3
ACHI 2018 : The Eleventh International Conference on Advances in Computer-Human Interactions

A.
B.
C.
D.
E.
Figure 2. Examples of activities extracted from the MVN Studio software. A: WAIT, B: REACH, C: CARRY, D: PLACE, E: WALK.
image processing to extract motion features. Conversely, in
optical motion capture, the 3D trajectories of markers placed
on the user are directly retrieved. A major problem of vision-
based systems is that the user must always be in the camera’s
ﬁeld of view, which limits the applicability in complex spaces.
Occlusion of markers is another issue for robust motion
detection, especially in cluttered environments.
To avoid these issues that are frequent in industrial con-
ditions, wearable sensors can be used. Such sensors are then
directly attached to the user, and no external sensor is needed.
Inertial sensors placed on the limbs and torso of the user are
the most commonly used [10] [11].
B. Algorithms for activity recognition
Classiﬁcation algorithms have been widely used to recog-
nize human daily activities, such as walking, sitting or lying
[12][10]. Using three inertial sensors placed on the chest, right
thigh and left ankle, Attal et al. [10] compared the k-Nearest
Neighbor (k-NN), Support Vector Machines (SVM), and HMM
algorithms for both supervised and unsupervised learning.
They showed that with supervised learning k-NN gives the
best performance, whereas with unsupervised learning HMM
performs best. They showed that main advantage of using
HMM was that the model took the temporal aspect into
account. Dubois and Charpillet [12] showed that HMM can
efﬁciently discriminate falls from other daily life activities.
Mandery et al. [13] used HMM to identify the best performing
sets of features for dimensionality reduction of motion capture
data. They showed that a small subset of features was sufﬁcient
to perform accurate recognition. Interestingly, the velocity of
the whole-body center of mass was always included in the
relevant features.
All the aforementioned studies only used motion capture
data. Conversely, W¨achter and Asfour [14] used optical devices
to track not only humans but also objects motions. The distance
between the human and tracked objects was used to detect
contact and pre-segment the data before the recognition step.
Coupet´e et al. [15] addressed the problem of real-time activity
recognition in an industrial environment using HMM and
object-related information. They used depth-cameras to capture
human motion and inertial sensors to track tools manipulated
by the worker. They showed that the tool-related information
improved the classiﬁcation performance from 80% to 94%.
III.
METHOD
A. Experimental protocol
1) Material: We used two wearable systems: the MVN
Link suit from Xsens [16] (Figure 1a) and the e-glove from
Emphasis Telematics [17] (Figure 1b). The XSens MVN suit
was used to capture the whole-body human motion with
17 wireless inertial sensors embedded in a lycra suit. The
suit information was combined with a glove containing force
sensors on the ﬁngertips of thumb, index and middle ﬁngers
and on the palm. However, in this paper, we used only the
palm force information to detect contacts.
The sample rates of the MVN suit and the glove are 240 Hz
and 100 Hz, respectively. To synchronize the data, both systems
used the same wireless network during data collection, and
each recorded sample was associated to an absolute time-
stamps value.
2) Task description: To evaluate our method, we designed
a pick-and-place task of a 6 kg bar, inspired by packaging tasks
on assembly lines in manufacturing industry.
One male participant performed 8 sequences of the task,
with each sequence consisting of 6 to 8 pick-and-place. Each
sequence started and ended in the same neutral pose. The bar
was initially placed at a height of 45 cm on a 100 × 50 cm ﬂat
support. The participant was instructed to take the bar with
both hands, carry it to the other side of the support, place the
bar there and return to the initial position to perform the next
iteration. Each sequence lasted around one minute. In order
to add variability in the data, the participant was instructed to
change the position of his hands on the bar, and to follow two
different paths when going to and coming from the bar ﬁnal
position.
We deﬁned seven states/activities (Figure 2):
•
WAIT: standing still
•
REACH: bending forward, without the bar
•
PICK: standing up straight while holding the bar
•
CARRY: walking while carrying the bar in the hands
•
PLACE: bending forwards with the bar in the hands
•
RELEASE: standing up straight with empty hands
•
WALK: walking without holding the bar
B. Activity recognition algorithm
1) Hidden Markov Model: We use HMM-based supervised
learning to recognize the activities with the hmmlearn [18]
library in Python. The model is deﬁned by N states repre-
senting the activities, such as WALK or PLACE (all activities
are presented in Section III-A2), with S = {s1, s2, ..., sN}
being the set of possible states. Each recorded sequence
k ∈ [1, K = 8] is represented by a series of discrete states
Qk = {qk
0, qk
1, ..., qk
t , ...qk
T } and a series of T observations
Xk = {xk
1, ..., xk
t , ..., xk
T } corresponding to motion capture
and glove data. For each instant, the goal is to infer the current
hidden state, such as qk
t = si.
Three parameters {Π, A, B} represent the model. Π =
{π1, π2, ..., πN} denotes the initial state probabilities and is
learned from the training set. For each state, πi is equal
230
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-616-3
ACHI 2018 : The Eleventh International Conference on Advances in Computer-Human Interactions

Figure 3. Probability transition diagram. The graph is not fully connected
because some transitions are impossible. For instance, to PLACE an object, it
ﬁrst needs to be PICKED.
to the frequency of appearance of this state at the start of
a sequence as in (1). A = {aij} is the transition matrix
probability, where aij is equal to the frequency of si following
sj in the training set (2) The transition matrix can also be
represented by a probability transition diagram (Figure 3).
B = {b1, ..., bN} represents the emission distribution that
models the observation (3). For each state, the emission bi
is computed as a multivariate Gaussian N(X, µi, Σi) with
µi and Σi the mean vector and the covariance matrix of the
Gaussian variable, respectively. µi and Σi are learned from the
observations X related to the state si in the training set.
πi = p(q0 = si) =
K−1
P
k=1
qk
0 = si
K
, i ∈ [1, N]
(1)
aij =
K−1
P
k=1
TP
t=1
(qk
t = sj).(qk
t−1 = si)
K−1
P
k=1
TP
t=1
(qk
t−1 = si)
, i, j ∈ [1, N]
(2)
bi = p(X|qt = si) = N(µi, σ2
i ), i ∈ [1, N]
(3)
In this paper, we used N = 7, while T is different for each
recorded sequence, from 444 samples for the shortest sequence
to 715 samples for the longest one.
2) Normalization: As the data (observations) have different
units (e.g., Cartesian position, velocity, contact information),
they need to be normalized. ext is the vector containing the
data xt normalized within the range [-1,1]:
ext = 2. xt − xmin
xmax − xmin
− 1
(4)
where xmin and xmax are constants computed from the data
in the training set.
34
36
38
40
42
1.0
0.5
0.0
0.5
1.0
Contact (normalized)
Contact and Center of Mass position during pick and place task
34
36
38
40
42
1.0
0.5
0.0
0.5
1.0
CoM vertical position (normalized)
34
36
38
40
42
Time (seconds)
walk
reach
pick
carry
place release
walk
Figure 4. Example of normalized data used to train the model. Top:
Time-series of the contact information from the e-glove; Middle: Time-series
of the center of mass vertical position; Bottom: Manually labeled activities.
0
20
40
60
Time (s)
carry
pick
place
reach
release
wait
walk
Activities
Comparison of prediction with annotation on one sequence
Figure 5. Comparison of manually annotated activities (red) and predicted
activities (green).
3) Sliding window: In order to decrease the computational
complexity and to reduce noise in the data, a sliding window
ﬁlter is applied to the recorded motion capture and glove data.
For each time window, the observation vector contains the
average values of the data across this window. A 60 samples
window is used, with an overlap of 30 samples between each
window. As the frequency of the MVN Link system is 240 Hz,
a window is 250 ms long. Given the 30 frames overlap, there
is a new observation every 125 ms. This rate is sufﬁcient since
each activity lasts more than one second.
231
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-616-3
ACHI 2018 : The Eleventh International Conference on Advances in Computer-Human Interactions

WITHOUT CONTACT INFORMATION:
Carry
Pick
Place
Reach
Release
Wait
Walk
Predicted labels
Walk
Wait
Release
Reach
Place
Pick
Carry
Real labels
0
0
0
0.31
0
0
99.69
0
0
0
0
0
82.54 17.46
0
0
1.31
0
87.43
0
11.25
0
0.29
0
90.24
0
0.38
9.08
3.92
1.42
89.01
3.76
0
0
1.88
4.25
94.71
0
0
0.26
0
0.78
86.86
0.14
0.45
0.25
0.07
0
12.23
Recall
Carry
Pick
Place
Reach
Release
Wait
Walk
Predicted labels
Walk
Wait
Release
Reach
Place
Pick
Carry
Real labels
0
0
0
1.16
0
0
87.33
0
0
0
0
0
86.85
0.95
0
0
0.85
0
99.39
0
2.29
0
0.3
0
93.96
0
0.65
2.05
1.61
2.04
97.44
4.18
0
0
0.4
2.04
97.23
0
0
0.3
0
0.12
96.35
0.42
1.7
0.7
0.3
0
6.87
Precision
WITH CONTACT INFORMATION:
Carry
Pick
Place
Reach
Release
Wait
Walk
Predicted labels
Walk
Wait
Release
Reach
Place
Pick
Carry
Real labels
0
0
0
0.7
0
0
99.3
0
0
0
0.88
0
82.07 17.05
0
0
0.52
1.72
87.94
0
9.82
0
0.26
0
96.05
0
0.37
3.32
4.83
0
92.87
2.3
0
0
0
4.45
95.55
0
0
0
0
0
99.86
0.07
0.07
0
0
0
0
Recall
Carry
Pick
Place
Reach
Release
Wait
Walk
Predicted labels
Walk
Wait
Release
Reach
Place
Pick
Carry
Real labels
0
0
0
2.11
0
0
95.49
0
0
0
0.51
0
86.83
1.02
0
0
0.27
1.25
100
0
2.49
0
0.31
0
94.04
0
0.67
1
1.5
0
99.55
2.08
0
0
0
1.11
99.5
0
0
0
0
0
97.39
0.18
0.18
0
0
0
0
Precision
Figure 6. Recall and Precision scores for each state without (top) and with (bottom) contact information.
4) Data annotation: Given that we used supervised learn-
ing, the recorded sequences were manually segmented and
annotated with the start and end of the different activities by
using the Anvil annotation software [19]. .
C. Features selection
Based on the results of [13] and our pilot tests, we include
the following motion-related features in the observation X:
the vertical position of the center of mass, the 3D velocity
of the center of mass, and the 3D velocity of both hands. The
contact information is the normal force on the palm. Therefore,
the observation vector xt at each instant t is xt ∈ R10 without
the contact information, and xt ∈ R11 with it.
D. Data analysis
1) Evaluation:
The
model
is
evaluated
with
cross-
validation on the dataset consisting of K
= 8 recorded
sequences. At each iteration, the database is split between the
training set that contains K − 1 sequences and the test set
that contains 1 sequence. The training set is used to learn the
parameters of the model, while the test set is used for the
evaluation.
2) Metrics: In order to evaluate the advantage of adding
the contact information, the recognition performances with
and without contact information are compared on the same
sequences. The model is evaluated with the Recall score,
Precision score and F1-score (harmonic mean of precision and
recall) as in (5 - 7):
Recalli = Number of samples correctly classiﬁed as class i
Number of samples that belong to the class i
(5)
Precisioni = Number of samples correctly classiﬁed as class i
Number of samples classiﬁed as class i
(6)
F1-scorei = 2. Precisioni.Recalli
Precisioni + Recalli
(7)
For each metric, an average score is computed on all the
iterations of the cross-validation.
IV.
RESULTS
A. First insight
Some patterns are observable when the data of the vertical
position of the center of mass and contact information are
compared to the annotated activities (Figure 4). When the par-
ticipant performs the activities WALK, REACH and RELEASE,
232
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-616-3
ACHI 2018 : The Eleventh International Conference on Advances in Computer-Human Interactions

TABLE I. RECOGNITION PERFORMANCE (%) WITH AND WITHOUT
CONTACT INFORMATION (LEFT AND MIDDLE COLUMNS), AND
PERFORMANCE COMPARISON WITH WILCOXON TEST (RIGHT COLUMN).
No contact
Contact
p-value
Recall
90.07
93.38
0.06
Precision
94.08
96.12
0.16
F1-score
91.99
94.71
0.05
the contact information data are equal to the minimum possible
value. Whereas during PICK, CARRY and PLACE activities, the
contact data have positive values. We can also identify when
the user bends forwards and stands up straight by looking at
the center of mass position. During the WALK and CARRY
states, there are oscillations around a value of 0.5.
B. Margin of error
Most activities are correctly identiﬁed (Figure 5), but the
transition between two states does not always happen at the
exact same frame on the real and the predicted case. This
kind of error is not relevant, as our future application does
not require a 125 ms accuracy. Therefore, the performance
scores are computed with a margin of error of one 125 ms time
window before and after each observation. The classiﬁcation is
considered correct if the predicted state at time t corresponds
to the annotated state at either time t − 1, t, or t + 1.
C. Overall results
The performance for each score with and without glove is
presented in Table I. For each score, the contact information
improves the results. The results are compared with a Wilcoxon
signed-rank test and we found a signiﬁcant difference for the
F1-score (p-value = 0.05).
D. Performance for each activity
Figure 6 presents the precision and recall scores for each
states. There is mainly a confusion between the RELEASE and
WALK activities. This is mainly due to the fact that at the end
of each sequence, the subject returns to a neutral position with
a little step after placing the last bar. In the annotation, it was
labeled RELEASE then WAIT without WALK transition, but the
step was classiﬁed by the model as walking (this error can also
be seen in Figure 5).
Figure 7 presents a comparison of the scores for each state
for both conditions. We computed a Wilcoxon signed-rank test
on the measure performance of recall, precision and F1-score.
For most of the measures, there is no signiﬁcant difference
with or without the use of the contact information. We found
a signiﬁcant difference with the F1-score of PICK task, the
precision score of PLACE activity, the recall score of REACH
task and the precision and F1-score of WALK activity. However,
when the contact information is added, there is an improvement
of the performances for the recognition of each activity and a
reduction of the uncertainty.
Overall, our results show that using both kinetics in-
formation and contact information is beneﬁcial for activity
recognition with our HMM-based model.
Carry
Pick
Place
Reach
Release
Wait
Walk
Activity
60
70
80
90
100
Score (%)
Recall
Condition
No Contact
Contact
Carry
Pick
Place
Reach
Release
Wait
Walk
Activity
60
70
80
90
100
Score (%)
Precision
Condition
No Contact
Contact
Carry
Pick
Place
Reach
Release
Wait
Walk
Activity
60
70
80
90
100
Score (%)
F1-score
Condition
No Contact
Contact
Figure 7. Classiﬁcation performance with and without contact information
for each activity: Recall score (top), Precision score (middle), and F1-score
(bottom).
V.
CONCLUSION
This paper presents a contextual use of Hidden Markov
Model for activity recognition, applied to a pick-and-place
task. We evaluate the beneﬁt of using the hand-object contact
information in addition to kinetics information to improve
the classiﬁcation performance. The overall performance is
better by about 3 % when the contact information is added.
The contact information could also increase the robustness
of the recognition in case of data loss. For instance, if data
corresponding to the PICK activity is unavailable, WALK and
CARRY can easily be confused when the recognition is based
233
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-616-3
ACHI 2018 : The Eleventh International Conference on Advances in Computer-Human Interactions

on motion features only. With the contact information, the
differentiation becomes trivial.
Nevertheless, the dataset we used consists of a single task.
In order to test if the usefulness of the contact information
can be generalized, future work includes collecting a dataset
with more tasks and more variability in the tasks and related
activities. Finally, the automation of the recognition process
could be increased by adding an automatic selection of relevant
features among all the available ones.
ACKNOWLEDGEMENTS
This work was supported by the European Union’s Hori-
zon 2020 Research and Innovation Programme under Grant
Agreement No. 731540 (project AnDy). The authors wish to
thank Lars Fritzsche and Emphasis Telematics SA (Dr. Giorgos
Papapanagiotakis, Dr. Michalis Miatidis, Panos Stogiannos,
Giannis Kantaris, Dimitris Potiriadis) for their support with
the e-glove.
REFERENCES
[1] E. Schneider, X. Irastorza, M. Bakhuys Roozeboom, and
I. Houtman, “Osh in ﬁgures: occupational safety and health in
the transport sector-an overview,” 2010.
[2] G. Li and P. Buckle, “Current techniques for assessing physical
exposure to work-related musculoskeletal risks, with emphasis
on posture-based methods,” Ergonomics, vol. 42, no. 5, pp. 674–
695, 1999.
[3] K. Schaub, G. Caragnano, B. Britzke, and R. Bruder, “The Eu-
ropean Assembly Worksheet,” Theoretical Issues in Ergonomics
Science, vol. 14, no. 6, pp. 616–639, 2013.
[4] T. Bossomaier, A. G. Bruzzone, A. Cimino, F. Longo, and
G. Mirabelli, “Scientiﬁc approaches for the industrial worksta-
tions ergonomic design: A review.” in ECMS, 2010, pp. 189–
199.
[5] S. Ivaldi, L. Fritzsche, J. Babic, F. Stulp, M. Damsgaard,
B. Graimann, G. Bellusci, and F. Nori, “Anticipatory models
of human movements and dynamics: the roadmap of the andy
project,” in Proc. International Conf. on Digital Human Models
(DHM), 2017.
[6] O. D. Lara and M. A. Labrador, “A Survey on Human Activity
Recognition using Wearable Sensors,” IEEE Communications
Surveys & Tutorials, vol. 15, no. 3, 2013.
[7] H. Junker, O. Amft, P. Lukowicz, and G. Tr¨oster, “Gesture
spotting
with
body-worn
inertial
sensors
to
detect
user
activities,” Pattern Recognition, vol. 41, no. 6, pp. 2010–2024,
Jun. 2008. [Online]. Available: http://www.sciencedirect.com/
science/article/pii/S0031320307005110
[8] J. K. Aggarwal and L. Xia, “Human activity recognition from
3D data: A review,” Pattern Recognition Letters, vol. 48, no.
Supplement C, pp. 70–80, Oct. 2014. [Online]. Available: http://
www.sciencedirect.com/science/article/pii/S0167865514001299
[9] L.
Lo
Presti
and
M.
La
Cascia,
“3D
skeleton-
based
human
action
classiﬁcation:
A
survey,”
Pattern
Recognition,
vol.
53,
no.
Supplement
C,
pp.
130–147,
May 2016. [Online]. Available: http://www.sciencedirect.com/
science/article/pii/S0031320315004392
[10] F. Attal, S. Mohammed, M. Dedabrishvili, F. Chamroukhi,
L. Oukhellou, and Y. Amirat, “Physical human activity recog-
nition using wearable sensors,” Sensors, vol. 15, no. 12, pp.
31 314–31 338, 2015.
[11] A. Y. Yang, S. Iyengar, P. Kuryloski, and R. Jafari, “Distributed
segmentation and classiﬁcation of human actions using a wear-
able motion sensor network,” in Computer Vision and Pattern
Recognition Workshops, 2008. CVPRW’08. IEEE Computer
Society Conference on.
IEEE, 2008, pp. 1–8.
[12] A. Dubois and F. Charpillet, “Human activities recognition with
RGB-Depth camera using HMM,” in Engineering in Medicine
and Biology Society (EMBC), 2013 35th Annual International
Conference of the IEEE.
IEEE, 2013, pp. 4666–4669.
[13] C. Mandery, M. Plappert, J. Borras, and T. Asfour, “Dimension-
ality reduction for whole-body human motion recognition,” in
Information Fusion (FUSION), 2016 19th International Confer-
ence on.
IEEE, 2016, pp. 355–362.
[14] M. W¨achter and T. Asfour, “Hierarchical segmentation of
manipulation actions based on object relations and motion char-
acteristics,” in Advanced Robotics (ICAR), 2015 International
Conference on.
IEEE, 2015, pp. 549–556. [Online]. Available:
http://ieeexplore.ieee.org/abstract/document/7251510/
[15] E. Coupet´e, F. Moutarde, S. Manitsaris, and O. Hugues, “Recog-
nition of Technical Gestures for Human-Robot Collaboration in
Factories,” in The Ninth International Conference on Advances
in Computer-Human Interactions, 2016.
[16] D. Roetenberg, H. Luinge, and P. Slycke, “Xsens MVN: full
6DOF human motion tracking using miniature inertial sensors,”
Xsens Motion Technologies BV, Tech. Rep, 2009.
[17] “e-glove | Emphasis Telematics,” URL: http://www.emphasisnet.
gr/e-glove/ [accessed: 2018-01-30].
[18] “hmmlearn
0.2.1
documentation,”
URL:
http://hmmlearn.
readthedocs.io/ [accessed: 2018-01-30].
[19] M. Kipp, L. F. von Hollen, M. C. Hrstka, and F. Zamponi,
“Single-person and multi-party 3d visualizations for nonverbal
communication analysis.” in LREC, 2014, pp. 3393–3397.
234
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-616-3
ACHI 2018 : The Eleventh International Conference on Advances in Computer-Human Interactions

