Student Learning and Student Perceptions of Learning from Interactive Modules  
Vanessa Slinger-Friedman, Lynn M. Patterson 
Department of Geography & Anthropology 
Kennesaw State University 
Kennesaw, United States of America 
vslinger@kennesaw.edu and lpatters@kennesaw.edu 
 
Abstract— The constructivist approach to online module 
design, whereby the learner constructs knowledge through 
activity, appears to offer instructors and students a way of 
achieving learning outcomes. However, analysis and evaluation 
of these new learning environments is lacking, especially in the 
area of how academic content in interactive environments 
impacts student actual learning and perceptions of learning. 
This paper documents the process to improve module design 
using focus groups and to test the effectiveness of the 
interactive elements based upon assessments of student 
learning outcomes tailored to online learning environments. 
Results from student learning assessments enable instructors 
to optimize instructional design to maximize learning 
opportunities and achievement in online environments.  
Keywords—multimedia 
instruction; 
interactive 
online 
modules; cognitive learning; student perceptions of learning; 
human geography  
I. 
 INTRODUCTION  
By its very nature, geography is a visual and interactive 
subject [1, 2].  Traditional resources for classes in this 
subject area offer only limited interactive opportunities that 
challenge students to apply geographic concepts to real-
world situations [2, 3].  For our introductory human 
geography course and as part of the development of a 
completely online textbook, we have developed a series of 
online interactive learning modules.  These modules include 
imagery, custom videos, readings, discussions, animations, 
interactive exercises, and assessments.  In these modules the 
integration of theory and applications takes place through 
activities in which theories and ideas are applied for use in 
practical situations to answer real-world geographic 
questions, bringing the course material “alive” for students. 
The purpose of this research is to investigate how these 
interactive e-learning activities affect student learning and 
student perceptions of their learning. 
 
This paper begins with the literature associated with 
student interactivity and its importance related to student 
learning.  Within the literature review, the theoretical 
framework for the design of the online interactive modules 
is presented.  Specific examples of disciplines that have 
used interactive designs and how it has been applied to 
assist in student learning are included.  Then, the design of 
the modules is detailed. Next, we outline the use of student 
focus groups and module testing using a control group 
experimental design.   Finally, we conclude with the broader 
implications of this research on optimizing instructional 
design to maximize learning opportunities and achievement 
in future online and distance learning environments  
II. 
LITERATURE 
With the application of concepts in real-world situations, 
the intent is to engage students with the course materials to 
improve student learning.  Much of the literature discussing 
interaction in online classes addresses either the interaction 
between student and instructor and among students [e.g., 4, 
5, 6] or the level of interaction of students with the 
technology as determined by frequency counts and access 
rates [e.g., 7, 8, 9]. Less attention, however, has been given 
to studying the interaction between students and course 
content and achievement of learning outcomes. As 
technology has developed and become a more integral part 
of the distance learning environment, and, even in the 
traditional classroom setting, it has impacted the distribution 
of content, learning tasks, and assignments [10]. The ways 
by which information is presented and also the way in 
which students interact with that material is important.  
Furthermore, the medium employed can motivate and 
engage students as active and collaborative learners rather 
than just providing information to them. Multimedia 
instruction rather than “flat resources,” such as static text 
documents, have been identified as an important element of 
high-level interactive engagement and student satisfaction 
[9].  
 
The design of the online interactive modules for this 
study is based on a cognitive theory framework that 
supports multimedia design of educational materials [11, 12, 
13]. 
Mayer’s 
research 
on 
cognitive 
theory-based 
assumptions regarding the way that people learn from words 
and pictures indicates that animation and narration (what 
Mayer considers the two elements of the “Dual Channel 
Assumption) in computer-based multimedia presentations 
results in deeper understanding in learners [13].  Mayer also 
presented, but did not test, the “Active Processing 
Assumption” which states that students engage in 
meaningful learning when they actively process material 
through “selecting relevant words and pictures, organizing 
them into coherent pictorial and verbal models, and 
integrating them with each other and appropriate prior 
knowledge” [13]. This research seeks to study the impact on 
learning of actively processing content through interaction. 
1
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-253-0
eLmL 2013 : The Fifth International Conference on Mobile, Hybrid, and On-line Learning

Hence it attempts to expand upon the research that studies 
the link between cognition and instruction [13]. 
In the fields of computer programming, nursing, and 
biology, modules with various levels of interactive ‘learning 
objects’ 
have 
been 
designed 
to 
improve 
student 
understanding and learning [11, 12, 14]. In a Java 
programming course, Bradley and Boyle [14] made their 
learning objects optional resources. They found that 
students accessed the learning objects in large numbers and, 
in a survey students indicated that the learning objects 
helped them to learn the concepts being addressed. While 
they experienced an increase in the percentage points 
achieved on the modules, the authors felt that the exact 
contribution of the learning objects was difficult to assess 
because they were used as components in larger pedagogical 
systems [14]. Maag [12] found that while there were no 
statistically significant increases in math-test scores from 
the pre- and post-test with the use of interactive multimedia, 
those students who had used the interactive multimedia 
reported the highest satisfaction score. Black et al. [11] 
focused on the creation of interactive objects and did not 
report on the impact of the interactivity. 
 
This concept of knowledge transmission is based on a 
constructivist point of view where knowledge is constructed 
by the learner through activity [10]. This construction has 
led to the development of “new learning environments” or 
what Martens et al. [10] call “constructivist e-learning 
environments” (CEEs) in which activities are created to 
challenge students and provide them with realistic contexts 
so that students become intrinsically motivated to explore 
and control their own learning process.  
 
Guzley et al. [4] suggest that students’ motivations are 
linked to their satisfaction with distance learning as a mode 
of instruction, in turn affecting their perceptions and 
influencing the overall effectiveness of the learning. This 
makes students’ satisfaction with, and perceptions about, the 
learning environment and process critical [10]. Since 
measurements of the causal effect of pedagogical techniques 
on student learning can be difficult to isolate, student self-
reported learning gains also have been identified as a useful 
indicator of actual learning [15, 16, 17, 18].  The literature 
on student perceptions of learning indicates that student 
perceptions may be more important than reality since 
decisions are often based on perceptions [15]. Furthermore, 
Chesebro and McCroskey [15] concluded in their research 
that, “students can provide reasonably accurate reports of 
the extent to which they are learning in their classrooms” 
(301). 
 
Designing new learning environments is challenging. 
Much of the available research shows an emphasis on 
delivery of these new learning environments rather than on 
analysis or evaluation [20]. Designers of these tasks rarely 
gain knowledge of how students will perceive the tasks 
before they are delivered to the students. Greenberg [21] 
asserts that quality assessments should be taking place 
during the design of the course and include the course 
creators. Finally, while claims about the positive results 
obtained using these new learning environments have been 
made, strong empirical research regarding their influence on 
students’ perceptions and the motivational impact of CEEs 
are lacking [10]. 
III. 
MODULE DESIGN 
Each interactive multi-media module is designed using a 
similar structure, requiring approximately 30 minutes for 
completion. Using a web-based format, the module begins 
with a short reading providing an overview of the applied 
topic and lists the learning objectives.  This reading is 
approximately 1-2 paragraphs in length.  Next, a 3 minute 
narrated animation illustrates a key concept.  This is 
followed by a five minute interview with an expert in the 
field discussing the geographic implications of the topic.  
Finally, a series of interactive exercises allows the student to 
explore the topic using geographic tools (e.g., visual 
examination, verbal descriptions, digital mapping, cognitive 
perceptions, and mathematical modeling). For each module 
element described above, an interactive textbox appears to 
the right where the student is encouraged to take notes.  The 
module ends with a self-assessment. This self-assessment is 
required for completion of the module.   
IV. 
MODULE IMPROVEMENT USING STUDENT FOCUS 
GROUPS 
To improve the e-learning modules, we will use focus 
groups to investigate student perceptions of learning and 
teaching effectiveness [e.g., 22, 23, 24, 25, 26].  For 
example, Kingston et al. [23] utilized mobile technologies 
and virtual fieldtrips to teach physical geography.  Students 
who had taken the old module and completed the new 
module were given questionnaires and then participated in a 
focus group to investigate the effectiveness of the new 
technologies.  Lederman [26] also suggests that focus 
groups can be very useful for pre-testing educational 
materials as they “provide an opportunity for extensive 
commentary, unrestrained by the limits of a survey 
questionnaire or the student-teacher relationship which may 
affect course evaluations at the end of a class” (126). 
 
The interactive modules will be tested with focus groups, 
comprised of approximately 5-7 student volunteers in each 
group.  Each student in the group will be asked to complete 
a common module in advance of the focus group interview.   
Based upon established learning outcomes for the modules, 
students will provide feedback on how the interactive 
exercises affected their learning.  The semi-structured focus 
group interviews also cover topics of engagement, clarity of 
concepts, ease and usefulness of exercises, and suggested 
improvements (Fig. 1).  To ensure data acquisition both 
members of the research team will be present – one to serve 
 
2
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-253-0
eLmL 2013 : The Fifth International Conference on Mobile, Hybrid, and On-line Learning

Introductions 
Facilitator introduces members of the research team and each of the group 
members introduce themselves.  The facilitator provides the background 
and ground rules (confidential and anonymous reporting, honest opinions, 
etc.).  The facilitator will inform the group that we would like to collect 
notes made by the participants during the session to ensure we collected as 
much 
feedback 
as 
possible, 
if 
the 
participants 
are 
willing. 
Issues and Discussion Questions (Semi-structured)* 
Overall Impressions 
 
Please share with us overall how you felt about the modules? 
 
What did you like about the modules?  What didn’t you like about the 
modules? 
Engagement 
 
What about the material (videos, photos, readings) did you find the 
most engaging? 
 
How did the interactive exercises affect your interest in the content? 
 
Did any of the material or exercises make you want to learn more 
about the topic?  If so, which and how? 
Clarity and Ease of Use of Elements  
 
What concepts or parts of the module were the most clear? The least 
clear? 
 
What aspect of the interactive exercises did you find the 
clearest/easiest?  What aspects were unclear/more difficult? 
Learning 
 
Overall, how useful did you find the exercises? 
 
How did the interactive exercises assist you in understanding course 
content?  In applying course content? 
 
How did the interactive exercises challenge you? 
Improvements 
 
What improvements could we make to improve the elements of the 
modules? 
Summary of what we have heard   
 
Have we missed anything? 
Collect notes (to review later). 
*Questions may be modified based upon results from post-module 
questionnaire. 
 
Figure 1. Focus Group Questions 
as moderator and the other as a note taker who records 
speakers, comments and significant non-verbal behavior 
[27].  A summary of the issues will be presented to the 
group at the conclusion to ensure no notable comments were 
excluded.   
 
Concerns about the use of focus groups persist, including 
“groupthink” [28].  We have two mechanisms to minimize 
this.  First, students will each fill out a short questionnaire at 
the completion of the module (Fig. 2).  The questionnaire 
allows us to obtain individual feedback that may not come 
out in the group discussion but that may be vital to 
improving the e-learning modules.  Second, we will ask the 
focus group members to jot down notes during the group 
interview. These notes will be collected at the end – in the 
event that members did not get a chance to share their 
comments.   
 
For the analysis of the focus group interviews, we will 
code the data, create categories emerge and develop 
summary statements which capture the essence of the 
responses [26, 29].  The results of the coding offer two 
outcomes.  First, the student responses will identify which of 
the interactive exercises have greater perceived value to 
students.  We will compare these responses to student 
performance on the various assessments to see if there is a 
correlation 
between 
perceptions 
of 
learning 
and 
performance. The modules will then be revised to address 
weaknesses. 
V. 
MODULE TESTING IN CLASSES 
The revised modules will then be implemented using a 
pre-test/post-test 
control 
group 
design 
to 
test 
for 
effectiveness of the interactive components on student 
learning and perceptions of learning.   In one semester, two 
separate classes (approximately 40 students in each class) 
will be presented with two of the applied geography topics.  
The control group (Class 1) will have access to only the 
multi-media elements and the experimental group (Class 2) 
will receive the full interactive module.  The modules will be 
completed within 2 days to alleviate threats to external 
validity with exposure to the subject material from the pre-
test.  Both groups will be tested at beginning of the module 
and at the conclusion of the modules based upon the learning 
objectives.  The pre-test will enable the researchers to 
determine existing knowledge base, which the post-test will 
allow for determination of learned knowledge.  Differences 
between the control group and the experimental group will 
illuminate the effect of the interactive elements.  
 
 
 
1.  The interactive exercises helped me to (learning outcome #1).   
Strongly agree Agree Neither agree nor disagree Disagree 
Strongly disagree  
 
2.  The interactive exercises helped me to (learning outcome #2).   
Strongly agree Agree Neither agree nor disagree Disagree 
Strongly disagree 
 
3.  Overall, the interactive activities:  
Made no difference to how I learned Helped me learn more Were 
detrimental to my learning process 
 
4.  The interactive activities in these modules are challenging 
Strongly agree Agree Neither agree nor disagree Disagree 
Strongly disagree  
 
5.  I am comfortable with the interactive activities in these modules 
Strongly agree Agree Neither agree nor disagree Disagree 
Strongly disagree  
 
6.  I would like to have more interactive exercises in my courses  
Strongly agree Agree Neither agree nor disagree Disagree 
Strongly disagree  
 
7.  Please comment on how specifically the interactive exercises can be 
improved. 
 
Note: The questions will be modified to reflect each module’s learning 
outcomes. 
 
Figure 2.  Perceptions of Learning Questions 
 
3
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-253-0
eLmL 2013 : The Fifth International Conference on Mobile, Hybrid, and On-line Learning

Independent t-tests will be conducted to examine the mean 
values of the control and experimental group scores and the 
gain scores for the control and experimental groups will be 
analyzed for ANCOVA (analysis of covariance) [30].    
 
Beyond assessment of student learning from end of 
module quizzes, students from the experiment group will 
also be asked about their perceptions of learning using the 
questions from Fig. 2.   The results of these questions will be 
presented as descriptive statistics.  Finally, data from student 
notes recorded next to the module elements will be coded.  
The student perceptions of learning and engagement of 
students (documented through note-taking) will be compared 
to student post-test scores to look for correlations. 
 
The researchers will then review the results from the pre-
test/post-test control group design assessments of learning 
and perceptions of learning to complete final revisions of the 
interactive modules. 
VI. 
CONCLUSION AND FUTURE WORK 
Educational delivery models for college courses have 
changed from primarily the traditional lecture in the 1980s. 
Contemporary educational delivery models include online 
and distance education; however, there has been a gap in the 
assessment of these learning technologies of their impact on 
student learning [19]. As new generations of students arrive 
at institutions of higher education with, “a greater reliance 
on visual imagery and on participating actively in the 
learning process that probably stem from experience with 
electronic media during formative years” [11], this type of 
interactivity with course content has become increasingly 
important. Given the rising importance of the computer and 
interactive learning, how should multimedia be designed 
and 
integrated 
into 
teaching 
to 
promote 
deeper 
understanding and learning for students? Educational 
research of this nature tackles the fundamental question of 
how to optimize instructional design to maximize learning 
opportunities and achievement in online and distance 
learning environments [5]. Knowledge about the outcome of 
interactive activities in distance learning instruction will be 
valuable for educators and researchers to make more 
informed decisions about future online and distance learning 
course development and implementation [10].  Thus, by 
enlisting students in curriculum development, we expect to 
improve the module content and interactive activities by 
directing revision based on student perception of learning. 
More broadly, this research will be a contribution to the 
existing literature that has been limited in its analysis of 
how students learn in interactive e-learning environments. 
Future research will include a study to better understand the 
specific learning benefits and constraints involved in student 
interaction with a variety of interactive elements and 
combinations of interactive elements in the online 
environment. 
 
ACKNOWLEDGMENT 
We wish to thank Tommy and Beth Holder for their 
support through the KSU Foundation Holder Award, the 
Center for Teaching and Learning at Kennesaw State 
University for providing technical assistance, and the 
College of Humanities and Social Sciences Office of 
Distance Education for research and travel funding.  
REFERENCES 
[1] G. Rose, On the need to ask how, exactly, is Geography “visual”? 
Antipode, Vol. 35(2), March 2003, pp 212-221. 
[2] C. Jain and A. Getis. The Effectiveness of internet-based instruction: 
an experiement in physical geography. Journal of Geography in 
Higher Education, Vol. 27(2), July 2003, pp. 153-167. 
[3] A.S. Posey, Automated Geography and the next generation. 
Professional Geographer, Vol. 45(4), Nov 1993, pp. 455-456. 
[4] R. Guzley, S. Avanzino, and A. Bor, “Simulated computer-
mediated/video-interactive distance learning: A test of motivation, 
interaction satisfaction, delivery, learning & perceived effectiveness.” 
Journal of Computer-Mediated Communication, Vol. 6(3), Apr. 2001. 
Online: http://jcmc.indiana.edu/vol6/issue3/ 
guzley.html. 
[5] S.D. Johnson, S.R. Aragon, N. Shaik, and N. Palma-Rivas, 
“Comparative analysis of learner satisfaction and learning outcomes 
in online and face-to-face learning environments.” Journal of 
Interactive Learning Research, vol. 11(1), 2000, pp. 29-49. 
[6] A. Sher, “Assessing the relationship of student-instructor and student-
student interaction to student learning and satisfaction in Web-based 
Online Learning Environment.” Journal of Interactive Online 
Learning, vol. 8(2), Summer 2009, pp. 102 – 120. 
[7] D. Davidson-Shivers, “Frequency and types of instructor interactions 
in online instruction.” Journal of Interactive Online Learning, vol. 
8(1), Spring 2009, pp. 23 – 40. 
[8] C.J. Grandzol, and J.R. Grandzol, “Interaction in online courses: 
more is NOT always better.” Online Journal of Distance Learning 
Administration, vol. 13(2), Summer 2010. 
[9] M. Murray, J. Pérez, D. Geist, A. Hedrick, “Student interaction with 
online course content: Build it and they might come.” Journal of 
Information Technology Education: Research, vol. 11, 2012, pp. 125 
– 140. 
[10] R. Martens, T. Bastiaens, and P.A. Kirschner, “New learning design 
in distance education: The impact on student perception and 
motivation.” Distance Education, vol. 28(1), May 2007, pp. 81-93. 
[11] B.L. Black, H. Heatwole, and H. Meeks, Using multimedia in 
interactive learning objects to meet emerging academic challenges. In 
Learning Objects: Theory, Praxis, Issues, and Trends. Koohang, A. & 
Harman, K. (Eds.), Santa Rosa, California, 2007, pp. 209 – 257. 
[12] M. Maag, “The Effectiveness of an interactive multimedia learning 
tool on nursing students’ math knowledge and self-efficacy.” CIN: 
Computers, Informatics, Nursing, vol. 22 (1), January-February 2004, 
pp. 26-33. 
[13] R.E. Mayer, “Cognitive theory and the design of multimedia 
instruction: An example of the two-way street between cognition and 
instruction.” New Directions for Teaching and Learning, no 9, Spring 
2002, pp. 55-71. 
[14] C. Bradley and T. Boyle, Students’ use of learning objects. 
Interactive Multimedia Electronic Journal of Computer-Enhanced 
Learning. Wake Forest University, USA, ISSN 1525-9102. Vol. 6 
(2), 2004. Online: http://imej.wfu.edu/articles/2004/2/01/index.asp 
[15] J.L. Chesebro and J.C. McCroskey, “The relationship between 
students’ reports of learning and their actual recall of lecture material: 
a validity test.” Communication Education, vol. 49(3), Jul. 2000, pp. 
297 – 301. 
4
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-253-0
eLmL 2013 : The Fifth International Conference on Mobile, Hybrid, and On-line Learning

[16] T.L. Jones, “Self-reported Instrument for measuring student learning 
outcomes.” Proc.American Society for Engineering Education Annual 
Conference & Exposition, Session 2793, Jun. 2003.  
[17] L.A. Schmidt, “Psychometric evaluation of the writing-to-learn 
attitude survey.” Journal of Nursing Education, vol. 43(10), Oct. 
2004, pp. 458 – 465. 
[18] V. Slinger-Friedman and L. Patterson, “Writing in Geography: 
Student attitudes and assessment.” Journal of Geography in Higher 
Education, vol. 36(2), May 2012, pp. 179-195. 
[19] J. O’Malley, “Students perceptions of distance learning, online 
learning and the traditional classroom.” Online Journal of Distance 
Learning Administration, vol. 2(4), Winter 1999, pp. 1-9.  
[20] J.J.G. van Merrienboer & R. Martens, “Computer-based tools for 
instructional design.” Educational Technology, Research and 
Development, vol. 50, 2002, pp. 5-9. 
[21] G. Greenberg, “Conceptions of quality in course design for web-
supported education.” Proc. of the 26th Annual Conference 
onDistance Teaching & Learning. Aug. 2010, Madison, WI. 
Available from http://www.uwex.edu/disted/conference/Resource_ 
library/proceedings/28667_10.pdf   
[22] D. A. Armstrong, “Students’ perceptions of online learning and 
instructional tools: a qualitative study of undergraduate students use 
of online tools.” The Turkish Online Journal of Educational 
Technology, vol. 10(3), Jul. 2011, pp. 222-226. 
[23] D.G. Kingston, W.J. Eastwood, P.I. Jones, R. Johnson, S. Marshall, 
and D.M. Marshall, “Experiences of using mobile technologies and 
virtual fieldtrips in Physical Geography: implications for hydrology 
education.” Hydrology and Earth System Sciences, Vol. 10, May 
2012, pp. 1281-1286. 
[24] M. Mawdesley, G. Long, A. Al-jibouri, and D. Scott, “The 
enhancement of simulation based learning exercises through 
formalized reflection, focus groups and group presentation.” 
Computers & Education, vol. 56, Jan. 2011, pp. 44-52. 
[25] S. Zutshi, M. Mitchell, and D. Weaver, “Undergraduate student 
acceptance of a unit design for developing independent learning 
abilities.” Journal of University Teaching & Learning Practice, vol. 
8(3), 2011, pp. 1-16. 
[26] L.C. Lederman, “Assessing the educational effectiveness:  The focus 
group interview as a technique for data collection.” Communication 
Education, vol. 38, 1990, pp. 117-127. 
[27] P.S. Kidd and M. B. Parshall, “Getting the focus and the group: 
Enhancing analytical rigor in focus group research.” Qualitative 
Health Research, Vol. 10, May 2000, pp. 293-308.   
[28] L. Janis, “Victims of Groupthink: a psychological study of foreign-
policy decisions and fiascoes”. Boston: Houghton Mifflin, 1972. 
[29] M. A. Carey, “Comment:  Concerns in the analysis of focus group 
Data.” Qualitative Health Research, vol. 5(4), Nov. 1995, pp. 487-
495. 
[30] D. M. Dimitrov and P. D. Rumrill, Jr., “Pretest-posttest designs and 
measurement of change.” Work, vol. 20, 2003, pp. 159-165. 
 
5
Copyright (c) IARIA, 2013.     ISBN:  978-1-61208-253-0
eLmL 2013 : The Fifth International Conference on Mobile, Hybrid, and On-line Learning

