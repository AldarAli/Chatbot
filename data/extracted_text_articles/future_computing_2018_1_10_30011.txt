A Novel Training Algorithm based on Limited-Memory quasi-Newton Method with
Nesterov’s Accelerated Gradient for Neural Networks
Shahrzad Mahboubi and Hiroshi Ninomiya
Department of Information Science, Shonan Institute of Technology
Email: mahboubi.shaa@gmail.com,
ninomiya@info.shonan-it.ac.jp
Abstract—This paper describes a novel training algorithm based
on Limited-memory quasi-Newton method (LQN) with Nesterov’s
accelerated gradient for faster training of neural networks.
Keywords–Limited-memory quasi-Newton method; Nesterov’s
accelerated gradient method; neural networks; training algorithm.
I.
INTRODUCTION
Neural networks have been recognized as a useful tool for
the function approximation problems with high-nonlinearity
[1][2]. Training is the most important step in developing a neu-
ral network model. Gradient algorithm based on the ﬁrst order
approximation such as Steepest gradient (SG), Momentum and
Nesterov’s accelerated gradient (NAG) methods are popularity
used for this purpose [1]-[3]. With the progress of AI tech-
nologies the characteristics between inputs and desired outputs
of the training samples become increasingly complex. Then
neural networks have to train the highly-nonlinear functions.
Under such circumstances the ﬁrst order methods converge too
slowly and optimization error cannot be effectively reduced
within ﬁnite time in spite of its advantage [2]. The quasi-
Newton (QN) training, which is one of the most effective
optimization based on the second order approximation [4] is
widely utilized as the robust training algorithm for highly-
nonlinear function approximation. However, the QN iteration
includes the approximated Hessian, that is QN needs the
massive computer resources of memories as the scale of neural
network becomes larger. To deal with this problem, it is
noteworthy that QN incorporating Limited-memory scheme is
effective for large-scale problems.
In this paper, the acceleration technique of Limited-
memory QN (LQN) is proposed using Nesterov’s accelerated
gradient. In [2], the QN training was drastically accelerated by
Nesterov’s accelerated gradient that is called Nesterov’s accel-
erated quasi-Newton (NAQ) training. Therefore, the proposed
algorithm can be accelerated in cooperating the similar scheme
of NAQ into LQN. The method is referred to as Limited-
memory NAQ (LNAQ). The proposed algorithm is demon-
strated through the computer simulations for a benchmark
problem compared with the conventional training methods.
II.
FORMULATION OF TRAINING AND LIMITED-MEMORY
QUASI-NEWTON METHOD
This section describes an error function of the neural
network training and conventional training algorithms beased
on gradients such as Back propagetion and Limited-memory
quasi-Newton method.
A. Formulation of training
Let dp, op and w ∈ RD be the p-th desired, output, and
weight vectors, respectively the error function E(w) is deﬁned
as the mean squared error (MSE) of
E(w) =
1
|Tr|
∑
p∈Tr
Ep(w), Ep(w) = 1
2∥dp − op∥2,
(1)
where T r denotes a training data set {xp, dp}, p ∈ Tr and |Tr|
is the number of training samples. Among the gradient-based
algorithms, (1) is minimized by wk+1 = wk+vk+1, where k is
the iteration count and vk+1 is the update vector. SG, so-called
Back propagation method has vk+1 = −αk∇E(wk) with the
step size αk and the gradient vector at wk of ∇E(wk).
B. Limited-memory quasi-Newton training (LQN)
The update vector of QN is deﬁned as
vk+1 = −αkHk∇E(wk).
(2)
where Hk is the symmetric positive deﬁnite matrix and itera-
tively approximated by the Broyden-Fletcher-Goldfarb-Shanno
(BFGS) formula [4]. For the purpose of reducing the amount
of memory used in QN a sophisticated technique incorporating
the limited-memory scheme is widely utilized for the calcula-
tion of vk+1 of LQN. Speciﬁcally, this method is useful for
solving problems whose Hk (inverse of approximated Hessian)
matrices in (2) cannot be computed at a reasonable cost [4].
Furthermore, instead of storing D×D matrix of Hk, only 2×t
vectors of the dimension D (2×t ×D) have to be stored. Here,
t is deﬁned by users and t ≪ D.
III.
PROPOSED ALGORITHM -LNAQ
Nesterov’s accelerated quasi-Newton (NAQ) training was
derived by the quadratic approximation of (1) around wk +
µvk, and µ was the momentum coefﬁcient whereas QN used
the approximation of (1) around wk [2]. NAQ drastically
improved the convergence speed of QN using the gradient
vector at wk + µvk of ∇E(wk + µvk) called Nesterov’s
accelerated gradient vector [3]. In this paper, we apply the
limited-memory method into NAQ, that is called LNAQ. The
proposed method can be expected to cope with large-scale
optimization problems like LQN, maintaining the fast training
of NAQ. The update vector of NAQ is
vk+1 = µvk − αk ˆHk∇E(wk + µvk),
(3)
1
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-608-8
FUTURE COMPUTING 2018 : The Tenth International Conference on Future Computational Technologies and Applications

and the matrix ˆHk was updated by,
ˆHk+1 = ˆHk − ( ˆHkqk)pT
k + pk( ˆHkqk)T
pT
k qk
+
(
1 + qT
k ˆHkqk
pT
k qk
)
pkpT
k
pT
k qk
,
(4)
where
pk = wk+1 − (wk + µvk),
(5)
qk = ∇E(wk+1) − ∇E(wk + µvk).
(6)
(4) was equivalent to the BFGS formula of [4] by replacing
pk and qk into sk = wk+1 − wk and yk = ∇E(wk+1) −
∇E(wk), respectively. Therefore, the limited-memory scheme
can be straightly applied to NAQ as illustrated in Algorithms 1
and 2. In Algorithm 1, two calculations of gradient vectors of
∇E(wk+µvk) and ∇E(wk+1) were needed within a training
loop whereas LQN was calculated once. This is a disadvantage
of LNAQ, but the algorithm can farther shorten the iteration
counts to cancel out the effect of this shortcoming.
Algorithm1: The proposed LNAQ
1. k = 1;
2. w1 = rand[−0.5, 0.5](uniform random numbers);
3. While(E(wk) > ε and k < kmax)
(a) Calculate ∇E(wk + µvk);
(b) Calculate the direction vector ˆck using Algorithm 2;
(c) Calculate αk using Armijo’s condition;
(d) Update wk+1 = wk + µvk − αkˆck;
(e) Calculate ∇E(wk+1);
(f) k = k + 1;
4. return wk;
Algorithm2: Direction Vector of LNAQ
1. ˆck = −∇E(wk + µvk);
2. for i : k, k − 1, . . . , k − min(k, (t − 1));
(a) ˆβi = pT
i ˆck/pT
i qi;
(b) ˆck = ˆck − ˆβiqi;
3. if k > 1,ˆck = (pT
k qk/qT
k qk)ˆck;
4. for i : k − min(k, (t − 1)), . . . , k, k − 1;
(a) ˆτ = qT
i ˆck/qT
i pi;
(b) ˆck = ˆck − (ˆβi − ˆτ)pi;
5. return ˆck;
IV.
SIMULATION RESULTS
In this paper, we demonstrated the effectiveness of the
proposed LNAQ for the training of neural networks. Levy
function as shown in (7) is used for the function approximation
problem. Levy function is a multimodal function and has the
highly-nonlinear characteristic [2].
f(x1 . . . xn) = π
n
{ n−1
∑
i=1
[(xi − 1)2(1 + 10 sin2(πxi+1))]
+10 sin2(πx1) + (xn − 1)2
}
, xi ∈ [−4, 4], ∀i.
(7)
Here the structure of neural network is 5-50-1, that is, the
network has 5 inputs, 1 output and 50 hidden neurons. The
dimension of w is 351. The number of training data is
0
50000
10−4
10−3
10−2
10−1
Training Error :E(w)
Iteration count :k
NAG
ADAM
LQN
LNAQ (propsed) 
Figure 1. The average training errors for iteration count.
|Tr| = 5000, which are generated by uniformly random
number in xi ∈ [−4, 4]. The storage amount of t is 30 for
LQN and LNAQ. Ten simulation runs are performed from
different initial values. The propose LNAQ is compared with
NAG, ADAM and LQN. ADAM is one of the latest and the
most effective ﬁrst order training method [5]. The momentum
coefﬁcient µ of NAG and LNAQ is experimentally set to
0.95 in the simulations. The termination conditions are set to
E(w) ≤ 10−4 and kmax = 5×105. Figure 1 shows the training
error E(w) of NAG, ADAM, LQN and LNAQ for the iteration
count in the early stage of training. From this ﬁgure, it can be
seen that NAG cannot converge within 5 × 104 iterations and
the iteration counts of LNAQ is drastically improved. Next, the
averages of iteration counts and CPU times (sec) for NAG,
ADAM, LQN and LNAQ are illustrated in Table 1. In the
simulations, the trainings continued until each training error of
E(w) was less than 1×10−4. This is an important point of the
function approximation problem. That is, the trained network
with the small MSE of E(w) can become an accurate neural
network model. All algorithms can obtain the neural network
model with the small training error. However, the ﬁrst order
methods (NAG and ADAM) need more iteration counts and
CPU times than LQN and LNAQ. Furthermore, LNAQ was
able to improve the convergence speed compare with LQN. As
a result, it is conﬁrmed that the proposed LNAQ is efﬁcient
and practical for the training of neural networks.
TABLE 1. SIMULATION RESULTS FOR THE AVERAGE ITERATION
COUNTS AND CPU TIMES.
Algorithm
NAG
ADAM
LQN
LNAQ
Iteration counts
335,403
48,680
17,892
4,514
CPU times (sec)
1,820
263
142
60
V.
CONCLUSIONS
In this research, we proposed a novel training algorithm
called LNAQ which was developed based on Limited-memory
method of QN using Nesterov’s accelerated gradients. The
effectiveness of the proposed LNAQ was demonstrated through
2
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-608-8
FUTURE COMPUTING 2018 : The Tenth International Conference on Future Computational Technologies and Applications

the computer simulations compared with the conventional
algorithms such as NAG, ADAM and LQN. It helps provide
accurate neural network models much fast. In the future, the
validity of the proposed algorithm for more highly nonlinear
function approximation problems and the much huge scale
problems including deep networks will be demonstrated.
ACKNOWLEDGMENT
This work was supported by Japan Society for the Promo-
tion of Science (JSPS), KAKENHI (17K00350).
REFERENCES
[1]
S. Haykin, Neural Networks and Learning Machines 3rd, Pearson,
2009.
[2]
H. Ninomiya, ”A novel quasi-Newton-Optimization for neural network
training incorporating Nesterov’s accelerated gradient”, IEICE NOLTA
Journal, vol.E8-N, no.4, p.289-301, Oct. 2017.
[3]
Y. Nesterov, ”Introductory Lectures on Convex Optimization: A Basic
Course”, Kluwer Boston, 2004.
[4]
J. Nocedal, and S.J. Wright., Numerical Optimization Second Edition,
Springer, 2006.
[5]
S. Ruder,”An overview of gradient descent optimization algorithms”,
arXiv 1609.04747, 15. Jun. 2017.
3
Copyright (c) IARIA, 2018.     ISBN:  978-1-61208-608-8
FUTURE COMPUTING 2018 : The Tenth International Conference on Future Computational Technologies and Applications

