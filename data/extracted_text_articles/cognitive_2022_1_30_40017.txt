Reasoning and Arguments in Negotiation 
Developing a formal model 
Mare Koit 
Institute of Computer Science 
University of Tartu 
Tartu, Estonia 
e-mail: mare.koit@ut.ee 
 
Abstract—Our aim is to develop a model of negotiation 
where two participants present arguments for/against of 
doing an action. The choice of an argument depends, on one 
hand, on the beliefs about the positive and negative aspects 
of doing the action and the needed resources, and on the 
other hand, on the result of reasoning affected by these 
beliefs. The model is based on the analysis of human-human 
negotiations (in this paper, we consider telemarketing calls). 
A limited version of the model is implemented as a dialogue 
system. The computer attempts to influence the reasoning of 
the user by its arguments in order to convince the user to 
make a decision.  
Keywords—reasoning; 
beliefs; 
negotiation; 
argument; 
dialogue system. 
I.  INTRODUCTION  
Negotiation is a form of interaction in which a group 
of agents, with a desire to cooperate but with potentially 
conflicting interests try to come to a mutually acceptable 
division 
of 
scarce 
resources 
[1]. 
Negotiation 
is 
simultaneously a linguistic and a reasoning problem, in 
which intent must be formulated and then verbally 
realized. A variety of agents have been created to 
negotiate with people within a large spectrum of settings 
including the number of parties, the number of 
interactions, and the number of issues to be negotiated [2]. 
Negotiation dialogues contain both cooperative and 
adversarial elements, and their modelling require agents to 
understand, plan, and generate utterances to achieve their 
goals [3]. 
Our aim is to develop a model of conversational agent 
that interacts with a user in Estonian and carries out 
negotiation. We start with the analysis of human-human 
negotiations aiming to model the reasoning processes 
which 
people 
go 
through 
when 
pursuing 
their 
communicative goals and coming to a decision. 
The rest of the paper is organized as follows. Section 2 
describes related work. In Section 3, we analyze a kind of 
human-human negotiation dialogues – telemarketing calls, 
in order to explain how do people reason and argue when 
negotiating about doing an action. In Section 4, we 
introduce our model of conversational agent that takes 
into account the results of the analysis of human-human 
negotiations, and an implementation – a simple Dialogue 
System (DS). Section 5 discusses the model and the DS. 
In Section 6, we draw conclusions and plan future work. 
II. RELATED WORK 
A conversational agent, or DS, is a computer system 
intended to interact with a human using text, speech, 
graphics, gestures and other modes for communication. It 
will have both dialogue modelling and dialogue 
management components [4]. A dialogue manager is a 
component of a DS that controls the conversation. Four 
kinds of dialogue management architectures are most 
common: plan-based, finite-state, frame-based, and 
information-state [5]. An information state includes 
beliefs, assumptions, expectations, goals, preferences and 
other attitudes of a dialogue participant that may influence 
the 
participant’s 
interpretation 
and 
generation 
of 
communicative behavior. The functions of the dialogue 
manager can be formalized in terms of information state 
update [6].  
Rahwan et al. [7] discuss three approaches to 
automated negotiation: game-theoretic, heuristic-based 
and 
argumentation-based. 
Argumentation-based 
approaches to negotiation allow agents to ‘argue’ about 
their beliefs and other mental attitudes during the 
negotiation process. Argumentation-based negotiation is 
the process of decision-making through the exchange of 
arguments [3].  
In negotiation, an argument can be considered as a 
piece of information that may allow an agent to: (a) justify 
its negotiation state; or (b) influence another agent’s 
negotiation state [8]. Amgoud and Cayrol define an 
argument as a pair (H, h) where: (i) H is a consistent 
subset of the knowledge base, (ii) H implies h, (iii) H is 
minimal, so that no subset of H satisfying both (i) and (ii) 
exists. H is called the support and h the conclusion of the 
argument [9].  
Automated negotiation agents capable of negotiating 
efﬁciently with people must rely on a good opponent 
modelling component to model their counterpart, adapt 
their behavior to their partner, influencing the partner’s 
opinions and beliefs [10]. NegoChat is the first 
negotiation agent successfully developed to use a natural 
chat interface while considering its impact on the agent’s 
negotiation strategy [2]. A virtual human that negotiating 
with a human helps people learn negotiation skills. For 
virtual agents, the expression of attitudes in groups is a 
key element to improve the social believability of the 
virtual worlds that they populate as well as the user’s 
experience, for example in entertainment or training 
applications [11][12][13]. 
17
Copyright (c) IARIA, 2023.     ISBN:  978-1-61208-950-8
COGNITIVE 2022 : The Fourteenth International Conference on Advanced Cognitive Technologies and Applications

An interesting and useful kind of DSs are embodied 
conversational agents [11][14][15].  
III. ANALYSIS OF HUMAN-HUMAN NEGOTIATIONS 
Our further aim is to implement a DS which interacts 
with a user in Estonian and carries out negotiations like a 
human does. For that, we are studying human-human 
negotiations using the Estonian dialogue corpus [16]. All 
the dialogues are recorded in authentic situations and then 
transliterated by using the transcription of Conversation 
Analysis [17]. A sub-corpus of telemarketing calls is 
chosen for the current study. In the dialogues, two official 
persons are communicating – a sales clerk of an 
educational company (its changed name is Tiritamm, he is 
the initiator of a call), and a manager or a personnel 
officer of another institution (she is here a customer). 
Tiritamm offers training courses (management, sale, etc.) 
which can be useful for the employees of the customer’s 
institution. The communicative goal of a sales clerk is to 
convince the customer to decide to take a course. Several 
typical phases can be differentiated in telemarketing 
negotiations 
[18]. 
The 
most 
important 
phase 
is 
argumentation. A sales clerk (A) presents different 
arguments that take into account the actual needs of the 
customer (B) explained by him (A) before or during the 
call. A tries to bring out the factors that are essential for 
the customer, in order to convince her to make a positive 
decision (Example 1). If B accepts these factors then A 
will demonstrate/prove how the proposed course will 
solve B’s problem. In an ideal case, the customer will 
agree with the proof offered by the clerk and will decide 
to take the course. 
Example 1 (transcription of Conversation Analysis is 
used in the examples) 
A: /---/(1.0) .hh sest loomu`likult et=ee `töökogemuste kaudu: 
õpib ka: alati aga .hh a `sageli ongi just `see (0.5) mt ee 
`kursused pakuvad sellise `võimaluse kus saab siis `teiste .hh 
oma hh `ala `spetsia`listidega samuti `kokku=ja `rääkida nendest 
`ühistest prob`leemidest ja samas siis ka .hh ee `mõtteid ja `ideid 
ee hh ee=`Tiritamme poolt sinna `juurde. 
because, 
of 
course, 
one 
can 
learn 
from 
experience but frequently training courses make 
it possible to meet other specialists in the 
field and discuss common problems; additional 
thoughts and ideas come from Tiritamm    argument 
(.) 
B: £ `jah?  
yes                                    accept 
The behaviour of sales clerks and customers is 
different when they are arguing for/against a course. A 
sales clerk when having the initiative provides his 
arguments for taking the course either asserting something 
(then a customer typically accepts the assertion, Example 
1). A customer, to the contrary, does not accept 
assertions/arguments of a sales clerk when arguing against 
taking a course (Example 2).  
Example 2 
/---/ B: aga jah ei mul on see läbi ´vaadatud=ja (.) ´kahjuks ma 
pean ütlema=et (.) et ´teie (.) seda meile (.) ´ei suuda ´õpetada (.) 
mida (.)´mina: (.) tahan. 
but I have looked through your catalogue of 
courses and unfortunately, I have to say that 
you can’t teach what is needed for us     counter 
argument 
/---/ 
A: .h ja mida kon´kreetselt=ee ´teie tahate.  
and what do you want                                          question 
(0.8) mida te ´silmas ´peate.   
what do you have in view                                  question 
B: noo (0.2) ´meie (.) äri´tegevus on (.) ´ehitamine.  
well, our business is house-building       answer 
/---/ 
A: nüüd kas (0.2) näiteks (0.5) ´lepingute ´saamisel (0.5) mt ee 
´tegelete te ka: läbi´rääkimistega.  
well, do you need to carry out negotiations in 
order to achieve agreements                                 question 
B: noo ikka.  
yes, of course                                                                answer 
(0.8) 
A: mt et see=on ka üks ´valdkond mida me: (0.2) ´käsitleme.  
but that is one of our fields which we cover 
                                            argument 
The argumentation continues in a similar way. A 
attempts to convince B preparing his new arguments by 
questions. Either A constructs his arguments during the 
conversation or he chooses suitable arguments from an 
existing set of possible arguments collected by previous 
experience with customers.  
When modelling negotiation, a good way seems to 
follow the sales clerks’ strategy: try to take and hold the 
initiative and propose ’hard’ arguments for the strived 
action, i.e., the statements that do not provoke the 
partner’s rejection but accept. In order to have such 
arguments at disposal, it is necessary to know as possible 
more about the partner in relation to the goal action.  
IV. MODELLING CONVERSATIONAL AGENT  
Results of the analysis of human-human negotiations 
motivated our model of conversational agent. Let us 
consider negotiation between two participants A and B 
where A is the initiator. Let his communicative goal be to 
bring B to the decision to do an action D. When 
convincing B, he is using a partner model (a picture of the 
communication partner) that gives him grounds to believe 
that B will agree to do the action. A starts the dialogue by 
proposing B to do D. If B, as the result of her reasoning, 
refuses, then A must influence her in the following 
negotiation, continuously correcting the partner model and 
trying to guess in which reasoning step B reached her 
negative decision. In this way, a dialogue – a sequence of 
utterances will be generated together by A and B. 
A. 
Information States in Negotiation Process 
Let A and B be conversational agents. Let us assume 
the following [18]: 
1) a set G of communicative goals where both 
participants choose their own initial goals (GA and GB, 
respectively). In our case, GA = “B decides to do D”  
2) a set S of communicative strategies of the 
participants. A communicative strategy is an algorithm 
used by a participant for achieving his/her communicative 
18
Copyright (c) IARIA, 2023.     ISBN:  978-1-61208-950-8
COGNITIVE 2022 : The Fourteenth International Conference on Advanced Cognitive Technologies and Applications

goal. This algorithm determines the activity of the 
participant at each communicative step 
3) a set T of communicative tactics, i.e., methods of 
influencing the partner when applying a communicative 
strategy. For example, A can entice, persuade, or threaten 
B in order to achieve the goal GA, i.e., A attempts to 
demonstrate that achieving this goal is, accordingly, 
pleasant, useful or obligatory for B 
4) a set R of reasoning models, which are used by 
participants when reasoning (here: about doing an action 
D). A reasoning model is an algorithm, which returns the 
positive or negative decision about the reasoning object 
(the action D) 
5) a set P of participant models, i.e., a participant’s 
depiction of the beliefs of himself/herself and his/her 
partner in relation to the reasoning object: P = {PA(A), 
PA(B), PB(A), PB(B)} 
6) a set of world knowledge  
7) a set of linguistic knowledge. 
A conversational agent passes several information 
states during interaction starting from initial state and 
going to every next state by applying update rules. 
Information states represent cumulative additions from 
previous actions in the dialogue, motivating future 
actions. There are two parts of an information state of a 
conversational agent [7] – private (information accessible 
only for the agent) and shared (accessible for both 
participants). 
Two categories of update rules will be used by a 
conversational agent for moving from current information 
state into the next one: (1) for interpreting the partner’s 
turns and (2) for generating its own turns. 
B. Reasoning Model  
The initial version of our reasoning model is 
introduced in [19]. In general, it follows the ideas realized 
in the BDI (Belief-Desire-Intention) model [20]. The 
reasoning process of a subject about doing an action D 
consists of steps where the resources, positive and 
negative aspects of D will be weighed. A communication 
partner can only implicitly take part in this process by 
presenting arguments to stress the positive and downgrade 
the negative aspects of D.  
Our reasoning model includes two parts: (1) a model 
of (human) motivational sphere that represents the beliefs 
of a reasoning subject in relation to the aspects of the 
action under consideration, and (2) reasoning procedures. 
1) Model of Motivational Sphere 
We represent the model of motivational sphere of a 
communication participant as a vector with (here: 
numerical) coordinates that express the beliefs of the 
participant in relation to different aspects of the action D:  
wD = (w(resourcesD), w(pleasantD), w(unpleasantD), 
w(usefulD), w(harmfulD), w(obligatoryD), w(prohibitedD), 
w(punishment-doD), w(punishment-notD)).  
The value of w(resourcesD) is 1 if the reasoning 
subject has all the resources needed for doing D, and 0 if 
some of them are missing. The value of w(obligatoryD) or 
w(prohibitedD) is 1 if the action is obligatory or, 
respectively, prohibited for the subject (otherwise 0). The 
values of the other coordinates can be numbers on the 
scale from 0 to 10 – w(pleasantD), w(unpleasantD), etc, 
indicate the values of the pleasantness, unpleasantness, 
etc. of D or its consequences; w(punishment-doD) is the 
punishment 
for 
doing 
a 
prohibited 
action 
and 
w(punishment-notD) – the punishment for not doing an 
obligatory action.  
2) Reasoning Procedures 
The reasoning itself depends on the determinant, 
which triggers it. With respect to the used theory, there 
are three kinds of determinants that can cause humans to 
reason about an action D: wish, need and obligation [21]. 
Therefore, 
three 
different 
prototypical 
reasoning 
procedures can be described – WISH, NEEDED, and 
MUST. Every procedure consists of steps passed by a 
reasoning subject and it finishes with a decision: do D or 
not. When reasoning, the subject considers his/her 
resources as well as different positive and negative 
aspects of doing D. If the positive aspects (pleasantness, 
usefulness, 
etc.) 
weigh 
more 
than 
negative 
(unpleasantness, harmfulness, etc.) then the decision will 
be “do D” otherwise “do not do D”. The reasoning subject 
checks primarily his/her wish, thereafter need and then 
obligation and he/she triggers the corresponding reasoning 
procedures. If no one procedure returns the decision “do 
D” then the reasoning ends with the decision “do not do 
D”.  
In Figure 1, we present the reasoning procedure 
NEEDED, which is triggered by the need of the reasoning 
subject to do the action D (i.e., doing the action is more 
useful than harmful for the subject) The procedure is 
presented as a step-form algorithm. We do not more 
indicate the action D. 
Presumption: w(useful)  w(harmful). 
1) Is w(resources) = 1? If not then go to 8. 
2) Is w(pleasant) > w(unpleasant)? If not then go to 5.  
3) Is w(prohibited) = 1? If not then go to 7. 
4) Is w(pleasant) + w(useful) > w(unpleasant) + w(harmful) + 
w(punishment-do)? If yes then go to 7 otherwise go to 8. 
5) Is w(obligatory) = 1? If not then go to 8. 
6) Is w(pleasant) + w(useful) + w(punishment-not) > 
w(unpleasant) + w(harmful)? if not then go to 8. 
7) Decide: do D. End. 
8) Decide: do not do D. 
Figure 1. Reasoning procedure NEEDED. 
We use two vectors wB and wAB, which capture the 
beliefs of communication participants in relation to the 
action D under consideration. Here wB is the model of 
motivational sphere of B who has to make a decision 
about doing D; the vector includes B’s (actual) 
evaluations (beliefs) of D’s aspects. These values are used 
by B when reasoning about doing D. The other vector wAB 
is the partner model that includes A’s beliefs concerning 
B’s beliefs in relation to the action. It is used by A when 
planning next turns in dialogue. We suppose that A has 
some preliminary knowledge about B in order to compose 
19
Copyright (c) IARIA, 2023.     ISBN:  978-1-61208-950-8
COGNITIVE 2022 : The Fourteenth International Conference on Advanced Cognitive Technologies and Applications

the initial partner model before making the initial 
proposal.  
Both the models will change as influenced by the 
arguments presented by both the participants in 
negotiation. For example, every argument presented by A 
targeting the usefulness of D will increase the 
corresponding values of wB(useful) as well as wAB(useful).  
C. Implementation 
A simple dialogue system is developed that carries out 
negotiations with a user in a natural language about doing 
an action [18]. The participants can have different initial 
goals: the initiator (either DS or a user) tries to achieve the 
decision of the partner to do the action but the partner’s 
goal can be opposite. DS interacts with a user using texts 
in a natural language. There are two work modes. In one 
case, the computer is playing A’s and in the other – B’s 
role. 
Both A and B have access to a common set of 
reasoning procedures. They also use fixed sets of dialogue 
acts and the corresponding utterances in a natural 
language, which are pre-classified semantically, e.g., the 
set Pmissing_resources for indicating that some resources for 
doing a certain action D are missing (e.g., I don't have 
proper dresses, see example 3 in the next section), 
Pincreasing_resources for indicating that there exist resources for 
(e.g., The company will cover all your expenses), 
Pincreasing_usefulness for stressing the usefulness of D (e.g., You 
can be useful for the company), etc. Therefore, no 
linguistic analysis or generation will be made during a 
dialogue. The utterances will be accidentally chosen by 
conversational agent from the suitable semantic classes (in 
our implementation, every utterance can be used only 
once). However, these restrictions will bring along that 
the generated dialogues are not quite coherent.  
If A’s goal is “B will do D” and B’s goal is opposite 
then A, starting interaction, generates, by using his 
knowledge a partner model wAB and determines the 
communicative tactics T, which he will use (e.g., 
persuasion), i.e., he accordingly fixes the reasoning 
procedure R, which he will try to trigger in B’s mind (e.g., 
NEEDED). B has her own model wB (which exact values 
A does not know). B in her turn determines a reasoning 
procedure RB that she will use in order to make a decision 
about doing D, and her communicative tactics TB. 
D. Reasoning and Arguments 
When attempting to direct B’s reasoning to the 
positive decision (do D), A presents several arguments 
stressing the positive and downgrading the negative 
aspects of D. Preparing an argument, A triggers a 
reasoning procedure in his partner model wAB, in order to 
be sure that the reasoning will give the positive decision. 
When opposing, B can use the same or a different 
reasoning procedure triggering it in the model of herself 
wB. After the changes made by both the participants in the 
two models during a dialogue, the models will approach 
each to other but, in general, do not equalise. Although, 
the results of reasoning in both models can be (or not be) 
equal. 
Let us consider a dialogue with our DS (Example 3). 
Here A is the conversational agent playing the role of the 
boss of a company and B is the user playing the role of an 
employee of the company who is at the same time 
studying at a university. A presents arguments for doing D 
by B (D=travel to N. in order to conclude a contract 
there). It succeeds to decline B’s counter arguments and 
convince B to accept its goal. 
Example 3 
1. A: The company offers you a trip to N. 
Our company needs to conclude a contract there.  
2. B: I don't have proper dresses.  
3. A: The company will pay your executive 
expenses. You can be useful for the company.  
4. B: I can have some problems at my 
university.  
5. A: 
It's 
all 
right–your 
examinations 
period will be extended. The company will 
evaluate your contribution.  
6. B: OK, I’ll do it.  
7. A: I am glad.  
Let us consider how the partner model is used in the 
dialogue. A will implement the tactics of persuasion and 
generates a partner model, let it be 
wAB={wAB(resources)=1, 
wAB(pleasant)=4, 
wAB(unpleasant)=2, 
wAB(useful)=5, 
wAB(harmful)=2, 
wAB(obligatory)=0, wAB(prohibited)=0, wAB(punishment-
do)=0, wAB(punishment-not)=0}. 
The reasoning procedure NEEDED (Figure 1) yields 
a positive decision in this model. A’s initial information 
state is as follows. 
Private part 
• 
initial partner model wAB = (1, 4, 2, 5, 2, 0, 0, 0, 
0) 
• 
the tactics chosen by A–persuasion 
• 
A will use the reasoning procedure NEEDED, the 
presumption is fulfilled: wAB(useful) > wAB(harmful) 
• 
the set of dialogue acts at A’s disposal: 
{proposal, arguments for increasing/decreasing values of 
different coordinates of wAB, accept, reject} 
• 
the set of utterances for expressing the dialogue 
acts at A’s disposal: {The company offers you a trip to N, 
You can be useful for the company, etc.}. 
Shared part  
• 
the reasoning procedures WISH, NEEDED, 
MUST  
• 
the tactics of enticement, persuasion, threatening 
• 
dialogue history–an empty set. 
Let us suppose that every statement (argument) 
presented in dialogue will increase or respectively, 
decrease the corresponding value in the model of beliefs 
by one unit. Still, this is a simplification because different 
arguments might have different weights for different 
dialogue participants. 
Conversational agent A starts the dialogue with a 
proposal. Using the tactics of persuasion and attempting to 
trigger the reasoning procedure NEEDED in B, it adds an 
argument for increasing the usefulness to the proposal 
(turn 1). In the same time, it increases the initial value of 
the usefulness in its partner model wAB by 1. The current 
reasoning procedure NEEDED still gives a positive 
20
Copyright (c) IARIA, 2023.     ISBN:  978-1-61208-950-8
COGNITIVE 2022 : The Fourteenth International Conference on Advanced Cognitive Technologies and Applications

decision in the updated model. A does not know the actual 
values of attitudes, which B has assigned in the model wB 
of herself. As caused by every counter argument presented 
by B, A has to update the partner model wAB. However, 
B’s counter argument (turn 2) demonstrates that B actually 
has resources missing (I don't have proper dresses) 
therefore, A has to decrease the value of wAB(resources) 
from 1 to 0 in its partner model. Now A must find an 
argument indicating that the resources are available: it 
selects an utterance from the set Pincreasing_resources (The 
company will pay your executive expenses) and following 
the tactics of persuasion it adds an argument for 
increasing the usefulness (You can be useful for the 
company) in turn 3. The value of wAB(resources) will now 
be 1 and the value of wAB(useful) will be increased by 1 in 
the partner model. The reasoning in the updated model 
gives a positive decision. Nevertheless, B has a new 
counter argument indicating the harmfulness of the action: 
I can have some problems at my university (turn 4).  
Now A has to increase the value wAB(harmful) in the 
partner model, it turns out that by 6 not by 1 as we 
assumed. Let us explain why. So far, A was supposing that 
D is not prohibited for B. This assumption proves to be 
wrong because otherwise it is impossible for B to indicate 
the harmfulness of D (if she is applying the reasoning 
procedure NEEDED as A supposes). Therefore, B 
supposedly compares the values of beliefs at the step 4 of 
the procedure and makes a negative decision. B can come 
to the step 4 only after the step 3 where she detects that D 
is prohibited and doing D involves a punishment (turn 4). 
Therefore, A changes the value of wAB(prohibited) from 0 
to 1 and increases the value of wAB(punishment-do) in the 
partner model at least by 1. (Being optimistic, A increases 
the value exactly by 1 and not more.) Now A checks, how 
to change the value of the harmfulness in the partner 
model in order to get the negative decision like B did. 
According to the reasoning procedure NEEDED A 
calculates that the value has to be increased (at least) by 6. 
Therefore, wAB(harmful) will be 2+6=8. 
Responding to B’s counter argument A decreases the 
value of wAB(harmful) by 1 using the utterance It's all 
right - your examinations period will be extended, and 
increases the value of wAB(useful) once more using the 
utterance The company will evaluate your contribution 
(turn 5). The reasoning procedure NEEDED gives a 
positive decision in the updated partner model. Now it 
turns out that B has made this same decision (turn 6). A 
has achieved its communicative goal and finishes the 
dialogue (turn 7). 
Example 3 demonstrates how A is updating the 
partner model wAB in negotiation with B. The final model 
will be wAB = (1, 4, 2, 8, 7, 0, 1, 0, 1). As compared with 
the initial model, the values of four aspects have 
increased. All the changes are caused by A’s arguments 
and B’s counter arguments.  
In this way, A is able to convince B to do D if he has 
enough arguments for doing D and his initial picture of B 
does not radically differentiate from B’s actual beliefs. 
Both the beliefs in the partner model wAB and B’s actual 
beliefs in the model wB of herself (if B is a conversational 
agent similarly with A) are changing during the dialogue 
as influenced by the arguments presented by the 
participants. Although the models wAB and wB do not 
necessarily coincide at the end of the dialogue, the 
proportions of the values of the positive (pleasantness, 
etc.) and negative aspects of doing D (unpleasantness, 
etc.) will be similar. Still, if B is a human user then she is 
not obliged to use models and algorithms (although can) 
but can choose utterances from suitable semantic classes. 
V. DISCUSSION 
Our model of conversational agent is motivated by the 
analysis of human-human negotiations. We consider the 
dialogues where two participants A and B negotiate doing 
an action D. In the analysed telemarketing calls, the 
communicative goal of a sales clerk of the educational 
company is to convince a customer to take a training 
course offered by the company. If the participants are 
collaborative and one of them presents his/her argument 
then the partner mostly accepts it. If the participants are 
antagonistic then at least one of them does not agree with 
the opinion of the partner and presents his/her 
counterargument(s). The more the clerk knows about the 
customer, the more convincing arguments is he able to 
choose. Asking questions is a way to learn more. 
When reasoning about doing an action, a subject is 
weighing different aspects of the action (its pleasantness, 
usefulness, etc.), which are included into his/her model of 
motivational sphere. In the model presented here, we 
evaluate these aspects by giving them discrete numerical 
values on the scale from 0 to 10. Still, people do not use 
numbers but rather words of a natural language, e.g., 
excellent, very pleasant, harm, etc. Further, when 
reasoning, people do not operate with exact values of the 
aspects of an action but they rather make ‘fuzzy 
calculations’, for example, they suppose/believe that 
doing an action is more pleasant than unpleasant and 
therefore they wish to do it. Another problem is that the 
aspects of actions considered here are not fully 
independent. For example, harmful consequences of an 
action as a rule are unpleasant. In addition, if the 
reasoning object is different (not doing an action like in 
our case) then the attitudes of a reasoning subject can be 
characterized by a different set of aspects. 
When attempting to direct B’s reasoning to the 
desirable decision, A presents several arguments stressing 
the positive and downgrading the negative aspects of D. 
The choice of A’s argument is based on one hand, on the 
partner model, which captures A’s knowledge about B, 
and on the other hand, on the (counter) argument 
presented by B. Still, B is not obliged to present any 
counter argument but she can only refuse (I do not do this 
action). When choosing the next argument for D, A 
triggers a reasoning procedure in his partner model 
depending on the chosen communicative tactics, in order 
to be sure that the reasoning will give a positive decision 
after presenting this argument. B herself can use the same 
or a different reasoning procedure triggering it in her own 
model. After the updates made both by A and B in the two 
models during a dialogue, the models will approach each 
21
Copyright (c) IARIA, 2023.     ISBN:  978-1-61208-950-8
COGNITIVE 2022 : The Fourteenth International Conference on Advanced Cognitive Technologies and Applications

to another but, in general, do not equalize. Nevertheless, 
the results of reasoning in both models can be similar, as 
demonstrated example 3. Therefore, A can convince B to 
do D even if not having a perfect picture of her. 
Our dialogue model considers only a limited kind of 
dialogues but although, it illustrates the situation where 
the dialogue participants are able to change their beliefs 
related to the negotiation object and bring them closer one 
to another by using arguments. The initiator A does not 
need to know whether the counter arguments presented by 
the partner B have been caused by B’s opposite initial goal 
or are there simply obstacles before their common goal, 
which can be eliminated by A’s arguments. A’s goal, on 
the contrary is not hidden from B. Secondly, the different 
communicative tactics used by A are aimed to trigger 
different reasoning procedures in B’s mind. A can fail to 
trigger the pursued reasoning procedure in B but however 
he can achieve his communicative goal when having a 
sufficient number of arguments supporting his initial goal. 
In our implemented DS, the user interacts with the 
computer, 
choosing 
ready-made, 
semantically 
pre-
classified sentences as arguments and counter arguments 
for and against doing a certain action. We suppose that 
this kind of software is useful when training the skills of 
finding arguments and counter arguments for and against 
of doing an action. The computer can establish certain 
restrictions on the argument types and on the order in their 
use. Still, when interacting with the computer, a human 
user does not use neither a formal partner model, nor a 
formal model of herself, nor reasoning procedures. 
However, both implementation modes allow study how 
the beliefs of the participants are changing in negotiation. 
VI. CONCLUSION AND FUTURE WORK 
We are considering the dialogues where two (human 
or artificial) agents A and B negotiate doing an action D 
by one of them (B). We analyse human negotiations in 
order to explain how arguments are used to convince a 
dialogue partner. Initial communicative goals of the 
participants can be similar or opposite. The partners 
present arguments for and against of doing D. The 
arguments of initiator A are based on his partner model 
wAB whilst B’s arguments – on her model of herself wB. 
Both models include beliefs about the resources, positive 
and negative aspects of doing D that have numerical 
values in our implementation. Both models are updated 
during a dialogue.  
Our further aim is to develop the DS concentrating 
foremost on the reasoning model. So far, we are using an 
intuitive reasoning theory. However, there are several 
other approaches to model change of a person’s opinion, 
e.g., Elaboration Likelihood Model, Social Judgment 
Theory, and Social Impact Theory. Some of the theories 
can be better to model human reasoning. Our further 
research will explain this. 
ACKNOWLEDGMENT 
This work was supported by the European Union 
through the European Regional Development Fund 
(Centre of Excellence in Estonian Studies). 
REFERENCES 
[1] I. Rahwan, P. McBurney, and L. Sonenberg, “Towards a theory of 
negotiation strategy (a preliminary report),” Game-Theoretic and 
Decision-Theoretic 
Agents 
(GTDT), 
S. 
Parsons 
and 
P. 
Gmytrasiewicz, Eds. Proc. of an AAMAS-2003 Workshop, pp. 73–
80, 2003. Melbourne, Australia. 
[2] A. Rosenfeld, I. Zuckerman, E. Segal-Halevi, O. Drein, and S. 
Kraus, “NegoChat: A Chat-based Negotiation Agent,” Proc. of the 
International Conference on Autonomous Agents and Multi-agent 
Systems AAMAS’14, pp. 525–532, 2014. 
[3] M. Lewis, D. Yarats, Y.N. Dauphin, D. Parikh, and D. Batra, “Deal 
or No Deal? End-to-End Learning for Negotiation Dialogues,” 
Proc. of the Conference on Empirical Methods in Natural 
Language Processing, pp. 2443–2453, 2017. Copenhagen, 
Denmark, Assoc. for Computational Linguistics. 
[4] D. Traum, Computational Approaches to Dialogue, The Routledge 
Handbook of Language and Dialogue, 2017. 
[5] D. Jurafsky and J. M. Martin, Speech and Language Processing: 
An Introduction to Natural Language Processing, Computational 
Linguistics, and Speech Recognition. Prentice Hall, 2009. 
[6] D. Traum and S. Larsson, “The Information State Approach to 
Dialogue Management,” Current and New Directions in Discourse 
and Dialogue, pp. 325–353, 2003. 
[7] I. Rahwan et al., “Argumentation-based negotiation”, The 
Knowledge Engineering Review, vol. 18(4), pp. 343–375, 
Cambridge 
University 
Press, 
2004. 
Available 
from 
http://dx.doi.org/10.1017/S0269888904000098 
[8] N. R. Jennings, S. Parsons, P. Noriega, and C. Sierra, “On 
argumentation-based negotiation,” Proc. of the International 
Workshop on Multi-Agent Systems, Boston, pp. 1–7, 1998. 
[9] L. Amgoud and C. Cayrol, “A Reasoning Model Based on the 
Production of Acceptable Arguments”, Ann. Math. Artif. Intell. 
34(1-3), pp. 197–215, 2002. 
[10] C Hadjinikolis, Y Siantos, S Modgil, E Black, and P. McBurney, 
“Opponent modelling in persuasion dialogues,” Proc. of the 23rd 
IJCAI, pp. 164-170, 2013. 
[11] B. Ravenet, A. Cafaro, B .Biancardi, M. Ochs, and C. Pelachaud, 
“Conversational Behavior Reflecting Interpersonal Attitudes in 
Small Group Interactions”, IVA, pp. 375–388, 2015. 
[12] J. Gratch, S. Hill, L.-P. Morency, D. Pynadath, and D. Traum, 
“Exploring the Implications of Virtual Human Research for 
Human-Robot Teams,” VAMR’15, International Conference on 
Virtual, Augmented and Mixed Reality, R. Shumaker and S. 
Lackey, Eds. Springer, pp. 186–196, 2015, doi: 10.1007/978-3-
319-21067-4_20 
[13] M. Saberi, S. DiPaola, and U. Bernardet, “Expressing Personality 
Through Non-verbal Behaviour in Real-Time Interaction,” 
Frontiers in Psychology, vol 12, pp. 54–74, 2021. 
[14] S. Dermouche, “Computational Model for Interpersonal Attitude 
Expression”, ICMI, pp. 554–558, 2016. 
[15] K. Jokinen, “Exploring Boundaries among Interactive Robots and 
Humans”, Conversational Dialogue Systems for the Next Decade, 
L. F. D’Haro, Z. Callejas, and S. Nakamura, Eds. Springer: 
Singapore, LNEE, vol. 704, pp. 271-275, 2020. 
[16] T. Hennoste et al, “From Human Communication to Intelligent 
User Interfaces: Corpora of Spoken Estonian,” Proc. of LREC’08, 
European Language Resources Association (ELRA), pp. 2025–
2032, 2008, Marrakech, Morocco. Available from www.lrec-
conf.org/proceedings/¬lrec2008 
[17] J. Sidnell, Conversation Analysis: An Introduction, London: 
Wiley-Blackwell, 2010. 
[18] M. Koit, “Reasoning and communicative strategies in a model of 
argument-based 
negotiation,” 
Journal 
of 
Information 
and 
Telecommunication TJIT, Taylor & Francis Online, 2, 14 p. 2018. 
[19] M. Koit and H. Õim, “A Computational Model of Argumentation 
in Agreement Negotiation Processes”, Argument & Computation, 
Taylor & Francis Online, 5 (2-3), pp. 209–236, 2014. 
[20] M. E. Bratman, “Intention, Plans, and Practical Reason,” CSLI 
Publications, 1999. 
[21] H. Õim, “Naïve Theories and Communicative Competence: 
Reasoning in Communication,” Estonian in the Changing World, 
pp. 211–231, 1996. 
22
Copyright (c) IARIA, 2023.     ISBN:  978-1-61208-950-8
COGNITIVE 2022 : The Fourteenth International Conference on Advanced Cognitive Technologies and Applications

