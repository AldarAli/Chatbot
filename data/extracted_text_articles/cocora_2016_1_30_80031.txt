Optimal Action for Cognitive Radio User under Constrained Energy
and Data Trafﬁc Uncertainty
Hiep Vu-Van, Young-Doo Lee and Insoo Koo
School of Electrical Engineering
University of Ulsan, Ulsan, South Korea
Email: vvhiep@gmail.com, leeyd1004@naver.com and iskoo@ulsan.ac.kr
Abstract—In cognitive radio network, cognitive radios should
operate with a small battery. Operation schedule for cognitive
radio user (CU) strongly affects performance of a device with a
ﬁnite capacity battery. An energy harvester that harvests energy
from the environment to recharge the battery can be utilized to
extend the lifetime of the CU. While the CU consumes a similar
energy for sensing and transmitting data in all active slots, its
reward (i.e., throughput) may not be the same because of different
data trafﬁc (i.e., the amount of data that needs to be transmitted).
Therefore, in order to maximize performance of the cognitive
radio network, the CU needs to consider the current data trafﬁc
to determine its optimal action policy in terms of sleeping or
active. In this paper, we formulate the problem of choosing an
action by the CU by using a partially observable Markov decision
process (POMDP) in which the CU’s three states in terms of data
trafﬁc, belief and remaining energy are utilized as main factors
to decide an optimal action of the CU in current time slot, and
the effect of current action to future reward will be considered
through POMDP. Simulation results prove the efﬁciency of the
proposed scheme.
Keywords—cognitive radio; energy harvesting; energy efﬁciency;
optimal action policy; POMDP.
I. INTRODUCTION
Cognitive radio (CR) technology can improve spectrum
utilization by allowing cognitive radio users (CU) to share
the frequency assigned to a licensed user, called the primary
user (PU). In order to avoid interference with the operation of
the licensed user, the CU is allowed to be active only when
the frequency is free. Otherwise, when the presence of the PU
is detected, the CU has to vacate their occupied frequency.
In the CR network, the CU often has a small battery that
can maintain operations of the CU in a short time. Therefore,
the performance of the CR network strongly depends on how
effectively the CU uses its electric power. The problem of
optimal energy management has been considered previously
in [1], [2] where an optimal energy management scheme for a
sensor node with an energy harvester to maximize throughput
was proposed. In [3], [4], a scheme to ﬁnd an optimal action
policy including sleeping to save energy or active to take
opportunity of transmitting data is proposed. Due to the
energy-constrained of the CU, the proposed scheme applies
the partially observable Markov decision process (POMDP)
[5], [6] to determine that optimal action policy.
In most of the previous works, data trafﬁc (i.e., the amount
of data that needs to be transmitted) was not considered in
order to decide the optimal action for energy efﬁciency. It
is often assumed that the CU always has data to transmit.
However, this assumption may be not real in practice. For
example, in wireless sensor network, in order to save energy,
only the change of data (e.g., the change of monitoring envi-
ronment) is tracked and reported. In this case, the data trafﬁc
varies in time. Therefore, the performance of CR network can
be strongly affected by the data trafﬁc. In order to maximize
performance of the cognitive radio network, the CU needs to
consider the current data trafﬁc to determine its optimal action
policy in terms of sleeping or active. In sleeping mode, the CU
is silent and waits until the next time slot for another action
round. In active mode, the CU ﬁrst determines the status of
the PU signal, if the PU signal is absent, the CU is allowed
to access the considered channel to transmit data. If the CU
wants to switch to active mode, it must have enough energy
for all operations of the mode (i.e., spectrum sensing and data
transmitting). In both sleeping and active mode, the CU can
harvest energy from the environment to recharge the battery.
In this paper, we formulate the problem of choosing an
action by the CU by using a partially observable Markov
decision process (POMDP) in which the CU’s three states in
terms of data trafﬁc, belief and remaining energy are utilized as
main factors to decide an optimal action of the CU in current
time slot, and the effect of current action to future reward
will be considered through POMDP. It is expected that the
proposed scheme based on POMDP theory will provide the
CR system an improved performance.
This paper is organized as follows. Section 2 describes
the system model that we consider in the paper. Section 3
details the proposed optimal action decision scheme based
on POMDP. Section 4 introduces simulation models and
simulation results of the proposed scheme. Finally, Section
5 concludes this paper.
II. SYSTEM MODEL
We consider a CR network and a PU that is assumed
to operate in a time slotted model. The status of the PU
changes between two states of the Markov chain model, that
is, Presence (P) and Absence (A) as shown in Figure 1.
The transition probability of the PU from state P to state
A and from state A to itself are deﬁned as PP A and PAA,
respectively.
The data that needs to be transmitted is stored in a data
buffer of the CU; the amount of data D in the buffer is deﬁned
as data trafﬁc of the CU. The buffer can store maximum Bmax
units of data. At the time t, data trafﬁc of the CU can be
deﬁned as,
10
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-456-5
COCORA 2016 : The Sixth International Conference on Advances in Cognitive Radio

D (t) ∈ {c1, c2, ..., cξd}
(units of data),
(1)
where 0 ≤ c1 < c2 < ... < cξd ≤ Bmax and ξd is the number
of possible states of data trafﬁc.
At each time slot, there are uncertainty amount of data din
coming the buffer. din can take its values from the set of
possible coming data as,
din (t) ∈

cin
1 , cin
2 , ..., cin
ξin
	
(units of data),
(2)
where 0 ≤ cin
1 < cin
2 < ... < cin
ξin and ξin is the number of
possible states of coming data.
The probability mass function (PMF) of the coming data is
given as follows:
Pdin (k) = Pr

din(t) = cin
k

, k = 1, 2, ..., ξin.
(3)
We assume that the coming data follows the stochastic
process that is marked by the Poisson process. Subsequently,
din(t) is a Poisson random variable with mean din
mean. The
PMF in (3) can be equivalent to:
Pdin (k) ≈ e−din
mean
decision policy in terms of sleeping or active is formulated
as the framework of POMDP. For POMDP, we deﬁne the
value function V (D, p, er) as the maximum total discounted
throughput from the current time slot when the current state
of the CU is S(k) = {D(k), p(k), er(k)} where D(k), p(k)
and er(k) are data trafﬁc, belief and the remaining energy at
the beginning of the kth time slot. The value function is given
by:
V (S (k)) = max
a(k) E
( ∞
X
t=k
αt−kR (S (t) , a (t)) |S (k)
)
,
(10)
where 0
≤
α
<
1 is the discount factor, S (k)
=
{D (k) , p (k) , er (k)}. R (S(t), a(t)) is the throughput of the
CU achieved at the tth time slot, which is mainly dependent
on state S(t) and action decision a(t).
A. Sleeping Mode (φ1)
If the CU decides to remain sleeping, no throughput is
achieved, then R (S(t), S |φ1 ) = 0.
State S(t + 1) = {D(t + 1), p(t + 1), er(t + 1)} of the CU
will be updated for the next time slot. Firstly, data trafﬁc will
be updated as,
D(t + 1) = min

V (S (k)) = max
ak {
∞
P
t=k
αt−k
P
φi∈a(t)
Pr (φi) P
e(t+1)
Pr (e (t) → e (t + 1) |φi )
P
D(t+1)
Pr (D(t) → D(t + 1) |φi )R (S (t) , a (t) |φi ) |S (k)} .
(30)
TABLE I. SIMULATION PARAMETERS
Symbol
Description
Value
γ
SNR of the sensing channel
−10 dB
Pr(H0)
Average absence
probability of the PU
0.5
PAA
Transition probability
from state A to itself
0.8
PP A
Transition probability
from state P to state A
0.2
Eca
Total capacity of battery
10 units of energy
eh
Harvested energy
1 units of energy
τh
Success probability of energy harvester
0.8
et
Transmission energy
2 units of energy
es
Sensing energy
1 units of energy
Bmax
Capacity of data buffer
10 units of data
smax
Transmission capacity
5 units of data
din
mean
Mean value of coming data
1 units of data
The remaining energy of the CU for the next time slot can
be updated similar to the case of observation φ3.
According to those observations, the value function in (10)
can be expressed as (30). In order to ﬁnd an optimal mode
policy for maximizing throughput, the optimization problem
in (30) will be solved by using the value iterations method
[8].
IV. SIMULATION RESULTS
In this section, we present simulation results to prove the
efﬁciency of the proposed scheme. Myopic scheme only con-
siders the current time slot for the value function (i.e., α = 0)
to decide the CU’s action. This means that unless the CU has
not enough energy for spectrum sensing and data transmitting
or there is no data in the data buffer, Myopic scheme will
always allow the CU to be active. Simulation results of Myopic
will be provided for reference. The parameters for simulations
are shown in Table I.
4
5
6
7
8
9
10
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
Capacity of battery (Emax)
Reward (units of data)
 
 
The proposed scheme
Myopic scheme
Fig. 2. Reward versus battery capacity Emax.
In order to evaluate the performance of the proposed
scheme, we deﬁne Reward of the CU as the average number
of units of data successfully transmitted in each time slot.
Figure 2 shows the Reward of the considered schemes
according to the capacity of the CU battery Emax. It can
be seen that the increase of battery capacity may help the
CU achieve higher Reward. However, when the battery is big
enough (i.e., it has enough space to store all harvested energy),
the increase of battery will not affect the Reward.
Figure 3 presents the relation between Reward and harvested
energy eh of the CU. Higher amount of eh provide more
energy for active mode of the CU, so that the CU can get
more Reward. However, when eh is high enough for active
mode of the CU in all time, eh has no more effect on the CU’s
Reward. In this case, the energy constraint will disappear; and
then the action of the proposed scheme and Myopic scheme
will be the same. That is the reason why they have the same
performance when the harvested energy is high.
Transmission energy et may give strong effect to the Reward
of the proposed scheme, as shown in Figure 4. More energy is
consumed by transmitting data, less Reward the CU achieves.
Figures 5 and 6 illustrate the Reward according to the
change of maximum transmission capacity smax of the CU
(i.e., the maximum amount of data that the CU can transmit
in whole duration of a time slot) and the change of SNR
in the sensing channel, respectively. These ﬁgures show that
better transmission capacity or better SNR will improve the
performance of the proposed scheme.
The simulations results shown in all ﬁgures prove that the
proposed scheme can offer the CU better performance than
the conventional Myopic scheme. That beneﬁt of the proposed
scheme is achieved by considering future Reward on deciding
current action.
1
1.5
2
2.5
3
3.5
4
4.5
5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
Harvested energy (e)
Reward (units of data)
 
 
The proposed scheme
Myopic scheme
Fig. 3. Reward versus harvested energy eh.
13
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-456-5
COCORA 2016 : The Sixth International Conference on Advances in Cognitive Radio

1
1.5
2
2.5
3
3.5
4
4.5
5
0.3
0.4
0.5
0.6
0.7
0.8
Transmission energy (et)
Reward (units of data)
 
 
The proposed scheme
Myopic scheme
Fig. 4. Reward versus transmission energy et.
V. CONCLUSION
In this paper, we proposed a scheme to decide an optimal
action to maximize Reward of the CU on energy-constrained
and uncertainty data trafﬁc manner. By focusing on uncertainty
data trafﬁc, the proposed scheme is more practical than in
the previous studies. On the other hand, the proposed scheme
can take consideration into the effect of the future Reward
on current action of the CU by applying POMDP theory.
Simulation results show that the proposed scheme can obtain
better performance than conventional Myopic scheme.
ACKNOWLEDGEMENT
This work was supported by the KRF funded by the MEST
(NRF-2014R1A1A2005378).
REFERENCES
[1] V. Sharma, U. Mukherji, V. Joseph, and S. Gupta, “Optimal energy man-
agement policies for energy harvesting sensor nodes,” IEEE Transactions
on Wireless Communications,, vol. 9, no. 4, pp. 1326–1336, 2010.
[2] S. Mao, M. H. Cheung, and V. Wong, “An optimal energy allocation algo-
rithm for energy harvesting wireless sensor networks,” in Communications
(ICC), 2012 IEEE International Conference on, 2012, pp. 265–270.
[3] A. Sultan, “Sensing and transmit energy optimization for an energy
harvesting cognitive radio,” Wireless Communications Letters, IEEE,
vol. 1, no. 5, pp. 500–503, 2012.
3
3.5
4
4.5
5
5.5
6
6.5
7
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0.7
Maximum transmission capacity (smax)
Reward (units of data)
 
 
The proposed scheme
Myopic scheme
Fig. 5. Reward versus maximum transmission capacity smax.
-12
-11.5
-11
-10.5
-10
-9.5
-9
-8.5
-8
0.55
0.6
0.65
0.7
SNR
Reward (units of data)
 
 
The proposed scheme
Myopic scheme
Fig. 6. Reward versus SNR of the sensing channel γ.
[4] S. Park, J. Heo, B. Kim, W. Chung, H. Wang, and D. Hong, “Optimal
mode selection for cognitive radio sensor networks with rf energy harvest-
ing,” in Personal Indoor and Mobile Radio Communications (PIMRC),
2012 IEEE 23rd International Symposium on, 2012, pp. 2155–2159.
[5] X. Cao and X. Guo, “Partially observable markov decision processes with
reward information,” in Decision and Control, 2004. CDC. 43rd IEEE
Conference on, vol. 4, 2004, pp. 4393–4398.
[6] L. P. Kaelbling, M. L. Littman, and A. R. Cassandra, “Planning and
acting in partially observable stochastic domains,” ARTIFICIAL INTEL-
LIGENCE, vol. 101, pp. 99–134, 1998.
[7] J. Ma and Y. Li, “Soft combination and detection for cooperative spectrum
sensing in cognitive radio networks,” in IEEE Global Telecommunications
Conference (GLOBECOM), 2007, pp. 3139–3143.
[8] D. P. Bertsekas, Dynamic Programming and Optimal Control.
Athena
Scientic, 2nd edition, 2001, vol. 1 and 2.
14
Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-456-5
COCORA 2016 : The Sixth International Conference on Advances in Cognitive Radio

