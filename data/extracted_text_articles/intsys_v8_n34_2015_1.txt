233
International Journal on Advances in Intelligent Systems, vol 8 no 3 & 4, year 2015, http://www.iariajournals.org/intelligent_systems/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Pictogram Creation with Sensory Evaluation Method  
Based on Multiplex Sign Languages 
 
Naotsune Hosono 
Keio University, 
Oki Consulting Solutions 
Tokyo, Japan 
naotsune@mx-keio.net 
 
Hiromitsu Inoue 
Chiba Prefectural University  
of Health Sciences, 
Chiba, Japan 
cal32840@topaz.plala.or.jp 
 
Miwa Nakanishi 
Keio University 
Kanagawa, Japan 
miwa_nakanishi@ae.keio.ac.jp 
 
 
Yutaka Tomita 
Keio University 
Kanagawa, Japan 
tomita@z3.keio.ac.jp 
 
 
Abstract— Human sign languages are originally designed for 
use by hearing impaired people, and they include semantic 
expressions in their scope. This paper discusses an original 
method to create pictograms based on multiplex local sign 
languages with the concept of “Context of Use” on dialogue, by 
applying Multivariate Analysis (MVA). Since pictograms are 
universal communication tools, Human-Centred Design (HCD) 
and context analysis with a Persona model are applied. The 
experiments consist of three steps with seven phases. The first 
step is to measure the similarity of a selected word among 
seven different local sign languages using MVA. The second 
step is to guide a pictogram designer to create a new common 
pictogram, by exploiting results from the first step result. The 
final step is to validate the newly created pictogram with MVA. 
Under the cycle of HCD, pictogram designers will summarize 
the expression of several local sign languages using this method. 
The acquisition of this experience is to be included as a 
pictogram design guideline within the context of universal 
communications, such as emergency and traveling situations. 
Through the proposed method, the relationship between 
selected words and local sign languages are initially explained 
by sensory evaluation of the subjects. The outcome of 
pictograms or icons of this experiment are implemented on 
smartphones with a touch panel. The final system is evaluated 
by hearing impaired subjects and foreigners, to compare 
qualitative 
measures 
of 
effectiveness, 
efficiency, 
and 
satisfaction based on context of use.  
Keywords- Context of Use; Computer Human Interface; 
Human-Centred Design; Pictogram; Universal Communication; 
Sensory Evaluation; Smartphone. 
I.  INTRODUCTION 
Quite often a designer must face the challenge of 
developing a new machine or software without any 
guidelines. Often only conventional design processes provide 
a starting point, where the process tends to start from an 
initial perception of needs. The target specification are then 
created and measured only by the experts of the area. The 
original design resources are derived from proprietary 
technologies (Seeds). Then, subsequent experimental and 
manufacturing development stages are based on this 
predetermined specification. The prototype machine is 
developed to meet mass production standard of value 
engineering (VE) within resource constraints. This initial 
machine tends to be an origin or source of mass-production 
version neglecting market needs. Finally, the machine is 
refined and shipped to a predetermined market. This 
introduction into a market may be the first opportunity that 
the end users have to examine and determine the efficiency 
of the machine. Necessary feature requests and candid 
feedback from end uses (Needs) are only available to the 
designers after once the machine has been introduced into 
the real market. 
Because of the initial lack of a specific requirement 
balance, computer based interface designers are required to 
measure and analyze the value of users’ comfort level [1]. In 
order to improve such long an expensive cycles, the 
International Standard Organization (ISO) proposed an 
international standard (IS) 9241-210 for Human-Centred 
Design [2]. This standard is a revision of IS 13407, which 
was prepared by ISO/TC159 in June 1999 [3] and provides 
requirements and recommendations for human-centred 
design principles and activities throughout the life cycle of 
computer-based interactive systems. It is intended to be used 
by the manager of the design processes, and is concerned 
with ways in which both hardware and software components 
of 
interactive 
systems 
can 
enhance 
human–system 
interaction. The application 
of 
human 
factors and 
ergonomics 
to 
interactive 
systems 
design 
enhances 
effectiveness, efficiency, and improves human working 
conditions. The benefits can include: increased productivity, 
enhanced quality of work, reduction in support and training 
costs and improves user satisfaction. The aim of IS is to help 
those responsible for managing hardware and software 
design processes, and to identify and plan effective and 
timely 
Human-centred 
design 
(HCD) 
activities. 
It 
complements existing design approaches and methods. The 
major additional portion is that IS 9241-210 includes User 
Experience (UX) concept with emotional factors [4]. 
However, the above mentioned IS does not provide 
detailed coverage of the method and technologies for 
determining the design. Because of the lack of a specific 
requirement balance, computer based interface designers are 
required to measure and analyze the value of users’ comfort 
levels. Because of generally scarce resources, designers must 
trade-off various elements of basic requirements by 
themselves. In the initial stages of development, designers 
will have little or no direct experience in developing a 
specific outcome to meet the exact requirement of icons or 
pictograms. 

234
International Journal on Advances in Intelligent Systems, vol 8 no 3 & 4, year 2015, http://www.iariajournals.org/intelligent_systems/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Sense, feeling, impression, and emotion are words of 
subjective notions, and deeply relate to happiness, anger, 
sorrow and enjoyment that are fundamental factors of UX. 
Sensory analysis [5] provides the basis for an examination of 
organoleptic attributes of a product by the sensory organs. 
This paper explores a sensory analysis method and discusses 
a method to create pictograms or icons to be used not only by 
hearing impaired, but also hearing people required to 
interpret a variety of local sign languages, with the concept 
of context of use on dialogue and Multivariate Analysis 
(MVA) [6].  
The structure of this paper is as follows. Section II 
described the research motivation, which arises with hearing 
impaired people issues in the face of emergencies. Simple 
and easy to use pictograms or icons are assumed to provide a 
basis for an efficient language-independent communication 
tool. In Sections III, IV, and V, the design of pictograms or 
icons is discussed, by applying sensory evaluation methods. 
The validity of the newly created pictograms or icons is 
discussed in Section VI, where the outcome of the results is 
implemented on a smartphone with touch panel. Then the 
efficiency of the system is evaluated by hearing impaired and 
foreign subjects. Section VII summarizes conclusions. 
II. RESEARCH PURPOSE AND ISSUES 
The purpose of this research is to develop a method to 
create meaningful pictograms or icons [7] referring to several 
local sign languages [8]. Sign language (SL) is basically a 
communication method from one person to another for 
hearing impaired persons. A significant difference between 
sign language and vocal language is that characters are 
represented by a hand shape.  Since there is no uniform SL in 
reality, and each SL expression is not exactly the same, and 
can vary from country to country because of their native 
cultures. Whereas sensory related expressions of SL such as 
happiness, angry, sorrow and enjoyment are tend to be 
universal regardless cultures. Those sensory related words 
are closely relating to UX. 
The main factors of almost all sign languages consist of 
the hand shape, location, movement and additional face 
expressions. However, there is a dilemma that SL is a 
language with motion whereas pictograms or icons are static. 
The development and learning of current sign languages and 
their expression has drawn a lot of attention from both 
signers and sign language designers. Then hand shapes and 
locations are drawn by an animation and movements are 
done by arrows referring to a snapshot of the related local 
sign languages [9-20]. With this background, this paper 
discusses a method to create icons and pictograms by 
choosing and selecting the common features among several 
sign languages for a designer who has little experience. In 
this study, the referring seven sign languages are;  
A: American Sign Language (ASL) 
B: British Sign Language (BSL) 
C: Chinese Sign Language (CSL) 
E: Spanish Sign Language (ESL) 
F: French Sign Language (FSL) 
J: Japanese Sign Language (JSL) 
K: Korean Sign Language (KSL) 
III. RESEARCH PROCEDURES 
This research scope covers not only linguistic studies of 
sign language but also HCD and context of use, since 
pictograms or icons are universal communication tools. The 
research was started in order to investigate the context of 
universal communication through local sign languages. HCD 
[2, 3] is based on the context of use [21], which is organized 
by four factors; “user,” “product,” “task,” and “environment” 
in use for goal (Figure 1). HCD and context analysis using 
the Persona model by Alan Cooper [22] are applied in this 
research.  
With this framework, the following three steps with 
seven phase research procedures are developed; 
Step1:  
Phase 1:  Concept Generation 
Phase 2:  Persona Model and Scenario Creation 
Phase 3:  Key words extraction on the situations 
Phase 4:  Initial Sensory Evaluation with Seven Local 
Sign Languages 
Step 2: 
Phase 5:  Summarized Pictogram Design. 
Step 3:  
Phase 6:  Second Sensory Evaluation with Seven Local 
Sign Languages with a Summarized Pictogram to prove the 
outcome 
Phase 7:  Conclude the Method  
 
Phase 1: Context Generation  
Based on the framework described above, two context 
situations have been selected: 1) emergencies and 2) 
travelling.  
Alan Cooper proposed the Persona Model related to 
HCD, where several situations representing Personas are 
imagined to be in certain contexts in order to simulate and 
find how they will behave. This method is highly accepted 
by manufacturers in the creation of new product plans and 
has also been applied to service science. 
 
 
 
 
 
Figure 1. Context of use of ISO 9241-11 
 
 

235
International Journal on Advances in Intelligent Systems, vol 8 no 3 & 4, year 2015, http://www.iariajournals.org/intelligent_systems/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Phase 2: Persona Model and Scenario Creation  
The first step is to create Personas by applying the 
Persona Model under HCD. A created Persona is a hearing 
impaired person in a situation where he suffers from a 
sudden illness while commuting in the morning, and is 
carried to the hospital by an ambulance (Figure 2).  
Diary like scenarios underlying Personas are described 
from discussions with colleagues utilizing the Brain 
Storming Method. These scenarios mainly pay attention to 
the dialogues between the target Persona and people 
surrounding them. The main goal of the target Persona is to 
describe his emergency situation in order to solicit help. The 
scenario of the hearing impaired person in an emergency 
consists of about 600 words (equivalent to 3000 Japanese 
characters). 
 
 
 
 
 
 
 
 
Profile
• Communication methods are Sign Langue, Lip Reading. Write Message is 
used  when complicated.
–
Write message is particularly used  in a convenience store, a gas 
station or a coffee shop.
–
Write on the palm without tools of writing.
• Parents will support when emergency.
–
By using mobile mail to the parents with explaining the situation, 
then ask parents to contact to colleagues or insurance company.
–
Telephone can not be used to call an ambulance or police station.
• Write message or gesture are used when consulting with medical doctors.
–
Ask repeatedly by write message when the explanations by the 
doctor or nurse are complicated.
–
Lip reading is not useful since they wear face guard mask.
–
Normally at the first visit to hospital , a sign interpreter will be 
accompanied.
• Always prepare tools of writing for the accidents or disaster.
• Mobile mail is daily used.
Goal
• Wish to inform current status or wish precisely.
• Wish to understand of the antagonist dialogue.
• Wish to use a simple and easy communication method when emergency.
Personal Profile
 Occupation：SE
 Address：Tokyo, Japan
 Age：24
 Characters：Cheerful, 
precise and friendly
 Symptom： Adult Deafened
Mobile usage situation
 Mobile Phone
‒Daily used
‒Mobile mail and Internet
‒Internet sites are 
Routing or weather 
forecasting 
 Other used functions
‒Digital Camera
Takuya  Kobayashi
“Try to communicate with all methods”
 Adult Deafened
 Friendly Character
 Identity for deaf
SCOPE
Persona A
 
 
 
Figure 2. An Example of Persona Model 
 
 
 
 
 
 
 
 
 
 
 

236
International Journal on Advances in Intelligent Systems, vol 8 no 3 & 4, year 2015, http://www.iariajournals.org/intelligent_systems/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Phase 3: Key word extraction on the situations  
The former phase is focused upon dialogues with several 
subjects and refers to observations from the view point of the 
provider and the receiver under the dialogue principle [23]. 
In this phase, the goal is to extract words that are 
fundamentally essential to establish the dialogues of the 
scenarios. In the context of a Persona in such an emergency 
situation, the most commonly used words are determined. In 
this case, 37 words were selected and categorized by the 
Brain Storming Method. Selected words are essential to the 
dialogues of the scenarios. The categorized 37 words were as 
follows;  
・Ten greeting words that trigger the initial dialogues: thank 
you, hello, goodbye, indeed, yes, no, I am deaf, do not 
understand, sorry, please do. 
・The next step of the dialogues start commonly with seven 
interrogative pronoun words: where ?, how much ?, how 
many ?, when ?, what ?, which one ?, who ?. 
・Twelve associated reply words for interrogative pronouns 
are: toilet, country, numeric (0, 1, 2, 3, 5, and 10), 
yesterday, today, tomorrow, name. 
・Three essential adjectives are: painful, different, expensive. 
・Five essential verbs are: want to, go, come, buy, reserve.  
 
A word “Painful” is a typical example among the 
selected seven words; Thank you, Goodbye, When? Where? 
Painful, Expensive, and Toilet through the Brain Storming 
Method by co-authors, representatives of the fire brigade and 
hearing impaired architects. This paper explains the 
development of our method by a selected word: “Painful” as 
a representative, since it closely relates to the usage context 
and UX concept and coincides to the survey results of Tokyo 
Fire Department and Keio University hospital (introduced in 
the next section). 
 
 
 
 
 
Figure 3. Sign Language Figures of “Painful” 
 
 
Abbreviations: 
  
A: American Sign Language (ASL) 
J: Japanese Sign Language (JSL) 
F: French Sign Language (FSL) 
C: Chinese Sign Language (CSL) 
B: British Sign Language (BSL) 
K: Korean Sign Language (KSL) 
E: Spanish Sign Language (ESL) 
S: Summarized and newly designed 
pictogram 
S
A
J
F
C
B
K
E

237
International Journal on Advances in Intelligent Systems, vol 8 no 3 & 4, year 2015, http://www.iariajournals.org/intelligent_systems/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
Phase 4: Initial Sensory Evaluation with Seven Local Sign 
Languages  
The overall research is initially focused on the creation of 
pictograms or icons to support dialogues, since the 
fundamentals of sign language are hand shape, location and 
motion. References are made to a collection of animation 
figures, extracted from seven local sign languages used by 
deaf architect. This architect provided enthusiastic support to 
the research by supplying and permitting reference to the 
database. The seven local sign languages are of American, 
British, Chinese, French, Korean, Japanese, and Spanish [8].  
In the experiments, the subjects are first shown an 
expression with the collection of animation figures extracted 
from seven local sign languages (Figure 3). Subsequently, 
the subjects are informed of the meaning of the sign, and 
then they are requested to vote with 19 tokens to express 
which of the seven different local sign language expressions 
(samples) best coincides with the original image. They are 
asked to use all 19 tokens, but that they are permitted 
eventually zero voting on some samples (Figures 4 and 5). 
This sensory evaluation method can easily make relative 
comparisons between the seven expressions of local sign 
languages and is more effective than the Ordering Method or 
Pair Comparison Method. Correspondence Analysis (CA) of 
MVA by IBM SPSS Statistics Ver.18 [24] is then applied. 
 The outcome is plotted on a plane such as similar local 
sign languages are plotted close together. The outcome of 
CA of MVA is fundamentally no name of Eigenvalue axes. 
Because of the characteristics of CA, the subjects who have 
general and standard ideas are positioned in the centre, 
whereas those who have extreme or specialized ideas are 
positioned away from the centre of position (0.0). The center 
crossing point of the first and second Eigenvalues is also 
called “centre of the gravity” or “average”. Then CA 
establishes a way to graphically examine the relationship 
between local sign languages and subjects [25]. 
Figure 6 is an example of an outcome chart where 
“painful” is plotted. Focusing on Subject No.3 and looking at 
the voting sheet, the image of “painful” by the subject will be 
appropriate to Spanish Sign Language (ESL) and American 
Sign Language (ASL) since the subject put more tokens on 
ESL and ASL than other sign languages. In the plot chart of 
CA, the subject is observed to have extreme or specialized 
ideas from other subjects, since the subject plot position is 
remote from the centre in Figure 7. In practice, the subject is 
then positioned closed to Spanish Sign Language (ESL) and 
American Sign Language (ASL) whereas far away from 
French Sign Language (FSL), British Sign Language (BSL), 
Japanese Sign Language (JSL), Korean Sign Language 
(KSL), or Chinese Sign language (CSL).  
On the other hand, in Figure 8, Subject No.8 is positioned 
centre, since the subject voted equally on all the sign 
languages of ESL, ASL, FSL, BSL, JSL, KSL and CSL (cf. 
Subject No.3). Hence, the above two plots provide examples 
of CA with the relationships between subjects and samples 
(sign languages), and explains the relations both between 
subjects to other subjects and between sign languages and 
other sign languages. Figure 9 shows the position of Subject.  
 
Figure 4. Inquiry Sheet Example of “Painful” 
 
 
 
Figure 5. A View of Experiment by the Subject 
 
 
 
Figure 6. A Plot of “Painful” with 7 Sign Languages 
 

238
International Journal on Advances in Intelligent Systems, vol 8 no 3 & 4, year 2015, http://www.iariajournals.org/intelligent_systems/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
No.3 is an outlier with respect to all other subjects and thus it 
is concluded that the subject idea is extreme or has 
specialized ideas compared with the others. 
The first experiment subjects are 13 people in their 
twenties, including nine science course students, and four 
humanity course students. Some have experience living 
overseas and sign language interpreting. The reason to 
exclude hearing impaired people in subjects is that they are 
biased their own sign language. After voting with the tokens, 
all the subjects are asked about their confidence level using 
the Semantic Differential (SD) method [5]. 
 
Phase 5: Summarized Pictogram Design  
The analysis of an outcome CA plot chart of “painful” is 
shown in Figure 6; with seven sign languages (SL), voted 
subjects create three clusters of “FSL and BSL”, “ASL and 
ESL”, “JSL, KSL and CSL” sign languages. 
Following the cycle process of HCD [2, 3], an original 
designer is asked to summarize and design an original 
pictogram for JSL, KSL and CSL by reflecting of the 
outcome by the sensory evaluation mentioned above. Then 
the newly designed pictogram, which is “S” in Figure 10, is 
added to the previous seven local sign languages. 
 
 
 
Phase 6:  Second Sensory Evaluation with Seven Local Sign 
Languages with a Summarized Pictogram to prove the 
outcome 
The first experiment is done with seven sign languages. 
The subject is permitted up to seven tokens for each sign 
language, for a total of 49 possible votes. Each subject is 
given 19 tokens, which is about 40% of total 49 voting 
locations. The second experiment is done with seven sign 
languages, plus one newly created sign language. Then the 
total voting locations becomes 56. Each subject is given 23 
tokens, which are again about 40% of total 56 voting 
location.  
After subjects are informed of the intended sign meaning, 
they are requested to vote with 23 tokens, which of the eight 
different local sign language expressions including the newly 
designed language, which has a pictogram “S” as show in 
Figure 10. This process was the same as the first sensory 
evaluation step of phase 4, and the Correspondence Analysis 
of Multivariate Analysis (MVA) by SPSS is once again 
performed [24].  
 
 
 
 
 
 
Figure 7. Subject 3 is plotted near to ESL and ASL 
 

239
International Journal on Advances in Intelligent Systems, vol 8 no 3 & 4, year 2015, http://www.iariajournals.org/intelligent_systems/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
 
 
Figure 8. Subject 8 is plotted further from all SL’s 
 
 
 
 
Figure 9. Subject 3 is plotted further from all other Subjects 

240
International Journal on Advances in Intelligent Systems, vol 8 no 3 & 4, year 2015, http://www.iariajournals.org/intelligent_systems/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
 
 
Figure 10. A Supplementary Treatment Plot of “Painful” with 7+1 Sign 
Languages, where S is a summarized one. 
 
 
The outcome, including the newly designed pictogram, is 
plotted with other seven local sign languages in order to 
measure 
whether 
the 
newly 
created 
pictogram 
is 
representative of the dominant cluster. The second 
experiment subjects are 20 engineering department students 
in their twenties including two female students. Almost all 
except three are different subjects from the first experience. 
After voting by the tokens, all the subjects are again asked of 
their confidence level using the Semantic Differential (SD) 
method [5]. Figure 10 is an example of outcome chart where 
“painful” is plotted. The newly designed symbol with a 
coloured green flag will represent JSL, KSL and CSL sign 
languages since it is plotted closer to those three sign 
languages. Whereas FSL, BSL, ASL and ESL are plotted 
further down.  
In order to demonstrate the outcome more precisely, 
supplementary treatment of MVA by SPSS is also applied by 
adding a newly designed supplementary category to the 
seven sign languages (active categories). Supplementary 
categories do not influence the analysis but are represented 
in the space defined by the active categories. Supplementary 
categories play no role in defining the dimensions. These 
versions of the plots are similar in seven sign languages and 
those created from summarized pictogram experiments. 
 
Phase 7:  Concluding the Method 
Phase 4 explains how to determine similarity amongst 
seven sign languages. Then the newly created icon or 
pictogram provides a basis to measure the similarity. Phase 7 
explains how that outcome is confirmed, since it plotted 
closed to the referred sign languages. 
 
 
Table I.  Complains Survey of Tokyo Fire Department and Keio Hospital 
 
 
 
 
Comparing the two outcomes of Phase 4 (Figure 6) with 
seven local sign languages and of Phase 5 (Figure 10) with 
seven local ones plus one, one can conclude the following, 
- Seven summarized pictograms are created for seven 
aggregated words respectively such as “painful.” 
- Then newly designed pictograms by a designer are all 
positioned in the centre of the related local sign 
languages cluster. 
- Even though most subjects are different in the first and 
second experiments, the outcome plot patterns by CA 
hold similar patterns in space. 
- the oriental sign languages of JSL, KSL and CSL are 
plotted closely together. 
- The outcome is confirmed by Supplementary Treatment 
of MVA by SPSS. 
 
IV. CONCEPT AND SYSTEM DESIGN OF THE 
SMARTPHONE WITH TOUCH PANEL 
The data of patient issues collected by Tokyo Fire 
Department (TFD, 290,471 patients) and Keio University 
hospital (2,421 patients) were analyzed (Table I). The 
collected data were not only hearing impaired, but all kind 
patients. From both datasets, more than 30% of complaints 
were determined to be about pain problems. Ten selected 
patient 
complaint 
items 
were 
pain/ache/grief, 
unconsciousness, difficulty breathing, fever, faintness, 
convulsions, vomiting, difficulty standing and walking, 
cardiopulmonary problems and external injury. 
The “Context of Use” referring to ISI9241-11 [21] is 
composed of four factors: user, product, task and 
environment for the goal.  At the time when a designed is to 
create icons or pictograms, it is necessary to bear in mind 
how such symbols are to be used under the context; 
otherwise the outcomes will be hard to be recognized by the 
users. They must be useful, effective and particularly 

241
International Journal on Advances in Intelligent Systems, vol 8 no 3 & 4, year 2015, http://www.iariajournals.org/intelligent_systems/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
efficient for both the hearing impaired and language 
dysfunction people and foreigners. Here the research 
intended to particularly focus on the communication method 
[23] of complaint of pain/ache/grief by hearing impaired 
patients.    
The pain or ache sources were assumed to be positioned 
in either the head, face, chest, back, belly, waist, arm/hand 
and leg/foot. In addition, three ache depths come from 
surface skin, visceral and bone. The hearing impaired and 
language dysfunction people complaint of pain/ache/grief 
and external injury was to be drawn by pictograms and icons 
that were easy to understand, even in emergency situations 
with help of minimum selected key words with MVA. 
Aching places were drawn in two dimensions. Ache depth 
and severe pain were in the third dimension. The hearing 
impaired and people will simply touch the designated icon or 
pictogram to communicate to support people in such 
emergency situation by ubiquitously carrying a smartphone 
with touch panel.  
The modern smartphones with touch panel are primarily 
equipped the following functions 1) tap to select, 2) double 
tap to do scaling, 3) drag to jump, 4) flick or swipe to move 
next page, 5) pinch in/out with double fingers, 6) accelerate 
sensor to position upright, 7) photo browsing to display icons 
or pictograms, 8) backlight for dark place usage, 9) GPS/Wi-
Fi positioning, and 10) wireless function to download the 
new contents and applications.  
The framework is implemented on the smartphones of 
with touch panel applying functions above such as iOS and 
Android devices to enable hearing impaired or language 
dysfunction people to communicate the remote supporter 
place of the nearest fire brigade in such urgent situations 
through the ICT clouds. The smartphone with touch panel 
will produce a text message for e-mail through simply 
tapping on icons or pictograms. The mail text is to be 
instantly sent to the remote supporters. 
Currently, 31 screen contents on the smartphone are 
implemented by the Software Development Kit (SDK) of 
MIT APP Inventor [26] and distributed onto Android touch 
panel terminal by DeployGete [27] for the evaluation (Figure 
11).  JAVA and Eclipse are used for the detailed 
programming part. It is also implemented on iOS using 
Objective-C with Xcode. The screen transition process is 
based on the telephone dialogues of the command console of 
the Kasuga Onojo Nakagawa Fire Department in Fukuoka 
Prefecture. The overall process is analyzed and drawn by 
Freeplane by Mind Mapping [28]. The touch panel includes 
the cognitive design method on Automated Teller Machine 
(ATM) for elderly people since under such an urgent 
situation, people would be upset and find it hard to 
communicate just like a cognitive decline [29, 30]. The 
repertoire of possible actions is as follows: 
(1) Simple selection with limited choices 
(2) Explicit sliding at the time of screen change 
(3) Step by step procedures after selection 
 
 
 
 
 
 
 
Figure 11. Pictograms on the Smartphone with Touch Panel representing 
Degree of Pains 
 
 
 
 
Figure 12. The Evaluation Experiment Scene by the Hearing Subjects 
 
V. EXPERIMENTAL EVALUATION 
The proposed smartphone with touch panel was 
evaluated by hearing impaired people in a manner similar to 
the usability test under the working hypothesis related to 
desire of communication outcome. The communication 
method for hearing impaired people or language dysfunction 
depended only on the sign language. Nowadays, since 
hearing impaired people are starting to use smartphones with 
text messaging. This improves their communication 
behaviour and quality of life (QOL) [31].  
The five tasks were prepared to compare performance 
with and without a service to call ambulance and fire 
brigades. This evaluation focused on efficiency by 
comparing two task groups; those that apply tapping icons or 
pictograms on the smartphone, and those that send text 
message by e-mail by means of personal computer (PC) key 
board. The evaluation test was performed by four hearing 

242
International Journal on Advances in Intelligent Systems, vol 8 no 3 & 4, year 2015, http://www.iariajournals.org/intelligent_systems/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
impaired subjects as described in the following five tasks. 
All experiment instructions are introduced to hearing 
impaired subjects by a sign interpreter (Figure 12). Subjects 
are permitted to use memo notes.  
•Task-1: Fire report by tapping icons on the smartphone. 
This scenario was that “The forest is on fire. I recognize a 
flame but no smoke. I am safe since I am away from the 
fire spot. There was no injury. Please help”. 
•Task-2: Fire report by e-mail text message 
The scenario was “This building is on fire. My floor is 
different from the fire spot. I cannot recognize flame but 
see smoke. There were some injuries. Please help”. 
•Task-3: Ambulance request by e-mail text message 
The scenario was “Please call ambulance since I was run 
over by a car. I am middle aged male. I am conscious but 
my leg is broken and bleeding. It is quite painful. Please 
send an ambulance soon”. 
• Task-4: Ambulance request by tapping icons on the 
smartphone 
The scenario was “My daughter is severely sick. She is a 
pregnant adult. She may have a preterm birth. She is 
conscious but indicates severe pain in the belly. She was 
once suffering from gallstones.  Please send an ambulance 
soon”. 
•Task-5: Interviews after four tasks 
An exercise to fill out a personal data sheet such as name, 
address, age and issues of input methods by a soft key 
board or hand writing. 
 
VI. EVALUATION RESULTS 
The results must be analyzed under “the Context of Use” 
[21] whose result was measured by the effectiveness, 
efficiency, and satisfaction. This evaluation focused 
particularly on the efficiency with comparing two task 
groups, and found applying tapping icons or pictograms on 
the smartphone was about three times quicker than text 
message by e-mail (Table II).  
It is natural that simple tapping icons input must be 
quicker than text input, however, the purpose of the 
evaluation is to measure how much quicker on the speed 
ratio utilizing smartphone comparing to text e-mail. In 
practice, it must be much quicker than three times since to 
input texts by telephone dials of ten keys is more 
complicated comparing to PC keyboard. For instance, it is 
necessary to push the “2” button three times to input single 
“c” character in 3G type mobile phones. The smartphones 
are equipped the soft keyboard on the touch panel. However, 
the character array is the same as PC keyboard, key widths 
are narrow and sometimes without any feedback. The current 
technology is not really effective yet. 
During interviews after the evaluation, many hearing 
impaired people pointed out that this smartphone service 
would ease their predicted mental anguish during any 
emergency. This concept is closely aligned to the 
Satisfaction in the Context of Use or User Experience (UX) 
[4]. 
 
 
Table II.  The Evaluation Result on Efficiency 
 
 
 
VII. CONCLUSION 
This paper consists of two parts. The first part of this 
paper discusses a method to extract the summarized 
expression of several local sign languages in order to draw 
pictograms or icons by applying the sensory evaluation with 
MVA. The experiments consist of three steps. The first step 
is to select a pictogram as a majority common expression 
upon a word among seven local sign languages. Considering 
this first step, this method seems valid in practice since 
oriental sign languages, Japanese, Chinese and Korean are 
similar by historical background, and in fact and they emerge 
as similar to each other. The second step is to confirm that 
the experimental characteristics of the pictogram represent 
the meaning of the word. The final step is to validate the 
newly created pictogram by MVA. Since almost all of the 
newly designed pictograms were positioned close to the 
cluster it can concluded that they were representative of the 
clusters.  
In the example of the concept of “painful” taken from 
seven sign languages, and analysis with the supplementary 
treatment of MVA, the newly designed pictogram was close 
to those oriental sign languages since it was plotted close to 
those sign languages on the plane. The last part of this paper 
discussed how the use of the outcome of pictograms or icons 
of this experiment was implemented on a modern 
smartphone with a touch panel display for computer-human 
interaction. Through the proposed method, the relationship 
between selected words and local sign languages were 
initially explained by sensory evaluation by the subjects. 
Under the cycle of HCD, the pictogram designer will 
perform to summarize the expression of several local sign 
languages by this method. We showed how the acquisition of 
user experience can provide design guidance, for instance, in 
the context of emergency and traveling situations.   
In order to show stability and repeatability, the 
experiments were done twice. The first one was to confirm 
the similarity among seven sign languages for any selected 
word. The second experiment was done with different 
subjects, in order to demonstrate independence of outcome. 
This is confirmed in the second experimental results. By 
applying the method, the selected seven words; Thank you, 
Goodbye, When? Where? Painful, Expensive, and Toilet 
were analyzed. All the summarized outcomes were 
positioned close to the referred ones on the plot. The results 
of the second experiment confirmed the outcome design by 

243
International Journal on Advances in Intelligent Systems, vol 8 no 3 & 4, year 2015, http://www.iariajournals.org/intelligent_systems/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
supplementary treatment of the CA procedure. The proposed 
method was quite an original one and thus provided one of 
the guidelines to create pictograms by referring to several 
sign languages. The relevant methods can be considered as a 
ranking method and a pair comparison method. A ranking 
method is relatively simple, however, when increasing the 
number of items to more than ten, it becomes hard to do 
ranking in middle: e.g., the seventh or eighth is hard to 
distinguish. A pair comparison method requires a large 
number of comparisons, e.g., 45 comparisons for ten items 
[5].  
The second part of this paper introduces an experiment to 
implement the outcome of icons or pictograms on the 
smartphones with touch panel. At the beginning they were 
printed in the form of a booklet to be used the 
communications between hearing impaired and hearing 
people as well as between foreigners and Japanese. The 
suffixes were created and translated into English, Spanish, 
Korean, Chinese and Portuguese for the foreign people use 
referring to nationality population composition in Japan. The 
results by the foreign people with the booklet were that the 
time to collaborate was 20% shorter and the messages of the 
dialogue were 20% more precise than simple gesture 
communication. In practice, the booklet was published 6,000 
copies and currently mounted on several ambulances in the 
local fire departments to aid to collaborate between the 
hearing impaired patient and emergency response personnel. 
The current prototype terminal was implemented on the 
smartphone with touch panel. The system then connects over 
a network to remote supporters of the nearest fire brigade. It 
was implemented on both iOS by Apple and Android mobile 
terminals with GPS positioning technology.  
The outcome of this research will be proposed to the 
Japanese government to make easily available, and to be 
used such a crisis for urgent communication or to be used 
during the next Olympics, to be held in the year of 2020 in 
Tokyo. 
 
ACKNOWLEDGMENT 
This research is supported and funded by the Project 
organized by Fire and Disaster Management Agency under 
Ministry of Internal Affairs and Communications (MIC) of 
Japan. The original idea of the booklet was proposed by a 
hearing impaired architect, Mr. M. Suzuki.  A collection of 
local sign language database is supplied and permitted for 
research use by Mr. M. Akatsuka of Architectural 
Association of Japanese DEAF (AAJD). Dr. H. Akatsu of 
Oki Electric Ind. Co., Ltd. introduced the cognitive design 
method on ATM for elderly people. Mr. T. Inaba and Mr. F. 
Miyajima of Kasuga Onojo Nakagawa Fire Department in 
Fukuoka Prefecture and Mr. M. Nishijima of Oki Consulting 
Solutions Co., Ltd. for the implementation on the 
smartphone in this research. Special thanks to Prof. Dr. R. 
Goebel, Alberta University, Canada who supported to 
perform the native check of the article. 
 
 
 
REFERENCES 
[1] N. Hosono, H. Inoue, Y. Nagashima, and Y. Tomita, 
“Sensory Evaluation Method to create Pictograms based on 
Multiplex Sign Languages,” in Proc. ACHI, 2013, pp. 13-16, 
Nice, France. 
[2] International Organization for Standardization: ISO9241-210: 
2010, “Ergonomic requirements for Office Work with Visual 
Display Terminals (VDTs).” Available from: 
http://www.iso.org/iso/home/store/catalogue_tc/catalogue_det
ail.htm?csnumber=52075, 2015.11.8. 
[3] International Organization for Standardization: ISO9241-210 
(former 
ISO13407:1999), 
“Ergonomics 
Human-centred 
design processes for interactive systems,” 2010. 
[4] R. Hartson and P.S. Pyla, “The UX Book: Process and 
Guidelines for Ensuring a Quality User Experience,” Morgan 
Kaufmann Publishers, 2012. 
[5] H. Inoue, “Sensory Evaluation,” Juse-P, ISBN978-4-817-
9435-0,  Japanese version, 2013. 
[6] A. Field, “Discovering Statistics Using SPSS 3rd edition,” 
Sage Publications, 2009. 
[7] W. Horton, “The Icon Book,” John Wiley & Sons, Inc., 1994. 
[8] M. Akatsuka, “Seven Sign Languages for Tourists: Useful 
Words 
and 
Expressions,” 
Chinese-Japanese-American 
Working Group, 2005. 
[9] N. Hosono, M. Suzuki, S. Hori, and Y. Tomita, “SOS Placard 
for the Hearing Disabled,” in Proc. IEA2006, Maastrecht, 
Netherlands. 
[10] N. Hosono, M. Suzuki, S. Hori, and Y. Tomita, “SOS Placard 
in Universal Use,” The 2nd International Conference for 
Universal Design, IAUD, Kyoto, Japan, 2006.  
[11] N. Hosono, M. Suzuki, R. Kimura, H. Miki, and Y. Tomita,  
“Emergency Communication Service on Mobile Phone for 
Universal Ubiquitous Use,” in Proc. AHFE2008, Las Vegas, 
USA. 
[12] N. Hosono, H. Miki, M. Suzuki, and Y. Tomita, “Urgent 
Collaboration Service for Inclusive Use,” in Proc. HCII2009, 
San Diego, USA. 
[13] N. Hosono, H. Inoue, H. Miki, M. Suzuki, and Y. Tomita, 
“Universal Communication through Touch Panel,” in Proc. 
AHFE2010, Miami, USA. 
[14] N. Hosono, H. Inoue, H. Miki, M. Suzuki, Y. Nagashima,  
and Y. Tomita, “Universal Communication Service for 
Inclusive Use,” in Proc. SICE2010, Taipei, Taiwan. 
[15] N. Hosono, H. Miki, M. Suzuki, and Y. Tomita, “Universal 
Communication System for Urgent Use,” in Proc. IAUD 2010, 
Hamamatsu, Japan. 
[16] M. Hirayama, M. Suzuki, and N. Hosono, “An Emergency 
Communication Pad for Hearing Impaired Persons,” ICCIT 
2010, Soeul, Korea. 
[17] N. Hosono, H. Inoue, and Y. Nagashima, “Context Analysis 
of Universal Communication through Local Sign Languages 
applying Multivariate Analysis,” ICCHP2010, Vienna, 
Austria.  
[18] N. Hosono, H. Inoue, H. Miki, M. Suzuki, Y. Nagashima,  Y. 
Tomita, and S. Yamamoto, “Service Science Method to create 
Pictograms referring to Sign Languages,” in Proc. HCII2011, 
Orland, USA. 
[19] N. Hosono, H. Inoue, H. Miki, M. Suzuki, Y. Nagashima, Y. 
Tomita, and S. Yamamoto, “Service Service Method to create 
Pictograms referring to Sign Languages,” in Proc. HCII2011. 
[20] N. Hosono, F. Miyajima, T. Inaba, M. Nishijima, M. Suzuki,  
H. Miki, and Y. Tomita, “The Urgent Communication System 
for Deaf and Language Dysfunction People,” in Proc.  
HCII2013, Las Vegas, USA. 

244
International Journal on Advances in Intelligent Systems, vol 8 no 3 & 4, year 2015, http://www.iariajournals.org/intelligent_systems/
2015, © Copyright by authors, Published under agreement with IARIA - www.iaria.org
[21] International Organization for Standardization, ISO9241-11, 
“Ergonomic requirements for Office Work with Visual 
Display Terminals (VDTs), Guidance on usability,” 1998. 
[22] A. Cooper, “About Face 3,” Wiley, 2007. 
[23] International Organization for Standardization, ISO9241-110, 
“Ergonomic requirements for Office Work with Visual 
Display Terminals (VDTs), Dialogue principles,” 2006. 
[24] SPSS, “Categories in Statistical Package for Social Science 
ver.18,” 2009. 
[25] N. Hosono, H. Inoue, and Y. Tomita, “Sensory Analysis 
Method applied to develop Initial Machine Specification,”  
Measurement, vol. 32, pp. 7-13, 2002. 
[26] MIT, “APP Inventor,”  Available from: 
http://appinventor.mit.edu/, 2015.11.8. 
[27] Deploygate, Available from:  https://deploygate.com/, 
2015.11.8. 
[28] Freeplane, Available from: 
http://freeplane.sourceforge.net/wiki/index.php/Main_Page, 
2015.11.8. 
[29] H. Akatsu, A. Komatsubara, “Auditory and Visual Guidance 
for Reducing Cognitive Load,” in Proc. HCII 2007, Beijin, 
China. 
[30] H. Akatsu, H. Miki, and N. Hosono, “Designing Adaptive 
ATM based on universal design,” in Proc. The 2nd 
International Conference for Universal Design, IAUD, Kyoto, 
Japan, 2006, pp.793-800, Kyoto, Japan. 
[31] H. Miki, and N. Hosono, “Universal Design with Information 
Technology,” 
Japanese 
version, 
Maruzen, 
2005.  
 
 
 

