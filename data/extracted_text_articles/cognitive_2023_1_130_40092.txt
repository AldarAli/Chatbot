Local-Global Reaction Map: Classiﬁcation of Listeners by Pupil Response
Characteristics when Listening to Sentences Including Emotion Induction Words
– Toward Adaptive Design of Auditory Information –
Katsuko T. Nakahira
Nagaoka University of Technology
Nagaoka, Niigata, Japan
Email: katsuko@vos.nagaokaut.ac.jp
Munenori Harada
Nagaoka University of Technology
Nagaoka, Niigata, Japan
Email: s193369@stn.nagaokaut.ac.jp
Muneo Kitajima
Nagaoka University of Technology
Nagaoka, Niigata, Japan
Email: mkitajima@kjs.nagaokaut.ac.jp
Abstract— When a person acquires a text as auditory infor-
mation and derives the meaning of the text, he or she may
simultaneously generate an emotion in response to the content
of the text. Emotions are said to have a certain relationship with
decision-making and memory. Therefore, it is expected that even
sentences with the same meaning will be remembered differently
depending on the emotion evoked. This study aims to clarify the
relationship between the emotions that arise when listening to
a text and the memory of the presented text. The classiﬁcation
of emotional states held by people is performed by a method
based on subjective quantities by impression rating or by a
method based on objective quantities by biometric information.
In this study, we focus on pupil response, which is biological
information that has been suggested to change with emotion.
Based on this, this paper proposes the Local-Global Reaction
Map (LGR-Map) as a classiﬁcation method for pupil changes
accompanying emotional changes, as a basic research for the
construction of adaptive content design methods that utilize the
degree of human emotional arousal. The LGR-Map is generated
by capturing the emotional changes during listening to a text
from the following two perspectives; Those generated by words
in a speciﬁc region of a sentence (Local reaction); those generated
by the context of the entire sentence (Global reaction). The total
pupil diameter change within a certain time period is obtained
as the characteristic quantity for each response. Error ellipses
are deﬁned for the distribution of listeners in the LR-GR for the
presented text (LGR-Map), and classiﬁed into ﬁve types based
on the rotation angle and ﬂattening ratio of the error ellipses.
The basic properties of the LGR-Map were investigated by using
auditory stimuli presented in short sentences containing Affective
Norm for English Words (ANEW).
Keywords— Local-Global Reaction Map; Pupil Response; Affec-
tive Norm for English Words; Emotion Induction; Contents Design
of Auditory Information.
I. INTRODUCTION
With the penetration of mobile devices and the development
of eXtended Reality (XR)technology, we are surrounded by
an increasing number of services that disseminate content via
electronic media. Many of these services are designed to enrich
the experience of individuals, and their range of application
is wide, from sensory experiences, such as sightseeing and
movies to educational materials that make it easier for people
to acquire knowledge. In recent years, there has been a move-
ment to expand content provision services from an inclusivity
perspective (e.g., [1]).
Content design is essential to content provision in the sense
of striving to convey what is to be conveyed as accurately
as possible. Content design has the issue of the quality and
quantity of the presenting stimulus as the material contained
in the content. Visual and auditory information are the central
presenting stimuli, and how to handle their quality and quantity
is one of the key factors.
Regarding the amount of content, since perceptual informa-
tion is basically a physical quantity, the amount of processing
is determined by the structure of the human cognitive system
itself, and individual differences are usually negligible. This
is described by Hirabayashi et al. [2] as the relationship
between the amount of information and the timing at which
the information is given, and it is possible to maximize human
memory by giving visual and auditory information, or explicit
and implicit information in the appropriate order and intervals.
Furthermore, the quality of content is largely related to the
viewer’s cognitive process. The cognitive process depends on
the richness of information nodes and the state of node connec-
tivity of the information receiver, and thus varies from person
to person. Murakami et al. [3][4] discussed the quality of con-
tent for short auditory information. We classiﬁed the emotions
of short sentences into positive, negative, and other categories
(in this case, we assign neutral), and calculated memory scores
for each category, suggesting that short sentences belonging to
a speciﬁc category improve memory scores. We also suggested
the possibility of using pupil response to measure human emo-
tion induction from short sentences. In addition, Moriya et al.
[5] found that pupil responses to Affective Norm for English
Words (ANEW) contained in short auditory information may
be characterized based on ANEW categories.
Therefore, in order to design content that facilitates better
emotional experiences and knowledge acquisition, it is desir-
able to be able to adaptively provide content according to
the viewer’s cognitive characteristics. For this purpose, it is
necessary to monitor the viewer’s emotional state in real time.
Biometric information is a suitable indicator for this purpose.
There are many types of biometric information on emotion
(e.g., Jim et al. [6], Shu et al. [7]), but considering the time
scale and ease of measurement, the pupillary response is the
most promising.
Based on the above, this paper focuses on
pupillary response and proposes the Local-Global Reaction
79
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-046-9
COGNITIVE 2023 : The Fifteenth International Conference on Advanced Cognitive Technologies and Applications

(a)
(b)
Figure 1. Cognitive model of this paper based on CI model. (a) Input - Cognitive process. (b) Working memory processing - output process.
Map (LGR-Map) as a classiﬁcation method for pupillary
changes associated with emotional changes.
This paper is organized as follows. In Section II, we
construct a base cognitive model and propose an LGR-Map
based on it. In Section III, we show the usefulness of the
LGR-Map by actually applying it to the HUCAPP 2023 data
[5]. In Section IV, we discuss the usefulness of LGR-Map.
II. DESIGNING LGR-MAP BASED ON
CONSTRUCTION-INTEGRATION MODEL
A.
Basic Design
In this paper, we construct a reaction model for human emo-
tion based on the Construction-Integration Model (CI-model,
e.g., [8][9][10]) proposed by Kintsch. The CI-model is a theory
of discourse comprehension consisting of a construction step
and an integration step.
The scenario in this paper is modeled based on the CI-model
as shown in Figure 1. Figure 1 (a) shows the construction
process that encodes information (packet of sound waves)
input from the outside world, retrieves information stored in
long-term memory using it as a clue, and constructs a network.
Figure 1 (b) shows the integration process in which the
retrieved information is pruned and integrated in the working
memory by pruning information that does not ﬁt the context,
and the physical response is output.
First, when a single stimulus (a packet of sound waves in the
auditory case) is perceived from a sensory organ, it is sent as
encoded perceptual information from the sensory organ to the
working memory. The sent information is matched with a large
number of nodes (knowledge concepts) in the brain’s long-
term memory. The corresponding knowledge concept and its
associated knowledge concept are then returned to the working
memory. In this case, the information of the chunk of emotion
(deﬁned by valence and arousal) rpi associated with the knowl-
edge concept is also returned, so that the working memory
temporarily retains the emotion of the perceived packet of
sound waves. Based on the returned rpi, the cognitive process
via the working memory activates the motor process in each
part of the body, and a response is generated. The pupillary
response we focus on in this paper is produced by the activity
of the pupillary sphincter and pupillary dilator muscles, which
are considered to be one of their responses. The story so far
can be expressed as follows.
Let K be a row vector of ∀word concepts (knowledge
concepts) in the long-term Memory (LTM) of ∃person, and
the word concept i input at time tj is denoted by the element
Ki(tj) in K. where Ki(tj) are the values of valence Vi and
arousal Ari that characterize the emotion [11]. The number
of elements is i = 1 · · · nK (nK is the total number of word
concepts), with only one i value of 1 for some time tj. Here,
Vi or Ari or both may have no value (∼ 0) (in that case,
Vi = 0, Ari = 0). The range of values for Vi and Ari is
1 ≤ Vi ≤ 9, 1 ≤ Ari ≤ 9.
Next, let A be a column vector of ∀emotion concepts in
the LTM of ∃people, consisting of elements Ak(Vk, Ark).
The number of elements is k = 1, · · · , mA (mA is the total
number of emotion concepts), and there always exist Vk, Ark
values.
The K and A are connected by a n × m matrix W (tj)
that shows their connectivity at time tj. The element wik(tj)
of W (tj) indicates the degree of coupling between Ki(tj)
and Ak. If Ki(t) in K is input and co-occurs with Ak in
A on tj, the probability that Ki(tj) retrieved from LTM is
p(Ki(tj)) and that Ak retrieved from LTM is p(Ak(tj)), the
probability of Ak being retrieved from LTM is expressed by
the following equation.
wik(tj) = wik(p(Ki(tj)), p(Ak(tj)))
In this case, the temporary emotion E(tj) generated from the
input ∃packet of sound waves is (1).
E(tj) = D(tj)
nK
∑
i=1
mA
∑
k=1
Ki(tj)wik(tj)Ak
(1)
80
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-046-9
COGNITIVE 2023 : The Fifteenth International Conference on Advanced Cognitive Technologies and Applications

visual
stimuli
rpd
pupillary
response
near reaction 
caused by 
gaze fixation 
define average rpd
as baseline 
near reaction 
caused by 
animation
mydriasis
miosis
mydriasis
miosis
near reaction 
caused by 
beep
audio
stimuli
silent interval
play the narration
duration time
beep
silent interval
t3
t4
trial
start
t1
narration
start
beep
ANEW
start
narration
finish
assess narration
impression
ANEW
finish
event
t2
t6
t7
t8
ta
Δt
ta
Δt
analyzing timeline
t5
offset
Figure 2. The full pupillary response when auditory information is given.
Here, D(tj) is the damping factor. The above explanation
represents the construction process.
The sentence, which consists of nwp packets of sound
waves, repeats the process of (1) as one cycle up to this
point, and continues to return the emotion associated with the
knowledge concept to the working memory. In the process,
E(tj) may or may not be integrated between tj depending
on the presence or absence of active sources and contextual
relations. The damping factor is introduced as a quantity that
indicates the degree of such emotional integration. When nwp
packets of sound waves are listened to, the emotion arises in
the form of integration of E(tj) that has been cultivated up
to that point. It is usually at the end of a sentence where the
packets of sound waves are interrupted. This is the integration
process in this research situation.
Based on this, we consider the 2D plane shown in Fig-
ure 1(b). We thought that we could show the characteristics
of the emotion that occurs in listeners when they listen to
narration by plotting the information on human reactions
in this plane. The rpi in the ﬁgure indicates the emotional
reaction to a speciﬁc packet of sound waves. rpi,i+1 indicates
the emotional reaction generated by the integration of the emo-
tional reactions generated by multiple packets of sound waves.
By treating it in this way, two measured reaction quantities can
be plotted on a plane as (rpi, rpi,i+1). In this paper, we call
this plane as Local-Global Response Map (LGR-Map).
B.
Representation of Pupillary Response based on CI-model
In order to apply the LGR-Map to pupillary responses,
the measurement design of pupillary response should keep
an adequate time interval by both inducing time interval
both inducing a speciﬁc emotion induction word in narrating
(instantaneous response) and context of narration (integrated
response). Figure 2 represents a measurement design of pupil-
lary response when listening to the narration stimuli based on
Figure 1. Here, we adopt the Japanese version of ANEW [14],
which induce emotion as the result of instantaneous response.
For the integrated response, we assumed that the effect appears
at the end of the sentence. We measured participants’ pupillary
responses to short sentences containing one ANEW word.
The auditory stimuli are adjusted for the event speciﬁed in
Figure 2 as follows. The beep sound for the mental preparation
to initiate auditory stimuli is uttered at t2. Narration starts at
t3 and ANEW is uttered at t4. After that, auditory stimuli
are terminated at t7. During this period, the auditory elements
related to the evocation of emotion are the ANEW and the
atmospheres at the end of the sentence. When analyzing the
pupillary response, it is necessary to analyze the data in the
vicinity of these elements.
Next, pupil diameter rpd(t) at elapsed time t is processed
as follows, in the following order: determination of baseline,
calculation of pupil diameter change, and total pupil diameter
change.
First, when we set ∆tb as the interval necessary to calculate
baseline, baseline ˜rpd is calculated as follows.
˜rpd =
1
∆tb
∫ tns
tns−∆tb
rpd(t) dt
(2)
Here, pupil diameter change value in t ∆rpd(t) is calculated
by the equation(3).
∆rpd(t) = rpd(t) − ˜rpd
(3)
The pupil diameter change δr(t) between the duration time
t and δt is calculated by the (4).
δr(t) = ∆rpd(t + δt) − ∆rpd(t)
(4)
Mydriasis (dilation) and miosis (constriction) are typical
quantities that show pupillary response. Since the instanta-
neous changes in either of them are minute, we represent the
81
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-046-9
COGNITIVE 2023 : The Fifteenth International Conference on Advanced Cognitive Technologies and Applications

total amount of change in only mydriasis or only miosis at
[ta, ta + ∆t]. These can be expressed as total amount of
mydriasis rmyd. The total amount of miosis rmio is calculated
by the equation (5).
rmyd or rmio =
∫ ta+∆t
ta
δr(t) dt
(5)
In order to obtain a clearer picture of the change in
pupillary response, it is better to capture the absolute change
in rmyd, rmio. Here, we deﬁne rall as the total change in
pupillary response calculated by (6).
rall = |rmyd| + |rmio|
(6)
In LGR-Map, rall is assumed to be a local (instantaneous) or
global (integrated) reaction around calculated R. The pupillary
response analysis start time ta and analysis interval ∆t can be
arbitrarily determined. In the LGR-Map, we set ta and ∆t
using the event time in Figure 2 as follows: For the local
reaction, ta is set to t5, the offset time at which the pupil
response is expected to start after the appearance of the ANEW
that causes the reaction. For the global reaction, ta is set where
the integrated effect can be easily conﬁrmed. In this paper, ta
is set a little before t7, when the narration ends.
Since the
actual narration has ns sets of calculated points or consists of
ns sentences, at most ns points are plotted on the LGR-Map.
C.
Typology based on LGR-Map
rall distribution on LGR-Map is regarded as a description
of induced emotion by narration stimuli for each participant.
We design the method of categorization of typology for rall
distribution on LGR-Map. rall distribution has x axis for local
response and y axis for global response. rall is the information
including the individual differences. Now, each individual
difference is assumed to obey a normal distribution. If the
distribution obeys a two-dimensional Gaussian distribution, we
can draw the error ellipsoid on LGR-Map.
The error ellipsoid is represented by the following equation
using the transformed coordinates u, v. Hence σ2
u, σ2
v are the
variances of the transformed coordinates with respect to the
respective axes.
u2
σ2u
+ v2
σ2v
= c2
Here, σ2
u, σ2
v, and rotation angle of error ellipsoid α can be
converted as (7) – (9) using σ2
x as variance for local response,
σ2
y as variance for global response, σxy as covariance of local-
global response.
σ2
u
=
σ2
x + σ2
y +
√
(σ2x − σ2y)2 + 4σ2xy
2
(7)
σ2
v
=
σ2
x + σ2
y −
√
(σ2x − σ2y)2 + 4σ2xy
2
(8)
tan α
=
σxy
σ2u − σ2y
(0 < α < 180◦)
(9)
We consider the shape of error ellipsoid depending on the
behavior of σ2
x, σ2
y, σxy. First, we relate σ2
x and σ2
y as (10).
σ2
y = γσ2
x (γ > 0)
(10)
The error ellipsoid can then be classiﬁed by the value of
γ. First, we can set σ2
x = σ2
y = σ2
0 when γ = 1. Therefore,
α = 45◦ as shown in the following calculation.
σ2
u
=
σ2
0 + σ2
0 +
√
4σ2xy
2
= σ2
0 + σxy
σ2
v
=
σ2
0 + σ2
0 −
√
4σ2xy
2
= σ2
0 − σxy
tan α
=
σxy
σ2
0 + σxy − σ2
0
=
1
If σxy
∼ 0, the distribution has a circle shape; if it has a
large value, the distribution has an ellipsoid shape.
Next, we consider the case of γ ̸= 1 in (10), where we apply
the observed data properties to the variables in (7) – (9). Since
σ2
x, σ2
y, and σxy are at most on the order of 10−2 given the
experimental environment, the σxy term is on the order of
10−4. Therefore, we can ignore the σxy term. Equation (7) –
(9) can be approximated by the following equations.
σ2
u
=
σ2
x + σ2
y +
√
(σ2x − σ2y)2
2
∼ σ2
x
(11)
σ2
v
=
σ2
x + σ2
y −
√
(σ2x − σ2y)2
2
∼ σ2
y = γσ2
x (12)
tan α
=
σxy
σ2u − σ2y
=
σxy
(1 − γ)σ2x
(13)
In the situation, considering the range of γ and signum of
σxy, we can predict the following categories. Hence, L, G
represent local or global reaction, and +, − after the L or G
represent strong or weak effect. −, − represent the spreading
to lower or upper side of data.
• case σxy ∼ 0:
– γ ∼ 1 : L0G0
The error ellipsoid distribution has circle shape.
– 0 < γ ≪ 1 :L+G–
The shape becomes parallel to the x axis, and α ∼
0◦.
– γ ≫ 1 : L–G+
The shape becomes parallel to the y axis, and α ∼
90◦.
• case σxy > 0 : L−G−
The shape becomes parallel to the x (in case of 0 < γ <
1) or y (in case of γ > 1) axis, and 0◦ ≪ α < 90◦.
• case σxy < 0 : L−G−
The shape becomes parallel to the x (in case of 0 < γ <
1) or y (in case of γ > 1) axis, and 90◦ ≪ α < 180◦.
In case of σ2
x ∼ σ2
y(γ ∼ 1), α ∼ 135◦.
82
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-046-9
COGNITIVE 2023 : The Fifteenth International Conference on Advanced Cognitive Technologies and Applications

TABLE I. THE RESULTS OF THE EMOTIONAL AROUSAL EFFECT OF
SENTENCES AND THE IMPRESSION EVALUATION.V DENOTES VALENCE,
At DENOTES ATMOSPHERE, SI DENOTES SCORE OF IMPRESSION.
V
At
V = SI(%)
At = SI(%)
Number of Trial
VNN
AtN
38
38
50
V−−
At−
58
58
54
V++
At+
54
54
49
VNN
At−
16
39
24
VNN
At+
17
54
34
V−−
At+
8
28
29
V++
At−
4
52
49
III.
TYPOLOGY OF PUPILLARY RESPONSE BASED ON
LGR-MAP
To evaluate the validity of the LGR-Map designed in section
II, we analyzed the pupillary response. The LGR-Map analysis
was conducted using the pupillary response data measured for
the controlled narration in the form of Figure 2.
A.
Characteristics of Data for Generating LGR-Map
The data used are those obtained by [5]. The data proﬁle
is as follows. The narration source used in the experiment is
designed as shown in Figure 2.
1) The narration is played back in Japanese, and is a short
sentence consisting of about 30 syllables.
2) One ANEW corresponding to either high-positive va-
lence
V++, high-negative valence
V−−, or neutral
valence VN was placed at tvs in one sentence.
3) After the appearance of an ANEW, we assigned an
expression that characterizes the mood of the whole sen-
tence as positive(At+) / neutral(AtN) / negative(At−).
4) After t4, the analysis interval from ta as t5 to ∆t, where
the pupillary response is expected to start, was set as
analysis interval 1.
5) The response that occurs at 0.5∆t before and after the
end of narration was deﬁned as analysis interval 2.
Therefore, analysis interval 1 was deﬁned as local reaction
(instantaneous reaction) and analysis interval 2 as global
reaction (integrated reaction). Twenty-one participants in their
20s were included, but data of two participants were excluded
due to inaccuracy.
Table I shows the results of subjective evaluation of
narration stimuli by participants. The narration stimuli are
composed of V−−, VNN, V++ and At−, AtN, At+. The
participants listened to each stimulus and then evaluated their
impressions on a 7-point scale from high negative to high
positive, indicating whether their ratings were consistent with
the valence or the atmosphere. However, cases in which the
impression matched less than 10 participants were excluded.
B.
LGR-Map to Represent Individual Participants’ Response
Sensitivity
For each participant, an LGR-Map was created for all
narration stimuli for the pupillary responses obtained under
the above conditions. In order to conﬁrm that the distribu-
tion was independent of the size of the individual pupillary
TABLE II. THE α AND FLATTENING RATE OF THE ERROR ELLIPSE IN THE
LGR-MAP FOR THE CHARACTERISTICS OF THE NARRATION STIMULUS.
Fee DENOTES THE FLATNESS.
V
At
α
Fee
V
At
α
Fee
VNN
At−
63.6◦
0.506
V−−
At−
28.4◦
0.091
VNN
AtN
43.1◦
0.350
V−−
At+
12.3◦
0.434
VNN
At+
32.4◦
0.246
V++
At−
52.2◦
0.182
V++
At+
−7.43◦
0.194
response, median-normalized values within analysis interval 1
and analysis interval 2 were used for the plots.
From (1), we expect that the distribution of individual
participants’ pupillary responses in the LGR-Map can be
classiﬁed into ﬁve types. Figure 3 shows a representative
example of an LGR-Map created using the pupillary responses
of individual participants to narration stimuli. As shown in
section II, (a) in Figure 3 is L + G−, same as (b) is L0G0,
(c) is L−G−, (d) is L−G−, and (e) is L − G+. When
creating the LGR-Map for individual participants, we also
examined whether there was a bias in the pupillary response
to a particular valence or atmosphere, but no bias was found.
IV. DISCUSSION: IMPLICATIONS OF LGR-MAP
A.
LGR-Map for Characterizing Individual Participant
The classiﬁcation of individual participants was not char-
acterized by a distinctive response to the combination of
(valence, atmosphere), which indicates emotion, suggesting
that it was simply determined by the distribution of wik(tj),
which is indicated by equation (1). The intensity of wij(tj)
is considered to change depending on the intensity of the
individual’s experience of emotion. If the overall experience of
emotion is weak, or if the experience of emotion is weak for
some reason and almost no emotion is generated, the response
of L−G− is expected to be shown. When the reaction is
triggered by either valence or atmosphere, it is considered to
have a reaction of L+G– or L–G+. If the reaction is equally
distributed between valence and atmospheres, the reaction is
considered to be L−G−. If the reaction is completely random,
it is considered to be L0G0.
B.
LGR-Map for Categorizing Narrations
Next, we consider human responses to ANEWs used as
narration stimuli. Since ANEWs are basically emotion refer-
ences elicited when people hear the word, we believe that it is
possible to evaluate the validity of narration stimuli that show
the same atmospheres as ANEWs by using the LGR-Map type
classiﬁcation.
Table II shows the values of α and ﬂattening Fee, which
are the features of LGR-Map. The features in the LGR-Map
are created by combining the valence and atmospheres into
9 patterns. There were three responses to each stimulus pair.
Trials with fewer than 10 trials showing the level of response
to a stimulus pair were excluded from the analysis, considering
them to be less signiﬁcant even if an error ellipse was written.
The table shows the following characteristics. For V−−At−,
Fee is almost zero, indicating that it is a circular distribution.
Therefore, (a) is classiﬁed as L0G0. For V−−At+, α is 13.2◦,
83
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-046-9
COGNITIVE 2023 : The Fifteenth International Conference on Advanced Cognitive Technologies and Applications

0.0
0.5
1.0
1.5
2.0
2.5
0.0
0.5
1.0
1.5
2.0
2.5
tot.change ANEW[arb.unit]
tot.change ATMS[arb.unit]
0.0
0.5
1.0
1.5
2.0
2.5
0.0
0.5
1.0
1.5
2.0
2.5
0.0
0.5
1.0
1.5
2.0
2.5
0.0
0.5
1.0
1.5
2.0
2.5
0.0
0.5
1.0
1.5
2.0
2.5
0.0
0.5
1.0
1.5
2.0
2.5
tot.change ANEW[arb.unit]
tot.change ATMS[arb.unit]
0.0
0.5
1.0
1.5
2.0
2.5
0.0
0.5
1.0
1.5
2.0
2.5
0.0
0.5
1.0
1.5
2.0
2.5
0.0
0.5
1.0
1.5
2.0
2.5
0.0
0.5
1.0
1.5
2.0
2.5
0.0
0.5
1.0
1.5
2.0
2.5
tot.change ANEW[arb.unit]
tot.change ATMS[arb.unit]
0.0
0.5
1.0
1.5
2.0
2.5
0.0
0.5
1.0
1.5
2.0
2.5
0.0
0.5
1.0
1.5
2.0
2.5
0.0
0.5
1.0
1.5
2.0
2.5
0.0
0.5
1.0
1.5
2.0
2.5
0.0
0.5
1.0
1.5
2.0
2.5
tot.change ANEW[arb.unit]
tot.change ATMS[arb.unit]
0.0
0.5
1.0
1.5
2.0
2.5
0.0
0.5
1.0
1.5
2.0
2.5
0.0
0.5
1.0
1.5
2.0
2.5
0.0
0.5
1.0
1.5
2.0
2.5
0.0
0.5
1.0
1.5
2.0
2.5
0.0
0.5
1.0
1.5
2.0
2.5
tot.change ANEW[arb.unit]
tot.change ATMS[arb.unit]
0.0
0.5
1.0
1.5
2.0
2.5
0.0
0.5
1.0
1.5
2.0
2.5
0.0
0.5
1.0
1.5
2.0
2.5
0.0
0.5
1.0
1.5
2.0
2.5
(a)
(b)
(c)
(d)
(e)
Figure 3. Examples of LGR-Maps for individual participants. The LGR-Map for individual participant are normalized by median of rall near ANEW and
near the end of the sentence, respectively. The oval lines indicate 66%, 90%, and 95% conﬁdence levels from the inside. The categories in LGR-Map are as
below: (a) L + G−, (b) L0G0, (c) L−G−, (d) L−G−, (e) L − G+.
almost parallel to the x axis, so it is classiﬁed as L + G−.
For VNNAtN, it is classiﬁed as L−G−, because α ∼ 45◦.
VNNAt−, VNNAt+ are not certain because the value of α is
ambiguous, but we can classify them as L−G− for the reason
described later. Since V++At−, V++At+ have ambiguous α
values and Fee values are not circular, we cannot indicate
which type they can be classiﬁed into at this time.
From the above, the following possibilities are considered
for V−−At−, V−−At+, VNNAtN. For V−−At−, values of
valence and atmosphere have negative each. In this case,
valence and atmosphere are the same characteristics, so that
we anticipate that participants’ pupillary responses are almost
uniform as indicated by Murakami et al [3][4]. Taken together,
these results suggest that the distribution of the LGR-Map is
random, centered on a representative LGR-Map value.
For V−−At+, the response is negative valence, with positive
atmosphere. This, together with V−−At−, can be interpreted
as follows. The response to the negative valence was scattered,
but the response to the pupillary response in the atmospheres
was reasonably consistent, resulting in the values in the y axis
being almost consistent and the distribution in the x axis being
broadened. This is thought to be due to the broadening of the
x-axis distribution.
Next, we consider the case of VNNAtN. Both valence
and atmosphere were neutral, that is no emotion is induced.
It indicates that no matter where in the instantaneous or
integrated area the pupillary response is measured, no change
in emotion occurs for the same person or narration. Therefore,
both pupillary responses show almost similar values, which is
a good sign that the distribution is close to a straight line with
y = x.
C.
Understanding Social Phenomena Using LGR-Map
Image language plays a greater role than symbolic language
in real-time communication. However, memes, which are
words, play a major role in the transmission and accumulation
of knowledge over a long period of time [15]. In knowledge
represented by a network, a meme, or a symbolic node,
develops links to image nodes associated with it. Image nodes
are formed in response to an individual’s actual perceptual,
cognitive, and motor experiences, and therefore represent
something unique to each individual. This is very different
from symbolic nodes, which are shared within a single culture.
Communication through memes (words) is a form of commu-
nication through language nodes that aggregate a large amount
of information, allowing for the exchange of a large amount of
information with a small amount of information (words). This
is achieved through the activation of image nodes that spread
around the language node. However, it is not guaranteed that
the spread of image nodes centered on the language node is
consistent on both sides of the interlocutor. Therefore, errors in
the transmission of information due to this are inevitable [16].
For example, the proliferation of Social Networking System
(SNS) allows transmission errors to be ampliﬁed in an ex-
tremely short period of time. While taking these characteristics
of SNS into account, it is necessary to establish a method
84
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-046-9
COGNITIVE 2023 : The Fifteenth International Conference on Advanced Cognitive Technologies and Applications

to realize verbal communication that does not cause trans-
mission errors, in order to build a community where people
can communicate in a healthy manner. Toward this end, an
approach that focuses on the activation of knowledge centered
on language nodes, as shown in this study, is promising.
V. CONCLUSIONS
In this paper, we focused on the pupillary response and
proposed the LGR-Map as a classiﬁcation method for pupillary
changes associated with emotional changes. The LGR-Map
indicates whether an individual’s pupil response to a stimulus
is more likely to respond to local informaion or contextual
information.
In order to propose the LGR-Map, we needed a cognitive
model that describes how people’s emotions are induced in
response to stimuli from the outside world. Thus, we con-
structed a model of human emotional responses based on the
CI-model. We assumed that the input stimuli have the feature
of auditory information, that is, transient information. The
input information was assumed to be We assumed a situation
where the valence of the whole sentence is determined by the
sentence-ﬁnal expression. For each of them, we considered an
emotional response appeared based on the CI-model. When
considering the above situations, we thought that there was
some kind of pupillary response for each emotional reaction.
We described the pupil response to ANEW as “local” reaction,
and the pupil response to the end of a sentence as “global”
reaction. In this case, the local and global pupil responses can
be represented in a two-dimensional plane. Based on this idea,
we proposed the LGR-Map. The shape classiﬁcation of the
LGR-Map was based on the variance of the error ellipsoid.
The results indicated that the LGR-Map could be classiﬁed
into ﬁve types according to the covariance of the local and
global pupil responses and the trend of the dispersion of the
local pupil response.
Based on a series of ideas, 36 auditory stimuli with var-
ious characteristics embedded in ANEWs and sentence-ﬁnal
expressions were actually given to 19 participants, and LGR-
Maps were created. As a result, we conﬁrmed that ﬁve types
of shapes were recognized for individual pupil responses.
Application of the LGR-Map will make it possible to pro-
vide adaptive content for individual person. The method of
implementation will be an issue for the future.
ACKNOWLEDGEMENT
This work was supported by JSPS KAKENHI Grant
Number 19K12246 / 19K12232 / 20H04290 / 22K12284 /
23K11334 , and National University Management Reform
Promotion Project. The authors would like to thank Editage
(www.editage.com) for English language editing. MH also
wants to thank to Nagai N · Promotion Foundation For Science
of Perception for their ﬁnantial support.
REFERENCES
[1] N. Vallez et al., “Automatic museum audio guide,” Sensors, vol. 20,
no. 3, 2020. [Online]. Available: https://doi.org/10.3390/s20030779
[2] R. Hirabayashi, M. Shino, K. T. Nakahira, and M. Kitajima, “How
auditory information presentation timings affect memory when watching
omnidirectional movie with audio guide,” in Proceedings of the 15th
International Joint Conference on Computer Vision, Imaging and Com-
puter Graphics Theory and Applications (VISIGRAPP 2020), vol. 2,
2020, pp. 162–169.
[3] M. Murakami, M. Shino, K. T. Nakahira, and M. Kitajima, “Effects
of emotion-induction words on memory of viewing visual stimuli with
audio guide,” in Proceedings of the 16th International Joint Conference
on Computer Vision, Imaging and Computer Graphics Theory and
Applications (VISIGRAPP 2021), vol. 2, 2021, pp. 89–100.
[4] M. Murakami, M. Shino, M. Harada, K. T. Nakahira, and M. Kita-
jima, “Effects of emotion-induction words on memory and pupillary
reactions while viewing visual stimuli with audio guide,” in Computer
Vision, Imaging and Computer Graphics Theory and Applications, A. A.
de Sousa et al., Eds.
Cham: Springer International Publishing, 2023,
pp. 69–89.
[5] S. Moriya, K. T. Nakahira, M. Harada, M. Shino, and M. Kitajima,
“Can pupillary responses while listening to short sentences containing
emotion induction words explain the effects on sentence memory?” in
Proceedings of the 18th International Joint Conference on Computer
Vision, Imaging and Computer Graphics Theory and Applications,
VISIGRAPP 2023, Volume V: HUCAPP, Online Streaming, February
19-21, 2023.
SCITEPRESS, 2023, pp. 213–220.
[6] J. Z. Lim, J. Mountstephens, and J. Teo, “Emotion recognition using eye-
tracking: Taxonomy, review and current challenges,” Sensors, vol. 20,
no. 8, 2020. [Online]. Available: https://doi.org/10.3390/s20082384
[7] L. Shu et al., “A review of emotion recognition using physiological
signals,” Sensors, vol. 18, no. 7, 2018. [Online]. Available: https:
//doi.org/10.3390/s18072074
[8] W. Kintsch, “The use of knowledge in discourse processing: A
construction-integration model,” Psychological Review, vol. 95, 1988,
pp. 163–182.
[9] W. Kintsch, Comprehension: A paradigm for cognition.
Cambridge,
UK: Cambridge University Press, 1998.
[10] C. Wharton and W. Kintsch, “An overview of construction-integration
model: A theory of comprehension as a foundation for a new cognitive
architecture,” SIGART Bull., vol. 2, no. 4, jul 1991, p. 169–173.
[Online]. Available: https://doi.org/10.1145/122344.122379
[11] J. Russell, “A circumplex model of affect,” Journal of Personality and
Social Psychology, vol. 39, 12 1980, pp. 1161–1178.
[12] K. T. Nakahira, M. Harada, and M. Kitajima, “Analysis of the
relationship between subjective difﬁculty of a task and the efforts
put into it using biometric information,” in Proceedings of the
17th International Joint Conference on Computer Vision, Imaging
and Computer Graphics Theory and Applications, VISIGRAPP 2022,
Volume 2: HUCAPP, Online Streaming, February 6-8, 2022, A. Paljic,
M. Ziat, and K. Bouatouch, Eds.
SCITEPRESS, 2022, pp. 241–248.
[Online]. Available: https://doi.org/10.5220/0010906800003124
[13] P.
Jerˇci´c,
C.
Sennersten,
and
C.
Lindley,
“Modeling
cognitive
load and physiological arousal through pupil diameter and heart
rate,” vol. 79, no. 5, pp. 3145–3159. [Online]. Available: https:
//doi.org/10.1007/s11042-018-6518-z
[14] M. M. Bradley, L. S. Miccoli, M. A. Escrig, and P. J. Lang, “The
pupil as a measure of emotional arousal and autonomic activation.”
Psychophysiology, vol. 45, no. 4, 2008, pp. 602–607. [Online].
Available: https://doi.org/10.1111/j.1469-8986.2008.00654.x
[15] D. C. Dennett, From Bacteria to Bach and Back: The Evolution of
Minds.
W W Norton & Co Inc, 2 2018.
[16] M. Kitajima, M. Toyota, and J. Dinet, “How Resonance Works for
Development and Propagation of Memes,” International Journal on
Advances in Systems and Measurements, vol. 14, 2021, pp. 148–161.
85
Copyright (c) IARIA, 2023.     ISBN:  978-1-68558-046-9
COGNITIVE 2023 : The Fifteenth International Conference on Advanced Cognitive Technologies and Applications

